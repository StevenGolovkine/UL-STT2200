[
  {
    "objectID": "slides/02-revisions-algebre-slides.html#plan-de-la-pr√©sentation",
    "href": "slides/02-revisions-algebre-slides.html#plan-de-la-pr√©sentation",
    "title": "R√©visions",
    "section": "Plan de la pr√©sentation",
    "text": "Plan de la pr√©sentation\n\n\nPropri√©t√©s matricielles\nValeurs et vecteurs propres\nDiagonalisation de matrices"
  },
  {
    "objectID": "slides/02-revisions-algebre-slides.html#notations-de-base",
    "href": "slides/02-revisions-algebre-slides.html#notations-de-base",
    "title": "R√©visions",
    "section": "Notations de base",
    "text": "Notations de base\n\n\n\\(M_{n, m}(\\mathbb{R})\\) : matrices √† \\(n\\) lignes et \\(m\\) colonnes\n\\(M_{n}(\\mathbb{R})\\) : matrices carr√©es de taille \\(n\\)\n\\(I_n\\) : matrice identit√© de taille \\(n\\)\n\\(u, v \\in \\mathbb{R}^n\\) : vecteurs colonnes"
  },
  {
    "objectID": "slides/02-revisions-algebre-slides.html#propri√©t√©s-du-d√©terminant",
    "href": "slides/02-revisions-algebre-slides.html#propri√©t√©s-du-d√©terminant",
    "title": "R√©visions",
    "section": "Propri√©t√©s du d√©terminant",
    "text": "Propri√©t√©s du d√©terminant\n\n\n\n\nPropri√©t√©s\n\n\n\n\\(\\det(A^\\top) = \\det(A)\\)\n\\(\\det(AB) = \\det(A)\\det(B)\\)\n\\(\\det(A^{-1}) = \\frac{1}{\\det(A)}\\)"
  },
  {
    "objectID": "slides/02-revisions-algebre-slides.html#propri√©t√©s-du-d√©terminant-1",
    "href": "slides/02-revisions-algebre-slides.html#propri√©t√©s-du-d√©terminant-1",
    "title": "R√©visions",
    "section": "Propri√©t√©s du d√©terminant",
    "text": "Propri√©t√©s du d√©terminant\n\nCas particulier o√π \\(A \\in M_{2}(\\mathbb{R})\\).\nSoit \\(A = \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}\\). Alors\n\\[\\det(A) = ad - bc.\\]"
  },
  {
    "objectID": "slides/02-revisions-algebre-slides.html#propri√©t√©-du-d√©terminant-exemple-wooclap-kffpdj",
    "href": "slides/02-revisions-algebre-slides.html#propri√©t√©-du-d√©terminant-exemple-wooclap-kffpdj",
    "title": "R√©visions",
    "section": "Propri√©t√© du d√©terminant : exemple (Wooclap: KFFPDJ)",
    "text": "Propri√©t√© du d√©terminant : exemple (Wooclap: KFFPDJ)\n\n\\[A = \\begin{pmatrix} 1 & 2 \\\\ 3 & 4 \\end{pmatrix}, \\quad B = \\begin{pmatrix} 2 & 0 \\\\ 1 & 1 \\end{pmatrix}\\]\n\nOn trouve que \\(\\det(A) = 4 - 6 = -2\\), \\(\\det(B) = 2 - 0 = 2\\). Donc\n\\[\\det(AB) = \\det(A) \\det(B) = -4\\]"
  },
  {
    "objectID": "slides/02-revisions-algebre-slides.html#inverse-de-matrices",
    "href": "slides/02-revisions-algebre-slides.html#inverse-de-matrices",
    "title": "R√©visions",
    "section": "Inverse de matrices",
    "text": "Inverse de matrices\n\n\n\n\nPropri√©t√©\n\n\nSi \\(A\\) et \\(B\\) sont inversibles, alors : \\((AB)^{-1} = B^{-1} A^{-1}\\)."
  },
  {
    "objectID": "slides/02-revisions-algebre-slides.html#inverse-de-matrices-1",
    "href": "slides/02-revisions-algebre-slides.html#inverse-de-matrices-1",
    "title": "R√©visions",
    "section": "Inverse de matrices",
    "text": "Inverse de matrices\n\nCas particulier o√π \\(A \\in M_{2}(\\mathbb{R})\\).\nSoit \\(A = \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}\\). Alors\n\\[A^{-1} = \\frac{1}{ad - bc}\\begin{pmatrix} d & -b \\\\ -c & a \\end{pmatrix}.\\]"
  },
  {
    "objectID": "slides/02-revisions-algebre-slides.html#inverse-de-matrices-exemple-wooclap-kffpdj",
    "href": "slides/02-revisions-algebre-slides.html#inverse-de-matrices-exemple-wooclap-kffpdj",
    "title": "R√©visions",
    "section": "Inverse de matrices : exemple (Wooclap: KFFPDJ)",
    "text": "Inverse de matrices : exemple (Wooclap: KFFPDJ)\n\n\\[A = \\begin{pmatrix} 2 & 0 \\\\ 0 & 3 \\end{pmatrix}, \\quad B = \\begin{pmatrix} 1 & 1 \\\\ 0 & 1 \\end{pmatrix}\\]\n\nOn a : \\(AB = \\begin{pmatrix} 2 & 2 \\\\ 0 & 3 \\end{pmatrix},\\) donc \\((AB)^{-1} = \\begin{pmatrix} 1/2 & -1/3 \\\\ 0 & 1/3 \\end{pmatrix}\\).\nEt effectivement : \\(B^{-1}A^{-1} = \\begin{pmatrix} 1 & -1 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1/2 & 0 \\\\ 0 & 1/3 \\end{pmatrix} = \\begin{pmatrix} 1/2 & -1/3 \\\\ 0 & 1/3 \\end{pmatrix}\\)."
  },
  {
    "objectID": "slides/02-revisions-algebre-slides.html#propri√©t√©s-de-la-trace",
    "href": "slides/02-revisions-algebre-slides.html#propri√©t√©s-de-la-trace",
    "title": "R√©visions",
    "section": "Propri√©t√©s de la trace",
    "text": "Propri√©t√©s de la trace\n\nLa trace de la matrice \\(A \\in M_{n}(\\mathbb{R})\\) est la somme des √©l√©ments diagonaux \\[\\text{tr}(A) = \\sum_{i=1}^{n} a_{ii}.\\]"
  },
  {
    "objectID": "slides/02-revisions-algebre-slides.html#propri√©t√©s-de-la-trace-1",
    "href": "slides/02-revisions-algebre-slides.html#propri√©t√©s-de-la-trace-1",
    "title": "R√©visions",
    "section": "Propri√©t√©s de la trace",
    "text": "Propri√©t√©s de la trace\n\n\n\n\nPropri√©t√©s\n\n\n\n\\(\\text{tr}(A) = \\text{tr}(A^{\\top})\\)\n\\(\\text{tr}(A + B) = \\text{tr}(A) + \\text{tr}(B)\\)\n\\(\\text{tr}(MN^{\\top}) = \\text{tr}(N^{\\top}M)\\)"
  },
  {
    "objectID": "slides/02-revisions-algebre-slides.html#propri√©t√©s-de-la-trace-exemple-wooclap-kffpdj",
    "href": "slides/02-revisions-algebre-slides.html#propri√©t√©s-de-la-trace-exemple-wooclap-kffpdj",
    "title": "R√©visions",
    "section": "Propri√©t√©s de la trace : exemple (Wooclap: KFFPDJ)",
    "text": "Propri√©t√©s de la trace : exemple (Wooclap: KFFPDJ)\n\nSoit \\(X \\in M_{n, p}(\\mathbb{R})\\) avec \\(p &lt; n\\) et de rang \\(p\\), et donc telle que \\(X^\\top X\\) est de dimension \\(p \\times p\\) et inversible. Quelle est la valeur de la trace de \\(X (X^\\top X)^{-1} X^\\top\\) ?\n\n\\[\\begin{align*}\n\\text{tr}(X (X^\\top X)^{-1} X^\\top) &= \\text{tr}(X^\\top X (X^\\top X)^{-1})\\\\\n&= \\text{tr}(I_p) \\\\\n&= p.\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/02-revisions-algebre-slides.html#matrices-sp√©ciales",
    "href": "slides/02-revisions-algebre-slides.html#matrices-sp√©ciales",
    "title": "R√©visions",
    "section": "Matrices sp√©ciales",
    "text": "Matrices sp√©ciales\n\n\n\nD√©finitions\n\n\nMatrice d√©finie positive : \\(A\\) est d√©finie positive si \\[\\text{pour tout } u \\neq 0,\\quad u^\\top A u &gt; 0.\\]\nMatrice orthogonale : \\(A\\) est orthogonale si \\[A^\\top A = A A^\\top = I_n.\\]"
  },
  {
    "objectID": "slides/02-revisions-algebre-slides.html#comment-montrer-quune-matrice-est-d√©finie-positive",
    "href": "slides/02-revisions-algebre-slides.html#comment-montrer-quune-matrice-est-d√©finie-positive",
    "title": "R√©visions",
    "section": "Comment montrer qu‚Äôune matrice est d√©finie positive ?",
    "text": "Comment montrer qu‚Äôune matrice est d√©finie positive ?\n\nSoit \\(A = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix}\\) et pour tout \\(u = \\begin{pmatrix} x \\\\ y \\end{pmatrix} \\neq 0\\), on a : \\[\\begin{align*}\nu^\\top A u &= \\begin{pmatrix} x & y \\end{pmatrix} \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix} \\\\\n  &= 2x^2 + 2xy + 2y^2 \\\\\n  &= x^2 + y^2 + (x+y)^2 &gt; 0.\n\\end{align*}\\]\nDonc \\(A\\) est d√©finie positive."
  },
  {
    "objectID": "slides/02-revisions-algebre-slides.html#valeurs-et-vecteurs-propres-1",
    "href": "slides/02-revisions-algebre-slides.html#valeurs-et-vecteurs-propres-1",
    "title": "R√©visions",
    "section": "Valeurs et vecteurs propres",
    "text": "Valeurs et vecteurs propres\n\nFaire un dessin pour Au = lu.\n\n\n\n\nD√©finitions\n\n\nSoit \\(A \\in M_{n}(\\mathbb{R})\\). Le scalaire \\(\\lambda \\in \\mathbb{R}\\) est une valeur propre de \\(A\\) s‚Äôil existe \\(u \\neq 0\\) tel que : \\(Au = \\lambda u\\).\n\\(u\\) est le vecteur propre correspondant √† \\(\\lambda\\).\nLe spectre de \\(A\\), not√© \\(\\text{sp}(A)\\), est l‚Äôensemble des valeurs propres."
  },
  {
    "objectID": "slides/02-revisions-algebre-slides.html#valeurs-et-vecteurs-propres-exemple",
    "href": "slides/02-revisions-algebre-slides.html#valeurs-et-vecteurs-propres-exemple",
    "title": "R√©visions",
    "section": "Valeurs et vecteurs propres : exemple",
    "text": "Valeurs et vecteurs propres : exemple\n\\[A = \\begin{pmatrix} 3 & 1 \\\\ 0 & 2 \\end{pmatrix}\\]\n\nCalcul des valeurs propres : \\[\\det(A - \\lambda I) = \\det\\begin{pmatrix} 3-\\lambda & 1 \\\\ 0 & 2-\\lambda \\end{pmatrix} = (3-\\lambda)(2-\\lambda)\\]\nDonc \\(\\lambda_1 = 3\\) et \\(\\lambda_2 = 2\\)."
  },
  {
    "objectID": "slides/02-revisions-algebre-slides.html#valeurs-et-vecteurs-propres-exemple-1",
    "href": "slides/02-revisions-algebre-slides.html#valeurs-et-vecteurs-propres-exemple-1",
    "title": "R√©visions",
    "section": "Valeurs et vecteurs propres : exemple",
    "text": "Valeurs et vecteurs propres : exemple\n\\[A = \\begin{pmatrix} 3 & 1 \\\\ 0 & 2 \\end{pmatrix}\\]\n\nCalcul des vecteurs propres :\n\nPour \\(\\lambda_1 = 3\\) : \\(u_1 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\\).\nPour \\(\\lambda_2 = 2\\) : \\(u_2 = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}\\)."
  },
  {
    "objectID": "slides/02-revisions-algebre-slides.html#propri√©t√©s-des-vecteurs-propres",
    "href": "slides/02-revisions-algebre-slides.html#propri√©t√©s-des-vecteurs-propres",
    "title": "R√©visions",
    "section": "Propri√©t√©s des vecteurs propres",
    "text": "Propri√©t√©s des vecteurs propres\n\n\n\nPropri√©t√©s\n\n\n\nSi \\(u\\) est vecteur propre pour \\(\\lambda\\), alors \\(cu\\) l‚Äôest aussi (pour \\(c \\neq 0\\)).\nPour matrices sym√©triques : Si \\(u_1\\) et \\(u_2\\) sont deux vecteurs propres correspondent √† des valeurs propres diff√©rentes, alors : \\[u_1^\\top u_2 = 0 \\text{ (orthogonalit√©)}.\\]"
  },
  {
    "objectID": "slides/02-revisions-algebre-slides.html#preuve-de-lorthogonalit√©",
    "href": "slides/02-revisions-algebre-slides.html#preuve-de-lorthogonalit√©",
    "title": "R√©visions",
    "section": "Preuve de l‚Äôorthogonalit√©",
    "text": "Preuve de l‚Äôorthogonalit√©\nSoient \\(\\lambda_1 \\neq \\lambda_2\\) et \\(Au_1 = \\lambda_1 u_1\\), \\(Au_2 = \\lambda_2 u_2\\).\n\nOn a \\[\\lambda_1 u_1^\\top u_2 = u_1^\\top A u_2 = \\lambda_2 u_1^\\top u_2.\\]\n\n\nDonc \\[(\\lambda_1 - \\lambda_2)u_1^\\top u_2 = 0\\]\n\n\nComme \\(\\lambda_1 \\neq \\lambda_2\\), on a n√©cessairement \\(u_1^\\top u_2 = 0\\)\n‚Üí Cette propri√©t√© est cruciale pour l‚ÄôACP !"
  },
  {
    "objectID": "slides/02-revisions-algebre-slides.html#caract√©risation-des-valeurs-propres",
    "href": "slides/02-revisions-algebre-slides.html#caract√©risation-des-valeurs-propres",
    "title": "R√©visions",
    "section": "Caract√©risation des valeurs propres",
    "text": "Caract√©risation des valeurs propres\n\n\n\n\nPropri√©t√©s\n\n\n\nMatrice sym√©trique ‚Üí toutes les valeurs propres sont r√©elles.\nMatrice d√©finie positive ‚Üí toutes les valeurs propres sont strictement positives."
  },
  {
    "objectID": "slides/02-revisions-algebre-slides.html#caract√©risation-des-valeurs-propres-exemple",
    "href": "slides/02-revisions-algebre-slides.html#caract√©risation-des-valeurs-propres-exemple",
    "title": "R√©visions",
    "section": "Caract√©risation des valeurs propres : exemple",
    "text": "Caract√©risation des valeurs propres : exemple\n\nSoit \\(A = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix} \\text{ (sym√©trique et d√©finie positive)}\\).\n\nValeurs propres : \\(\\lambda_1 = 3, \\lambda_2 = 1\\) (toutes r√©elles et &gt; 0).\nVecteurs propres : \\(u_1 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}, u_2 = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}\\).\nV√©rifions l‚Äôorthogonalit√© : \\(u_1^\\top u_2 = 1 \\times 1 + 1 \\times (-1) = 0\\)."
  },
  {
    "objectID": "slides/02-revisions-algebre-slides.html#diagonalisation-de-matrices",
    "href": "slides/02-revisions-algebre-slides.html#diagonalisation-de-matrices",
    "title": "R√©visions",
    "section": "Diagonalisation de matrices",
    "text": "Diagonalisation de matrices\n\n\n\n\nD√©finition\n\n\nUne matrice \\(A\\) est diagonalisable s‚Äôil existe \\(P\\) inversible et \\(D\\) diagonale telles que : \\[A = P D P^{-1} \\Leftrightarrow P^{-1} A P = D.\\]\n\n\n\n\nInterpr√©tation : On peut ‚Äúsimplifier‚Äù \\(A\\) en changeant de base."
  },
  {
    "objectID": "slides/02-revisions-algebre-slides.html#th√©or√®me-de-d√©composition-spectrale",
    "href": "slides/02-revisions-algebre-slides.html#th√©or√®me-de-d√©composition-spectrale",
    "title": "R√©visions",
    "section": "Th√©or√®me de d√©composition spectrale",
    "text": "Th√©or√®me de d√©composition spectrale\n\n\n\n\nTh√©or√®me\n\n\nPour toute matrice sym√©trique \\(A\\), il existe une matrice orthogonale \\(P\\) telle que : \\(A = P \\Lambda P^\\top\\) o√π \\(\\Lambda = \\text{diag}(\\lambda_1, \\ldots, \\lambda_n)\\).\n\n\n\nLa matrice \\(P\\) est form√©e des vecteurs propres de \\(A\\) et \\(\\Lambda\\) est la matrice diagonale des valeurs propres associ√©es."
  },
  {
    "objectID": "slides/02-revisions-algebre-slides.html#exemple",
    "href": "slides/02-revisions-algebre-slides.html#exemple",
    "title": "R√©visions",
    "section": "Exemple",
    "text": "Exemple\nEn reprenant l‚Äôexample pr√©c√©dent.\nSoit \\(A = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix}\\). On a \\(P = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 & 1 \\\\ 1 & -1 \\end{pmatrix}\\) et \\(\\Lambda = \\begin{pmatrix} 3 & 0 \\\\ 0 & 1 \\end{pmatrix}.\\)\nOn peut v√©rifier le th√©or√®me de d√©composition spectrale :\n\\[P \\Lambda P^\\top = \\frac{1}{2}\\begin{pmatrix} 1 & 1 \\\\ 1 & -1 \\end{pmatrix}\\begin{pmatrix} 3 & 0 \\\\ 0 & 1 \\end{pmatrix}\\begin{pmatrix} 1 & 1 \\\\ 1 & -1 \\end{pmatrix} = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix} = A.\\]"
  },
  {
    "objectID": "slides/02-revisions-algebre-slides.html#lien-avec-le-d√©terminant-et-la-trace",
    "href": "slides/02-revisions-algebre-slides.html#lien-avec-le-d√©terminant-et-la-trace",
    "title": "R√©visions",
    "section": "Lien avec le d√©terminant et la trace",
    "text": "Lien avec le d√©terminant et la trace\n\n\n\n\nPropri√©t√©s\n\n\nSi la matrice \\(A\\) a pour valeurs propres \\(\\lambda_1, \\ldots, \\lambda_n\\) :\n\n\\(\\det(A) = \\prod_{i=1}^{n} \\lambda_i\\).\n\\(\\text{tr}(A) = \\sum_{i=1}^{n} \\lambda_i\\)."
  },
  {
    "objectID": "slides/02-revisions-algebre-slides.html#lien-avec-le-d√©terminant-et-la-trace-exemple",
    "href": "slides/02-revisions-algebre-slides.html#lien-avec-le-d√©terminant-et-la-trace-exemple",
    "title": "R√©visions",
    "section": "Lien avec le d√©terminant et la trace : exemple",
    "text": "Lien avec le d√©terminant et la trace : exemple\n\nSoit \\(A = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix}\\). On a vu que ses valeurs propres sont \\(\\lambda_1 = 3\\) et \\(\\lambda_2 = 1\\).\nOn peut donc v√©rifier les formules pour le d√©terminant et la trace :\n\\[\\det(A) = 4 - 1 = 3 = 3 \\times 1 = \\lambda_1 \\lambda_2,\\]\n\\[\\text{tr}(A) = 2 + 2 = 4 = 3 + 1 = \\lambda_1 + \\lambda_2.\\]"
  },
  {
    "objectID": "slides/04-dimension-pca-slides.html#pourquoi-r√©duire-la-dimension",
    "href": "slides/04-dimension-pca-slides.html#pourquoi-r√©duire-la-dimension",
    "title": "Dimension",
    "section": "Pourquoi r√©duire la dimension ?",
    "text": "Pourquoi r√©duire la dimension ?\nTravailler avec un grand nombre de variables pose plusieurs d√©fis :\n\nVisualisation compliqu√©e : impossible de repr√©senter au-del√† de 3 dimensions\nS√©paration des classes difficile : structure cach√©e dans des combinaisons\nCo√ªt computationnel √©lev√© : mod√®les complexes difficiles √† ajuster\nCorr√©lations fortes : variables redondantes, mod√®les instables\n\nQuestion centrale : Peut-on r√©duire la dimension sans perdre trop d‚Äôinformation ?"
  },
  {
    "objectID": "slides/04-dimension-pca-slides.html#lapproche-pca",
    "href": "slides/04-dimension-pca-slides.html#lapproche-pca",
    "title": "Dimension",
    "section": "L‚Äôapproche PCA",
    "text": "L‚Äôapproche PCA\nPrincipe : Ne pas simplement supprimer des variables, mais construire de nouvelles variables.\nM√©thode : Cr√©er des combinaisons lin√©aires des variables initiales qui r√©sument l‚Äôinformation essentielle.\nAnalyse en Composantes Principales (ACP) : M√©thode non-supervis√©e pour r√©duire la dimension tout en conservant le maximum d‚Äôinformation.\nIntroduite par Hotelling (1933)."
  },
  {
    "objectID": "slides/04-dimension-pca-slides.html#applications-courantes",
    "href": "slides/04-dimension-pca-slides.html#applications-courantes",
    "title": "Dimension",
    "section": "Applications courantes",
    "text": "Applications courantes\n\nVisualisation : Repr√©senter des donn√©es multidimensionnelles\nR√©duction dimensionnelle : Passer de \\(p\\) √† \\(k \\ll p\\) variables\nCompression : Images ou signaux\nExploration : Donn√©es biologiques, textuelles, environnementales"
  },
  {
    "objectID": "slides/04-dimension-pca-slides.html#exemples-concrets",
    "href": "slides/04-dimension-pca-slides.html#exemples-concrets",
    "title": "Dimension",
    "section": "Exemples concrets",
    "text": "Exemples concrets\n\nSport : Comparer des √©quipes de hockey sur 6 statistiques\nCriminologie : R√©sumer la criminalit√© entre provinces sur 7 types de crimes\nImagerie : Compresser des images de \\(1084 \\times 1084\\) pixels\nBiologie : Identifier des variants de tumeurs √† partir de millions de g√®nes"
  },
  {
    "objectID": "slides/04-dimension-pca-slides.html#formulation-math√©matique",
    "href": "slides/04-dimension-pca-slides.html#formulation-math√©matique",
    "title": "Dimension",
    "section": "Formulation math√©matique",
    "text": "Formulation math√©matique\nSoit \\(X = (X_1, \\ldots, X_p)^{\\top}\\) un vecteur al√©atoire centr√© avec matrice de covariance \\(\\Sigma\\).\nOn cherche une combinaison lin√©aire : \\[Y_1 = \\alpha_1^{\\top} X = \\sum_{k=1}^p \\alpha_{1k} X_k\\]\nObjectif : Maximiser \\(\\text{Var}(Y_1)\\) sous contrainte \\(\\|\\alpha_1\\| = 1\\)"
  },
  {
    "objectID": "slides/04-dimension-pca-slides.html#probl√®me-doptimisation",
    "href": "slides/04-dimension-pca-slides.html#probl√®me-doptimisation",
    "title": "Dimension",
    "section": "Probl√®me d‚Äôoptimisation",
    "text": "Probl√®me d‚Äôoptimisation\n\\[\\max_{\\alpha_1^{\\top} \\alpha_1 = 1} \\text{Var}(Y_1) = \\max_{\\alpha_1^{\\top} \\alpha_1 = 1} \\alpha_1^{\\top} \\Sigma \\alpha_1\\]\nSolution (multiplicateurs de Lagrange) : \\[\\Sigma \\alpha_1 = \\lambda_1 \\alpha_1\\]\no√π \\(\\lambda_1\\) est la plus grande valeur propre de \\(\\Sigma\\) et \\(\\alpha_1\\) le vecteur propre associ√©."
  },
  {
    "objectID": "slides/04-dimension-pca-slides.html#construction-des-composantes",
    "href": "slides/04-dimension-pca-slides.html#construction-des-composantes",
    "title": "Dimension",
    "section": "Construction des composantes",
    "text": "Construction des composantes\nLes composantes suivantes sont construites par orthogonalit√© :\n\\[\\Sigma \\alpha_k = \\lambda_k \\alpha_k, \\quad \\lambda_1 \\geq \\lambda_2 \\geq \\ldots \\geq \\lambda_p\\]\nComposantes principales : \\[Y_k = \\alpha_k^{\\top} X\\]\no√π \\(\\alpha_k\\) est le vecteur propre associ√© √† \\(\\lambda_k\\)."
  },
  {
    "objectID": "slides/04-dimension-pca-slides.html#repr√©sentation-matricielle",
    "href": "slides/04-dimension-pca-slides.html#repr√©sentation-matricielle",
    "title": "Dimension",
    "section": "Repr√©sentation matricielle",
    "text": "Repr√©sentation matricielle\nSoit \\(A = (\\alpha_1, \\ldots, \\alpha_p) \\in \\mathbb{R}^{p \\times p}\\) la matrice des vecteurs propres.\n\\[Y = A^{\\top} X\\]\nPropri√©t√©s de \\(A\\) : - \\(A^{\\top} A = AA^{\\top} = I_p\\) (orthogonale) - \\(A^{\\top} = A^{-1}\\) - \\(\\Sigma A = A \\Lambda\\) o√π \\(\\Lambda = \\text{diag}(\\lambda_1, \\ldots, \\lambda_p)\\)"
  },
  {
    "objectID": "slides/04-dimension-pca-slides.html#variance-des-composantes",
    "href": "slides/04-dimension-pca-slides.html#variance-des-composantes",
    "title": "Dimension",
    "section": "Variance des composantes",
    "text": "Variance des composantes\n\\[\\text{Var}(Y) = A^{\\top} \\Sigma A = \\Lambda\\]\nCons√©quences : - \\(\\text{Cov}(Y_k, Y_l) = 0\\) si \\(k \\neq l\\) (orthogonalit√©) - \\(\\text{Var}(Y_k) = \\lambda_k\\) (variance d√©croissante) - \\(\\text{Var}(Y_1) \\geq \\text{Var}(Y_2) \\geq \\ldots \\geq \\text{Var}(Y_p)\\)"
  },
  {
    "objectID": "slides/04-dimension-pca-slides.html#proportion-de-variance-expliqu√©e",
    "href": "slides/04-dimension-pca-slides.html#proportion-de-variance-expliqu√©e",
    "title": "Dimension",
    "section": "Proportion de variance expliqu√©e",
    "text": "Proportion de variance expliqu√©e\nVariation totale : \\[\\text{tr}(\\Sigma) = \\sum_{i=1}^p \\lambda_i = \\sum_{k=1}^p \\text{Var}(Y_k)\\]\nProportion expliqu√©e par \\(Y_k\\) : \\[\\frac{\\lambda_k}{\\lambda_1 + \\cdots + \\lambda_p} = \\frac{\\text{Var}(Y_k)}{\\text{tr}(\\Sigma)}\\]\nProportion expliqu√©e par les \\(m\\) premi√®res : \\[100\\% \\times \\frac{\\sum_{k=1}^m \\lambda_k}{\\sum_{k=1}^p \\lambda_k}\\]"
  },
  {
    "objectID": "slides/04-dimension-pca-slides.html#estimation-pratique",
    "href": "slides/04-dimension-pca-slides.html#estimation-pratique",
    "title": "Dimension",
    "section": "Estimation pratique",
    "text": "Estimation pratique\nEn pratique, \\(\\Sigma\\) est inconnue. On estime par :\n\\[\\widehat{\\Sigma} = \\frac{1}{n-1} \\sum_{i=1}^n (X_i - \\overline{X})(X_i - \\overline{X})^{\\top}\\]\nD√©composition spectrale : \\[\\widehat{\\Sigma} = \\widehat{A} \\widehat{\\Lambda} \\widehat{A}^{\\top}\\]\nComposantes estim√©es : \\[Y_i = \\widehat{A}^{\\top} X_i\\]"
  },
  {
    "objectID": "slides/04-dimension-pca-slides.html#sensibilit√©-√†-l√©chelle",
    "href": "slides/04-dimension-pca-slides.html#sensibilit√©-√†-l√©chelle",
    "title": "Dimension",
    "section": "Sensibilit√© √† l‚Äô√©chelle",
    "text": "Sensibilit√© √† l‚Äô√©chelle\nProbl√®me : Variables avec grandes variances dominent l‚ÄôACP.\nExemple : Distance en m√®tres vs kilom√®tres multiplie la variance par \\(10^6\\).\nSolution : Standardiser les variables si elles ont des unit√©s diff√©rentes.\n√âquivalence : ACP sur matrice de corr√©lations au lieu de covariances."
  },
  {
    "objectID": "slides/04-dimension-pca-slides.html#interpr√©tation-g√©om√©trique",
    "href": "slides/04-dimension-pca-slides.html#interpr√©tation-g√©om√©trique",
    "title": "Dimension",
    "section": "Interpr√©tation g√©om√©trique",
    "text": "Interpr√©tation g√©om√©trique\nRotation des axes : \\(Y = A^{\\top}X\\) repr√©sente une rotation orthogonale.\nNouveaux axes : Directions de variation maximale successive.\nScores : \\(Y_{ik} = \\alpha_k^{\\top} X_i\\) (coordonn√©es dans le nouveau syst√®me).\nQualit√© de repr√©sentation : \\[Q_{ik} = \\frac{Y_{ik}^2}{\\sum_{j=1}^p Y_{ij}^2}\\]"
  },
  {
    "objectID": "slides/04-dimension-pca-slides.html#choix-du-nombre-de-composantes",
    "href": "slides/04-dimension-pca-slides.html#choix-du-nombre-de-composantes",
    "title": "Dimension",
    "section": "Choix du nombre de composantes",
    "text": "Choix du nombre de composantes\n1. R√®gle des 80% : Retenir assez de composantes pour expliquer 80% de la variance.\n2. R√®gle de Kaiser : Conserver les composantes avec \\(\\lambda_k &gt; 1\\) (si matrice de corr√©lations).\n3. R√®gle de Joliffe : Variante plus stricte, \\(\\lambda_k &gt; 0.7\\).\n4. R√®gle de Cattell (coude) : Chercher un point de rupture dans le graphique des valeurs propres."
  },
  {
    "objectID": "slides/04-dimension-pca-slides.html#r√®gles-de-choix---d√©tails",
    "href": "slides/04-dimension-pca-slides.html#r√®gles-de-choix---d√©tails",
    "title": "Dimension",
    "section": "R√®gles de choix - D√©tails",
    "text": "R√®gles de choix - D√©tails\nR√®gle des 80% : - Seuil arbitraire mais pratique - Bon √©quilibre information/simplicit√©\nR√®gle de Kaiser : - Bas√©e sur la moyenne des valeurs propres = 1 - Applicable uniquement avec standardisation\nR√®gle du coude : - Graphique \\(\\lambda_k\\) vs rang \\(k\\) - Chercher changement de pente - Plus subjective mais intuitive"
  },
  {
    "objectID": "slides/04-dimension-pca-slides.html#consid√©rations-pratiques",
    "href": "slides/04-dimension-pca-slides.html#consid√©rations-pratiques",
    "title": "Dimension",
    "section": "Consid√©rations pratiques",
    "text": "Consid√©rations pratiques\nUsage en pr√©diction : Utiliser les \\(k\\) premi√®res composantes comme pr√©dicteurs.\nAvantages : - R√©duit la dimension - √âlimine la multicolin√©arit√© - Conserve l‚Äôinformation essentielle\nCompromis : - Trop de composantes : pas de simplification - Trop peu : perte d‚Äôinformation pertinente"
  },
  {
    "objectID": "slides/04-dimension-pca-slides.html#conclusion",
    "href": "slides/04-dimension-pca-slides.html#conclusion",
    "title": "Dimension",
    "section": "Conclusion",
    "text": "Conclusion\nL‚ÄôACP est un outil puissant pour la r√©duction de dimension qui :\n\nPr√©serve l‚Äôinformation maximale\n√âlimine la redondance\nFacilite la visualisation et la mod√©lisation\nN√©cessite des choix m√©thodologiques importants\n\nLe succ√®s d√©pend du choix appropri√© du nombre de composantes selon le contexte d‚Äôapplication."
  },
  {
    "objectID": "slides/04-dimension-mca-slides.html#introduction-√†-lacm",
    "href": "slides/04-dimension-mca-slides.html#introduction-√†-lacm",
    "title": "Dimension",
    "section": "Introduction √† l‚ÄôACM",
    "text": "Introduction √† l‚ÄôACM\nL‚ÄôAnalyse des Correspondances Multiples (ACM) est un prolongement de l‚ÄôAFC pour analyser plus de deux variables qualitatives.\nApplications classiques : - Questionnaires √† choix multiples (\\(Q\\) questions) - Visualisation de r√©ponses d‚Äôenqu√™tes - Variables mixtes (quantitatives + qualitatives)\nPrincipe fondamental : Transformation pr√©alable par codage disjonctif complet."
  },
  {
    "objectID": "slides/04-dimension-mca-slides.html#extension-de-lafc",
    "href": "slides/04-dimension-mca-slides.html#extension-de-lafc",
    "title": "Dimension",
    "section": "Extension de l‚ÄôAFC",
    "text": "Extension de l‚ÄôAFC\nL‚ÄôACM peut √™tre vue comme :\n\nAFC g√©n√©ralis√©e : Plus de 2 variables qualitatives\nACP adapt√©e : Variables mixtes (quanti + quali)\nAnalyse de questionnaires : R√©ponses √† choix multiples\n\nTransformation cl√© : Passer des variables originales aux modalit√©s binaires."
  },
  {
    "objectID": "slides/04-dimension-mca-slides.html#notation-de-base",
    "href": "slides/04-dimension-mca-slides.html#notation-de-base",
    "title": "Dimension",
    "section": "Notation de base",
    "text": "Notation de base\n\n\\(n\\) : nombre d‚Äôindividus (observations)\n\\(Q\\) : nombre de variables (questions)\n\n\\(J_q\\) : nombre de modalit√©s de la variable \\(q\\)\n\\(J\\) : nombre total de modalit√©s (\\(J = \\sum_{q=1}^Q J_q\\))\n\nObjectif : Repr√©senter graphiquement les relations entre toutes ces modalit√©s."
  },
  {
    "objectID": "slides/04-dimension-mca-slides.html#codage-disjonctif-complet",
    "href": "slides/04-dimension-mca-slides.html#codage-disjonctif-complet",
    "title": "Dimension",
    "section": "Codage disjonctif complet",
    "text": "Codage disjonctif complet\n\n\n\n\n\n\nAvertissement\n\n\nD√©finition : Tableau binaire \\(Z\\) (\\(n \\times J\\)) contenant uniquement des 0 et des 1.\n\n\n\nPrincipe : - Chaque variable ‚Üí ses modalit√©s - Individu poss√®de modalit√© ‚Üí code 1 - Individu ne poss√®de pas modalit√© ‚Üí code 0 - Disjonctif : au plus une modalit√© par variable - Complet : au moins une modalit√© par variable"
  },
  {
    "objectID": "slides/04-dimension-mca-slides.html#exemple-concret---donn√©es",
    "href": "slides/04-dimension-mca-slides.html#exemple-concret---donn√©es",
    "title": "Dimension",
    "section": "Exemple concret - Donn√©es",
    "text": "Exemple concret - Donn√©es\nProduits avec Type et Prix :\n\n\n\nProduit\nType\nPrix ($)\n\n\n\n\nNike Tech Fleece\nHoodie\n256.72\n\n\nPuma Joggers\nJoggers\n221.26\n\n\nOff-White Hoodie\nHoodie\n198.45\n\n\nSupreme Hoodie\nHoodie\n235.50\n\n\nJordan 1 High\nSneakers\n298.22\n\n\nNike Dunk Low\nSneakers\n273.00\n\n\nNike Tech Fleece\nHoodie\n162.38\n\n\n\nVariable Prix ‚Üí 3 classes : &lt;200\\(, [200\\)-250\\(], &gt;250\\)"
  },
  {
    "objectID": "slides/04-dimension-mca-slides.html#exemple---codage-disjonctif-complet",
    "href": "slides/04-dimension-mca-slides.html#exemple---codage-disjonctif-complet",
    "title": "Dimension",
    "section": "Exemple - Codage disjonctif complet",
    "text": "Exemple - Codage disjonctif complet\nTableau transform√© \\(Z\\) :\n\n\n\nProduit\nHoodie\nJoggers\nSneakers\n&lt;200$\n[200-250\\(] | &gt;250\\)\n\n\n\n\n\nNike Tech Fleece\n1\n0\n0\n0\n0\n1\n\n\nPuma Joggers\n0\n1\n0\n0\n1\n0\n\n\nOff-White Hoodie\n1\n0\n0\n1\n0\n0\n\n\nSupreme Hoodie\n1\n0\n0\n0\n1\n0\n\n\nJordan 1 High\n0\n0\n1\n0\n0\n1\n\n\nNike Dunk Low\n0\n0\n1\n0\n0\n1\n\n\nNike Tech Fleece\n1\n0\n0\n1\n0\n0\n\n\n\nChaque ligne somme √† \\(Q = 2\\) (2 variables)."
  },
  {
    "objectID": "slides/04-dimension-mca-slides.html#propri√©t√©s-du-tableau-z",
    "href": "slides/04-dimension-mca-slides.html#propri√©t√©s-du-tableau-z",
    "title": "Dimension",
    "section": "Propri√©t√©s du tableau Z",
    "text": "Propri√©t√©s du tableau Z\n\n\n\n\n\n\nImportant\n\n\nPropri√©t√©s fondamentales :\n\nSomme par ligne : \\(\\sum_{j=1}^J z_{ij} = Q\\) (constante)\nSomme totale : \\(\\sum_{i,j} z_{ij} = nQ\\)\nSomme par colonne : \\(\\sum_{i=1}^n z_{ij} = n_j\\) (effectif modalit√© \\(j\\))\n\n\n\n\nCes propri√©t√©s d√©coulent directement du codage disjonctif complet."
  },
  {
    "objectID": "slides/04-dimension-mca-slides.html#perte-dinformation",
    "href": "slides/04-dimension-mca-slides.html#perte-dinformation",
    "title": "Dimension",
    "section": "Perte d‚Äôinformation",
    "text": "Perte d‚Äôinformation\n\n\n\n\n\n\nAstuce\n\n\nVariables quantitatives ‚Üí Classes : Perte d‚Äôinformation in√©vitable.\n\nValeur pr√©cise ‚Üí Appartenance √† une classe\nExemple : 256.72$ ‚Üí ‚Äú&gt;250$‚Äù\nCompromis : Granularit√© vs interpr√©tabilit√©\n\n\n\n\nEnjeu : Choisir des classes pertinentes pour le probl√®me √©tudi√©."
  },
  {
    "objectID": "slides/04-dimension-mca-slides.html#tableau-de-burt",
    "href": "slides/04-dimension-mca-slides.html#tableau-de-burt",
    "title": "Dimension",
    "section": "Tableau de Burt",
    "text": "Tableau de Burt\n\n\n\n\n\n\nAvertissement\n\n\nD√©finition : \\(B = Z^\\top Z\\) (matrice \\(J \\times J\\))\n\n\n\nStructure du tableau de Burt : - Blocs diagonaux : \\(B_{qq} = Z_q^\\top Z_q\\) (effectifs par modalit√©) - Blocs non-diagonaux : \\(B_{qq'} = Z_q^\\top Z_{q'}\\) (tableaux de contingence) - Sym√©trique : \\(B_{q'q} = B_{qq'}^\\top\\)"
  },
  {
    "objectID": "slides/04-dimension-mca-slides.html#propri√©t√©s-du-tableau-de-burt",
    "href": "slides/04-dimension-mca-slides.html#propri√©t√©s-du-tableau-de-burt",
    "title": "Dimension",
    "section": "Propri√©t√©s du tableau de Burt",
    "text": "Propri√©t√©s du tableau de Burt\nTaille : \\(J \\times J\\) (nombre total de modalit√©s)\nBlocs diagonaux : Matrices diagonales - √âl√©ments = effectifs de chaque modalit√©\nBlocs non-diagonaux : Tableaux de contingence\n- Croisement entre variables \\(q\\) et \\(q'\\)\nSym√©trie : \\(B = B^\\top\\)"
  },
  {
    "objectID": "slides/04-dimension-mca-slides.html#√©quivalence-math√©matique",
    "href": "slides/04-dimension-mca-slides.html#√©quivalence-math√©matique",
    "title": "Dimension",
    "section": "√âquivalence math√©matique",
    "text": "√âquivalence math√©matique\nPropri√©t√© fondamentale : L‚ÄôACM peut s‚Äôeffectuer sur : - Le tableau disjonctif complet \\(Z\\) - Le tableau de Burt \\(B = Z^\\top Z\\)\nR√©sultat : Les deux approches donnent les m√™mes facteurs !\nCette √©quivalence offre une flexibilit√© computationnelle."
  },
  {
    "objectID": "slides/04-dimension-mca-slides.html#√©l√©ments-propres-de-z",
    "href": "slides/04-dimension-mca-slides.html#√©l√©ments-propres-de-z",
    "title": "Dimension",
    "section": "√âl√©ments propres de Z",
    "text": "√âl√©ments propres de Z\nAnalyse directe - Vecteurs propres de : \\[S = \\frac{1}{Q} Z^\\top Z D_J^{-1}\\]\no√π \\(D_J = \\text{diag}(n_1, \\ldots, n_J)\\)\nCoordonn√©es profils-lignes : \\[\\Phi_k = n Z D_J^{-1} u_k\\]\nAnalyse duale - Vecteurs propres de : \\[T = \\frac{1}{Q} Z D_J^{-1} Z^\\top\\]"
  },
  {
    "objectID": "slides/04-dimension-mca-slides.html#√©l√©ments-propres-de-b",
    "href": "slides/04-dimension-mca-slides.html#√©l√©ments-propres-de-b",
    "title": "Dimension",
    "section": "√âl√©ments propres de B",
    "text": "√âl√©ments propres de B\nTableau de Burt sym√©trique ‚Üí Analyse directe = duale\nVecteurs propres de : \\[S' = \\frac{1}{Q^2} B^\\top D_J^{-1} B D_J^{-1}\\]\nPropri√©t√© remarquable : \\[S' = \\frac{1}{Q^2} Z^\\top Z D_J^{-1} Z^\\top Z D_J^{-1}\\]"
  },
  {
    "objectID": "slides/04-dimension-mca-slides.html#relation-entre-valeurs-propres",
    "href": "slides/04-dimension-mca-slides.html#relation-entre-valeurs-propres",
    "title": "Dimension",
    "section": "Relation entre valeurs propres",
    "text": "Relation entre valeurs propres\nTh√©or√®me fondamental :\nSi \\(\\lambda\\) est valeur propre de \\(S\\) (analyse de \\(Z\\)),\nalors \\(\\lambda^2\\) est valeur propre de \\(S'\\) (analyse de \\(B\\)).\nCons√©quence : - M√™mes vecteurs propres pour \\(Z\\) et \\(B\\) - Valeurs propres de \\(B\\) = carr√©s des valeurs propres de \\(Z\\)"
  },
  {
    "objectID": "slides/04-dimension-mca-slides.html#encodage-des-variables-quantitatives",
    "href": "slides/04-dimension-mca-slides.html#encodage-des-variables-quantitatives",
    "title": "Dimension",
    "section": "Encodage des variables quantitatives",
    "text": "Encodage des variables quantitatives\nChoix des bornes crucial pour la qualit√© de l‚Äôanalyse :\nApproches recommand√©es : - Analyser la distribution (histogrammes) - Bornes pertinentes au domaine d‚Äô√©tude - √âviter les classes peu informatives\nApproche d√©conseill√©e : - Effectifs √©gaux automatiques - Classes sans sens m√©tier"
  },
  {
    "objectID": "slides/04-dimension-mca-slides.html#encodage-des-variables-qualitatives",
    "href": "slides/04-dimension-mca-slides.html#encodage-des-variables-qualitatives",
    "title": "Dimension",
    "section": "Encodage des variables qualitatives",
    "text": "Encodage des variables qualitatives\nModalit√©s ‚Äúnaturelles‚Äù mais probl√®mes potentiels :\nEffectifs d√©s√©quilibr√©s : - Modalit√©s tr√®s rares vs tr√®s fr√©quentes - Impact sur la repr√©sentation\nSolutions : - Regroupements pertinents (connaissance du domaine) - √âviter la r√©partition al√©atoire - Pr√©server le sens des modalit√©s"
  },
  {
    "objectID": "slides/04-dimension-mca-slides.html#strat√©gies-dencodage",
    "href": "slides/04-dimension-mca-slides.html#strat√©gies-dencodage",
    "title": "Dimension",
    "section": "Strat√©gies d‚Äôencodage",
    "text": "Strat√©gies d‚Äôencodage\nVariables continues : 1. Examiner la distribution 2. Identifier des seuils naturels 3. Classes √©quilibr√©es ET pertinentes 4. √âviter trop de classes (complexit√©)\nVariables qualitatives : 1. Analyser les effectifs 2. Regrouper les modalit√©s rares 3. Pr√©server la logique m√©tier 4. Maintenir l‚Äôinterpr√©tabilit√©"
  },
  {
    "objectID": "slides/04-dimension-mca-slides.html#consid√©rations-pratiques",
    "href": "slides/04-dimension-mca-slides.html#consid√©rations-pratiques",
    "title": "Dimension",
    "section": "Consid√©rations pratiques",
    "text": "Consid√©rations pratiques\nNombre optimal de classes : - Trop peu ‚Üí Perte d‚Äôinformation - Trop ‚Üí Complexit√©, effectifs faibles\nValidation du codage : - V√©rifier la pertinence des classes - Tester diff√©rents d√©coupages - √âvaluer l‚Äôimpact sur les r√©sultats\nDocumentation : Justifier les choix d‚Äôencodage."
  },
  {
    "objectID": "slides/04-dimension-mca-slides.html#applications-typiques",
    "href": "slides/04-dimension-mca-slides.html#applications-typiques",
    "title": "Dimension",
    "section": "Applications typiques",
    "text": "Applications typiques\n√âtudes marketing : - Profils consommateurs √ó Pr√©f√©rences - Segmentation client√®le\nSciences sociales : - Enqu√™tes d‚Äôopinion - Comportements √ó Caract√©ristiques\nSant√© publique : - Facteurs de risque √ó Pathologies - Questionnaires de qualit√© de vie"
  },
  {
    "objectID": "slides/04-dimension-mca-slides.html#avantages-de-lacm",
    "href": "slides/04-dimension-mca-slides.html#avantages-de-lacm",
    "title": "Dimension",
    "section": "Avantages de l‚ÄôACM",
    "text": "Avantages de l‚ÄôACM\n\nVisualisation de relations complexes multi-variables\nTraitement unifi√© de variables h√©t√©rog√®nes\n\nR√©duction de dimension pr√©servant les associations\nInterpr√©tation intuitive des proximit√©s\nFlexibilit√© dans l‚Äôencodage des variables"
  },
  {
    "objectID": "slides/04-dimension-mca-slides.html#limites-et-pr√©cautions",
    "href": "slides/04-dimension-mca-slides.html#limites-et-pr√©cautions",
    "title": "Dimension",
    "section": "Limites et pr√©cautions",
    "text": "Limites et pr√©cautions\nPerte d‚Äôinformation : - Variables continues ‚Üí classes - Choix d‚Äôencodage critique\nSensibilit√© aux effectifs : - Modalit√©s rares peuvent dominer - √âquilibrage n√©cessaire\nInterpr√©tation : - Proximit√©s entre modalit√©s de variables diff√©rentes - Attention aux artefacts d‚Äôencodage"
  },
  {
    "objectID": "slides/04-dimension-mca-slides.html#conclusion",
    "href": "slides/04-dimension-mca-slides.html#conclusion",
    "title": "Dimension",
    "section": "Conclusion",
    "text": "Conclusion\nL‚ÄôACM √©tend l‚ÄôAFC au cas multi-variables gr√¢ce au codage disjonctif complet.\n√âl√©ments cl√©s : - Transformation binaire pr√©alable - √âquivalence \\(Z\\) ‚ÜîÔ∏é Tableau de Burt - Choix d‚Äôencodage fondamental - Visualisation des patterns complexes\nSucc√®s d√©pend de la qualit√© de l‚Äôencodage des variables originales."
  },
  {
    "objectID": "slides/03-generalities-stat-slides.html#structure-dun-projet",
    "href": "slides/03-generalities-stat-slides.html#structure-dun-projet",
    "title": "G√©n√©ralit√©s",
    "section": "Structure d‚Äôun projet",
    "text": "Structure d‚Äôun projet\n\n5 √©tapes principales :\n\nD√©finition des objectifs\nCollecte et pr√©paration des donn√©es\n√âlaboration et validation des mod√®les\nImpl√©mentation et mise en production\nSuivi de la performance et am√©lioration continue"
  },
  {
    "objectID": "slides/03-generalities-stat-slides.html#r√©partition-temps-vs-importance-pyle-1999",
    "href": "slides/03-generalities-stat-slides.html#r√©partition-temps-vs-importance-pyle-1999",
    "title": "G√©n√©ralit√©s",
    "section": "R√©partition temps vs importance (Pyle, 1999)",
    "text": "R√©partition temps vs importance (Pyle, 1999)\n\n\n\n\n√âtape\nTemps\nImportance\n\n\n\n\nComprendre le probl√®me\n10%\n15%\n\n\nExplorer la solution\n9%\n14%\n\n\nImplementer la solution\n1%\n51%\n\n\nPr√©parer les donn√©es\n60%\n15%\n\n\nAnalyse descriptive des donn√©es\n15%\n3%\n\n\nMod√©liser les donn√©es\n5%\n2%\n\n\n\n\nConstat: importance ‚â† temps pass√© !"
  },
  {
    "objectID": "slides/03-generalities-stat-slides.html#d√©finition-des-objectifs",
    "href": "slides/03-generalities-stat-slides.html#d√©finition-des-objectifs",
    "title": "G√©n√©ralit√©s",
    "section": "1. D√©finition des objectifs",
    "text": "1. D√©finition des objectifs\n\nPourquoi c‚Äôest crucial ?\n\nGuide la collecte des donn√©es.\nD√©finit le mod√®le ad√©quat.\nFacilite l‚Äôinterpr√©tation.\n√âvite les explorations aveugles."
  },
  {
    "objectID": "slides/03-generalities-stat-slides.html#d√©finition-des-objectifs-1",
    "href": "slides/03-generalities-stat-slides.html#d√©finition-des-objectifs-1",
    "title": "G√©n√©ralit√©s",
    "section": "1. D√©finition des objectifs",
    "text": "1. D√©finition des objectifs\n\nComment bien formuler ?\n\nClarifier les termes.\nIdentifier les utilisateurs.\nD√©finir la population cible.\nPr√©ciser les d√©cisions qui d√©pendent des r√©sultats."
  },
  {
    "objectID": "slides/03-generalities-stat-slides.html#exemples-dobjectifs",
    "href": "slides/03-generalities-stat-slides.html#exemples-dobjectifs",
    "title": "G√©n√©ralit√©s",
    "section": "1. Exemples d‚Äôobjectifs",
    "text": "1. Exemples d‚Äôobjectifs\n\n‚ùå Mauvais objectifs\n\nAnalyser les donn√©es clients.\nAnalyser les donn√©es des adversaires.\nAnalyser les donn√©es du m√©dicament.\n\n\n‚úÖ Bons objectifs\n\nPr√©dire quels clients ach√®teront le produit d‚Äô√©pargne.\nCaract√©riser le style de jeu pour identifier les faiblesses d‚Äôune √©quipe.\nConcevoir un protocole pour tester l‚Äôefficacit√© d‚Äôun m√©dicament."
  },
  {
    "objectID": "slides/03-generalities-stat-slides.html#sources-de-donn√©es",
    "href": "slides/03-generalities-stat-slides.html#sources-de-donn√©es",
    "title": "G√©n√©ralit√©s",
    "section": "2. Sources de donn√©es",
    "text": "2. Sources de donn√©es\nPlateformes g√©n√©riques :\n\nGoogle datasets\nKaggle\nUC Irvine ML Repository\n\nSources officielles :\n\nCanada: StatCan\nFrance: data.gouv.fr\nUSA: data.gov\n\nSources internes : bases clients, transactions, production, web analytics"
  },
  {
    "objectID": "slides/03-generalities-stat-slides.html#qualit√©-des-donn√©es",
    "href": "slides/03-generalities-stat-slides.html#qualit√©-des-donn√©es",
    "title": "G√©n√©ralit√©s",
    "section": "2. Qualit√© des donn√©es",
    "text": "2. Qualit√© des donn√©es\n\n\n‚ÄúGarbage in, garbage out‚Äù\n\n\nQuelques questions importantes :\n\nLes donn√©es sont-elles repr√©sentatives ?\nSont-elles exactes, compl√®tes, pertinentes ?\nY a-t-il des valeurs manquantes, doublons, incoh√©rences ?"
  },
  {
    "objectID": "slides/03-generalities-stat-slides.html#format-tidy-data",
    "href": "slides/03-generalities-stat-slides.html#format-tidy-data",
    "title": "G√©n√©ralit√©s",
    "section": "2. Format ‚ÄúTidy Data‚Äù",
    "text": "2. Format ‚ÄúTidy Data‚Äù\n\n3 principes fondamentaux :\n\nChaque variable = une colonne.\nChaque observation = une ligne.\nChaque cellule = une valeur unique.\n\n\n\nR : tidyr, readr, readxl, haven\n\nPython : pandas, polars"
  },
  {
    "objectID": "slides/03-generalities-stat-slides.html#exploration-pr√©liminaire---nettoyage",
    "href": "slides/03-generalities-stat-slides.html#exploration-pr√©liminaire---nettoyage",
    "title": "G√©n√©ralit√©s",
    "section": "2. Exploration pr√©liminaire - Nettoyage",
    "text": "2. Exploration pr√©liminaire - Nettoyage\n\n\nSupprimer les doublons.\nUniformiser les modalit√©s.\nV√©rifier les formats des valeurs sp√©ciales."
  },
  {
    "objectID": "slides/03-generalities-stat-slides.html#exploration-pr√©liminaire---exploration",
    "href": "slides/03-generalities-stat-slides.html#exploration-pr√©liminaire---exploration",
    "title": "G√©n√©ralit√©s",
    "section": "2. Exploration pr√©liminaire - Exploration",
    "text": "2. Exploration pr√©liminaire - Exploration\n\n\nModalit√©s rares/nombreuses.\nValeurs extr√™mes ou aberrantes.\nCorr√©lations fortes.\nClasses d√©s√©quilibr√©es.\nValeurs manquantes."
  },
  {
    "objectID": "slides/03-generalities-stat-slides.html#exploration-pr√©liminaire---exploration-1",
    "href": "slides/03-generalities-stat-slides.html#exploration-pr√©liminaire---exploration-1",
    "title": "G√©n√©ralit√©s",
    "section": "2. Exploration pr√©liminaire - Exploration",
    "text": "2. Exploration pr√©liminaire - Exploration\n\nPour faire l‚Äôexploration, on peut utiliser :\n\ndes statistiques descriptives unidimensionnelles\n\nnum√©rique: moyenne, extremum, tableau de fr√©quences, ‚Ä¶\ngraphique: histogramme, boxplot"
  },
  {
    "objectID": "slides/03-generalities-stat-slides.html#exploration-pr√©liminaire---exploration-2",
    "href": "slides/03-generalities-stat-slides.html#exploration-pr√©liminaire---exploration-2",
    "title": "G√©n√©ralit√©s",
    "section": "2. Exploration pr√©liminaire - Exploration",
    "text": "2. Exploration pr√©liminaire - Exploration\n\nPour faire l‚Äôexploration, on peut utiliser :\n\ndes statistiques descriptives bidimensionnelles\n\nnum√©rique: tableaux crois√©s, corr√©lation, ‚Ä¶\ngraphique: diagramme de dispersion, ‚Ä¶"
  },
  {
    "objectID": "slides/03-generalities-stat-slides.html#exploration-pr√©liminaire---exploration-3",
    "href": "slides/03-generalities-stat-slides.html#exploration-pr√©liminaire---exploration-3",
    "title": "G√©n√©ralit√©s",
    "section": "2. Exploration pr√©liminaire - Exploration",
    "text": "2. Exploration pr√©liminaire - Exploration\n\nPour faire l‚Äôexploration, on peut utiliser :\n\ndes transformations de variables\n\ntransformation simple: passer au \\(\\log\\), standardisation, ‚Ä¶\nregrouper des modalit√©s\ncombiner des variables"
  },
  {
    "objectID": "slides/03-generalities-stat-slides.html#exploration-pr√©liminaire---exploration-4",
    "href": "slides/03-generalities-stat-slides.html#exploration-pr√©liminaire---exploration-4",
    "title": "G√©n√©ralit√©s",
    "section": "2. Exploration pr√©liminaire - Exploration",
    "text": "2. Exploration pr√©liminaire - Exploration\n\nUne valeur extr√™me est une valeur √©loign√©e des autres valeurs de la population.\nüö® Peut correspondre √† un profil particulier et plus rare dans la population.\n\nQue faire ?\n\n\n\nUtiliser des m√©thodes robustes\nTransformer la variable, e.g.¬†avec un \\(\\log\\)."
  },
  {
    "objectID": "slides/03-generalities-stat-slides.html#exploration-pr√©liminaire---exploration-5",
    "href": "slides/03-generalities-stat-slides.html#exploration-pr√©liminaire---exploration-5",
    "title": "G√©n√©ralit√©s",
    "section": "2. Exploration pr√©liminaire - Exploration",
    "text": "2. Exploration pr√©liminaire - Exploration\n\nUne valeur aberrantes est une valeur erron√©e caus√©e par une erreur de saisie, une erreur de calcul, une mauvaise mesure ou une fausse d√©claration.\n\nQue faire ?\n\n\n\nLes remplacer par la bonne valeur.\nLes traiter comme des valeurs manquantes\nRetirer l‚Äôindividu."
  },
  {
    "objectID": "slides/03-generalities-stat-slides.html#mod√©lisation-4-composantes",
    "href": "slides/03-generalities-stat-slides.html#mod√©lisation-4-composantes",
    "title": "G√©n√©ralit√©s",
    "section": "3. Mod√©lisation : 4 composantes",
    "text": "3. Mod√©lisation : 4 composantes\n\n\n\nEspace de repr√©sentation (cadre math√©matique).\nDistance ou similarit√© (comparer les observations).\nMod√®le ou algorithme (m√©thode d‚Äôapprentissage).\nFonction de co√ªt (mesure de qualit√©)."
  },
  {
    "objectID": "slides/03-generalities-stat-slides.html#mise-en-production-data-engineering",
    "href": "slides/03-generalities-stat-slides.html#mise-en-production-data-engineering",
    "title": "G√©n√©ralit√©s",
    "section": "4. Mise en production : Data Engineering",
    "text": "4. Mise en production : Data Engineering\n\n\nAutomatiser la collecte et le nettoyage.\nInt√©grer le mod√®le dans l‚Äôapplication.\nG√©n√©rer rapports/pr√©dictions en temps r√©el.\nMaintenir la pipeline de donn√©es.\n\nR√¥le du Data Engineer : conception et maintenance de bout en bout."
  },
  {
    "objectID": "slides/03-generalities-stat-slides.html#suivi-et-am√©lioration",
    "href": "slides/03-generalities-stat-slides.html#suivi-et-am√©lioration",
    "title": "G√©n√©ralit√©s",
    "section": "5. Suivi et am√©lioration",
    "text": "5. Suivi et am√©lioration\n\nD√©fis :\n\nEvolution des donn√©es (data drift).\nHypoth√®ses initiales invalid√©es.\nNouvelles variables disponibles.\n\n\nSolutions :\n\nMonitoring r√©gulier des performances.\nR√©entra√Ænement avec donn√©es r√©centes.\nAm√©lioration continue du mod√®le."
  },
  {
    "objectID": "slides/03-generalities-stat-slides.html#conclusion",
    "href": "slides/03-generalities-stat-slides.html#conclusion",
    "title": "G√©n√©ralit√©s",
    "section": "Conclusion",
    "text": "Conclusion\n\nUn bon mod√®le :\n‚úÖ Performant √† un instant donn√©.\n‚úÖ Robuste dans le temps.\n‚úÖ Adaptable aux changements.\n‚Üí √âquilibre entre toutes les √©tapes, pas seulement la mod√©lisation !"
  },
  {
    "objectID": "slides/03-generalities-bias-variance-slides.html#objectif-de-mod√©lisation",
    "href": "slides/03-generalities-bias-variance-slides.html#objectif-de-mod√©lisation",
    "title": "G√©n√©ralit√©s",
    "section": "Objectif de mod√©lisation",
    "text": "Objectif de mod√©lisation\n\nMod√©liser la relation entre :\n\nVariable r√©ponse \\(Y\\) (quantitative, qualitative, etc.).\nVariables explicatives \\(X = (X_1, \\ldots, X_p)\\)."
  },
  {
    "objectID": "slides/03-generalities-bias-variance-slides.html#mod√®le-g√©n√©ral",
    "href": "slides/03-generalities-bias-variance-slides.html#mod√®le-g√©n√©ral",
    "title": "G√©n√©ralit√©s",
    "section": "Mod√®le g√©n√©ral",
    "text": "Mod√®le g√©n√©ral\n\n\\[Y = f(X) + \\varepsilon\\]\n\n\\(f\\) : fonction d√©terministe (information syst√©matique).\n\\(\\varepsilon\\) : terme d‚Äôerreur al√©atoire."
  },
  {
    "objectID": "slides/03-generalities-bias-variance-slides.html#hypoth√®ses-du-mod√®le",
    "href": "slides/03-generalities-bias-variance-slides.html#hypoth√®ses-du-mod√®le",
    "title": "G√©n√©ralit√©s",
    "section": "Hypoth√®ses du mod√®le",
    "text": "Hypoth√®ses du mod√®le\n\nSur le terme d‚Äôerreur \\(\\varepsilon\\) :\n\nInd√©pendant des variables explicatives \\(X\\).\n\\(\\mathbb{E}[\\varepsilon] = 0\\) (esp√©rance nulle).\n\\(\\mathrm{Var}(\\varepsilon) = \\sigma^2\\) (variance constante).\n\n\n‚Üí Cadre g√©n√©ral pour toutes les m√©thodes du cours."
  },
  {
    "objectID": "slides/03-generalities-bias-variance-slides.html#plan",
    "href": "slides/03-generalities-bias-variance-slides.html#plan",
    "title": "G√©n√©ralit√©s",
    "section": "Plan",
    "text": "Plan\n\n\nMesures de qualit√© - MSE et taux d‚Äôerreur\nCompromis biais/variance - D√©composition fondamentale\nFlexibilit√© des mod√®les - Rigidit√© vs adaptabilit√©\nOptimisation pratique - Trouver l‚Äô√©quilibre"
  },
  {
    "objectID": "slides/03-generalities-bias-variance-slides.html#comment-mesurer-la-qualit√©",
    "href": "slides/03-generalities-bias-variance-slides.html#comment-mesurer-la-qualit√©",
    "title": "G√©n√©ralit√©s",
    "section": "Comment mesurer la qualit√© ?",
    "text": "Comment mesurer la qualit√© ?\n\nUne fois \\(\\hat{f}\\) estim√©, comment √©valuer \\(\\hat{Y} = \\hat{f}(X)\\) ?\n\n\nId√©e : Mesurer √† quel point \\(\\hat{Y}\\) est proche de la vraie valeur \\(Y\\)."
  },
  {
    "objectID": "slides/03-generalities-bias-variance-slides.html#variables-quantitatives-eqm-mse",
    "href": "slides/03-generalities-bias-variance-slides.html#variables-quantitatives-eqm-mse",
    "title": "G√©n√©ralit√©s",
    "section": "Variables quantitatives : EQM (MSE)",
    "text": "Variables quantitatives : EQM (MSE)\n\n\n\nErreur Quadratique Moyenne (MSE)\n\n\n\\[\\text{MSE}(Y, \\hat{Y}) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{f}(x_i))^2\\]\n\n\n\n\nInterpr√©tation ‚Üí Distance moyenne entre valeurs observ√©es et pr√©dites.\n\n\nMSE faible ‚Üí Pr√©dictions proches des observations."
  },
  {
    "objectID": "slides/03-generalities-bias-variance-slides.html#variables-qualitatives-taux-derreur-er",
    "href": "slides/03-generalities-bias-variance-slides.html#variables-qualitatives-taux-derreur-er",
    "title": "G√©n√©ralit√©s",
    "section": "Variables qualitatives : Taux d‚Äôerreur (ER)",
    "text": "Variables qualitatives : Taux d‚Äôerreur (ER)\n\n\n\nTaux d‚Äôerreur (ER)\n\n\n\\[\\text{ER}(Y, \\hat{Y}) = \\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{1}(y_i \\neq \\hat{f}(x_i))\\]\n\n\n\n\nInterpr√©tation ‚Üí Proportion de mauvaises pr√©dictions.\n\n\nER faible ‚Üí Pr√©dictions proches des observations."
  },
  {
    "objectID": "slides/03-generalities-bias-variance-slides.html#exactitude-vs-interpr√©tabilit√©",
    "href": "slides/03-generalities-bias-variance-slides.html#exactitude-vs-interpr√©tabilit√©",
    "title": "G√©n√©ralit√©s",
    "section": "Exactitude vs Interpr√©tabilit√©",
    "text": "Exactitude vs Interpr√©tabilit√©\n\nMod√®les simples (ex: r√©gression lin√©aire) :\n\nFaciles √† interpr√©ter\nRelations complexes mal capt√©es\n\nMod√®les flexibles (ex: for√™t al√©atoire) :\n\nMeilleures pr√©dictions\nDifficiles √† interpr√©ter\n\n\nLe choix d√©pend de l‚Äôobjectif : compr√©hension ou performance ?"
  },
  {
    "objectID": "slides/03-generalities-bias-variance-slides.html#no-free-lunch-in-statistics",
    "href": "slides/03-generalities-bias-variance-slides.html#no-free-lunch-in-statistics",
    "title": "G√©n√©ralit√©s",
    "section": "‚ÄúNo Free Lunch in Statistics‚Äù",
    "text": "‚ÄúNo Free Lunch in Statistics‚Äù\n\n\n\n\nIl n‚Äôexiste pas de m√©thode universellement optimale !\n\n\nUne m√©thode performante dans un contexte peut √©chouer ailleurs.\n‚Üí Toujours adapter l‚Äôapproche au probl√®me."
  },
  {
    "objectID": "slides/03-generalities-bias-variance-slides.html#erreur-de-pr√©diction",
    "href": "slides/03-generalities-bias-variance-slides.html#erreur-de-pr√©diction",
    "title": "G√©n√©ralit√©s",
    "section": "Erreur de pr√©diction",
    "text": "Erreur de pr√©diction\n\nNotre vrai objectif : Minimiser l‚Äôerreur sur de nouvelles donn√©es\n\\[\\mathbb{E}[(Y - \\hat{Y})^2] = \\mathbb{E}[(Y - \\hat{f}(X))^2]\\]\n\n\nCette erreur se d√©compose en 3 parties‚Ä¶"
  },
  {
    "objectID": "slides/03-generalities-bias-variance-slides.html#d√©composition-biaisvariance",
    "href": "slides/03-generalities-bias-variance-slides.html#d√©composition-biaisvariance",
    "title": "G√©n√©ralit√©s",
    "section": "D√©composition biais/variance",
    "text": "D√©composition biais/variance\n\n\n\nD√©composition biais/variance\n\n\n\\[\\mathbb{E}[(Y - \\hat{f}(X))^2] = \\text{Biais}(\\hat{f}(X))^2 + \\text{Var}(\\hat{f}(X)) + \\sigma^2\\]\n\n\n\nTrois composantes :\n\nBiais\\(^2\\) ‚Üí Erreur syst√©matique d‚Äôapproximation.\nVariance ‚Üí Sensibilit√© aux fluctuations d‚Äô√©chantillon.\n\\(\\sigma^2\\) ‚Üí Erreur irr√©ductible (bruit intrins√®que)."
  },
  {
    "objectID": "slides/03-generalities-bias-variance-slides.html#d√©composition-biaisvariance-2",
    "href": "slides/03-generalities-bias-variance-slides.html#d√©composition-biaisvariance-2",
    "title": "G√©n√©ralit√©s",
    "section": "D√©composition biais/variance",
    "text": "D√©composition biais/variance\n\nMod√®le peu flexible (ex: r√©gression lin√©aire)\n\n‚úÖ Variance faible\n‚ùå Biais √©lev√©\n\nMod√®le tr√®s flexible (ex: r√©gression avec beaucoup de polyn√¥mes)\n\n‚ùå Variance √©lev√©e\n‚úÖ Biais faible\n\n\n‚Üí Besoin d‚Äôun √©quilibre optimal !"
  },
  {
    "objectID": "slides/03-generalities-bias-variance-slides.html#d√©composition-biaisvariance-3",
    "href": "slides/03-generalities-bias-variance-slides.html#d√©composition-biaisvariance-3",
    "title": "G√©n√©ralit√©s",
    "section": "D√©composition biais/variance",
    "text": "D√©composition biais/variance\n\n\nPar Bigbossfarin - Own work, CC0, Link."
  },
  {
    "objectID": "slides/03-generalities-bias-variance-slides.html#strat√©gies-doptimisation",
    "href": "slides/03-generalities-bias-variance-slides.html#strat√©gies-doptimisation",
    "title": "G√©n√©ralit√©s",
    "section": "Strat√©gies d‚Äôoptimisation",
    "text": "Strat√©gies d‚Äôoptimisation\n\nComment trouver l‚Äô√©quilibre ?\n\nValidation crois√©e ‚Üí Estimation de l‚Äôerreur de pr√©diction.\nS√©lection de mod√®le ‚Üí Comparer diff√©rentes des mod√®les avec diff√©rentes flexibilit√©s.\nM√©thodes d‚Äôensembles ‚Üí Combiner plusieurs mod√®les.\nR√©gularisation ‚Üí Contr√¥ler la complexit√© du mod√®le."
  },
  {
    "objectID": "slides/03-generalities-bias-variance-slides.html#implications-pour-la-pratique",
    "href": "slides/03-generalities-bias-variance-slides.html#implications-pour-la-pratique",
    "title": "G√©n√©ralit√©s",
    "section": "Implications pour la pratique",
    "text": "Implications pour la pratique\n\nQuestions √† se poser :\n\nAi-je assez de donn√©es pour un mod√®le flexible ?\nMon objectif est-il la pr√©diction ou la compr√©hension ?\nQuelle est la complexit√© r√©elle du ph√©nom√®ne √©tudi√© ?\nComment √©valuer la performance sur nouvelles donn√©es ?"
  },
  {
    "objectID": "slides/03-generalities-bias-variance-slides.html#conclusion",
    "href": "slides/03-generalities-bias-variance-slides.html#conclusion",
    "title": "G√©n√©ralit√©s",
    "section": "Conclusion",
    "text": "Conclusion\n\nErreur totale = Biais\\(^2\\) + Variance + Bruit irr√©ductible.\nCompromis entre biais et variance.\nMod√®les rigides ‚Üí Fort biais, faible variance.\nMod√®les flexibles ‚Üí Faible biais, forte variance.\nOptimum ‚Üí √âquilibre minimisant l‚Äôerreur totale.\nPas de solution universelle ‚Üí Il faut s‚Äôadapter au contexte.\n\n\nProchaine √©tape ‚Üí Validation et s√©lection de mod√®les."
  },
  {
    "objectID": "slides/01-introduction-slides.html#qui-suis-je",
    "href": "slides/01-introduction-slides.html#qui-suis-je",
    "title": "STT-2200: Analyse de donn√©es",
    "section": "Qui suis-je ?",
    "text": "Qui suis-je ?\n\n\nDipl√¥me d‚Äôing√©nieur en statistique + Maitrise en analyse de donn√©es massives √† l‚ÄôENSAI, France\nDoctorat en maths appliqu√©s, statistiques √† l‚ÄôENSAI, France\nPostdoc en statistique √† University of Limerick, Ireland\nActuellement, professeur adjoint √† ULaval\nRecherche en analyse de donn√©es fonctionnelles"
  },
  {
    "objectID": "slides/01-introduction-slides.html#qui-√™tes-vous",
    "href": "slides/01-introduction-slides.html#qui-√™tes-vous",
    "title": "STT-2200: Analyse de donn√©es",
    "section": "Qui √™tes-vous ?",
    "text": "Qui √™tes-vous ?\n\n\n\n\n\nAllez sur wooclap.com\nEntrez le code KFFPDJ"
  },
  {
    "objectID": "slides/01-introduction-slides.html#quest-ce-que-lanalyse-de-donn√©es",
    "href": "slides/01-introduction-slides.html#quest-ce-que-lanalyse-de-donn√©es",
    "title": "STT-2200: Analyse de donn√©es",
    "section": "Qu‚Äôest ce que l‚Äôanalyse de donn√©es ?",
    "text": "Qu‚Äôest ce que l‚Äôanalyse de donn√©es ?\n\n\n\n\n\nAllez sur wooclap.com\nEntrez le code KFFPDJ"
  },
  {
    "objectID": "slides/01-introduction-slides.html#quest-ce-que-lanalyse-de-donn√©es-1",
    "href": "slides/01-introduction-slides.html#quest-ce-que-lanalyse-de-donn√©es-1",
    "title": "STT-2200: Analyse de donn√©es",
    "section": "Qu‚Äôest ce que l‚Äôanalyse de donn√©es ?",
    "text": "Qu‚Äôest ce que l‚Äôanalyse de donn√©es ?\n\nL‚Äôanalyse de donn√©es est un ensemble de m√©thodes permettant de retirer de l‚Äôinformation d‚Äôun jeu de donn√©es. On parle aussi d‚Äôapprentissage statistique (statistical learning).\n\n\n\n\nExemple\n\n\nVous travaillez pour l‚ÄôONU. Votre mission est d‚Äôavoir de l‚Äôinformation sur l‚Äôesp√©rance de vie √† travers le monde. Pour cela, vous disposez de diverse donn√©es sur les diff√©rents pays du monde."
  },
  {
    "objectID": "slides/01-introduction-slides.html#ce-quon-ne-fait-pas-dans-ce-cours",
    "href": "slides/01-introduction-slides.html#ce-quon-ne-fait-pas-dans-ce-cours",
    "title": "STT-2200: Analyse de donn√©es",
    "section": "Ce qu‚Äôon ne fait PAS dans ce cours",
    "text": "Ce qu‚Äôon ne fait PAS dans ce cours\n\nmais qui pourrait √™tre inclus dans un cours d‚Äôanalyse de donn√©es.\n\nVisualisation de donn√©es\nD√©veloppement algorithmique\nDeep learning"
  },
  {
    "objectID": "slides/01-introduction-slides.html#ce-quon-fait-dans-ce-cours",
    "href": "slides/01-introduction-slides.html#ce-quon-fait-dans-ce-cours",
    "title": "STT-2200: Analyse de donn√©es",
    "section": "Ce qu‚Äôon fait dans ce cours",
    "text": "Ce qu‚Äôon fait dans ce cours\n\n\nCompr√©hension des concepts d‚Äôanalyse de donn√©es\nD√©veloppement math√©matique des m√©thodes classiques\nPractique de l‚Äôanalyse de donn√©es\nUtilisation des librairies classiques"
  },
  {
    "objectID": "slides/01-introduction-slides.html#√†-quoi-√ßa-sert",
    "href": "slides/01-introduction-slides.html#√†-quoi-√ßa-sert",
    "title": "STT-2200: Analyse de donn√©es",
    "section": "√Ä quoi √ßa sert ?",
    "text": "√Ä quoi √ßa sert ?\n\nLes m√©thodes enseign√©es servent √†\n\nr√©duire la dimension des donn√©es\nclassifier des individus automatiquement\nformer des groupes d‚Äôindividus"
  },
  {
    "objectID": "slides/01-introduction-slides.html#contenu-du-cours",
    "href": "slides/01-introduction-slides.html#contenu-du-cours",
    "title": "STT-2200: Analyse de donn√©es",
    "section": "Contenu du cours",
    "text": "Contenu du cours\n\n\nR√©visions / Rappels\nG√©n√©ralit√© sur l‚Äôanalyse de donn√©es\nR√©duction de la dimension\nClassication supervis√©e\nClassification non-supervis√©e\n√âthique"
  },
  {
    "objectID": "slides/01-introduction-slides.html#format-du-cours",
    "href": "slides/01-introduction-slides.html#format-du-cours",
    "title": "STT-2200: Analyse de donn√©es",
    "section": "Format du cours",
    "text": "Format du cours\n\n\nMardi : Laboratoire 1h (mais la salle est r√©serv√©e pour 2h)\nVendredi : Cours magistral 2h\nLes labs du mardi porteront sur le cours vu le vendredi. Vous pouvez ammener votre propre laptop pour les labs, cependant les ordinateurs de l‚Äôuniversit√© seront utilisable.\nLes labs consisteront en des exercices (th√©oriques et pratiques) √† faire en autonomie et je serais l√† pour r√©pondre √† vos questions."
  },
  {
    "objectID": "slides/01-introduction-slides.html#√©valuations",
    "href": "slides/01-introduction-slides.html#√©valuations",
    "title": "STT-2200: Analyse de donn√©es",
    "section": "√âvaluations",
    "text": "√âvaluations\n\n\nIntra le 24 octobre \\(\\longrightarrow\\) QCM (30%)\n\nReprise le 30 octobre\n\nExamen final le 12 d√©cembre (50%)\n\nReprise le 18 d√©cembre\n\nProjet en √©quipe √† rendre le 2 d√©cembre √† 17h00 (20%)\nPr√©sentation du projet le 9 d√©cembre au matin."
  },
  {
    "objectID": "slides/01-introduction-slides.html#logiciels",
    "href": "slides/01-introduction-slides.html#logiciels",
    "title": "STT-2200: Analyse de donn√©es",
    "section": "Logiciels",
    "text": "Logiciels\n\nLibre √† vous de choisir votre logiciel pr√©f√©r√© (ou celui que vous maitrisez le mieux)\n\n\n\n\n\nAllez sur wooclap.com\nEntrez le code KFFPDJ"
  },
  {
    "objectID": "slides/01-introduction-slides.html#communications",
    "href": "slides/01-introduction-slides.html#communications",
    "title": "STT-2200: Analyse de donn√©es",
    "section": "Communications",
    "text": "Communications\n\n\nSite du cours MonPortail\nNotes de cours : stt-2200.netlify.app\nUne question : forum ou courriel\nMon bureau : VCH-2209"
  },
  {
    "objectID": "slides/01-introduction-slides.html#mon-r√¥le",
    "href": "slides/01-introduction-slides.html#mon-r√¥le",
    "title": "STT-2200: Analyse de donn√©es",
    "section": "Mon r√¥le",
    "text": "Mon r√¥le\n\n\nMon but est de vous aider dans votre apprentissage.\nJ‚Äôessaie de vous poser des questions, de vous pr√©senter des exemples et de vous aider √† utiliser les ressources disponibles pour vous assurer d‚Äôavoir bien compris.\nMon objectif est de vous donner les bases et les comp√©tences n√©cessaires pour continuer √† apprendre sur ces sujets de mani√®re ind√©pendante, tout en ayant du recul sur l‚Äôutilisation des diff√©rentes m√©thodes."
  },
  {
    "objectID": "slides/01-introduction-slides.html#politique-sur-la-s√©curit√©-dans-les-laboratoires-informatiques",
    "href": "slides/01-introduction-slides.html#politique-sur-la-s√©curit√©-dans-les-laboratoires-informatiques",
    "title": "STT-2200: Analyse de donn√©es",
    "section": "Politique sur la s√©curit√© dans les laboratoires informatiques",
    "text": "Politique sur la s√©curit√© dans les laboratoires informatiques\n\nPour utiliser les laboratoires informatiques, il faut avoir pris connaissance de la Politique de s√©curit√© et vous engagez √† vous y conformer :\n\nwww.fsg.ulaval.ca/securite-labo-info"
  },
  {
    "objectID": "slides/01-introduction-slides.html#pour-finir-lintroduction",
    "href": "slides/01-introduction-slides.html#pour-finir-lintroduction",
    "title": "STT-2200: Analyse de donn√©es",
    "section": "Pour finir l‚Äôintroduction",
    "text": "Pour finir l‚Äôintroduction\n\n\nStatistics courses [‚Ä¶] tend to resemble horoscopes.\n\nR. McElreath - Statistical Rethinking"
  },
  {
    "objectID": "slides/03-generalities-model-evaluation-slides.html#introduction",
    "href": "slides/03-generalities-model-evaluation-slides.html#introduction",
    "title": "G√©n√©ralit√©s",
    "section": "Introduction",
    "text": "Introduction\n\nComment mesurer la performance pr√©dictive r√©elle de nos mod√®les ?\nM√©triques classiques :\n\nErreur quadratique moyenne (EQM / MSE) ‚Üí variables quantitatives.\nTaux d‚Äôerreur (ER) ‚Üí variables qualitatives.\n\n‚Üí Cependant, √©valuer uniquement sur les donn√©es d‚Äôentra√Ænement pose des probl√®mes fondamentaux."
  },
  {
    "objectID": "slides/03-generalities-model-evaluation-slides.html#plan",
    "href": "slides/03-generalities-model-evaluation-slides.html#plan",
    "title": "G√©n√©ralit√©s",
    "section": "Plan",
    "text": "Plan\n\n\nLe sur-ajustement\nLe sous-ajustement\nApproche par jeu de validation\nValidation crois√©e"
  },
  {
    "objectID": "slides/03-generalities-model-evaluation-slides.html#le-probl√®me-du-sur-ajustement",
    "href": "slides/03-generalities-model-evaluation-slides.html#le-probl√®me-du-sur-ajustement",
    "title": "G√©n√©ralit√©s",
    "section": "Le probl√®me du sur-ajustement",
    "text": "Le probl√®me du sur-ajustement\n\nNotre mod√®le s‚Äôadapte trop aux donn√©es d‚Äôentra√Ænement, y compris au bruit al√©atoire.\n\nDonc:\n\nErreur faible sur les donn√©es d‚Äôentra√Ænement.\nErreur √©lev√©e sur de nouvelles observations.\nMauvaise capacit√© de g√©n√©ralisation.\n\n\n\nPourquoi: L‚Äôestimateur \\(\\hat{f}\\) a √©t√© ajust√© pour minimiser l‚Äôerreur sur ces m√™mes donn√©es."
  },
  {
    "objectID": "slides/03-generalities-model-evaluation-slides.html#le-probl√®me-du-sous-ajustement",
    "href": "slides/03-generalities-model-evaluation-slides.html#le-probl√®me-du-sous-ajustement",
    "title": "G√©n√©ralit√©s",
    "section": "Le probl√®me du sous-ajustement",
    "text": "Le probl√®me du sous-ajustement\n\nNotre mod√®le ne s‚Äôadapte pas aux donn√©es d‚Äôentra√Ænement.\n\nDonc:\n\nErreur √©lev√©e sur les donn√©es d‚Äôentra√Ænement.\nErreur √©lev√©e sur de nouvelles observations.\nNe prend pas en compte les donn√©es.\n\n\n\nPourquoi: L‚Äôestimateur \\(\\hat{f}\\) a √©t√© impl√©ment√© sans prendre en compte les donn√©es."
  },
  {
    "objectID": "slides/03-generalities-model-evaluation-slides.html#sur-ajustement-vs-sous-ajustement",
    "href": "slides/03-generalities-model-evaluation-slides.html#sur-ajustement-vs-sous-ajustement",
    "title": "G√©n√©ralit√©s",
    "section": "Sur-ajustement vs Sous-ajustement",
    "text": "Sur-ajustement vs Sous-ajustement\n\nSur-ajustement (overfitting):\n\nMod√®le trop flexible.\nCapture le bruit plut√¥t que le signal.\n\nSous-ajustement (underfitting):\n\nMod√®le trop rigide.\nNe capture pas la structure des donn√©es.\n\nObjectif ‚Üí Trouver le bon compromis entre les deux."
  },
  {
    "objectID": "slides/03-generalities-model-evaluation-slides.html#approche-id√©ale",
    "href": "slides/03-generalities-model-evaluation-slides.html#approche-id√©ale",
    "title": "G√©n√©ralit√©s",
    "section": "Approche id√©ale",
    "text": "Approche id√©ale\n\nS√©parer compl√®tement les donn√©es d‚Äôentra√Ænement et d‚Äô√©valuation.\n\nJeu d‚Äôentra√Ænement : Utilis√© pour ajuster le mod√®le.\nJeu de test : Utilis√© pour √©valuer la performance pr√©dictive.\n\n\nProbl√®me pratique : Nous n‚Äôavons g√©n√©ralement pas acc√®s √† un jeu de test ind√©pendant.\n\n\nSolution ‚Üí Utiliser des m√©thodes de validation."
  },
  {
    "objectID": "slides/03-generalities-model-evaluation-slides.html#approche-par-jeu-de-validation",
    "href": "slides/03-generalities-model-evaluation-slides.html#approche-par-jeu-de-validation",
    "title": "G√©n√©ralit√©s",
    "section": "Approche par jeu de validation",
    "text": "Approche par jeu de validation\n\n\nApproche par jeu de validation."
  },
  {
    "objectID": "slides/03-generalities-model-evaluation-slides.html#limites-du-jeu-de-validation",
    "href": "slides/03-generalities-model-evaluation-slides.html#limites-du-jeu-de-validation",
    "title": "G√©n√©ralit√©s",
    "section": "Limites du jeu de validation",
    "text": "Limites du jeu de validation\n\nEstimation instable : L‚Äôerreur d√©pend fortement des observations dans le jeu de validation.\n\nPerte d‚Äôinformation : Moins de donn√©es pour ajuster le mod√®le, ce qui peut surestimer l‚Äôerreur r√©elle.\n\n\nVariabilit√© : Diff√©rents jeux de validation peuvent donner des r√©sultats tr√®s diff√©rents.\n\n\nSolution ‚Üí Validation crois√©e pour une approche plus robuste."
  },
  {
    "objectID": "slides/03-generalities-model-evaluation-slides.html#approche-par-validation-crois√©e",
    "href": "slides/03-generalities-model-evaluation-slides.html#approche-par-validation-crois√©e",
    "title": "G√©n√©ralit√©s",
    "section": "Approche par validation crois√©e",
    "text": "Approche par validation crois√©e\n\nApproche par validation crois√©e."
  },
  {
    "objectID": "slides/03-generalities-model-evaluation-slides.html#choix-du-param√®tre-k",
    "href": "slides/03-generalities-model-evaluation-slides.html#choix-du-param√®tre-k",
    "title": "G√©n√©ralit√©s",
    "section": "Choix du param√®tre K",
    "text": "Choix du param√®tre K\n\nR√®gle du pouce: \\(K = 5\\) ou \\(K = 10\\).\nCompromis √† consid√©rer :\n\nCo√ªt computationnel : Plus \\(K\\) est grand, plus c‚Äôest co√ªteux en temps de calcul.\nPr√©cision de l‚Äôestimation : Plus \\(K\\) est grand, moins de biais.\nStabilit√© : Valeurs mod√©r√©es de \\(K\\) r√©duisent la variance\n\n\nCas limite: \\(K = n\\) (Leave-One-Out Cross-Validation - LOOCV)."
  },
  {
    "objectID": "slides/03-generalities-model-evaluation-slides.html#conclusion",
    "href": "slides/03-generalities-model-evaluation-slides.html#conclusion",
    "title": "G√©n√©ralit√©s",
    "section": "Conclusion",
    "text": "Conclusion\n\nLa validation appropri√©e des mod√®les est essentielle pour √©viter une illusion de performance.\nLe sur-ajustement est un pi√®ge courant qui n√©cessite une √©valuation rigoureuse.\nLa validation crois√©e offre une solution robuste et pratique pour estimer la performance r√©elle.\nLe choix des param√®tres de validation doit √©quilibrer pr√©cision, stabilit√© et co√ªt computationnel.\n\n\nProchaine √©tape ‚Üí R√©duction de dimension."
  },
  {
    "objectID": "slides/03-generalities-space-slides.html#pourquoi-sint√©resser-√†-lespace-math√©matique",
    "href": "slides/03-generalities-space-slides.html#pourquoi-sint√©resser-√†-lespace-math√©matique",
    "title": "G√©n√©ralit√©s",
    "section": "Pourquoi s‚Äôint√©resser √† l‚Äôespace math√©matique ?",
    "text": "Pourquoi s‚Äôint√©resser √† l‚Äôespace math√©matique ?\n\nLe type de variables d√©termine :\n\nL‚Äôespace math√©matique dans lequel elle vit.\nLes mesures de distance qu‚Äôon peut utiliser.\nLes mod√®les pertinents √† utiliser.\n\n\n‚Üí Il est donc fondamental de conna√Ætre l‚Äôespace math√©matique avant toute mod√©lisation !"
  },
  {
    "objectID": "slides/03-generalities-space-slides.html#plan",
    "href": "slides/03-generalities-space-slides.html#plan",
    "title": "G√©n√©ralit√©s",
    "section": "Plan",
    "text": "Plan\n\n\nUnit√© statistique - Le choix du niveau d‚Äôanalyse.\nTypes de variables - 4 cat√©gories principales.\nEspaces associ√©s - Le cadre math√©matique \\(\\mathcal{X}\\)."
  },
  {
    "objectID": "slides/03-generalities-space-slides.html#unit√©-statistique",
    "href": "slides/03-generalities-space-slides.html#unit√©-statistique",
    "title": "G√©n√©ralit√©s",
    "section": "Unit√© statistique",
    "text": "Unit√© statistique\n\nD√©finition : √âl√©ment de base d√©finissant une observation.\n\nC‚Äôest un choix du mod√©lisateur qui d√©termine le niveau d‚Äôagr√©gation de l‚Äôanalyse.\n\n\nExemples :\n\nEnqu√™te sur les revenus\n√âtude d‚Äôun lyc√©e\nImagerie m√©dicale"
  },
  {
    "objectID": "slides/03-generalities-space-slides.html#attention-niveaux-danalyse-multiples",
    "href": "slides/03-generalities-space-slides.html#attention-niveaux-danalyse-multiples",
    "title": "G√©n√©ralit√©s",
    "section": "Attention : Niveaux d‚Äôanalyse multiples",
    "text": "Attention : Niveaux d‚Äôanalyse multiples\n\n\n\n\nExemple : Image m√©dicale\n\n\nUne image = collection de pixels\nChoix 1 :\nPixel comme unit√© ‚Üí Variables : RVB, opacit√© par pixel.\nChoix 2 :\nImage comme unit√© ‚Üí Variables : caract√©ristiques globales."
  },
  {
    "objectID": "slides/03-generalities-space-slides.html#types-de-variables-vue-densemble",
    "href": "slides/03-generalities-space-slides.html#types-de-variables-vue-densemble",
    "title": "G√©n√©ralit√©s",
    "section": "Types de variables : Vue d‚Äôensemble",
    "text": "Types de variables : Vue d‚Äôensemble\n\n4 types principaux pour les scalaires :\n\nNum√©rique (quantitative)\nOrdinale\nNominale sym√©trique\nNominale asym√©trique"
  },
  {
    "objectID": "slides/03-generalities-space-slides.html#variable-num√©rique",
    "href": "slides/03-generalities-space-slides.html#variable-num√©rique",
    "title": "G√©n√©ralit√©s",
    "section": "1. Variable num√©rique",
    "text": "1. Variable num√©rique\n\n\n\nD√©finition\n\n\nUne variable num√©rique repr√©sente une quantit√© mesurable.\nExemples :\n\nRevenu en dollars\nMasse corporelle\nTemp√©rature"
  },
  {
    "objectID": "slides/03-generalities-space-slides.html#variable-ordinale",
    "href": "slides/03-generalities-space-slides.html#variable-ordinale",
    "title": "G√©n√©ralit√©s",
    "section": "2. Variable ordinale",
    "text": "2. Variable ordinale\n\n\n\nD√©finition\n\n\nUne variable qualitative dont les modalit√©s peuvent √™tre ordonn√©es naturellement, sans que l‚Äô√©cart ne soit quantifiable.\nExemples :\n\nNiveau de revenu : faible &lt; moyen &lt; √©lev√©\nSatisfaction : pas du tout &lt; peu &lt; moyennement &lt; tr√®s &lt; tout √† fait"
  },
  {
    "objectID": "slides/03-generalities-space-slides.html#variable-nominale-sym√©trique",
    "href": "slides/03-generalities-space-slides.html#variable-nominale-sym√©trique",
    "title": "G√©n√©ralit√©s",
    "section": "3. Variable nominale sym√©trique",
    "text": "3. Variable nominale sym√©trique\n\n\n\nD√©finition\n\n\nUne variable qualitative o√π toutes les modalit√©s sont aussi informatives les unes que les autres.\nExemples :\n\nNationalit√© : fran√ßaise, canadienne, allemande‚Ä¶\nFili√®re : math√©matiques, physique, biologie‚Ä¶\nCouleur : rouge, vert, bleu‚Ä¶"
  },
  {
    "objectID": "slides/03-generalities-space-slides.html#variable-nominale-asym√©trique",
    "href": "slides/03-generalities-space-slides.html#variable-nominale-asym√©trique",
    "title": "G√©n√©ralit√©s",
    "section": "4. Variable nominale asym√©trique",
    "text": "4. Variable nominale asym√©trique\n\n\n\nD√©finition\n\n\nUne variable qualitative o√π une modalit√© a un statut particulier (souvent ‚Äúpar d√©faut‚Äù ou plus fr√©quente).\nExemples :\n\nPr√©sence/absence de sympt√¥me\nTransaction frauduleuse/normale\nD√©faut/bon fonctionnement"
  },
  {
    "objectID": "slides/03-generalities-space-slides.html#au-del√†-des-variables-classiques",
    "href": "slides/03-generalities-space-slides.html#au-del√†-des-variables-classiques",
    "title": "G√©n√©ralit√©s",
    "section": "Au-del√† des variables classiques",
    "text": "Au-del√† des variables classiques\n\nCourbes et signaux\nTextes et s√©quences de caract√®res\nImages et vid√©os\nR√©seaux et graphes\nDonn√©es g√©ospatiales\n\n‚Üí Le choix de repr√©sentation d√©pend de l‚Äôunit√© statistique."
  },
  {
    "objectID": "slides/03-generalities-space-slides.html#espaces-dobservation-mathcalx",
    "href": "slides/03-generalities-space-slides.html#espaces-dobservation-mathcalx",
    "title": "G√©n√©ralit√©s",
    "section": "Espaces d‚Äôobservation \\(\\mathcal{X}\\)",
    "text": "Espaces d‚Äôobservation \\(\\mathcal{X}\\)\n\n\n\nD√©finition\n\n\nEspace math√©matique dans lequel nos variables prennent leurs valeurs.\n\nCadre formel de travail.\nGuide les choix m√©thodologiques.\n\n\n\n\nNotation : \\(\\mathcal{X}\\) = espace d‚Äôobservation"
  },
  {
    "objectID": "slides/03-generalities-space-slides.html#variable-num√©rique-mathbbr",
    "href": "slides/03-generalities-space-slides.html#variable-num√©rique-mathbbr",
    "title": "G√©n√©ralit√©s",
    "section": "Variable num√©rique ‚Üí \\(\\mathbb{R}\\)",
    "text": "Variable num√©rique ‚Üí \\(\\mathbb{R}\\)\n\nCas g√©n√©ral : \\(\\mathcal{X} = \\mathbb{R}\\)\n\nCas restreints :\n\nTaille d‚Äôune personne : \\(\\mathcal{X} = [0, +\\infty)\\)\nProbabilit√© : \\(\\mathcal{X} = [0, 1]\\)\nPourcentage : \\(\\mathcal{X} = [0, 100]\\)"
  },
  {
    "objectID": "slides/03-generalities-space-slides.html#variable-nominale-ensemble-fini",
    "href": "slides/03-generalities-space-slides.html#variable-nominale-ensemble-fini",
    "title": "G√©n√©ralit√©s",
    "section": "Variable nominale ‚Üí Ensemble fini",
    "text": "Variable nominale ‚Üí Ensemble fini\n\nExemple : Lancer de d√©\n\\[\\mathcal{X} = \\{1, 2, 3, 4, 5, 6\\}\\]\nExemple : Fili√®re d‚Äô√©tude\n\\[\\mathcal{X} = \\{\\text{Math}, \\text{Physique}, \\text{Bio}, \\text{Info}\\}\\]"
  },
  {
    "objectID": "slides/03-generalities-space-slides.html#espaces-plus-complexes",
    "href": "slides/03-generalities-space-slides.html#espaces-plus-complexes",
    "title": "G√©n√©ralit√©s",
    "section": "Espaces plus complexes",
    "text": "Espaces plus complexes\n\nAnalyse de courbes : fonctions continues sur \\([a,b]\\)\n\\[\\mathcal{X} = \\mathcal{C}([a,b])\\]\nAnalyse de texte : alphabet\n\\[\\mathcal{X} = \\{A, B, C, \\ldots, Z\\}\\]"
  },
  {
    "objectID": "slides/03-generalities-space-slides.html#variables-multiples-produit-cart√©sien",
    "href": "slides/03-generalities-space-slides.html#variables-multiples-produit-cart√©sien",
    "title": "G√©n√©ralit√©s",
    "section": "Variables multiples : Produit cart√©sien",
    "text": "Variables multiples : Produit cart√©sien\n\nObservation simultan√©e : taille + poids + sexe\n\\[\\mathcal{X} = \\mathcal{X}_1 \\times \\mathcal{X}_2 \\times \\mathcal{X}_3\\]\nCas particulier : \\(p\\) variables num√©riques \\[\\mathcal{X} = \\mathbb{R}^p\\]"
  },
  {
    "objectID": "slides/03-generalities-space-slides.html#exemple-concret-√©tude-d√©tudiants",
    "href": "slides/03-generalities-space-slides.html#exemple-concret-√©tude-d√©tudiants",
    "title": "G√©n√©ralit√©s",
    "section": "Exemple concret : √©tude d‚Äô√©tudiants",
    "text": "Exemple concret : √©tude d‚Äô√©tudiants\n\n\nAge (num√©rique) : \\(\\mathcal{X}_1 = [16, 30]\\).\nFili√®re (nominale sym√©trique) : \\(\\mathcal{X}_2 = \\{\\text{Math}, \\text{Physique}\\}\\).\nSatisfaction (ordinale) : \\(\\mathcal{X}_3 = \\{\\text{Faible}, \\text{Moyen}, \\text{√âlev√©}\\}\\).\n\n\n\nEspace d‚Äôobservation :\n\n\\[\\mathcal{X} = [16, 30] \\times \\{\\text{Math}, \\text{Physique}\\} \\times \\{\\text{Faible}, \\text{Moyen}, \\text{√âlev√©}\\}\\]"
  },
  {
    "objectID": "slides/03-generalities-space-slides.html#r√©capitulatif",
    "href": "slides/03-generalities-space-slides.html#r√©capitulatif",
    "title": "G√©n√©ralit√©s",
    "section": "R√©capitulatif",
    "text": "R√©capitulatif\n\n\n\n\n\nType de variable\nExemple\nEspace typique\n\n\n\n\nNum√©rique\n√Çge, revenu\n\\(\\mathbb{R}\\), \\([0,+\\infty)\\)\n\n\nOrdinale\nSatisfaction\nEnsemble ordonn√© fini\n\n\nNominale sym√©trique\nNationalit√©\nEnsemble fini\n\n\nNominale asym√©trique\nPr√©sence/absence\n\\(\\{0, 1\\}\\)"
  },
  {
    "objectID": "slides/03-generalities-space-slides.html#conclusion",
    "href": "slides/03-generalities-space-slides.html#conclusion",
    "title": "G√©n√©ralit√©s",
    "section": "Conclusion",
    "text": "Conclusion\n\n\nL‚Äôunit√© statistique d√©finit le niveau d‚Äôanalyse.\nLe type de variable guide le choix de l‚Äôespace.\nL‚Äôespace d‚Äôobservation \\(\\mathcal{X}\\) est le cadre math√©matique.\nVariables multiples ‚Üí produit cart√©sien d‚Äôensemble.\n\n\n‚Üí Prochaine √©tape : D√©finir des distances dans ces espaces !"
  },
  {
    "objectID": "slides/02-revisions-algebre-slides-annotated.html#plan-de-la-pr√©sentation",
    "href": "slides/02-revisions-algebre-slides-annotated.html#plan-de-la-pr√©sentation",
    "title": "R√©visions",
    "section": "Plan de la pr√©sentation",
    "text": "Plan de la pr√©sentation\n\n\nPropri√©t√©s matricielles\nValeurs et vecteurs propres\nDiagonalisation de matrices"
  },
  {
    "objectID": "slides/02-revisions-algebre-slides-annotated.html#notations-de-base",
    "href": "slides/02-revisions-algebre-slides-annotated.html#notations-de-base",
    "title": "R√©visions",
    "section": "Notations de base",
    "text": "Notations de base\n\n\n\\(M_{n, m}(\\mathbb{R})\\) : matrices √† \\(n\\) lignes et \\(m\\) colonnes\n\\(M_{n}(\\mathbb{R})\\) : matrices carr√©es de taille \\(n\\)\n\\(I_n\\) : matrice identit√© de taille \\(n\\)\n\\(u, v \\in \\mathbb{R}^n\\) : vecteurs colonnes"
  },
  {
    "objectID": "slides/02-revisions-algebre-slides-annotated.html#propri√©t√©s-du-d√©terminant",
    "href": "slides/02-revisions-algebre-slides-annotated.html#propri√©t√©s-du-d√©terminant",
    "title": "R√©visions",
    "section": "Propri√©t√©s du d√©terminant",
    "text": "Propri√©t√©s du d√©terminant\n\n\n\n\nPropri√©t√©s\n\n\n\n\\(\\det(A^\\top) = \\det(A)\\)\n\\(\\det(AB) = \\det(A)\\det(B)\\)\n\\(\\det(A^{-1}) = \\frac{1}{\\det(A)}\\)"
  },
  {
    "objectID": "slides/02-revisions-algebre-slides-annotated.html#propri√©t√©s-du-d√©terminant-1",
    "href": "slides/02-revisions-algebre-slides-annotated.html#propri√©t√©s-du-d√©terminant-1",
    "title": "R√©visions",
    "section": "Propri√©t√©s du d√©terminant",
    "text": "Propri√©t√©s du d√©terminant\n\nCas particulier o√π \\(A \\in M_{2}(\\mathbb{R})\\).\nSoit \\(A = \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}\\). Alors\n\\[\\det(A) = ad - bc.\\]"
  },
  {
    "objectID": "slides/02-revisions-algebre-slides-annotated.html#propri√©t√©-du-d√©terminant-exemple-wooclap-kffpdj",
    "href": "slides/02-revisions-algebre-slides-annotated.html#propri√©t√©-du-d√©terminant-exemple-wooclap-kffpdj",
    "title": "R√©visions",
    "section": "Propri√©t√© du d√©terminant : exemple (Wooclap: KFFPDJ)",
    "text": "Propri√©t√© du d√©terminant : exemple (Wooclap: KFFPDJ)\n\n\\[A = \\begin{pmatrix} 1 & 2 \\\\ 3 & 4 \\end{pmatrix}, \\quad B = \\begin{pmatrix} 2 & 0 \\\\ 1 & 1 \\end{pmatrix}\\]\n\nOn trouve que \\(\\det(A) = 4 - 6 = -2\\), \\(\\det(B) = 2 - 0 = 2\\). Donc\n\\[\\det(AB) = \\det(A) \\det(B) = -4\\]"
  },
  {
    "objectID": "slides/02-revisions-algebre-slides-annotated.html#inverse-de-matrices",
    "href": "slides/02-revisions-algebre-slides-annotated.html#inverse-de-matrices",
    "title": "R√©visions",
    "section": "Inverse de matrices",
    "text": "Inverse de matrices\n\n\n\n\nPropri√©t√©\n\n\nSi \\(A\\) et \\(B\\) sont inversibles, alors : \\((AB)^{-1} = B^{-1} A^{-1}\\)."
  },
  {
    "objectID": "slides/02-revisions-algebre-slides-annotated.html#inverse-de-matrices-1",
    "href": "slides/02-revisions-algebre-slides-annotated.html#inverse-de-matrices-1",
    "title": "R√©visions",
    "section": "Inverse de matrices",
    "text": "Inverse de matrices\n\nCas particulier o√π \\(A \\in M_{2}(\\mathbb{R})\\).\nSoit \\(A = \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}\\). Alors\n\\[A^{-1} = \\frac{1}{ad - bc}\\begin{pmatrix} d & -b \\\\ -c & a \\end{pmatrix}.\\]"
  },
  {
    "objectID": "slides/02-revisions-algebre-slides-annotated.html#inverse-de-matrices-exemple-wooclap-kffpdj",
    "href": "slides/02-revisions-algebre-slides-annotated.html#inverse-de-matrices-exemple-wooclap-kffpdj",
    "title": "R√©visions",
    "section": "Inverse de matrices : exemple (Wooclap: KFFPDJ)",
    "text": "Inverse de matrices : exemple (Wooclap: KFFPDJ)\n\n\\[A = \\begin{pmatrix} 2 & 0 \\\\ 0 & 3 \\end{pmatrix}, \\quad B = \\begin{pmatrix} 1 & 1 \\\\ 0 & 1 \\end{pmatrix}\\]\n\nOn a : \\(AB = \\begin{pmatrix} 2 & 2 \\\\ 0 & 3 \\end{pmatrix},\\) donc \\((AB)^{-1} = \\begin{pmatrix} 1/2 & -1/3 \\\\ 0 & 1/3 \\end{pmatrix}\\).\nEt effectivement : \\(B^{-1}A^{-1} = \\begin{pmatrix} 1 & -1 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1/2 & 0 \\\\ 0 & 1/3 \\end{pmatrix} = \\begin{pmatrix} 1/2 & -1/3 \\\\ 0 & 1/3 \\end{pmatrix}\\)."
  },
  {
    "objectID": "slides/02-revisions-algebre-slides-annotated.html#propri√©t√©s-de-la-trace",
    "href": "slides/02-revisions-algebre-slides-annotated.html#propri√©t√©s-de-la-trace",
    "title": "R√©visions",
    "section": "Propri√©t√©s de la trace",
    "text": "Propri√©t√©s de la trace\n\nLa trace de la matrice \\(A \\in M_{n}(\\mathbb{R})\\) est la somme des √©l√©ments diagonaux \\[\\text{tr}(A) = \\sum_{i=1}^{n} a_{ii}.\\]"
  },
  {
    "objectID": "slides/02-revisions-algebre-slides-annotated.html#propri√©t√©s-de-la-trace-1",
    "href": "slides/02-revisions-algebre-slides-annotated.html#propri√©t√©s-de-la-trace-1",
    "title": "R√©visions",
    "section": "Propri√©t√©s de la trace",
    "text": "Propri√©t√©s de la trace\n\n\n\n\nPropri√©t√©s\n\n\n\n\\(\\text{tr}(A) = \\text{tr}(A^{\\top})\\)\n\\(\\text{tr}(A + B) = \\text{tr}(A) + \\text{tr}(B)\\)\n\\(\\text{tr}(MN^{\\top}) = \\text{tr}(N^{\\top}M)\\)"
  },
  {
    "objectID": "slides/02-revisions-algebre-slides-annotated.html#propri√©t√©s-de-la-trace-exemple-wooclap-kffpdj",
    "href": "slides/02-revisions-algebre-slides-annotated.html#propri√©t√©s-de-la-trace-exemple-wooclap-kffpdj",
    "title": "R√©visions",
    "section": "Propri√©t√©s de la trace : exemple (Wooclap: KFFPDJ)",
    "text": "Propri√©t√©s de la trace : exemple (Wooclap: KFFPDJ)\n\nSoit \\(X \\in M_{n, p}(\\mathbb{R})\\) avec \\(p &lt; n\\) et de rang \\(p\\), et donc telle que \\(X^\\top X\\) est de dimension \\(p \\times p\\) et inversible. Quelle est la valeur de la trace de \\(X (X^\\top X)^{-1} X^\\top\\) ?\n\n\\[\\begin{align*}\n\\text{tr}(X (X^\\top X)^{-1} X^\\top) &= \\text{tr}(X^\\top X (X^\\top X)^{-1})\\\\\n&= \\text{tr}(I_p) \\\\\n&= p.\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/02-revisions-algebre-slides-annotated.html#matrices-sp√©ciales",
    "href": "slides/02-revisions-algebre-slides-annotated.html#matrices-sp√©ciales",
    "title": "R√©visions",
    "section": "Matrices sp√©ciales",
    "text": "Matrices sp√©ciales\n\n\n\nD√©finitions\n\n\nMatrice d√©finie positive : \\(A\\) est d√©finie positive si \\[\\text{pour tout } u \\neq 0,\\quad u^\\top A u &gt; 0.\\]\nMatrice orthogonale : \\(A\\) est orthogonale si \\[A^\\top A = A A^\\top = I_n.\\]"
  },
  {
    "objectID": "slides/02-revisions-algebre-slides-annotated.html#comment-montrer-quune-matrice-est-d√©finie-positive",
    "href": "slides/02-revisions-algebre-slides-annotated.html#comment-montrer-quune-matrice-est-d√©finie-positive",
    "title": "R√©visions",
    "section": "Comment montrer qu‚Äôune matrice est d√©finie positive ?",
    "text": "Comment montrer qu‚Äôune matrice est d√©finie positive ?\n\nSoit \\(A = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix}\\) et pour tout \\(u = \\begin{pmatrix} x \\\\ y \\end{pmatrix} \\neq 0\\), on a : \\[\\begin{align*}\nu^\\top A u &= \\begin{pmatrix} x & y \\end{pmatrix} \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix} \\\\\n  &= 2x^2 + 2xy + 2y^2 \\\\\n  &= x^2 + y^2 + (x+y)^2 &gt; 0.\n\\end{align*}\\]\nDonc \\(A\\) est d√©finie positive."
  },
  {
    "objectID": "slides/02-revisions-algebre-slides-annotated.html#valeurs-et-vecteurs-propres-1",
    "href": "slides/02-revisions-algebre-slides-annotated.html#valeurs-et-vecteurs-propres-1",
    "title": "R√©visions",
    "section": "Valeurs et vecteurs propres",
    "text": "Valeurs et vecteurs propres\n\nFaire un dessin pour Au = lu.\n\n\n\n\nD√©finitions\n\n\nSoit \\(A \\in M_{n}(\\mathbb{R})\\). Le scalaire \\(\\lambda \\in \\mathbb{R}\\) est une valeur propre de \\(A\\) s‚Äôil existe \\(u \\neq 0\\) tel que : \\(Au = \\lambda u\\).\n\\(u\\) est le vecteur propre correspondant √† \\(\\lambda\\).\nLe spectre de \\(A\\), not√© \\(\\text{sp}(A)\\), est l‚Äôensemble des valeurs propres."
  },
  {
    "objectID": "slides/02-revisions-algebre-slides-annotated.html#valeurs-et-vecteurs-propres-exemple",
    "href": "slides/02-revisions-algebre-slides-annotated.html#valeurs-et-vecteurs-propres-exemple",
    "title": "R√©visions",
    "section": "Valeurs et vecteurs propres : exemple",
    "text": "Valeurs et vecteurs propres : exemple\n\\[A = \\begin{pmatrix} 3 & 1 \\\\ 0 & 2 \\end{pmatrix}\\]\n\nCalcul des valeurs propres : \\[\\det(A - \\lambda I) = \\det\\begin{pmatrix} 3-\\lambda & 1 \\\\ 0 & 2-\\lambda \\end{pmatrix} = (3-\\lambda)(2-\\lambda)\\]\nDonc \\(\\lambda_1 = 3\\) et \\(\\lambda_2 = 2\\)."
  },
  {
    "objectID": "slides/02-revisions-algebre-slides-annotated.html#valeurs-et-vecteurs-propres-exemple-1",
    "href": "slides/02-revisions-algebre-slides-annotated.html#valeurs-et-vecteurs-propres-exemple-1",
    "title": "R√©visions",
    "section": "Valeurs et vecteurs propres : exemple",
    "text": "Valeurs et vecteurs propres : exemple\n\\[A = \\begin{pmatrix} 3 & 1 \\\\ 0 & 2 \\end{pmatrix}\\]\n\nCalcul des vecteurs propres :\n\nPour \\(\\lambda_1 = 3\\) : \\(u_1 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\\).\nPour \\(\\lambda_2 = 2\\) : \\(u_2 = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}\\)."
  },
  {
    "objectID": "slides/02-revisions-algebre-slides-annotated.html#propri√©t√©s-des-vecteurs-propres",
    "href": "slides/02-revisions-algebre-slides-annotated.html#propri√©t√©s-des-vecteurs-propres",
    "title": "R√©visions",
    "section": "Propri√©t√©s des vecteurs propres",
    "text": "Propri√©t√©s des vecteurs propres\n\n\n\nPropri√©t√©s\n\n\n\nSi \\(u\\) est vecteur propre pour \\(\\lambda\\), alors \\(cu\\) l‚Äôest aussi (pour \\(c \\neq 0\\)).\nPour matrices sym√©triques : Si \\(u_1\\) et \\(u_2\\) sont deux vecteurs propres correspondent √† des valeurs propres diff√©rentes, alors : \\[u_1^\\top u_2 = 0 \\text{ (orthogonalit√©)}.\\]"
  },
  {
    "objectID": "slides/02-revisions-algebre-slides-annotated.html#preuve-de-lorthogonalit√©",
    "href": "slides/02-revisions-algebre-slides-annotated.html#preuve-de-lorthogonalit√©",
    "title": "R√©visions",
    "section": "Preuve de l‚Äôorthogonalit√©",
    "text": "Preuve de l‚Äôorthogonalit√©\nSoient \\(\\lambda_1 \\neq \\lambda_2\\) et \\(Au_1 = \\lambda_1 u_1\\), \\(Au_2 = \\lambda_2 u_2\\).\n\nOn a \\[\\lambda_1 u_1^\\top u_2 = u_1^\\top A u_2 = \\lambda_2 u_1^\\top u_2.\\]\n\n\nDonc \\[(\\lambda_1 - \\lambda_2)u_1^\\top u_2 = 0\\]\n\n\nComme \\(\\lambda_1 \\neq \\lambda_2\\), on a n√©cessairement \\(u_1^\\top u_2 = 0\\)\n‚Üí Cette propri√©t√© est cruciale pour l‚ÄôACP !"
  },
  {
    "objectID": "slides/02-revisions-algebre-slides-annotated.html#caract√©risation-des-valeurs-propres",
    "href": "slides/02-revisions-algebre-slides-annotated.html#caract√©risation-des-valeurs-propres",
    "title": "R√©visions",
    "section": "Caract√©risation des valeurs propres",
    "text": "Caract√©risation des valeurs propres\n\n\n\n\nPropri√©t√©s\n\n\n\nMatrice sym√©trique ‚Üí toutes les valeurs propres sont r√©elles.\nMatrice d√©finie positive ‚Üí toutes les valeurs propres sont strictement positives."
  },
  {
    "objectID": "slides/02-revisions-algebre-slides-annotated.html#caract√©risation-des-valeurs-propres-exemple",
    "href": "slides/02-revisions-algebre-slides-annotated.html#caract√©risation-des-valeurs-propres-exemple",
    "title": "R√©visions",
    "section": "Caract√©risation des valeurs propres : exemple",
    "text": "Caract√©risation des valeurs propres : exemple\n\nSoit \\(A = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix} \\text{ (sym√©trique et d√©finie positive)}\\).\n\nValeurs propres : \\(\\lambda_1 = 3, \\lambda_2 = 1\\) (toutes r√©elles et &gt; 0).\nVecteurs propres : \\(u_1 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}, u_2 = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}\\).\nV√©rifions l‚Äôorthogonalit√© : \\(u_1^\\top u_2 = 1 \\times 1 + 1 \\times (-1) = 0\\)."
  },
  {
    "objectID": "slides/02-revisions-algebre-slides-annotated.html#diagonalisation-de-matrices",
    "href": "slides/02-revisions-algebre-slides-annotated.html#diagonalisation-de-matrices",
    "title": "R√©visions",
    "section": "Diagonalisation de matrices",
    "text": "Diagonalisation de matrices\n\n\n\n\nD√©finition\n\n\nUne matrice \\(A\\) est diagonalisable s‚Äôil existe \\(P\\) inversible et \\(D\\) diagonale telles que : \\[A = P D P^{-1} \\Leftrightarrow P^{-1} A P = D.\\]\n\n\n\n\nInterpr√©tation : On peut ‚Äúsimplifier‚Äù \\(A\\) en changeant de base."
  },
  {
    "objectID": "slides/02-revisions-algebre-slides-annotated.html#th√©or√®me-de-d√©composition-spectrale",
    "href": "slides/02-revisions-algebre-slides-annotated.html#th√©or√®me-de-d√©composition-spectrale",
    "title": "R√©visions",
    "section": "Th√©or√®me de d√©composition spectrale",
    "text": "Th√©or√®me de d√©composition spectrale\n\n\n\n\nTh√©or√®me\n\n\nPour toute matrice sym√©trique \\(A\\), il existe une matrice orthogonale \\(P\\) telle que : \\(A = P \\Lambda P^\\top\\) o√π \\(\\Lambda = \\text{diag}(\\lambda_1, \\ldots, \\lambda_n)\\).\n\n\n\nLa matrice \\(P\\) est form√©e des vecteurs propres de \\(A\\) et \\(\\Lambda\\) est la matrice diagonale des valeurs propres associ√©es."
  },
  {
    "objectID": "slides/02-revisions-algebre-slides-annotated.html#exemple",
    "href": "slides/02-revisions-algebre-slides-annotated.html#exemple",
    "title": "R√©visions",
    "section": "Exemple",
    "text": "Exemple\nEn reprenant l‚Äôexample pr√©c√©dent.\nSoit \\(A = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix}\\). On a \\(P = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 & 1 \\\\ 1 & -1 \\end{pmatrix}\\) et \\(\\Lambda = \\begin{pmatrix} 3 & 0 \\\\ 0 & 1 \\end{pmatrix}.\\)\nOn peut v√©rifier le th√©or√®me de d√©composition spectrale :\n\\[P \\Lambda P^\\top = \\frac{1}{2}\\begin{pmatrix} 1 & 1 \\\\ 1 & -1 \\end{pmatrix}\\begin{pmatrix} 3 & 0 \\\\ 0 & 1 \\end{pmatrix}\\begin{pmatrix} 1 & 1 \\\\ 1 & -1 \\end{pmatrix} = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix} = A.\\]"
  },
  {
    "objectID": "slides/02-revisions-algebre-slides-annotated.html#lien-avec-le-d√©terminant-et-la-trace",
    "href": "slides/02-revisions-algebre-slides-annotated.html#lien-avec-le-d√©terminant-et-la-trace",
    "title": "R√©visions",
    "section": "Lien avec le d√©terminant et la trace",
    "text": "Lien avec le d√©terminant et la trace\n\n\n\n\nPropri√©t√©s\n\n\nSi la matrice \\(A\\) a pour valeurs propres \\(\\lambda_1, \\ldots, \\lambda_n\\) :\n\n\\(\\det(A) = \\prod_{i=1}^{n} \\lambda_i\\).\n\\(\\text{tr}(A) = \\sum_{i=1}^{n} \\lambda_i\\)."
  },
  {
    "objectID": "slides/02-revisions-algebre-slides-annotated.html#lien-avec-le-d√©terminant-et-la-trace-exemple",
    "href": "slides/02-revisions-algebre-slides-annotated.html#lien-avec-le-d√©terminant-et-la-trace-exemple",
    "title": "R√©visions",
    "section": "Lien avec le d√©terminant et la trace : exemple",
    "text": "Lien avec le d√©terminant et la trace : exemple\n\nSoit \\(A = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix}\\). On a vu que ses valeurs propres sont \\(\\lambda_1 = 3\\) et \\(\\lambda_2 = 1\\).\nOn peut donc v√©rifier les formules pour le d√©terminant et la trace :\n\\[\\det(A) = 4 - 1 = 3 = 3 \\times 1 = \\lambda_1 \\lambda_2,\\]\n\\[\\text{tr}(A) = 2 + 2 = 4 = 3 + 1 = \\lambda_1 + \\lambda_2.\\]"
  },
  {
    "objectID": "slides/02-revisions-programming-slides.html#philosophie-du-cours",
    "href": "slides/02-revisions-programming-slides.html#philosophie-du-cours",
    "title": "R√©visions",
    "section": "Philosophie du cours",
    "text": "Philosophie du cours\n\n\n\nLibert√© de choix\n\n\nCe cours n‚Äôimpose aucun langage sp√©cifique !\nVous √™tes libres de choisir :\n\nR, Python, Julia, ou m√™me SAS ou C++.\nLe langage qui vous convient le mieux.\n\n\n\n\n\nObjectif commun : Code lisible, compr√©hensible et r√©utilisable."
  },
  {
    "objectID": "slides/02-revisions-programming-slides.html#plan-de-la-pr√©sentation",
    "href": "slides/02-revisions-programming-slides.html#plan-de-la-pr√©sentation",
    "title": "R√©visions",
    "section": "Plan de la pr√©sentation",
    "text": "Plan de la pr√©sentation\n\n\nBonnes pratiques universelles\nComment choisir ?"
  },
  {
    "objectID": "slides/02-revisions-programming-slides.html#principes-fondamentaux",
    "href": "slides/02-revisions-programming-slides.html#principes-fondamentaux",
    "title": "R√©visions",
    "section": "Principes fondamentaux",
    "text": "Principes fondamentaux\n\n\nCode lisible : les autres (et vous dans 6 mois !) doivent comprendre\nDocumentation : commentaires clairs et concis\nStructure : organisation logique des fichiers et fonctions\nReproductibilit√© : m√™mes donn√©es ‚Üí m√™mes r√©sultats\nGestion des versions : Git pour suivre les modifications"
  },
  {
    "objectID": "slides/02-revisions-programming-slides.html#exemple-code-mal-√©crit-vs.-bien-√©crit",
    "href": "slides/02-revisions-programming-slides.html#exemple-code-mal-√©crit-vs.-bien-√©crit",
    "title": "R√©visions",
    "section": "Exemple : Code mal √©crit vs.¬†bien √©crit",
    "text": "Exemple : Code mal √©crit vs.¬†bien √©crit\n\n‚ùå Code difficile √† comprendre :\nimport pandas as pd\nd=pd.read_csv('data.csv')\nr=d.groupby('A').mean()['B']\nprint(r)\n\n‚úÖ Code clair et document√© :\nimport pandas as pd\n\n# Chargement des donn√©es de ventes\nsales_data = pd.read_csv('monthly_sales.csv')\n\n# Calcul de la moyenne des ventes par r√©gion\naverage_sales_by_region = sales_data.groupby('region').mean()['sales_amount']\n\nprint(f\"Ventes moyennes par r√©gion : {average_sales_by_region}\")"
  },
  {
    "objectID": "slides/02-revisions-programming-slides.html#structure-de-projet-recommand√©e",
    "href": "slides/02-revisions-programming-slides.html#structure-de-projet-recommand√©e",
    "title": "R√©visions",
    "section": "Structure de projet recommand√©e",
    "text": "Structure de projet recommand√©e\n\nmon_projet/\n‚îú‚îÄ‚îÄ data/           # Donn√©es brutes et nettoy√©es\n‚îú‚îÄ‚îÄ scripts/        # Scripts d'analyse\n‚îú‚îÄ‚îÄ functions/      # Fonctions r√©utilisables\n‚îú‚îÄ‚îÄ results/        # Graphiques, tableaux, documents\n‚îú‚îÄ‚îÄ docs/           # Documentation\n‚îî‚îÄ‚îÄ README.md       # Description du projet\n\nAvantages :\n\nOrganisation claire\nFacilite la collaboration\nProjet reproductible"
  },
  {
    "objectID": "slides/02-revisions-programming-slides.html#crit√®res-de-d√©cision",
    "href": "slides/02-revisions-programming-slides.html#crit√®res-de-d√©cision",
    "title": "R√©visions",
    "section": "Crit√®res de d√©cision",
    "text": "Crit√®res de d√©cision\n\n\nVotre exp√©rience : Quel langage connaissez-vous d√©j√† ?\nType de projet : Statistiques pures vs.¬†ML vs.¬†applications web\nPerformance : Avez-vous besoin de calculs tr√®s rapides ?\n√âcosyst√®me : Quels packages/biblioth√®ques vous faut-il ?\n√âquipe : Quel langage utilise votre groupe/entreprise ?"
  },
  {
    "objectID": "slides/02-revisions-programming-slides.html#apprentissage-recommand√©",
    "href": "slides/02-revisions-programming-slides.html#apprentissage-recommand√©",
    "title": "R√©visions",
    "section": "Apprentissage recommand√©",
    "text": "Apprentissage recommand√©\n\n\n\n\n\nMa√Ætrisez un langage √† fond plut√¥t que plusieurs superficiellement\nApprenez les concepts (statistiques, ML) avant la syntaxe\nPratiquez r√©guli√®rement : projets personnels\nLisez du code des autres : GitHub, Stack Overflow\nSuivez les bonnes pratiques d√®s le d√©but"
  },
  {
    "objectID": "slides/02-revisions-programming-slides.html#ressources-dapprentissage",
    "href": "slides/02-revisions-programming-slides.html#ressources-dapprentissage",
    "title": "R√©visions",
    "section": "Ressources d‚Äôapprentissage",
    "text": "Ressources d‚Äôapprentissage\n\nPour tous les langages :\n\nLearn X in Y minutes (guides rapides)\nStack Overflow (questions/r√©ponses)\nDocumentation officielle"
  },
  {
    "objectID": "slides/02-revisions-programming-slides.html#limportant-cest",
    "href": "slides/02-revisions-programming-slides.html#limportant-cest",
    "title": "R√©visions",
    "section": "L‚Äôimportant c‚Äôest‚Ä¶",
    "text": "L‚Äôimportant c‚Äôest‚Ä¶\n\n\n\n\nComprendre les concepts math√©matiques et statistiques\n√âcrire du code lisible et document√©\nReproduire ses analyses\nContinuer d‚Äôapprendre et s‚Äôadapter\n\n\n\n\nLe langage de programmation n‚Äôest qu‚Äôun outil pour exprimer vos id√©es !"
  },
  {
    "objectID": "tp/02-revision-tp.html",
    "href": "tp/02-revision-tp.html",
    "title": "TP: R√©vision",
    "section": "",
    "text": "Vous pouvez faire les exercices dans le langage de votre choix."
  },
  {
    "objectID": "tp/02-revision-tp.html#exercice-1-estimer-pi",
    "href": "tp/02-revision-tp.html#exercice-1-estimer-pi",
    "title": "TP: R√©vision",
    "section": "Exercice 1: Estimer \\(\\pi\\)",
    "text": "Exercice 1: Estimer \\(\\pi\\)\nDans cet exercice, on se propose d‚Äôestimer \\(\\pi\\) gr√¢ce √† la m√©thode de Monte-Carlo. Le m√©thode de Monte-Carlo est une m√©thode algorithmique permettant d‚Äôestimer des quantit√©s en utilisant des tirages al√©atoires. Pour estimer \\(\\pi\\), l‚Äôid√©e est de g√©n√©rer des points dans un carr√© de fa√ßon uniforme et ensuite de compter la proportion de ces points qui sont dans le cercle unit√©.\n\nG√©n√©rer un nombre \\(n\\) de points \\((x, y)\\) dans un carr√© de longueur \\(2\\) centr√© √† l‚Äôorigine, i.e.¬†\\(x \\in [-1, 1]\\) et \\(y \\in [-1, 1]\\).\nPour chaque point \\(n\\), d√©terminer si le point appartient au cercle unit√©.\nCalculer la proportion du nombre de points dans le cercle unit√©.\n√Ä partir du r√©sultat pr√©c√©dent, estimer \\(\\pi\\).\nNotons \\(\\widehat{\\pi}(n)\\), l‚Äôestimateur de \\(\\pi\\) utilisant \\(n\\) points g√©n√©r√©s. Tracer l‚Äôerreur d‚Äôestimation \\(\\left| \\pi - \\widehat{\\pi}(n) \\right|\\) en fonction de \\(n\\)."
  },
  {
    "objectID": "tp/02-revision-tp.html#exercice-2-estimation-dint√©grale",
    "href": "tp/02-revision-tp.html#exercice-2-estimation-dint√©grale",
    "title": "TP: R√©vision",
    "section": "Exercice 2: Estimation d‚Äôint√©grale",
    "text": "Exercice 2: Estimation d‚Äôint√©grale\nDans cette exercice, on se propose d‚Äôestimer \\[I = \\int_{0}^{1} \\sqrt{1 - x^2} dx .\\]\n\nCalculer \\(I\\) en utilisant une primitive.\nIl est possible d‚Äôestimer \\(I = \\int_{0}^{1} f(x) dx\\) √† l‚Äôaide des sommes de Riemann \\(\\widehat{I}(n) = \\frac{1}{n}\\sum_{i = 1}^{n} f(\\frac{i}{n}), i = 1, \\dots, n\\). Estimer \\(I\\) en utilisant les sommes de Riemann.\nL‚Äôint√©grale \\(I\\) peut √™tre vu comme l‚Äôesp√©rance d‚Äôun variable al√©atoire. \\(I = \\mathbb{E}[f(U)]\\), o√π \\(U \\sim \\mathcal{U}(0, 1)\\). On peut estimer \\(I\\) en utilisant \\(\\widetilde{I}(n) = \\frac{1}{n} \\sum_{i = 1}^{n} f(u_i)\\), o√π les \\(u_i\\) sont des r√©alisations de la variable al√©atoire \\(U\\). Estimer \\(I\\) en utilisant cette m√©thode.\nComparer la qualit√© de ces deux estimateurs de \\(I\\) en fonction de \\(n\\)."
  },
  {
    "objectID": "tp/02-revision-tp.html#exercice-3-la-loi-des-gaz-parfaits",
    "href": "tp/02-revision-tp.html#exercice-3-la-loi-des-gaz-parfaits",
    "title": "TP: R√©vision",
    "section": "Exercice 3: La loi des gaz parfaits",
    "text": "Exercice 3: La loi des gaz parfaits\nDans cet exercice, on se propose de v√©rifier la loi des gaz parfaits √† partir des donn√©es. On rappelle que la loi des gaz parfaits est donn√©e par \\(PV = nRT\\) o√π :\n\n\\(P\\) est la pression √† l‚Äôint√©rieur du volume consid√©r√© en Pascal (Pa);\n\\(V\\) est le volume du gaz en m\\(^3\\);\n\\(n\\) est la quantit√© de mati√®re en mole (mol);\n\\(R = 8.314\\) est la constante universelle des gaz parfaits en J.mol\\(^{-1}\\).K\\(^{-1}\\);\n\\(T\\) est la temp√©rature √† l‚Äôint√©rieur du volume consid√©r√© en Kelvin (K).\n\nOn conduit une exp√©rience consistant √† chauffer une quantit√© fixe de gaz dans un r√©cipient ferm√© de volume fix√©. La temp√©rature \\(T\\) en Kelvin et la pression \\(P\\) en kPa sont enregistr√©es. On trouve les r√©sultats suivants :\n\ntemperature = [\n  406, 296, 272, 449, 483, 439, 460, 276, 321, 462, 408, 322, 285,\n  411, 491, 359, 453, 486, 413, 350, 263, 456, 390, 462, 389, 494,\n  303, 496, 336, 460\n]\n\npression = [\n  1365, 982, 898, 1486, 1596, 1481, 1506, 906, 1085, 1542, 1367,\n  1072, 955, 1379, 1633, 1186, 1499, 1606, 1378, 1156, 867, 1514,\n  1306, 1525, 1287, 1665, 1020, 1635, 1118, 1529\n]\n\n\nTracer la pression en fonction de la temp√©rature. Est-ce que le graphique est lin√©aire ?\nConstruire la matrice \\(X = (1 | \\text{temperature})\\). La premi√®re colonne de \\(X\\) est une colonne de \\(1\\) et la deuxi√®me colonne de \\(X\\) est le vecteur des temp√©ratures.\nCalculer le vecteur \\(\\beta = (X^{\\top} X)^{-1} X^{\\top} Y\\) o√π \\(Y\\) est le vecteur des pressions.\nQuelle est l‚Äôinterpr√©tation physique des √©l√©ments du vecteur \\(\\beta\\) ?\n√Ä quelle valeur de \\(\\beta_0\\), le premier coefficient de \\(\\beta\\), devrait-on s‚Äôattendre dans le cadre d‚Äôun gaz parfait ? Est-ce le cas ici ? Pourquoi ?\nSupposons que le volume de gaz est de \\(10\\)dm\\(^3\\). Estimer la quantit√© de mati√®re \\(n\\) en mole utilis√© pour avoir les donn√©es."
  },
  {
    "objectID": "tp/03-generalities-tp.html",
    "href": "tp/03-generalities-tp.html",
    "title": "TP: G√©n√©ralit√©s",
    "section": "",
    "text": "Vous pouvez faire les exercices dans le langage de votre choix."
  },
  {
    "objectID": "tp/03-generalities-tp.html#exercice-1-nettoyage-et-exploration-de-donn√©es",
    "href": "tp/03-generalities-tp.html#exercice-1-nettoyage-et-exploration-de-donn√©es",
    "title": "TP: G√©n√©ralit√©s",
    "section": "Exercice 1 : Nettoyage et exploration de donn√©es",
    "text": "Exercice 1 : Nettoyage et exploration de donn√©es\n\nT√©l√©charger le jeu de donn√©es suivant: lien\nNettoyer le jeu de donn√©es. En particulier, on s‚Äôint√©ressera aux points suivant :\n\n\nencodage des valeurs manquantes;\ngestion des valeurs extr√™mes;\ngestion des valeurs abberantes;\ngestion des doublons;\nharmonisation des dates;\nsuppression des tirets, points, ‚Ä¶ dans les num√©ros de t√©l√©phone.\ngestion des titres (Mr., Ms., Dr., ‚Ä¶) dans les noms;\nharmonisation des pays;\nconversion des Oui/Non en TRUE/FALSE;\netc.\n\n\nFaire une exploration unidimensionelle/bidimensionelle. En particulier, on pourra faire :\n\n\ngraphiques des donn√©es quantitatives;\ncalcul de corr√©lations;\ntableaux de fr√©quences pour les variables qualitatives;\netc."
  },
  {
    "objectID": "tp/03-generalities-tp.html#exercice-2-compromis-biais-variance",
    "href": "tp/03-generalities-tp.html#exercice-2-compromis-biais-variance",
    "title": "TP: G√©n√©ralit√©s",
    "section": "Exercice 2 : Compromis biais-variance",
    "text": "Exercice 2 : Compromis biais-variance\nDans cet exercice, on se propose d‚Äôillustrer le compromis biais-variance √† l‚Äôaide de donn√©es simul√©es et d‚Äôun mod√®le de r√©gression. On fait l‚Äôhypoth√®se que le vrai mod√®le de nos donn√©es est \\[f(x) = 3 + 8 x + 2 x^2.\\]\n\nSimuler un ensemble de donn√©es \\((X, Y)\\) tel que \\(Y = f(X) + \\epsilon\\), avec \\(X \\sim \\mathcal{N}(0, 1)\\), \\(\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)\\) et \\(\\sigma^2 = 5\\).\nAjuster un mod√®le lin√©aire de la forme \\(Y = \\beta_{0} + \\beta_{1} X\\). En R, la fonction standard est lm. En Python, vous pouvez utiliser la fonction ols du package statsmodels.\nCalculer \\(\\widehat{Y}\\) en utilisant \\(X\\) g√©n√©r√© √† la question 1.\nCalculer l‚Äôerreur quadratique moyenne sur le jeu d‚Äôentra√Ænement.\nSimuler un jeu de donn√©es de validation et calculer l‚Äôerreur quadratique moyenne, le biais et la variance du mod√®le sur ce jeu de validation.\nRefaire les questions 2 √† 5, mais avec en ajustant un mod√®le lin√©aire avec un polyn√¥me d‚Äôordre 2.\nRefaire les questions 2 √† 5, mais avec en ajustant un mod√®le lin√©aire avec un polyn√¥me d‚Äôordre 10.\nConclure quant au compromis biais-variance et la complexit√© du mod√®le."
  },
  {
    "objectID": "tp/03-generalities-tp.html#exercice-3-validation-crois√©e",
    "href": "tp/03-generalities-tp.html#exercice-3-validation-crois√©e",
    "title": "TP: G√©n√©ralit√©s",
    "section": "Exercice 3 : Validation crois√©e",
    "text": "Exercice 3 : Validation crois√©e\nEn pratique, nous ne connaissons pas la vraie fonction \\(f(x)\\). Pour √©valuer la qualit√© de notre mod√®le, on peut utiliser la validation crois√©e. Notez que le but de cet exercice est de faire votre propre code et non d‚Äôutiliser des fonctions d√©j√† faites.\n\nSimuler un jeu de donn√©es en utilisant le mod√®le de l‚Äôexercice 2.\nFaire une validation crois√©e avec \\(K = 3\\) pour √©valuer le mod√®le de r√©gression lin√©aire simple (sans polyn√¥me d‚Äôordre sup√©rieur) en utilisant l‚Äôerreur quadratique moyenne.\nFaire la m√™me chose avec le mod√®le de r√©gression lin√©aire avec un polyn√¥me d‚Äôordre 2.\nFaire la m√™me chose avec le mod√®le de r√©gression lin√©aire avec un polyn√¥me d‚Äôordre 10.\nConclure quant √† l‚Äôutilisation de la validation crois√©e."
  },
  {
    "objectID": "tp/04-dimension-tp.html",
    "href": "tp/04-dimension-tp.html",
    "title": "TP: Dimension",
    "section": "",
    "text": "Vous pouvez faire les exercices dans le langage de votre choix."
  },
  {
    "objectID": "td/03-generalities-td.html",
    "href": "td/03-generalities-td.html",
    "title": "TD: G√©n√©ralit√©s",
    "section": "",
    "text": "Dans cette exercice, on se propose de finir la preuve que la distance de Jaccard est bien une distance. On s‚Äôint√©resse en particulier √† la preuve de l‚Äôin√©galit√© triangulaire. Pour cela, l‚Äôid√©e est de r√©√©crire la distance de Jaccard comme une distance entre des ensembles.\nConsid√©rons deux observations \\(x\\) et \\(y\\) de \\(K\\) variables binaires. Notons \\(X = \\{ i \\in \\{ 1, \\dots, K \\} \\mid x_i = 1 \\}\\) et \\(Y = \\{ i \\in \\{ 1, \\dots, K \\} \\mid y_i = 1 \\}\\).\n\nMontrer les trois premi√®res propri√©t√©s des distances pour la distance de Jaccard.\nFaire un dessin montrant \\(M_{11}\\), \\(M_{10}\\) et \\(M_{01}\\) √† l‚Äôaide des ensembles \\(X\\) et \\(Y\\).\n√âcrire l‚Äôindice de Jaccard entre \\(x\\) et \\(y\\) √† l‚Äôaide des ensembles \\(X\\) et \\(Y\\). En d√©duire la distance de Jaccard en fonction des ensembles \\(X\\) et \\(Y\\). On pourra noter \\(X \\Delta Y = (X \\backslash Y) \\cup (Y \\backslash X)\\).\nSoit \\(A = \\{ 1, 2 \\}\\), \\(B = \\{ 2, 3 \\}\\) et \\(C = \\{ 3, 4 \\}\\). Calculer les distances de Jaccard entre \\(A\\) et \\(B\\), \\(B\\) et \\(C\\) et \\(A\\) et \\(C\\) en utilisant la d√©finition ensembliste de cette distance.\nSoit une troisi√®me observation \\(z\\), notons \\(Z = \\{ i \\in \\{ 1, \\dots, K \\} \\mid z_i = 1 \\}\\). R√©√©crire l‚Äôin√©galit√© triangulaire pour la distance de Jaccard en utilisant la d√©finition ensembliste.\nMontrer que \\(X \\Delta Y \\subseteq (X \\Delta Z) \\cup (Y \\Delta Z)\\).\nMontrer que \\(X \\cup Y \\subseteq (X \\cup Z) \\cup (Y \\cup Z)\\).\nConclure sur l‚Äôin√©galit√© triangulaire. On pourra supposer que, pour tous nombres positifs \\(a_{1}, a_{2}, b_{1}, b_{2}\\), on a \\(\\frac{a_{1} + a_{2}}{b_{1} + b_{2}} \\leq \\frac{a_{1}}{b_{1}} + \\frac{a_{2}}{b_{2}}\\)."
  },
  {
    "objectID": "td/03-generalities-td.html#exercice-1-preuve-de-la-distance-de-jaccard",
    "href": "td/03-generalities-td.html#exercice-1-preuve-de-la-distance-de-jaccard",
    "title": "TD: G√©n√©ralit√©s",
    "section": "",
    "text": "Dans cette exercice, on se propose de finir la preuve que la distance de Jaccard est bien une distance. On s‚Äôint√©resse en particulier √† la preuve de l‚Äôin√©galit√© triangulaire. Pour cela, l‚Äôid√©e est de r√©√©crire la distance de Jaccard comme une distance entre des ensembles.\nConsid√©rons deux observations \\(x\\) et \\(y\\) de \\(K\\) variables binaires. Notons \\(X = \\{ i \\in \\{ 1, \\dots, K \\} \\mid x_i = 1 \\}\\) et \\(Y = \\{ i \\in \\{ 1, \\dots, K \\} \\mid y_i = 1 \\}\\).\n\nMontrer les trois premi√®res propri√©t√©s des distances pour la distance de Jaccard.\nFaire un dessin montrant \\(M_{11}\\), \\(M_{10}\\) et \\(M_{01}\\) √† l‚Äôaide des ensembles \\(X\\) et \\(Y\\).\n√âcrire l‚Äôindice de Jaccard entre \\(x\\) et \\(y\\) √† l‚Äôaide des ensembles \\(X\\) et \\(Y\\). En d√©duire la distance de Jaccard en fonction des ensembles \\(X\\) et \\(Y\\). On pourra noter \\(X \\Delta Y = (X \\backslash Y) \\cup (Y \\backslash X)\\).\nSoit \\(A = \\{ 1, 2 \\}\\), \\(B = \\{ 2, 3 \\}\\) et \\(C = \\{ 3, 4 \\}\\). Calculer les distances de Jaccard entre \\(A\\) et \\(B\\), \\(B\\) et \\(C\\) et \\(A\\) et \\(C\\) en utilisant la d√©finition ensembliste de cette distance.\nSoit une troisi√®me observation \\(z\\), notons \\(Z = \\{ i \\in \\{ 1, \\dots, K \\} \\mid z_i = 1 \\}\\). R√©√©crire l‚Äôin√©galit√© triangulaire pour la distance de Jaccard en utilisant la d√©finition ensembliste.\nMontrer que \\(X \\Delta Y \\subseteq (X \\Delta Z) \\cup (Y \\Delta Z)\\).\nMontrer que \\(X \\cup Y \\subseteq (X \\cup Z) \\cup (Y \\cup Z)\\).\nConclure sur l‚Äôin√©galit√© triangulaire. On pourra supposer que, pour tous nombres positifs \\(a_{1}, a_{2}, b_{1}, b_{2}\\), on a \\(\\frac{a_{1} + a_{2}}{b_{1} + b_{2}} \\leq \\frac{a_{1}}{b_{1}} + \\frac{a_{2}}{b_{2}}\\)."
  },
  {
    "objectID": "td/03-generalities-td.html#exercice-2-diff√©rent-sc√©narios",
    "href": "td/03-generalities-td.html#exercice-2-diff√©rent-sc√©narios",
    "title": "TD: G√©n√©ralit√©s",
    "section": "Exercice 2 : Diff√©rent sc√©narios",
    "text": "Exercice 2 : Diff√©rent sc√©narios\nPour chacun des sc√©narios suivants, dire si c‚Äôest un probl√®me de classification ou de r√©gression et si c‚Äôest un probl√®me d‚Äôinf√©rence ou de pr√©diction, donner le nombre d‚Äôobservations et l‚Äôensemble math√©matique dans lequel ces observations vivent, donner une distance possible pour comparer les observations.\n\nOn voudrait savoir si une personne hospitalis√©e au Qu√©bec √† une chance de d√©velopper une complication respiratoire. Pour cela, on se rend dans 5 h√¥pitaux de la province et on demande √† 100 patients dans chacun des h√¥pitaux s‚Äôils ont eu une complication respiratoire, ainsi que leur √¢ge, leur IMC et leur statut vaccinal.\nOn cherche √† mod√©liser le prix de vente des maisons √† Qu√©bec en fonction de leur surface, du nombres de chambres et de la pr√©sence d‚Äôun garage. Il y a \\(2000\\) ventes dans la base de donn√©es.\nOn s‚Äôint√©resse √† la concentration quotidienne de particules fines (PM2.5) dans l‚Äôair √† Qu√©bec sur une ann√©e. De plus, on mesure la temp√©rature, le vent et l‚Äôhumidit√©.\nUn sondage est men√© aupr√®s de \\(2000\\) r√©sidents canadiens pour √©valuer leur niveau de satisfaction vis-√†-vis des services publics (sant√©, √©ducation, transport, etc.). On recueille les r√©ponses sur une √©chelle de 1 (tr√®s insatisfait) √† 5 (tr√®s satisfait), ainsi que des informations d√©mographiques comme l‚Äô√¢ge, la province de r√©sidence, le revenu et le niveau d‚Äô√©ducation.\nOn veut identifier l‚Äôesp√®ce d‚Äôun animal √† partir de mesures morphologiques (longeur des pattes, poids, longueur du museau) collect√©es dans les parcs nationaux. On observe des castors, des ours noirs, des cerfs et des wapitis. En tout, on observe \\(1000\\) animaux."
  },
  {
    "objectID": "td/03-generalities-td.html#exercice-3-une-mise-en-situation",
    "href": "td/03-generalities-td.html#exercice-3-une-mise-en-situation",
    "title": "TD: G√©n√©ralit√©s",
    "section": "Exercice 3 : Une mise en situation",
    "text": "Exercice 3 : Une mise en situation\nUne chercheuse en sciences politiques souhaite √©tudier les caract√©ristiques de certains √©lus canadiens. Elle a mont√© une base de donn√©es avec \\(151\\) √©lus. Pour chaque √©lus, la base de donn√©es contient des variables qualitatives (genre, langue, parti politique, ‚Ä¶) et ordinales (niveau de scolarit√©, tranche d‚Äô√¢ge au moment de l‚Äô√©lection, ‚Ä¶). Elle souhaite visualiser ses donn√©es pour mieux comprendre les caract√©ristiques des √©lus et les regrouper selon leurs caract√©ristiques.\nProposer un plan de travail : identifier toutes les √©tapes de l‚Äôanalyse (en commen√ßant par la d√©finition de l‚Äôobjectif) et associer un nombre d‚Äôheures approximatif √† chaque √©tape."
  },
  {
    "objectID": "td/03-generalities-td.html#exercice-4-calcul-de-distance",
    "href": "td/03-generalities-td.html#exercice-4-calcul-de-distance",
    "title": "TD: G√©n√©ralit√©s",
    "section": "Exercice 4 : Calcul de distance",
    "text": "Exercice 4 : Calcul de distance\n\nUn robot commence √† la position \\((0, 0)\\). Calculer la distance la plus courte que le robot doive parcourir pour aller √† la position \\((8, 6)\\).\nUn taxi New-Yorkais dois aller de l‚Äôangle de la 5th avenue et 42nd street (New York public library) √† l‚Äôangle de la 1st avenue et 114th street (Thomas Jefferson park). Calculer la distance que le taxi va faire.\nUne entreprise de transport utilise les coordonn√©es GPS pour calculer les distances entre son entrep√¥t et deux de ses clients. Les coordonn√©es de l‚Äôentrep√¥t sont \\((10, 15)\\), les coordonn√©es du premier client sont \\((18, 22)\\) et les coordonn√©es du deuxi√®me client sont \\((5, 8)\\). Quel est le client le plus proche de l‚Äôentrep√¥t √† vol d‚Äôoiseau. Est-ce le m√™me client qui est le plus proche de l‚Äôentrep√¥t pour un camion qui suivrait des routes, r√©parties en grille.\nConsid√©rons un syst√®me de communication qui envoie des messages encod√©s sur 4 bits. Ce syst√®me envoie les messages suivants: ‚Äú1001‚Äù et ‚Äú1101‚Äù. Quelle est la distance de Hamming entre ces deux messages ? Si un syst√®me de d√©tection d‚Äôerreurs peut d√©tecter des erreurs de 2 bits, est-ce que la diff√©rence entre les deux messages est d√©tectable ?\nTrois utilisateurs ont aim√© les films suivants: \\(\\{ \\text{Titanic}, \\text{Avatar}, \\text{Star Wars}, \\text{Matrix}, \\text{Inception} \\}\\), \\(\\{ \\text{Avatar}, \\text{Matrix}, \\text{Batman}, \\text{Superman}, \\text{Inception} \\}\\) et \\(\\{ \\text{Inception}, \\text{Avengers}, \\text{Spiderman}, \\text{Superman}, \\text{Batman} \\}\\). Quels sont les utilisateurs les plus proches bas√©s sur la distance de Hamming ?"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STT-2200: Analyse de donn√©es",
    "section": "",
    "text": "Bienvenue √† tous au cours STT-2200 de l‚Äôautomne 2025.\nVous trouverez sur ce site toute les informations n√©cessaire pour les cours, les diff√©rents travaux et les examens. Les contenus seront mis √† jour au cours de la session avec les corrig√©s de certains travaux. Je vous invite √† regarder les diff√©rents modules avant les cours.\nCe site a √©t√© cr√©√© en s‚Äôaidant des notes de cours de Aur√©lien Nicosia et de Thierry Duchesne.\nCe site a √©t√© cr√©√© avec Quarto. Des larges mod√®les de langages (LLM), ChatGPT et Claude, ont √©t√© utilis√©s pour am√©liorer la fluidit√© des notes, pour corriger les typos et pour la mise en forme des figures. Toutes erreurs restantes me sont strictement et directement imputable.\n\n\n\nData trap (xkcd:2582)."
  },
  {
    "objectID": "contents/dimension/02-ca.html",
    "href": "contents/dimension/02-ca.html",
    "title": "Analyse factorielle des correspondances",
    "section": "",
    "text": "L‚Äôanalyse factorielle des correspondances (AFC) est une m√©thode d‚Äôanalyse exploratoire qui vise √† repr√©senter graphiquement les relations entre les modalit√©s de deux variables qualitatives. Elle permet de repr√©senter simultan√©ment les profils-lignes (dans \\(\\mathbb{R}^p\\)) et les profils-colonnes (dans \\(\\mathbb{R}^n\\)) d‚Äôun tableau de contingences, dans un espace de faible dimension, tout en pr√©servant au mieux la distance du \\(\\chi^2\\). L‚Äôobjectif de l‚ÄôAFC est de trouver une repr√©sentation bidimensionnelle (voir tridimensionnelle) dans laquelle les proximit√©s g√©om√©triques entre points refl√®tent au mieux les similarit√©s entre les modalit√©s.",
    "crumbs": [
      "Modules",
      "04 - Dimension",
      "Analyse factorielle des correspondances"
    ]
  },
  {
    "objectID": "contents/dimension/02-ca.html#notation",
    "href": "contents/dimension/02-ca.html#notation",
    "title": "Analyse factorielle des correspondances",
    "section": "Notation",
    "text": "Notation\nOn consid√®re un tableau de contingence \\(K = (k_{ij})\\), o√π \\(k_{ij}\\) est le nombre d‚Äôindividus appartenant √† la classe \\(i \\in \\{ 1, \\dots, n \\}\\) et √† la cat√©gorie \\(j \\in \\{ 1, \\dots, p \\}\\). On travaille ensuite avec le tableau des fr√©quences relatives en normalisant ce tableau. Comme les fr√©quences sont proportionnelles √† la taille d‚Äô√©chantillon \\(n\\), le tableau des fr√©quences relatives contient plus d‚Äôinformation. Notons \\(F = (f_{ij})\\), dans lequel \\[f_{ij} = \\frac{k_{ij}}{k_{\\bullet\\bullet}} = \\frac{k_{ij}}{\\sum_{l = 1}^{n} \\sum_{m = 1}^{p} k_{lm}}.\\]\nLes marges lignes (resp. colonnes) du tableau correspondent √† la somme des colonnes pour chaque ligne (resp. √† la somme des lignes pour chaque colonne): \\[\\begin{align}\nf_{i \\bullet} &= \\sum_{j = 1}^{p} f_{ij} = \\frac{k_{i \\bullet}}{k_{\\bullet\\bullet}}, \\quad 1 \\leq i \\leq n; \\\\\nf_{\\bullet j} &= \\sum_{i = 1}^{n} f_{ij} = \\frac{k_{\\bullet j}}{k_{\\bullet\\bullet}}, \\quad 1 \\leq j \\leq p.\n\\end{align}\\]\nOn a \\(f_{\\bullet \\bullet} = \\sum_{i = 1}^{n} f_{i \\bullet} = \\sum_{j = 1}^{p} f_{\\bullet j} = 1\\).\n\n\n\n\n\n\nNoteExemple\n\n\n\nComme exemple, on va consid√©rer la majeure et le type d‚Äôadmission des √©tudiants inscrit au cours STT-2200 √† l‚Äôautomne 2025.\n\nTableau de contingence des √©tudiants inscrit pour le cours STT-2200 (Automne 2025) croisant leur majeure et leur type d‚Äôadmission.\n\n\n\nColl√®ge\nUniversit√© Laval\nAutre universit√©\nHors Qu√©bec\n\n\n\n\nActuariat\n2\n0\n0\n1\n\n\nStatistique\n2\n4\n1\n0\n\n\nBio-info\n4\n2\n0\n2\n\n\nFinance\n2\n0\n0\n0\n\n\nMaths\n1\n0\n0\n0\n\n\nInfo\n2\n1\n0\n1\n\n\n\nIci, on trouve \\(k_{\\bullet \\bullet} = 25\\). C‚Äôest tout simplement le nombre d‚Äô√©tudiants inscrit au cours. On trouve donc le tableau de fr√©quences suivant :\n\nTableau de fr√©quences associ√© au tableau de contingence pr√©c√©dent.\n\n\n\n\n\n\n\n\n\n\n\nColl√®ge\nUniversit√© Laval\nAutre universit√©\nHors Qu√©bec\n\\(f_{i \\bullet}\\)\n\n\n\n\nActuariat\n0.08\n0\n0\n0.04\n0.12\n\n\nStatistique\n0.08\n0.16\n0.04\n0\n0.28\n\n\nBio-info\n0.16\n0.08\n0\n0.08\n0.32\n\n\nFinance\n0.08\n0\n0\n0\n0.08\n\n\nMaths\n0.04\n0\n0\n0\n0.04\n\n\nInfo\n0.08\n0.04\n0\n0.04\n0.16\n\n\n\\(f_{\\bullet j}\\)\n0.52\n0.28\n0.04\n0.16\n1",
    "crumbs": [
      "Modules",
      "04 - Dimension",
      "Analyse factorielle des correspondances"
    ]
  },
  {
    "objectID": "contents/dimension/02-ca.html#ind√©pendance-statistique",
    "href": "contents/dimension/02-ca.html#ind√©pendance-statistique",
    "title": "Analyse factorielle des correspondances",
    "section": "Ind√©pendance statistique",
    "text": "Ind√©pendance statistique\nLe tableau des fr√©quences relatives \\(F = (f_{ij})\\) peut √™tre interpr√©t√© comme une estimation des probabilit√©s conjointes des modalit√©s des deux variables qualitatives. Si les deux variables sont statistiquement ind√©pendantes, on s‚Äôattend √† ce que la probabilit√© conjointe s‚Äôapproche du produit des probabilit√©s marginales : \\[f_{ij} \\approx f_{i \\bullet} f_{\\bullet j}, \\quad i \\in \\{ 1, \\dots, n \\},~ j \\in \\{ 1, \\dots, p \\}.\\]\nPour tester si les √©carts observ√©s entre \\(f_{ij}\\) et \\(f_{i \\bullet} f_{\\bullet j}\\) sont significatifs, on utilise le test du \\(\\chi^2\\) d‚Äôind√©pendance : \\[T = \\sum_{i = 1}^{n} \\sum_{j = 1}^{p} \\frac{\\left( k_{ij} - \\mathbb{E}(k_{ij}) \\right)^2}{\\mathbb{E}(k_{ij})} = \\sum_{i = 1}^{n} \\sum_{j = 1}^{p} \\frac{\\left( k_{ij} - \\frac{k_{i \\bullet}k_{\\bullet j}}{k_{\\bullet\\bullet}} \\right)^2}{\\left( \\frac{k_{i \\bullet} k_{\\bullet j}}{k_{\\bullet\\bullet}} \\right)}.\\] Sous l‚Äôhypoth√®se d‚Äôind√©pendance, cette statistique suit approximativement une loi du \\(\\chi^2\\). Si les variables sont ind√©pendantes, la statistique \\(T\\) doit √™tre proche de \\(0\\).",
    "crumbs": [
      "Modules",
      "04 - Dimension",
      "Analyse factorielle des correspondances"
    ]
  },
  {
    "objectID": "contents/dimension/02-ca.html#profils-lignes-et-profils-colonnes",
    "href": "contents/dimension/02-ca.html#profils-lignes-et-profils-colonnes",
    "title": "Analyse factorielle des correspondances",
    "section": "Profils-lignes et profils-colonnes",
    "text": "Profils-lignes et profils-colonnes\nPour analyser les structures dans le tableau de contingence, on introduit la notion de profil. Chaque ligne du tableau peut √™tre vue comme un profil-ligne \\[L_i = \\left( \\frac{k_{i 1}}{k_{i \\bullet}}, \\dots, \\frac{k_{i p}}{k_{i \\bullet}} \\right) = \\left( \\frac{f_{i 1}}{f_{i \\bullet}}, \\dots, \\frac{f_{i p}}{f_{i \\bullet}} \\right).\\] Le profil-ligne repr√©sente la r√©partition des modalit√©s \\(i\\) de la premi√®re variable parmi les modalit√©s de la seconde.\nDe m√™me, chaque colonne du tableau peut √™tre vue comme un profil-colonne \\[C_j = \\left( \\frac{k_{1 j}}{k_{\\bullet j}}, \\dots, \\frac{k_{n j}}{k_{\\bullet j}} \\right) = \\left( \\frac{f_{1 j}}{f_{\\bullet j}}, \\dots, \\frac{f_{n j}}{f_{\\bullet j}} \\right)\n.\\] Le profil-colonne repr√©sente la r√©partition des modalit√©s \\(j\\) de la deuxi√®me variable parmi les modalit√©s de la premi√®re.\nOn peut ensuite s‚Äôint√©resser au profil-ligne moyen (resp. profil-colonne moyen) obtenus comme la moyenne pond√©r√©e des profils-lignes (resp. profils-colonnes). Autrement dit, ils correspond aux fr√©quences marginales colonnes (resp. fr√©quences marginales lignes). Le profil-ligne moyen est donn√© par \\[\\left( \\sum_{i = 1}^{n} f_{i \\bullet} \\frac{f_{i 1}}{f_{i \\bullet}}, \\dots, \\sum_{i = 1}^{n} f_{i \\bullet} \\frac{f_{i p}}{f_{i \\bullet}} \\right) = \\left( f_{\\bullet 1}, \\dots, f_{\\bullet p} \\right),\\] et le profil-colonne moyen est donn√© par \\[\\left( \\sum_{j = 1}^{p} f_{{\\bullet j}} \\frac{f_{1 j}}{f_{\\bullet j}}, \\dots, \\sum_{j = 1}^{p} f_{\\bullet j} \\frac{f_{n j}}{f_{\\bullet j}} \\right) = \\left( f_{{1 \\bullet}}, \\dots, f_{n \\bullet} \\right).\\]\nSi les variables sont ind√©pendantes, tous les profiles sont √©gaux √† leur profils moyens respectifs. Autrement dit, pour tout \\(i \\in \\{ 1, \\dots, n \\}\\) et \\(j \\in \\{ 1, \\dots, p \\}\\), \\[\\left( \\frac{f_{i 1}}{f_{i \\bullet}}, \\dots, \\frac{f_{i p}}{f_{i \\bullet}} \\right) = \\left( f_{\\bullet 1}, \\dots, f_{\\bullet p} \\right) \\quad\\text{et}\\quad\n\\left( \\frac{f_{1 j}}{f_{\\bullet j}}, \\dots, \\frac{f_{n j}}{f_{\\bullet j}} \\right) = \\left( f_{{1 \\bullet}}, \\dots, f_{n \\bullet} \\right).\\] Ainsi, plus les profils s‚Äô√©loignent de leurs moyennes, plus les variables montrent une d√©pendance.\nPour mesurer la diff√©rence entre deux profils-lignes, on utilise la distance du \\(\\chi^2\\) pond√©r√©e par les fr√©quences marginales : \\[d^2(L_i, L_{i^\\prime}) = \\sum_{j = 1}^{p} \\frac{1}{f_{\\bullet j}} \\left( \\frac{f_{ij}}{f_{i \\bullet}} - \\frac{f_{i^\\prime j}}{f_{i^\\prime \\bullet}} \\right)^2.\\] On peut faire de m√™me pour la diff√©rence entre deux profils-colonnes : \\[d^2(C_j, C_{j^\\prime}) = \\sum_{i = 1}^{n} \\frac{1}{f_{i \\bullet}} \\left( \\frac{f_{ij}}{f_{\\bullet j}} - \\frac{f_{i j^\\prime}}{f_{\\bullet j^\\prime}} \\right)^2.\\]\nOn peut √©crire cela sous forme matricielle. Notons \\(D_n = \\text{diag}(f_{i \\bullet})\\) la matrice diagonale des poids des lignes et \\(D_p = \\text{diag}(f_{\\bullet j})\\) la matrice diagonale des poids des colonnes. La matrice \\(D_n^{-1}F\\) a pour lignes les profils-lignes et la matrice \\(D_p^{-1}F^{\\top}\\) a pour lignes les profils-colonnes. la distance du \\(\\chi^2\\) entre deux profils-lignes \\(L_i\\) et \\(L_{i^\\prime}\\) s‚Äô√©crit alors \\[d^2(L_i, L_{i^\\prime}) = (L_i - L_{i^\\prime})^\\top D_p^{-1} (L_i - L_{i^\\prime}),\\] et de mani√®re analogue pour deux profils-colonnes \\(C_j\\) et \\(C_{j^\\prime}\\) \\[d^2(C_j, C_{j^\\prime}) = (C_j - C_{j^\\prime})^\\top D_n^{-1} (C_j - C_{j^\\prime}).\\]\nCes distances sont √† la base de la repr√©sentation g√©om√©trique dans l‚Äôanalyse des correspondances, o√π l‚Äôon cherche une projection des profils dans un espace de faible dimension qui conserve au mieux ces distances.",
    "crumbs": [
      "Modules",
      "04 - Dimension",
      "Analyse factorielle des correspondances"
    ]
  },
  {
    "objectID": "contents/dimension/02-ca.html#estimation-des-√©l√©ments-propres",
    "href": "contents/dimension/02-ca.html#estimation-des-√©l√©ments-propres",
    "title": "Analyse factorielle des correspondances",
    "section": "Estimation des √©l√©ments propres",
    "text": "Estimation des √©l√©ments propres\nL‚Äôanalyse des profils-lignes s‚Äôappelle l‚Äôanalyse directe. On consid√®re les profils-lignes contenus dans la matrice \\(D_n^{-1}F \\in \\mathbb{R}^{n \\times p}\\). On projette les profils-lignes dans un espace muni de la m√©trique du \\(\\chi^2\\) sur les colonnes, d√©finie par \\[\\left\\langle x, y \\right\\rangle = x^{\\top} D_p^{-1} y.\\]\nL‚Äôanalyse des profils-colonnes s‚Äôappelle l‚Äôanalyse duale. On consid√®re les profils-colonnes contenus dans la matrice \\(D_p^{-1} F^{\\top} \\in \\mathbb{R}^{p \\times n}\\). On projette les profils-colonnes dans un espace muni de la m√©trique du \\(\\chi^2\\) sur les lignes, d√©finie par \\[\\left\\langle x, y \\right\\rangle = x^{\\top} D_n^{-1} y.\\]\nPour l‚Äôanalyse directe, on cherche le premier axe factoriel, i.e.¬†la direction \\(u \\in R^p\\) qui maximise la variance projet√©e des profils-lignes, sous contrainte que \\(u\\) soit norm√©. On cherche donc \\[\\max_{u} u^{\\top} D_p^{-1} F^{\\top} D_n^{-1} F D_p^{-1} u, \\quad\\text{s.c.}\\quad u^{\\top} D_p^{-1} u = 1.\\] Ce probl√®me d‚Äôoptimisation revient √† chercher le premier vecteur propre de la matrice \\[S = F^{\\top} D_n^{-1} F D_p^{-1}.\\] La matrice \\(S\\) joue un r√¥le analogue √† la matrice de covariance dans l‚ÄôACP. Le premier vecteur propre \\(u_1\\) v√©rifie donc la relation \\[S u_{1} = F^{\\top} D_n^{-1} F D_p^{-1} u_1 = \\lambda_{1} u_1,\\] avec \\(\\lambda_1\\) la valeur propre associ√©e √† \\(u_1\\). Les vecteurs propres de la matrice \\(S\\) donnent les axes factoriels dans l‚Äôespace des colonnes. Les coordonn√©es des profils-lignes sur le premier axe factoriel sont obtenues par la relation \\[\\Phi_1 = D_n^{-1} F D_p^{-1} u_1.\\] On obtient les autres couples de valeurs propres et vecteurs propres, ainsi que les coordonn√©es des profils-lignes sur les axes factoriels associ√©s de mani√®re similaire.\nL‚Äôanalyse duale se fait de fa√ßon similaire. On cherche le premier vecteur propre de la matrice \\[T = F D_p^{-1} F^{\\top} D_n^{-1}.\\] Le premier vecteur propre \\(v_1\\) v√©rifie donc la relation \\[T v_{1} = F D_p^{-1} F^{\\top} D_n^{-1} v_1 = \\mu_1 v_1,\\] avec \\(\\mu_{1}\\) la valeur propre associ√©e √† \\(v_{1}\\). Les vecteurs propres de la matrice \\(T\\) donnent les axes factoriels dans l‚Äôespace des lignes. Les coordonn√©es des profils-colonnes sur le premier axe factoriel sont obtenues par la relation \\[\\Psi_1 = D_p^{-1} F^{\\top} D_n^{-1} v_1 .\\] On obtient les autres couples de valeurs propres et vecteurs propres, ainsi que les coordonn√©es des profils-colonnes sur les axes factoriels associ√©s de mani√®re similaire.\n\n\n\n\n\n\nImportantPropri√©t√©\n\n\n\nLes matrices \\(S\\) et \\(T\\) ont les m√™mes \\(r = \\min(n - 1, p - 1)\\) premi√®res valeurs propres positives. Cela garantit une repr√©sentation coh√©rente des lignes et des colonnes dans le m√™mes espace r√©duit. Pour \\(k = 1, \\dots, r\\), les relations entre les vecteurs propres \\(u_k\\) et \\(v_k\\) sont \\[u_k = \\frac{1}{\\sqrt{\\lambda_k}} F^{\\top} D_n^{-1} v_k \\quad \\text{et} \\quad v_k = \\frac{1}{\\sqrt{\\lambda_k}} F D_p^{-1} u_k.\\]\n\n\n\n\n\n\n\n\nAstucePreuve\n\n\n\n\n\nEn partant de l‚Äô√©quation \\[T v_{1} = F D_p^{-1} F^{\\top} D_n^{-1} v_1 = \\mu_1 v_1,\\] en multipliant √† gauche par \\(F^{\\top} D_n^{-1}\\), on obtient : \\[F^{\\top} D_n^{-1} F D_p^{-1} F^{\\top} D_n^{-1} v_1 = \\mu_1 F^{\\top} D_n^{-1} v_1.\\] Ainsi le vecteur \\(F^{\\top} D_n^{-1} v_1\\) est un vecteur propre de la matrice \\(F^{\\top} D_n^{-1} F D_p^{-1}\\) associ√©e √† la valeur propre \\(\\mu_1\\). Comme \\(\\lambda_1\\) est la plus grande valeur propre de \\(F^{\\top} D_n^{-1} F D_p^{-1}\\), on en d√©duit que \\(\\mu_1 \\leq \\lambda_{1}\\). En proc√©dant de la m√™me mani√®re, en partant de \\(S u_1 = \\lambda_{1} u_{1}\\), on d√©duit que \\(\\lambda_{1} \\leq \\mu_{1}\\). Donc \\(\\lambda_{1} = \\mu_{1}\\). On peut ensuite faire de m√™me pour les \\(r\\) premi√®res valeurs propres. On en d√©duit aussi les relations entre les valeurs propres.\n\n\n\n\n\n\n\n\n\nAstuceRemarque\n\n\n\nEn centrant les profils, on peut projeter les profils-lignes et les profils-colonnes dans un m√™me rep√®re, facilitant ainsi l‚Äôinterpr√©tation g√©om√©trique conjointe.",
    "crumbs": [
      "Modules",
      "04 - Dimension",
      "Analyse factorielle des correspondances"
    ]
  },
  {
    "objectID": "contents/dimension/02-ca.html#centre-de-gravit√©-et-inertie",
    "href": "contents/dimension/02-ca.html#centre-de-gravit√©-et-inertie",
    "title": "Analyse factorielle des correspondances",
    "section": "Centre de gravit√© et inertie",
    "text": "Centre de gravit√© et inertie\nDans les sorties des logiciels de statistique, le nuage des points issus d‚Äôune AFC est g√©n√©ralement centr√© en \\((0, 0)\\). Cette convention refl√®te une analyse relative aux centres de gravit√© des profils-lignes et des profils-colonnes. Ce centrage est √† la fois pratique et interpr√©table. En effet, il fait appara√Ætre les distances entre les modalit√©s par rapport √† leur moyenne poind√©r√©e, i.e.¬†par rapport au comportement moyen dans la population.\nChaque modalit√© (ligne ou colonne) est associ√©e √† un poids, correspondant √† sa fr√©quence marginale : le poids de la \\(i\\)e ligne est \\(f_{i \\bullet}\\) et le poids de la \\(j\\)e colonne est \\(f_{\\bullet j}\\). Le centre de gravit√© des lignes est la moyenne pond√©r√©e des profils-lignes :\n\\[G_L = \\left( g_{1}, \\dots, g_{p} \\right)^{\\top}, \\quad \\text{o√π}\\quad g_j = \\sum_{i = 1}^{n} f_{i \\bullet} \\frac{f_{i j}}{f_{i \\bullet}} = \\sum_{i = 1}^{n} f_{i j} = f_{\\bullet j}, j \\in \\{ 1, \\dots, p \\}.\\]\nDe m√™me, le centre de gravit√© des colonnes est \\[G_C = \\left( f_{1 \\bullet}, \\dots, f_{n \\bullet} \\right)^{\\top}.\\]\nPour recentrer les profils autour du centre de gravit√©, on soustrait leur valeur moyenne : \\[\\frac{f_{i j}}{f_{i \\bullet}} - g_{j} = \\frac{f_{i j}}{f_{i \\bullet}} - f_{\\bullet j} = \\frac{f_{i j} - f_{i \\bullet} f_{\\bullet j}}{f_{i \\bullet}}.\\] Ce centrage garantit que chaque profil-ligne \\(i \\in \\{ 1, \\dots, n \\}\\) est moyenn√© √† z√©ro : \\[\\sum_{j = 1}^{p} \\frac{f_{i j} - f_{i \\bullet}f_{\\bullet j}}{f_{i \\bullet}} = 0.\\]\nL‚ÄôAFC ne se fait donc plus sur la matrice \\(S\\) mais plut√¥t sur une matrice centr√©e \\(S^\\star = (s_{j j^\\prime}^\\star),\\) o√π \\[s_{j j^\\prime}^\\star = \\sum_{i = 1}^{n} \\frac{\\left( f_{i j} - f_{i \\bullet} f_{\\bullet j} \\right) \\left( f_{i j^\\prime} - f_{i \\bullet} f_{\\bullet j^\\prime} \\right)}{f_{i \\bullet} f_{\\bullet j^\\prime}}.\\]\nPar d√©finition, la trace de la matrice \\(S^\\star\\) donne l‚Äôinertie totale : \\[\\text{tr}(S^\\star) = \\sum_{j = 1}^{p} \\sum_{i = 1}^{n} \\frac{\\left( f_{i j} - f_{i \\bullet}f_{\\bullet j} \\right)^2}{f_{i \\bullet} f_{\\bullet j}}.\\] Celle-ci correspond √† la statistique du \\(\\chi^2\\) normalis√©e que l‚Äôon utilise pour tester l‚Äôind√©pendance entre les variables.\n\n\n\n\n\n\nImportantPropri√©t√©\n\n\n\nOn a que, pour tout \\(j, j^\\prime \\in \\{ 1, \\dots, p \\}\\), \\(s_{j j^\\prime}^\\star = s_{j j^\\prime} - f_{\\bullet j},\\) o√π \\[s_{j j^\\prime} = \\sum_{i = 1}^{n} \\frac{f_{i j}f_{i j^\\prime}}{f_{i \\bullet} f_{\\bullet j^\\prime}}.\\]\n\n\n\n\n\n\n\n\nAstucePreuve\n\n\n\n\n\n\n\n\n\nLa propri√©t√© pr√©c√©dente entraine que les matrices \\(S\\) et \\(S^\\star\\) one les m√™mes vecteurs propres pour les \\(p\\) premi√®res dimensions, ce qui permet d‚Äôeffectuer l‚Äôanalyse factorielle sur la version centr√©e.",
    "crumbs": [
      "Modules",
      "04 - Dimension",
      "Analyse factorielle des correspondances"
    ]
  },
  {
    "objectID": "contents/dimension/02-ca.html#coordonn√©es-factorielles",
    "href": "contents/dimension/02-ca.html#coordonn√©es-factorielles",
    "title": "Analyse factorielle des correspondances",
    "section": "Coordonn√©es factorielles",
    "text": "Coordonn√©es factorielles\nOn a que, pour tout \\(k = 1, \\dots, r\\), \\[\\Phi_k = D_n^{-1} F D_p^{-1} u_k \\quad \\text{et} \\quad \\Psi_k = D_p^{-1} F^{\\top} D_n^{-1} v_k.\\] Or, on a aussi vu les relations entre les vecteurs propres \\(u_k\\) et \\(v_k\\), \\[u_k = \\frac{1}{\\sqrt{\\lambda_k}} F^{\\top} D_n^{-1} v_k \\quad \\text{et} \\quad v_k = \\frac{1}{\\sqrt{\\lambda_k}} F D_p^{-1} u_k.\\] On en d√©duit donc les relations entre les coordonn√©es factorielles des profils-lignes et celles des profils-colonnes : \\[\\Phi_k = \\frac{1}{\\sqrt{\\lambda_k}} D_n^{-1} F \\Psi_k \\quad \\text{et} \\quad \\Psi_k = \\frac{1}{\\sqrt{\\lambda_k}} D_p^{-1} F^{\\top} \\Phi_k.\\]\nOn peut maintenant examiner ces relations sur chacune des composantes : \\[\\left[ \\Phi_k \\right]_i = \\frac{1}{\\sqrt{\\lambda_k}} \\sum_{j = 1}^{p} \\frac{f_{ij}}{f_{i \\bullet}} \\left[ \\Psi_k \\right]_j \\quad \\text{et} \\quad \\left[ \\Psi_k \\right]_j = \\frac{1}{\\sqrt{\\lambda_k}} \\sum_{i = 1}^{n} \\frac{f_{ij}}{f_{\\bullet j}} \\left[ \\Phi_k \\right]_i,\\] o√π \\(\\left[ \\Phi_k \\right]_i\\) d√©signe la coordonn√©e du profil-ligne \\(L_i\\) sur le \\(k\\)e axe factoriel et \\(\\left[ \\Psi_k \\right]_j\\) d√©signe la coordonn√©e du profil-colonne \\(C_j\\) sur le m√™me axe factoriel. Ces relations expriment, √† un facteur \\(1 / \\sqrt{\\lambda_k}\\) pr√®s, que chaque profil-ligne est au barycentre des projections des profils-colonnes affect√©s du poids de la colonne \\(j\\) dans la ligne \\(i\\) et que chaque profil-colonne est au barycentre des projections des profils-lignes affect√©s du poids de la ligne \\(i\\) dans la colonne \\(j\\).\n\n\n\n\n\n\nAstuceRemarque\n\n\n\nAinsi, en AFC, nous avons une double repr√©sentation barycentrique. Sur les axes factoriels, chaque point d‚Äôun nuage est au barycentre des points de l‚Äôautre nuage.",
    "crumbs": [
      "Modules",
      "04 - Dimension",
      "Analyse factorielle des correspondances"
    ]
  },
  {
    "objectID": "contents/dimension/03-mca.html",
    "href": "contents/dimension/03-mca.html",
    "title": "Analyse des correspondances multiples",
    "section": "",
    "text": "L‚Äôanalyse des correspondances multiples (ACM) peut √™tre pr√©sent√©e comme un prolongement de l‚ÄôAFC. Elle permet la representation graphique de tableaux de fr√©quences contnant plus de deux variables. Un exemple classique d‚Äôun tableau de fr√©quences avec plus de deux variables qualitatives est un tableau pr√©sentant les r√©ponses d‚Äôindividus √† un questionnaire contenant \\(Q\\) questions √† choix multiples. L‚ÄôACM est donc tr√®s utile pour visualiser les r√©sultats d‚Äôune √©tude par questionnaire.\nL‚ÄôACM peut aussi √™tre vue comme une version de l‚ÄôACP quand les variables sont mixtes, i.e.¬†comprenant √† la fois des variables quantitatives et des variables qualitatives. Le traitement conjoint de ces deux types de donn√©es repose sur leur transformation pr√©alable appel√©e codage disjonctif complet.",
    "crumbs": [
      "Modules",
      "04 - Dimension",
      "Analyse des correspondances multiples"
    ]
  },
  {
    "objectID": "contents/dimension/03-mca.html#notation",
    "href": "contents/dimension/03-mca.html#notation",
    "title": "Analyse des correspondances multiples",
    "section": "Notation",
    "text": "Notation\nNotons \\(n\\) le nombre d‚Äôindividus (ou d‚Äôobservations) et \\(Q\\) le nombre de variables (ou de questions dans le cas d‚Äôun questionnaire). Chaque variable poss√®de \\(J_q\\) modalit√©s et le nombre total de modalit√©s est √©gal √† \\(J\\).\n\n\n\n\n\n\nAvertissementD√©finition\n\n\n\nLe tableau binaire, i.e.¬†ne contenant que des \\(0\\) et des \\(1\\), √† \\(n\\) lignes et \\(J\\) colonnes est appel√© tableau de codage disjonctif complet. On le note \\(Z\\).\n\n\nAinsi, une variable n‚Äôest pas trait√©e telle quelle mais √† travers ses modalit√©s. Elle est d√©coup√©e en modalit√©s et tout individus est alors cod√© \\(1\\) pour la modalit√© qu‚Äôil poss√®de et \\(0\\) dans les autres (i.e.¬†qu‚Äôil ne poss√®de pas, les modalit√©s √©tant exclusives). Ce codage est imm√©diat pour des variables qualitatives. Cependant, pour une variable qualitative, on proc√®de en d√©coupant au pr√©alable la variable en classes. Ainsi, chaque individu n‚Äôappartient qu‚Äô√† une seule classe. Ce processus de transformation de l‚Äôinformation est appel√© codage disjonctif complet. Il s‚Äôagit bien d‚Äôun codage, car l‚Äôinformation initiale est transform√©e, disjonctif, car tout individu poss√®de au plus une modalit√©, et complet, car tout individu a au moins une modalit√©.\n\n\n\n\n\n\nNoteExemple\n\n\n\nPrenons par exemple un ensemble de produits avec diff√©rents types (Hoodie, Joggers et Sneakers) et diff√©rents prix. On a \\(7\\) observations (\\(7\\) produits). La variable Type est une variable qualitative et la variable Prix est une variable quantitative.\n\nJeu de donn√©es de produits.\n\n\nProduit\nType\nPrix ($)\n\n\n\n\nNike Tech Fleece\nHoodie\n256.72\n\n\nPuma Joggers\nJoggers\n221.26\n\n\nOff-White Hoodie\nHoodie\n198.45\n\n\nSupreme Hoodie\nHoodie\n235.50\n\n\nJordan 1 High\nSneakers\n298.22\n\n\nNike Dunk Low\nSneakers\n273.00\n\n\nNike Tech Fleece\nHoodie\n162.38\n\n\n\nPour coder l‚Äôinformation en tableau de codage disjonctif complet, on d√©finit trois classes de prix (prix inf√©rieur √† \\(200\\$\\), prix compris entre \\(200\\$\\) et \\(250\\$\\) et prix sup√©rieur √† \\(250\\$\\)). Ainsi, on peut encoder la variable Prix gr√¢ce aux classes pr√©c√©dentes. Ainsi, le tableau disjonctif complet est donn√© par la tableau suivant.\n\nJeu de donn√©es de produits en codage disjonctif complet.\n\n\n\n\n\n\n\n\n\n\n\nProduit\nHoodie\nJoggers\nSneakers\n\\(&lt;200\\$\\)\nentre \\(200\\$\\) et \\(250\\$\\)\n\\(&gt;250\\$\\)\n\n\n\n\nNike Tech Fleece\n1\n0\n0\n0\n0\n1\n\n\nPuma Joggers\n0\n1\n0\n0\n1\n1\n\n\nOff-White Hoodie\n1\n0\n0\n1\n0\n0\n\n\nSupreme Hoodie\n1\n0\n0\n0\n1\n0\n\n\nJordan 1 High\n0\n0\n1\n0\n0\n1\n\n\nNike Dunk Low\n0\n0\n1\n0\n0\n1\n\n\nNike Tech Fleece\n1\n0\n0\n1\n0\n0\n\n\n\n\n\n\n\n\n\n\n\nAstuceRemarque\n\n\n\nLorsque l‚Äôon veut transformer des variables quantitatives en tableau de codage disjonctif complet, on perd de l‚Äôinformation. En effet, comme on doit d√©couper les variables qualitatives en classes, l‚Äôappartenance √† une classe est moins informatif qu‚Äôune valeur pr√©cise d‚Äôune variable. Dans l‚Äôexemple pr√©c√©dent, on perd de l‚Äôinformation sur le prix.\n\n\n\n\n\n\n\n\nImportantPropri√©t√©s\n\n\n\n\nLa somme des √©l√©ments d‚Äôune m√™me ligne est constante et vaut \\(Q\\).\nLa somme de tous les √©l√©ments du tableau est √©gale √† \\(nQ\\).\nLa somme des √©l√©ments d‚Äôune m√™me colonne est √©gale √† l‚Äôeffectif \\(n_j\\) poss√©dant la modalit√© \\(j\\) de la variable \\(q\\).\n\n\n\n\n\n\n\n\n\nNotePreuve\n\n\n\n\n\nComme \\(Z\\) est un tableau disjonctif complet, on a \\[\\sum_{j = 1}^{J_q} z_{ij} = 1.\\] Donc, on trouve que \\[z_{i \\bullet} = \\sum_{j = 1}^{J} z_{ij} = \\sum_{q = 1}^{Q} \\sum_{j = 1}^{J_q} z_{ij} = Q, \\]\n\\[z_{\\bullet j} = \\sum_{i = 1}^{n} z_{ij} = n_j,\\]\n\\[z_{\\bullet \\bullet} = \\sum_{i = 1}^{n} \\sum_{j = 1}^{J} z_{ij} = nQ.\\]",
    "crumbs": [
      "Modules",
      "04 - Dimension",
      "Analyse des correspondances multiples"
    ]
  },
  {
    "objectID": "contents/dimension/03-mca.html#tableau-de-burt",
    "href": "contents/dimension/03-mca.html#tableau-de-burt",
    "title": "Analyse des correspondances multiples",
    "section": "Tableau de Burt",
    "text": "Tableau de Burt\n\n\n\n\n\n\nAvertissementD√©finition\n\n\n\nLe tableau de Burt, not√© \\(B\\), est le produit de la transpos√© de \\(Z\\) par \\(Z\\) : \\[B = Z^{\\top} Z.\\]\n\n\n\n\n\n\n\n\nImportantPropri√©t√©s de \\(B\\)\n\n\n\n\nLe tableau de Burt est carr√© et sa taille est √©gale au nombre total de modalit√©s \\(J\\) poss√©d√©es par les \\(Q\\) variables.\nLes blocs diagonaux de \\(B\\) sont eux-m√™mes des matrices diagonales. Ils sont donn√©s par \\(B_qq = Z^{\\top}_q Z_q\\) et leurs √©l√©ments diagonaux correspondent √† l‚Äôeffectif de chaque modalit√© pour la variable \\(q\\).\nLes blocs non-diagonaux de \\(B\\) sont donn√©s par \\(B_{q q^\\prime} = Z^{\\top}_q Z_{q^\\prime}, q \\neq q^\\prime\\). Ils correspondent aux tableaux de contingence croisant les variables \\(q\\) et \\(q^\\prime\\).\nLe tableau de Burt est sym√©trique car \\(B_{q^\\prime q} = Z^{\\top}_{q^\\prime} Z_{q}\\) est la transpos√© de \\(B_{q q^\\prime} = Z^{\\top}_q Z_{q^\\prime}\\).\n\n\n\nD‚Äôun point de vue math√©matique, l‚ÄôACM est une AFC effectu√©e sur la matrice logique \\(Z\\) ou sur le tableau de Burt \\(B\\). On peut d√©montrer que l‚Äôon obtient les m√™mes facteurs, et ce, peu importe la matrice utilis√© pour l‚Äôanalyse.",
    "crumbs": [
      "Modules",
      "04 - Dimension",
      "Analyse des correspondances multiples"
    ]
  },
  {
    "objectID": "contents/dimension/03-mca.html#√©l√©ments-propres-du-tableau-z",
    "href": "contents/dimension/03-mca.html#√©l√©ments-propres-du-tableau-z",
    "title": "Analyse des correspondances multiples",
    "section": "√âl√©ments propres du tableau \\(Z\\)",
    "text": "√âl√©ments propres du tableau \\(Z\\)\nOn peut calculer les √©l√©ments propres du tableau \\(Z\\) en utilisant la m√™me m√©thode que pour l‚ÄôAFC. Par analogie avec l‚ÄôAFC, on cherche donc les vecteurs propres de la matrice \\[S = \\frac{1}{Q} Z^{\\top} Z D_J^{-1},\\] o√π \\(D_J\\) est la matrice diagonale de terme \\(n_j, j = 1, \\dots, J\\). On peut calculer de la m√™me fa√ßon les coordonn√©es des profils-lignes sur les axes factoriels : \\[\\Phi_k = n Z D_J^{-1} u_k,\\] o√π \\(u_k\\) est le \\(k\\)e vecteur propre associ√© √† la valeur propre \\(\\lambda_k\\) de la matrice \\(S\\).\nOn peut aussi s‚Äôint√©resser √† l‚Äôanalyse duale du tableau \\(Z\\). Toujours par analogie avec l‚ÄôAFC, on cherche les vecteurs propres de la matrice \\[T = \\frac{1}{Q} Z D_J^{-1} Z^{\\top}.\\] De m√™me, on peut calculer les coordonn√©es des profils-colonnes sur les axes factoriels : \\[\\Psi_k = n D_J^{-1} Z^{\\top} v_k.\\]",
    "crumbs": [
      "Modules",
      "04 - Dimension",
      "Analyse des correspondances multiples"
    ]
  },
  {
    "objectID": "contents/dimension/03-mca.html#√©l√©ments-propres-du-tableau-de-burt-b",
    "href": "contents/dimension/03-mca.html#√©l√©ments-propres-du-tableau-de-burt-b",
    "title": "Analyse des correspondances multiples",
    "section": "√âl√©ments propres du tableau de Burt \\(B\\)",
    "text": "√âl√©ments propres du tableau de Burt \\(B\\)\nLe tableau de Burt √©tant sym√©trique, l‚Äôanalyse direct et l‚Äôanalyse duale co√Øcident. On peut aussi l‚Äôanalyse en analogie avec l‚ÄôAFC. La somme des √©l√©ments d‚Äôune m√™me ligne (ou d‚Äôune m√™me colonne) de \\(B\\) vaut \\(Q n_j\\) et la somme des √©l√©ments de \\(B\\) est \\(n Q^2\\). On cherche les vecteurs propres de la matrice \\[S^\\prime = \\frac{1}{Q^2} B^{\\top} D_J^{-1} B D_J^{-1}.\\]\nOn remarque alors que cette matrice \\(S^\\prime\\) a les m√™mes vecteurs propres que la matrice \\(S\\). En effet, \\[S^\\prime = \\frac{1}{Q^2} B^{\\top} D_J^{-1} B D_J^{-1} = \\frac{1}{Q^2} Z^{\\top} Z D_J^{-1} Z^{\\top} Z D_J^{-1}.\\] Et soit \\(u\\) et \\(\\lambda\\) v√©rifiant \\(Z^{\\top} Z D_J^{-1} = \\lambda u\\), alors \\[ Z^{\\top} Z D_J^{-1} Z^{\\top} Z D_J^{-1} u = Z^{\\top} Z D_J^{-1} \\lambda u = \\lambda^2 u.\\]\nFinalement, l‚Äôanalyse de \\(Z\\) ou de \\(B\\) fournit les m√™mes vecteurs propres et pour tout \\(k = 1, \\dots, Q\\), la \\(k\\)e valeur propre de \\(B\\) est la carr√© de la \\(k\\)e valeur propre de \\(Z\\).",
    "crumbs": [
      "Modules",
      "04 - Dimension",
      "Analyse des correspondances multiples"
    ]
  },
  {
    "objectID": "contents/dimension/03-mca.html#lencodage-des-variables",
    "href": "contents/dimension/03-mca.html#lencodage-des-variables",
    "title": "Analyse des correspondances multiples",
    "section": "L‚Äôencodage des variables",
    "text": "L‚Äôencodage des variables\nL‚Äôencodage des variables, et en particulier le choix des bornes des classes, est primordiale en ACM. Pour les variables continues, les bornes devraient √™tre pertinentes au regard du probl√®me √©tudi√©. Par exemple, on ne va pas d√©finir une classe \\(&gt; 1000\\$\\) dans l‚Äôexemple pr√©c√©dent. Pour obtenir des bornes pertinentes, on peut regarder les distributions des variables, e.g.¬†avec un histogramme. Dans certains cas particuliers, il est possible de d√©couper la variable en modalit√©s d‚Äôeffectifs √©gaux. Cependant, cette approche peut conduire √† des modalit√©s peu pertinentes.\nDans le cas de variables qualitatives, le choix des classes ne se pose pas; il est donn√© par la variable. Cependant, les modalit√©s ‚Äúnaturelles‚Äù peuvent conduire √† des effectifs (tr√®s) d√©s√©quilibr√©s. Dans ce cas, on doit g√©n√©ralement proc√©der √† des regroupements. Ici encore, une bonne connaissance du domaine √©tudi√©e est n√©cessaire. En tout cas, on pr√©ferera faire des regroupements de modalit√©s, plut√¥t que r√©partir de mani√®re al√©atoire les modalit√©s √† effectif faible dans les autres modalit√©s (ce qui est parfois propos√© dans les logiciels).",
    "crumbs": [
      "Modules",
      "04 - Dimension",
      "Analyse des correspondances multiples"
    ]
  },
  {
    "objectID": "contents/unsupervised/02-hierarchy.html",
    "href": "contents/unsupervised/02-hierarchy.html",
    "title": "M√©thodes hi√©rarchiques",
    "section": "",
    "text": "L‚Äôalgorithme \\(k\\)-means pr√©sente plusieurs limitations. Par exemple, celles-ci peuvent √™tre probl√©matique lorsque l‚Äôon ne dispose que d‚Äôune matrice de similiarit√© ou de distance entre les observations et que l‚Äôon n‚Äôa pas acc√®s aux donn√©es originales. Dans un tel contexte, les m√©thodes de classification hi√©rarchique sont pertinentes.\nLa classification hi√©rarchique permet d‚Äôobtenir une s√©rie de partitions imbriqu√©es, allant de la partition la plus fine (chaque observation dans son propre groupe) √† la plus grossi√®re (toutes les observations dans un seul groupe). Cette approche ne fournit donc pas une seule parition, mais une hi√©rarchie de partitions. Cette hi√©rarchie peut √™tre repr√©sent√© √† l‚Äôaide d‚Äôun dendogramme, un arbre r√©sumant comment ces partitions sont imbriqu√©es. Il existe deux types d‚Äôalgorithmes pour effectuer une classification hi√©rarchique:\nDans les deux cas, on obtient \\(n\\) partitions hi√©rarchiques constitu√©es de \\(1\\) √† \\(n\\) groupes.",
    "crumbs": [
      "Modules",
      "06 - Non-supervis√©e",
      "M√©thodes hi√©rarchiques"
    ]
  },
  {
    "objectID": "contents/unsupervised/02-hierarchy.html#algorithmes",
    "href": "contents/unsupervised/02-hierarchy.html#algorithmes",
    "title": "M√©thodes hi√©rarchiques",
    "section": "Algorithmes",
    "text": "Algorithmes\n\nAlgorithmes descendants\nLes algorithmes descendants commencent avec l‚Äôensemble des \\(n\\) observations r√©unis dans un seul groupe. √Ä chaque √©tape, le groupe jug√© le moins homog√©ne est divis√© en deux sous-groupes, en cherchant √† maximiser la dissimilarit√© entre les deux sous-groupes. On continue ainsi jusqu‚Äô√† ce que chaque observation soit isol√© dans son propre groupe.\nCe type d‚Äôalgorithme est co√ªteux en temps de calcul, car il faut √©valuer, √† chaque √©tape, toutes les mani√®res possibles de diviser un groupe en deux. On l‚Äôutilise donc rarement en pratique.\n\n\nAlgorithme ascendant\n√Ä l‚Äôinverse, les algorithmes ascendants d√©butent avec \\(n\\) groupes distincts, chacun contenant une seule observation. √Ä chaque √©tape, on fusionne les deux groupes les plus similaires, i.e.¬†ceux dont la dissimilarit√© est la plus faible selon un crit√®re choisi. L‚Äôalgorithme continue jusqu‚Äô√† ce qu‚Äôil ne reste plus qu‚Äôun seul groupe contenant toutes les observations.",
    "crumbs": [
      "Modules",
      "06 - Non-supervis√©e",
      "M√©thodes hi√©rarchiques"
    ]
  },
  {
    "objectID": "contents/unsupervised/02-hierarchy.html#distance-entre-groupes",
    "href": "contents/unsupervised/02-hierarchy.html#distance-entre-groupes",
    "title": "M√©thodes hi√©rarchiques",
    "section": "Distance entre groupes",
    "text": "Distance entre groupes\nPour mettre en oeuvre les algorithmes pr√©c√©dent, on doit d√©finir la distance entre deux groupes d‚Äôobservations \\(A\\) et \\(B\\), not√©e \\(d(A, B)\\). Si l‚Äôon sait g√©n√©ralement mesurer distance entre deux individus, on doit d√©finir une distance entre deux groupes contenant un nombre diff√©rents d‚Äô√©l√©ments. Il existe plusieurs fa√ßon de calculer une telle distance entre deux groupes.\n\nM√©thode du plus proche voisin (single linkage)\nDans cette approche, la distance entre deux groupes est d√©finie comme la plus petite distance entre un individu de \\(A\\) et un individu de \\(B\\) :\n\\[d(A, B) = \\min \\{ d_{i j}: i \\in A, j \\in B \\}.\\]\nDit autrement, deux groupes \\(A\\) et \\(B\\) sont consid√©r√©s comme proche si un √©l√©ment de \\(A\\) est proche d‚Äôun √©l√©ment de \\(B\\). Cette m√©thode pr√©sente plusieurs avantages. Elle donne de bons r√©sultats lorsque les variables sont de nature diff√©rente (e.g., quantitatives et qualitatives) et permet de construire des groupes aux formes irr√©guli√®re. De plus, elle est relativement robuste aux donn√©es aberrantes. Enfin, ses propri√©t√©s math√©matiques th√©oriques sont int√©ressantes.\nCependant, cette m√©thode tend √† cr√©er des groupes d√©s√©quilibr√©s : un grand groupe central entour√© de plusieurs petits groupes satellites. Elle est moins performante lorsque les groupes naturels sont de forme r√©guli√®re. Bien qu‚Äôelle est de bonnes propri√©t√©s math√©matiques, celles-ci ne se v√©rifie pas toujours empiriquement.\n\n\nM√©thode du voisin le plus distant (complete linkage)\n√Ä l‚Äôinverse de la m√©thode pr√©c√©dente, la distance entre deux groupes est d√©finie comme la plus grande distance entre un individu de \\(A\\) et un individu de \\(B\\) :\n\\[d(A, B) = \\max \\{ d_{i j}: i \\in A, j \\in B \\}.\\]\nDit autrement, deux groupes sont consid√©r√©s proches si tous les √©l√©ments de \\(A\\) sont proches de tous les √©l√©ments de \\(B\\). Cette m√©thode a tendence √† produire des groupes r√©guliers de taille homog√®ne. Comme la m√©thode du plus proche voisin, elle est bien adapt√©e aux variables de diff√©rents types. Cependant, elle est extr√™mement sensible aux donn√©es aberrantes. En effet, un seul individu peut augmenter artificiellement la distance entre deux groupes. De plus, elle a tendance √† forcer la formation de groupes de m√™me taille, ce qui n‚Äôest pas toujours justifi√© en pratique.\n\n\nM√©thode de la moyenne (average linkage)\nIci, la distance entre deux groupes est d√©finie comme la moyenne des distances entre toutes les paires d‚Äôindividus, issu de \\(A\\) et de \\(B\\) :\n\\[d(A, B) = \\frac{1}{n_{A}n_{B}} \\sum_{i \\in A} \\sum_{j \\in B} d(X_{i}, X_{j}).\\] o√π \\(n_{A}\\) est le nombre d‚Äôobservations dans le groupe \\(A\\) et \\(n_{B}\\) est le nombre d‚Äôobservations dans le groupe \\(B\\).\nCette m√©thode consiste √† consid√©rer toutes les interactions possibles entre les √©l√©ments des deux groupes, puis √† en faire la moyenne. Elle tend √† produire des groupes dont la variance interne est faible, i.e.¬†relativement homog√®ne. Toutefois, cette m√©thode privil√©gie la formation de groupes de variance similaire, ce qui n‚Äôest pas toujours justifi√© en pratique.\n\n\nM√©thode du centro√Øde (centroid method)\nPour cette m√©thode, la distance entre deux groupes est d√©finie comme la distance entre leurs centro√Ødes, i.e.¬†les moyennes des observations de chaque groupe :\n\\[d(A, B) = d(\\overline{X}_{A}, \\overline{X}_{B}).\\] o√π \\[\\overline{X}_{A} = \\frac{1}{n_{A}} \\sum_{i \\in A} X_i, \\quad\\text{et}\\quad \\overline{X}_B = \\frac{1}{n_{B}} \\sum_{j \\in B} X_j\\]\nApr√®s la fusion de \\(A\\) et de \\(B\\), le nouveau centro√Øde \\(\\overline{X}_{AB}\\) est donn√© par la moyenne pond√©r√©e : \\[\\overline{x}_{AB} = \\frac{n_{A} \\overline{x}_{A} + n_{B} \\overline{x}_{B}}{n_{A} + n_{B}}.\\]\nCette approche est assez robuste aux donn√©es aberrantes, mais elle est g√©n√©ralement peu performante lorsqu‚Äôil n‚Äôy en a pas.\n\n\nM√©thode de la m√©diane (median method)\nLa m√©thode de la m√©diane repose sur une mise √† jour des distances de fa√ßon r√©cursive. Lorsqu‚Äôon fusionne deux groupes \\(A\\) et \\(B\\), on d√©finit la distance entre le nouveau groupe \\(AB\\) et autre groupe \\(C\\) par la formule : \\[d(AB, C) = \\frac{d(A, C) + d(B, C)}{2} - \\frac{d(A, B)}{4}.\\]\nCette m√©thode est particuli√®rement robuste aux donn√©es aberrantes (davantage que la m√©thode du centro√Øde). Elle est cependant tr√®s peu efficace lorsque de telles valeurs extr√™mes sont absentes.\n\n\nM√©thode de Ward (Ward‚Äôs method)\nLa m√©thode de Ward est une variante de la m√©thode du centro√Øde. Elle est optimale dans le cas o√π les observations suivent des lois normales multivari√©es, de m√™me matrice de variance-covariance mais de moyennes diff√©rentes. Elle est bas√©e sur une mesure de l‚Äôinertie intra-groupe. Pour chaque groupe \\(A\\), groupe \\(B\\) et groupe \\(A \\cup B\\), not√© \\(AB\\), on d√©finit\n\\[SC_A = \\sum_{i \\in A} (X_i - \\overline{X}_A)^\\top (X_i - \\overline{X}_A),\\]\n\\[SC_B = \\sum_{j \\in B} (X_j - \\overline{X}_B)^\\top (X_j - \\overline{X}_B),\\]\n\\[SC_AB = \\sum_{k \\in A \\cup B} (X_k - \\overline{X}_{AB})^\\top (X_k - \\overline{X}_{AB}).\\]\no√π \\(\\overline{X}_A\\), \\(\\overline{X}_B\\) et \\(\\overline{X}_{AB}\\) sont calcul√©es comme dans la m√©thode du centro√Øde. On regroupe les groupes \\(A\\) et \\(B\\) qui minimisent l‚Äôaugmentation de l‚Äôinertie : \\[I_{AB} = SC_{AB} - SC_A - SC_B = \\frac{d^2(\\overline{X}_A, \\overline{X}_B)}{\\frac{1}{n_{A}} + \\frac{1}{n_{B}}}.\\]\nCette m√©thode est tr√®s efficace lorsque les groupes sont homog√®nes, de taille comparable et que les hypoth√®ses gaussiennes sont raisonnablement satisfaites. En revanche, elle est sensible aux donn√©es aberrantes et tend √† former des regroupements de m√™me taille.\n\n\nM√©thode flexible (flexible clustering)\nLa m√©thode flexible repose sur une formule g√©n√©rale permettant de repr√©senter plusieurs m√©thodes de mise √† jour des distances. Si l‚Äôon fusionne deux groupes \\(A\\) et \\(B\\) pour former \\(AB\\), et que l‚Äôon souhaite calculer la distance entre \\(AB\\) et autre groupe \\(C\\), on peut utiliser la relation \\[d(C, AB) = \\alpha_A d(C, A) + \\alpha_B d(C, B) + \\beta d(A, B) + \\gamma \\left| d(C, A) - d(C, B) \\right|.\\]\nSelon les valeurs choisies pour les coefficients \\(\\alpha_A\\), \\(\\alpha_B\\), \\(\\beta\\) et \\(\\gamma\\), on peut retrouver les formules de mise √† jour correspondant aux diff√©rentes m√©thodes pr√©c√©dentes. Le Table¬†1 pr√©sente les valeurs des diff√©rents coefficients √† choisir pour retrouver les diff√©rentes m√©thodes.\n\n\n\nTable¬†1: Coefficients pour retrouver les m√©thodes pr√©c√©dentes.\n\n\n\n\n\n\n\n\n\n\n\n\nM√©thode\n\\(\\alpha_A\\)\n\\(\\alpha_B\\)\n\\(\\beta\\)\n\\(\\gamma\\)\n\n\n\n\nPlus proche\n\\(1/2\\)\n\\(1/2\\)\n\\(0\\)\n\\(-1/2\\)\n\n\nPlus distant\n\\(1/2\\)\n\\(1/2\\)\n\\(0\\)\n\\(1/2\\)\n\n\nM√©diane\n\\(1/2\\)\n\\(1/2\\)\n\\(-1/4\\)\n\\(0\\)\n\n\nMoyenne\n\\(\\frac{n_A}{n_A+n_B}\\)\n\\(\\frac{n_B}{n_A+n_B}\\)\n\\(0\\)\n\\(0\\)\n\n\nCentro√Øde\n\\(\\frac{n_A}{n_A+n_B}\\)\n\\(\\frac{n_B}{n_A+n_B}\\)\n\\(-\\frac{n_An_B}{n_A + n_B}\\)\n\\(0\\)\n\n\nWard\n\\(\\frac{n_A+n_C}{n_A+n_B+n_C}\\)\n\\(\\frac{n_B+n_C}{n_A+n_B+n_C}\\)\n\\(-\\frac{n_C}{n_A+n_B+n_C}\\)\n\\(0\\)\n\n\n\n\n\n\nPour la m√©thode flexible, on impose arbitrairement les contraintes suivantes: \\[\\alpha_A + \\alpha_B + \\beta = 1, \\quad \\alpha_A = \\alpha_B, \\quad \\gamma = 0.\\]\nAinsi, on a \\(\\alpha_A = \\alpha_B = \\frac{1 - \\beta}{2}\\). Il ne reste qu‚Äôun seul param√®tre √† fixer. G√©n√©ralement, on choisit \\(\\beta = -0.25\\). Si l‚Äôon soup√ßonne la pr√©sence de donn√©es aberrantes, on peut opter pour \\(\\beta = -0.5\\) afin d‚Äôaccro√Ætre la robustesse de l‚Äôalgortihme.",
    "crumbs": [
      "Modules",
      "06 - Non-supervis√©e",
      "M√©thodes hi√©rarchiques"
    ]
  },
  {
    "objectID": "contents/unsupervised/02-hierarchy.html#quelle-partition-choisir",
    "href": "contents/unsupervised/02-hierarchy.html#quelle-partition-choisir",
    "title": "M√©thodes hi√©rarchiques",
    "section": "Quelle partition choisir ?",
    "text": "Quelle partition choisir ?\nUn algorithme de classification hierarchique, ascendant ou descendant, produit une s√©quence de \\(n\\) partitions imbriqu√©es, allant de \\(n\\) groupes (o√π chaque observation est isol√©e) √† un seul groupe contenant toutes les observations. En pratique, on se pose donc la question suivante : quelle partition faut-il consid√©rer ?\nPlusieurs pistes peuvent guider ce choix :\n\nUne des partitions est-elle particuli√®rement interpr√©table d‚Äôun point de vue scientifique ou m√©tier ?\nUne des partitions est-elle pertinente sur le plan op√©rationnel, par exemple pour orienter des d√©cisions ?\nCherche-t-on explicitement √† segmenter la population en un nombre de groupes \\(K\\), d√©termin√© √† l‚Äôavance ?\n\nLorsque ces consid√©rations pratiques ne suffisent pas √† trancher, on peut s‚Äôappuyer sur des crit√®res statistiques pour √©valuer la qualit√© des partitions et sugg√©rer un nombre ‚Äúoptimal‚Äù de groupes. Ces crit√®res sont surtout adapt√©s lorsque les variables sont continues.\nLa librairie R NbClust en propose une trentaine et la librairie Python sklearn une dizaine. On pr√©sente ensuite les crit√®res les plus fr√©quemment utilis√©s.\n\nCrit√®res bas√©s sur l‚Äôinertie\nL‚Äôinertie mesure la dispersion des observations appartenant √† un groupe par rapport √† son centro√Øde. L‚Äôinertie totale \\(I_{\\text{tot}}\\) de l‚Äôensemble des \\(n\\) observations est \\[I_{\\text{tot}} = \\frac{1}{n}\\sum_{i = 1}^{n} d(X_i, G),\\] o√π la fonction \\(d\\) est une distance et \\(G\\) le centre de gravit√© de l‚Äôensemble des observations. L‚Äôinertie totale \\(I_{tot}\\) se d√©compose en une inertie intra-groupe et une inertie inter-groupe de la fa√ßon suivante : \\[I_{\\text{tot}} = I_{\\text{intra-groupe}} + I_{\\text{inter-groupe}}.\\] L‚Äôinertie inter-groupe \\(I_{\\text{inter-groupe}}\\) est l‚Äôinertie des centres de gravit√© des groupes pond√©r√©s par le nombre d‚Äôobservations dans le groupe. Elle mesure la s√©paration entre les groupes. Elle est donn√© par \\[I_{\\text{inter-groupe}} = \\frac{1}{n}\\sum_{k = 1}^{K} n_k d(G_k, G),\\] o√π \\(G_k\\) est le centre de gravit√© du groupe \\(k\\) et \\(n_k\\) le nombre d‚Äôobservations dans ce groupe. L‚Äôinertie intra-groupe \\(I_{\\text{intra-groupe}}\\) est la somme des inerties des groupes. Elle mesure donc l‚Äôh√©t√©rog√©n√©it√© des groupes. Elle est donn√©e par \\[I_{\\text{intra-groupe}} = \\frac{1}{n}\\sum_{k = 1}^{K} \\sum_{i \\in C_k} d(X_i, G_k),\\] o√π l‚Äôensemble \\(C_k\\) contient les observations du groupe \\(k\\).\nPlus les groupes sont compacts, plus l‚Äôinertie intra-groupe est faible, ce qui signifie que l‚Äôinertie inter-groupe est √©lev√©. Une partition de bonne qualit√© maximise donc l‚Äôinertie inter-groupe. Ces crit√®res supposent g√©n√©ralement que les variables sont continues et ont √©t√© standardis√©es pour √©viter qu‚Äôune variable √† grande √©chelle ne domine les autres. Parmi les crit√®res se basant sur l‚Äôinertie, on peut citer : le pseudo-\\(R^2\\) et la statistique de Calinski-Harabasz (CH).\nLe pseudo-\\(R^2\\) mesure la proportion d‚Äôinertie expliqu√©e par la partition :\n\\[\\text{pseudo-}R^2 =\\frac{I_{\\text{inter-groupe}}}{I_{\\text{tot}}}.\\]\nUn pseudo-\\(R^2\\) √©lev√© indique que la parition capture une grande part de la structure des donn√©es. La statistique CH est une variation du pseudo-\\(R^2\\) normalis√© par le nombre de groupes : \\[\\text{CH} = \\frac{I_{\\text{inter-groupe}} / (K - 1)}{I_{\\text{intra-groupe}} / (n - K)}.\\] Un score √©lev√© de la statistique CH sugg√®re un bon √©quilibre entre groupes compactes et groupes s√©par√©s.\n\n\nCrit√®res bas√©s sur la distance\nOn peut aussi utiliser des crit√®res se basant sur la distance entre les observations pour mesurer la qualit√© d‚Äôune partition. Parmi les crit√®res se basant sur la distance, on peut citer : l‚Äôindice de Dunn et l‚Äôindice de silhouette.\nL‚Äôindice de Dunn cherche √† maximiser la distance minimale entre deux groupes, tout en minimisant la distance maximale entre les observations √† l‚Äôint√©rieur d‚Äôun groupe : \\[D= \\frac{\\text{Distance minimale entre 2 groupes}}{\\text{Distance maximale dans un groupe}}.\\] Ce crit√®re favorise des groupes denses et bien s√©par√©s. En ce qui concerne le crit√®re de silhouette, il mesure la qualit√© d‚Äôaffection d‚Äôune observation \\(X_i\\) √† son groupe : \\[S(X_i) = \\frac{b_i - a_i}{max(b_i, a_i)}\\] o√π \\(a_i\\) est la distance moyenne entre l‚Äôobservation \\(X_i\\) et les autres observations de son groupe et \\(b_i\\) est la distance moyenne entre l‚Äôobservation \\(X_i\\) et les observations du groupe le plus proche de \\(X_i\\). On souhaite maximiser la silhouette moyenne des observations. La silhouette moyenne sur toutes les observations est donc un bon indicateur de la coh√©rence globale de la partition.\nEn r√©sum√©, plusieurs crit√®res peuvent nous guider dans le choix du nombre de groupes, mais aucun n‚Äôest parfait. Il est souvent recommend√© de croiser connaissance m√©tier, visualisation des partitions et indicateurs statistiques pour prendre une d√©cision √©clair√©e.",
    "crumbs": [
      "Modules",
      "06 - Non-supervis√©e",
      "M√©thodes hi√©rarchiques"
    ]
  },
  {
    "objectID": "contents/generalities/01-stat.html",
    "href": "contents/generalities/01-stat.html",
    "title": "Projet d‚Äôanalyse de donn√©es",
    "section": "",
    "text": "On pr√©sente ici les diff√©rentes √©tapes d‚Äôun projet d‚Äôanalyse de donn√©es.",
    "crumbs": [
      "Modules",
      "03 - G√©n√©ralit√©s",
      "Projet d'analyse de donn√©es"
    ]
  },
  {
    "objectID": "contents/generalities/01-stat.html#projet-danalyse-donn√©es",
    "href": "contents/generalities/01-stat.html#projet-danalyse-donn√©es",
    "title": "Projet d‚Äôanalyse de donn√©es",
    "section": "Projet d‚Äôanalyse donn√©es",
    "text": "Projet d‚Äôanalyse donn√©es\nUn projet d‚Äôanalyse de donn√©es suit g√©n√©ralement une structure bien d√©finie en plusieurs √©tapes. Nous en d√©gagons ici cinq principales, chacune avec ses objectifs, ses enjeux et son importance relative dans le succ√®s du projet :\n\nD√©finition des objectifs\nCollecte et pr√©paration des donn√©es\n√âlaboration et validation des mod√®les\nImpl√©mentation et mise en production\nSuivi de la performance et am√©lioration continue\n\nLors de la planification d‚Äôun projet, il faut prendre en compte que chaque √©tape √† une importance diff√©rente, mais aussi que chacune ne prend pas le m√™me temps d‚Äôex√©cution. Pyle (1999) donne une estimation du temps de chaque √©tape, ainsi que de leur importance dans la r√©ussite du projet (donn√© en pourcentage du total, cf. Table¬†1).\n\n\n\nTable¬†1: D√©coupage d‚Äôun projet d‚Äôanalyse des donn√©es.\n\n\n\n\n\n√âtape\nTemps\nImportance\n\n\n\n\nComprendre le probl√®me\n\\(10\\%\\)\n\\(15\\%\\)\n\n\nExplorer la solution\n\\(9\\%\\)\n\\(14\\%\\)\n\n\nImplementer la solution\n\\(1\\%\\)\n\\(51\\%\\)\n\n\nPr√©parer les donn√©es\n\\(60\\%\\)\n\\(15\\%\\)\n\n\nAnalyser les donn√©es\n\\(15\\%\\)\n\\(3\\%\\)\n\n\nMod√©liser les donn√©es\n\\(5\\%\\)\n\\(2\\%\\)\n\n\n\n\n\n\nOn remarque deux faits importants. L‚Äôimportance d‚Äôune √©tape n‚Äôest pas proportionnelle au temps pass√© dessus. Par exemple, l‚Äôimpl√©mentation de la solution est une √©tape essentielle (sinon il n‚Äôy a pas de r√©sultat), mais peut n‚Äôexiger que peu de temps (parfois quelques lignes de code). √Ä l‚Äôinverse, la pr√©paration des donn√©es, souvent sous-estim√©e, est g√©n√©ralement chronophage, notamment pour g√©rer les donn√©es manquantes, les donn√©es aberrantes, ou encore les √©ventuels accents pour des donn√©es en fran√ßais.",
    "crumbs": [
      "Modules",
      "03 - G√©n√©ralit√©s",
      "Projet d'analyse de donn√©es"
    ]
  },
  {
    "objectID": "contents/generalities/01-stat.html#d√©finition-des-objectifs",
    "href": "contents/generalities/01-stat.html#d√©finition-des-objectifs",
    "title": "Projet d‚Äôanalyse de donn√©es",
    "section": "D√©finition des objectifs",
    "text": "D√©finition des objectifs\nToute analyse commence par une question claire : que cherche-t-on √† accomplir ? Visualiser des donn√©es ? Tester une hypoth√®se ? Pr√©dire un comportement ? Segmenter une population ? Une d√©finition pr√©cise des objectifs est essentielle pour orienter les √©tapes suivantes. Cela permet de guider la collecte et la structuration des donn√©es. Cela permet de d√©finir un mod√®le ad√©quat (e.g.¬†classification, r√©gression, ‚Ä¶). Cela permet de faciliter l‚Äôinterpr√©tation et la communication des r√©sultats. Cette phase √©vite aussi les explorations aveugles et les interpr√©tations biais√©es.\nComment fait-on en pratique pour formuler un bon objectif ? On pose des questions ! Tout d‚Äôabord, il faut clarifier les termes. Qui va utiliser le mod√®le et comment ? Quelle est la population cible ? Quelle d√©cision d√©pendra r√©sultats ?\n\n\n\n\n\n\nNoteExemple\n\n\n\nLa Banque National du Canada voudrait lancer un nouveau produit d‚Äô√©pargne et vous donne acc√®s √† sa base de donn√©es clients.\nMauvais objectif: Analyser les donn√©es de la base clients.\nMeilleur objectif: Peut-on pr√©dire quels clients sont susceptibles d‚Äôacheter ce nouveau produit d‚Äô√©pargne ?\n\n\n\n\n\n\n\n\nNoteExemple\n\n\n\nL‚Äô√©quipe de hockey des Canadiens de Montr√©al souhaite mieux conna√Ætre ses adversaires pour d√©velopper de nouvelles tactiques de jeu.\nMauvais objectif: Analyser les donn√©es des adversaires.\nMeilleur objectif: Peut-on caract√©riser le style de jeu des adversaires pour identifier leurs faiblesses ?\n\n\n\n\n\n\n\n\nNoteExemple\n\n\n\nPharmascience souhaite √©valuer l‚Äôefficacit√© d‚Äôun nouveau m√©dicament.\nMauvais objectif: Analyser les donn√©es du m√©dicament.\nMeilleur objectif: Peut-on concevoir un protocole statistique permettant de tester l‚Äôefficacit√© du m√©dicament ?",
    "crumbs": [
      "Modules",
      "03 - G√©n√©ralit√©s",
      "Projet d'analyse de donn√©es"
    ]
  },
  {
    "objectID": "contents/generalities/01-stat.html#donn√©es",
    "href": "contents/generalities/01-stat.html#donn√©es",
    "title": "Projet d‚Äôanalyse de donn√©es",
    "section": "Donn√©es",
    "text": "Donn√©es\nLes donn√©es sont le coeur du sujet. Pour √™tre utile, les donn√©es doivent √™tre disponibles et de bonnes qualit√©s. Une fois les objectifs d√©finis, on effectue une traitement pr√©liminaire et une exploration basique des donn√©es pour ensuite aller vers des mod√®les plus d√©velopp√©s.\n\nO√π trouver des donn√©es ?\nR√©ponse simple : Internet ! Voici une liste de sites (non-exhaustives) qui regroupent des jeux de donn√©es :\n\nGoogle datasets;\nKaggle;\nUC Irvine Machine Learning Repository;\nTime Series Machine Learning website;\nPhysionet Database.\n\nOn peut aussi regarder les sites officiels de sources de donn√©es que l‚Äôon peut trouver pour une grande partie des pays du monde :\n\nCanada: StatCan;\nFrance: data.gouv.fr;\nUSA: data.gov;\nAngleterre: data.gouv.uk;\netc.\n\nPour des donn√©es sur des sujets plus sp√©cifiques, les agences gouvernementales sont souvent de bonnes ressources. Par exemple, le Centre Canadien de cartographie et d‚Äôobservation de la terre fournit les donn√©es g√©ospatiales du Canada (ici).\nLorsque que l‚Äôon travaille pour une entreprise, on a g√©n√©ralement acc√®s aux sources de donn√©es internes, e.g.¬†base de donn√©es sur la production, les clients et les employ√©s, les listes de transactions et de clients potentiels, des informations sur les visites web, etc.\n\n\nQualit√©\nIl y a un dicton populaire en informatique, s‚Äôappliquant aussi en analyse de donn√©es: ‚ÄúGarbage in, garbage out‚Äù. M√™me le meilleur mod√®le ne peut compenser des donn√©es biais√©es, incompl√®tes ou erron√©es.\nPour nous assurer de la qualit√© des donn√©es, on pourra se poser les questions suivantes :\n\nLes donn√©es sont-elles repr√©sentatives de la population cible ?\nSont-elles exactes, compl√®tes, pertinentes ?\nY a-t-il des valeurs manquantes, des doublons, des incoh√©rences ?\n\n\n\nConstitution de la base de donn√©es\nUne fois nos donn√©es collect√©es, il faut les charger en m√©moire pour ensuite pouvoir faire des analyses. En Python, les librairies pandas et polars permettent de lire la plupart des formats de fichiers auxquels nous aurons affaire. En ce qui concerne R, plusieurs packages sont utilis√©s selon le format (cf. Table¬†2).\n\n\n\nTable¬†2: Diff√©rentes libraries pour diff√©rents formats de fichiers.\n\n\n\n\n\nFormat\nExtension\nLibrarie\n\n\n\n\nTexte\n.txt; .csv\nreadr\n\n\nExcel\n.xlsx\nreadxl\n\n\nSAS\n.sas7bdat\nhaven\n\n\nSPSS\n.sav; .zsav\nhaven\n\n\nJSON\n.json\njsonlite\n\n\n\n\n\n\nDepuis une dizaine d‚Äôann√©e, le concept de ‚Äútidy data‚Äù a emerg√© (cf. Wickham (2014)). Chaque jeu de donn√©es ‚Äútidy‚Äù respecte trois principes:\n\nChaque variable est une colonne du tableau.\nChaque observation est une ligne du tableau.\nChaque cellule du tableau contient une valeur unique.\n\nCela permet d‚Äôavoir une approche unifi√©e pour l‚Äôanalyse de donn√©es. De mani√®re g√©n√©ral, on essaiera toujours de mettre son jeu de donn√©es sous format ‚Äútidy‚Äù. Le package tidyr en R et les librairies pandas et polars en Python permettent de mettre en forme les donn√©es en format ‚Äútidy‚Äù.\n\n\nExploration et traitement pr√©liminaire\nUne fois les donn√©es charg√©es et mise sous le format ‚Äútidy‚Äù, une phase d‚Äôexploration pr√©liminaire est n√©cessaire avant l‚Äô√©tape de mod√©lisation. Cette √©tape, bien que souvent n√©glig√©e, est tr√®s importante, mais elle n‚Äôest pas le coeur de ce cours. Cette √©tape permet de d√©tecter les probl√®mes potentiels, de mieux comprendre la structure des donn√©es et d‚Äôorienter les choix m√©thodologiques. Voici quelques trucs √† faire concernant cette premi√®re exploration:\n\nNettoyage de donn√©es: supprimer les doublons, uniformiser les modalit√©s, v√©rifier le format des valeurs sp√©ciales, etc.\nExploration des donn√©es: identification des modalit√©s rares ou trop nombreuses, analyse des √©ventuelles asym√©tries, d√©tection des classes d√©s√©quilibr√©es, identification des valeurs extr√™mes ou aberrantes, recherche des corr√©lations fortes entre les variables, √©valuation des valeurs manquantes.",
    "crumbs": [
      "Modules",
      "03 - G√©n√©ralit√©s",
      "Projet d'analyse de donn√©es"
    ]
  },
  {
    "objectID": "contents/generalities/01-stat.html#√©laboration-et-validation-des-mod√®les",
    "href": "contents/generalities/01-stat.html#√©laboration-et-validation-des-mod√®les",
    "title": "Projet d‚Äôanalyse de donn√©es",
    "section": "√âlaboration et validation des mod√®les",
    "text": "√âlaboration et validation des mod√®les\nCe cours concerne l‚Äô√©laboration et la validation de mod√®les. Pour l‚Äôinstant, on peut retenir quatre composantes principales :\n\nUn espace (math√©matique) de repr√©sentation: il s‚Äôagit du cadre math√©matique dans lequel on travaille.\nUne distance (ou similarit√©): elle permet de comparer les observations entre elles.\nUn mod√®le (ou algorithme): c‚Äôest la m√©thode utilis√©e pour apprendre √† partir des donn√©es.\nUne fonction de co√ªt: elle mesure la qualit√© du mod√®le.\n\nCes √©l√©ments seront √©tudi√©s en d√©tails dans les sections suivantes du cours.",
    "crumbs": [
      "Modules",
      "03 - G√©n√©ralit√©s",
      "Projet d'analyse de donn√©es"
    ]
  },
  {
    "objectID": "contents/generalities/01-stat.html#mise-en-oeuvre",
    "href": "contents/generalities/01-stat.html#mise-en-oeuvre",
    "title": "Projet d‚Äôanalyse de donn√©es",
    "section": "Mise en oeuvre",
    "text": "Mise en oeuvre\nUne fois le mod√®le choisi et valid√©, il peut √™tre d√©ploy√© en production. La mise en production signifie le rendre op√©rationnel dans un environnement r√©el, souvent en automatisant l‚Äôensemble du processus de traitement des donn√©es. G√©n√©ralement, cela consiste √† automatiser la collecte, le nettoyage et la transformation des donn√©es, √† int√©grer le mod√®le cr√©√© dans une application ou un syst√®me d√©cisionnel, et √† g√©n√©rer des rapports ou des pr√©dictions en temps r√©el ou √† intervalles r√©guliers. Cette partie est le domaine du data engineering. Un data engineer con√ßoit et maintient la pipeline de traitement depuis la source des donn√©es jusqu‚Äô√† la sortie du mod√®le.",
    "crumbs": [
      "Modules",
      "03 - G√©n√©ralit√©s",
      "Projet d'analyse de donn√©es"
    ]
  },
  {
    "objectID": "contents/generalities/01-stat.html#suivi-de-la-performance-et-am√©lioration",
    "href": "contents/generalities/01-stat.html#suivi-de-la-performance-et-am√©lioration",
    "title": "Projet d‚Äôanalyse de donn√©es",
    "section": "Suivi de la performance et am√©lioration",
    "text": "Suivi de la performance et am√©lioration\nFinalement, une fois que le mod√®le est mis en production, il faut assurer un suivi de sa performance dans le temps. En effet, les donn√©es √©voluent, de m√™me que les comportements qu‚Äôelles d√©crivent. Ainsi, les distributions des donn√©es peuvent changer (un ph√©nom√®ne appel√© data drift), les hypoth√®ses initiales peuvent ne plus √™tre valides ou encore de nouvelles donn√©es ou de nouvelles variables peuvent am√©liorer la performance. Pour surveiller la performance du mod√®le, on peut faire un monitoring r√©gulier des performances. On peut aussi r√©entra√Æner le mod√®le avec des donn√©es r√©centes ou l‚Äôam√©liorer en int√©grant de nouvelles hypoth√®ses.\nUn bon mod√®le n‚Äôest donc pas seulement performant √† un instant donn√©, il est aussi robuste et adaptable dans le temps.",
    "crumbs": [
      "Modules",
      "03 - G√©n√©ralit√©s",
      "Projet d'analyse de donn√©es"
    ]
  },
  {
    "objectID": "contents/generalities/04-bias-variance.html",
    "href": "contents/generalities/04-bias-variance.html",
    "title": "Biais/Variance",
    "section": "",
    "text": "Cette section est bas√©e sur James et al. (2021), chapitre 2.",
    "crumbs": [
      "Modules",
      "03 - G√©n√©ralit√©s",
      "Biais/Variance"
    ]
  },
  {
    "objectID": "contents/generalities/04-bias-variance.html#quel-est-notre-objectif",
    "href": "contents/generalities/04-bias-variance.html#quel-est-notre-objectif",
    "title": "Biais/Variance",
    "section": "Quel est notre objectif ?",
    "text": "Quel est notre objectif ?\nNous souhaitons mod√©liser la relation entre une variable r√©ponse \\(Y\\), pouvant √™tre quantitative, qualitative ou de nature diff√©rente, et un ensemble de \\(p\\) variables explicatives \\(X = (X_{1}, \\dots, X_{p})\\), elles aussi de (potentiellement) diff√©rents types. L‚Äôid√©e centrale est qu‚Äôil existe un relation entre \\(Y\\) et les variables explicatives \\(X\\). De mani√®re g√©n√©rales, nous mod√©lisons cette relation par le mod√®le :\n\\[Y = f(X) + \\varepsilon. \\tag{1}\\]\nIci, \\(f\\) est une fonction d√©terministe (non-al√©atoire) repr√©sentant l‚Äôinformation syst√©matique que les variables explicatives \\(X_{1}, \\dots, X_{p}\\) apportent sur \\(Y\\), et \\(\\varepsilon\\) est un terme d‚Äôerreur al√©atoire, mod√©lisant les variations de \\(Y\\) non expliqu√©es par \\(X\\). Dans le cadre de ce cours, nous ferons les hypoth√®ses suivantes : la variable al√©atoire \\(\\varepsilon\\) est ind√©pendente de des variables explicatives \\(X\\), \\(\\mathbb{E}[\\varepsilon] = 0\\) et \\(\\mathrm{Var}(\\varepsilon) = \\sigma^2\\). Le mod√®le √âquation¬†1 est g√©n√©ral. Il sert de cadre pour l‚Äôensemble des m√©thodes que nous allons √©tudier, m√™me lorsque la forme explicite de \\(f\\) n‚Äôest pas connue.\nLa Figure¬†1 illustre les diff√©rents √©l√©ments du mod√®le: les donn√©es observ√©es \\((X_i, Y_i)\\), la fonction \\(f\\) (en bleu) et les √©carts al√©atoires \\(\\varepsilon_i\\) repr√©sent√©s par des lignes pointill√©es.\n\n\nCode\nlibrary(tibble)\nlibrary(dplyr)\n\ngenerate_noisy_data &lt;- function(n = 100, noise_levels = c(0, 0.1, 0.3, 0.5)) {\n  # x values (avoid 0 for log)\n  x_vals &lt;- seq(0.01, 0.99, length.out = n)\n  \n  # True function\n  f &lt;- function(x) 4 * x * (1 - x) * log(x) + 2\n  \n  # Generate data for each noise level\n  data &lt;- lapply(noise_levels, function(sigma) {\n    y_true &lt;- f(x_vals)\n    y_noisy &lt;- y_true + rnorm(n, mean = 0, sd = sqrt(sigma))\n    \n    tibble(\n      x = x_vals,\n      y = y_noisy,\n      noise = sigma\n    )\n  }) %&gt;% bind_rows()\n  \n  return(data)\n}\n\n# Example usage\nset.seed(123)\n\nnoise_levels &lt;- seq(0, 0.5, by=0.01)\ndf &lt;- generate_noisy_data(noise_levels = noise_levels)\nwrite.csv(df, './data.csv')\n\n\n\n\nCode\ndata = FileAttachment(\"../../include/data/bias-variance/data.csv\").csv({ typed: true })\n\nviewof noise = Inputs.range(\n  [0.01, 0.5], \n  {value: 0.25, step: 0.01, label: tex`\\sigma^2`}\n)\n\nfiltered = data.filter(function(df) {\n  return df.noise == noise;\n})\n\ntrue_curve = data.filter(function(df) {\n  return df.noise == 0;\n})\n\nerrors = filtered.map(f =&gt; {\n  const location = true_curve.find(loc =&gt; loc.x === f.x);\n  return { ...f, y_end: location?.y };\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nPlot.plot({\n  grid: true,\n  x: {\n    domain: [0, 1],\n    label: \"X\",\n  },\n  y: {\n    domain: [0, 3],\n    label: \"Y\",\n  },\n  marks: [\n    Plot.dot(filtered, {x: \"x\", y: \"y\", fill: \"#444444\", r: 3}),\n    Plot.line(true_curve, {x: \"x\", y: \"y\", stroke: \"#B0E1FA\", strokeWidth: 5}),\n    Plot.link(errors, {\n      x1: \"x\",\n      x2: \"x\",\n      y1: \"y\",\n      y2: \"y_end\",\n      stroke: \"#AAAAAA\",\n      strokeDasharray: \"5,5\",\n    })\n  ]\n})\n\n\n\n\n\n\n\n\n\nFigure¬†1: Les diff√©rents √©l√©ments du mod√®le. Les points repr√©sentent les donn√©es observ√©es \\((X_i, Y_i)\\). La courbe bleue repr√©sente la fonction \\(f\\) et les lignes pointill√©es repr√©sentent l‚Äôerreur associ√©e √† chaque observation.\n\n\n\n\nDans la suite du cours, nous verrons diff√©rentes m√©thodes permettant d‚Äôestimer la fonction \\(f\\) √† partir de donn√©es. Cependant avant d‚Äô√©tudier comment contruire un estimateur \\(\\widehat{f}\\) de \\(f\\), nous allons nous interroger sur la qualit√© d‚Äôun tel estimateur : que signifie ‚Äúbien estimer‚Äù \\(f\\) ? Et comment √©valuer la qualit√© de l‚Äôestimation ?\n\n\n\n\n\n\nNoteExemple : R√©gression lin√©aire simple\n\n\n\nDans ce cadre tr√®s simple, nous faisons l‚Äôhypoth√®se que la fonction \\(f\\) est de la forme : \\(f(x) = a x + b\\). Dans ce cas, l‚Äôestimation de la fonction \\(f\\) se r√©sume √† l‚Äôestimation des coefficients \\(a\\) et \\(b\\).\n\n\n\n\n\n\n\n\nAstuceRemarque : Compromis entre exactitude et interpr√©tabilit√©\n\n\n\nD√©pendant de l‚Äôobjectif de l‚Äô√©tude, nous devons g√©n√©ralement faire un choix entre l‚Äôexactitude de nos pr√©dictions et l‚Äôinterpr√©tabilit√© de notre mod√®le. Un mod√®le simple, comme la r√©gression lin√©aire, sera facile √† interpr√©ter mais capturera mal des relations complexes. √Ä l‚Äôinverse, un mod√®le plus flexible, comme une for√™t al√©atoire, aura de meilleur pr√©diction, mais sera plus difficilement interpr√©table. Le choix d√©pend donc de l‚Äôobjectif de l‚Äôanalyse : compr√©hension ou performance pr√©dictive ?\n\n\n\n\n\n\n\n\nAstuceRemarque : No free lunch in statistics\n\n\n\nPourquoi ne pas simplement utiliser le mod√®le ‚Äúultime‚Äù, celui qui serait toujours optimal quelque soit le jeu de donn√©es ? Parce qu‚Äôun tel mod√®le n‚Äôexiste pas ! Il n‚Äôy a pas de m√©thode universellement meilleure pour tous les jeux de donn√©es et tous les objectifs. Une m√©thode performante dans un contexte donn√© peut √©chouer ailleurs. Il faut donc toujours adapter l‚Äôapproche au probl√®me (explication, pr√©diction, classification, ‚Ä¶).",
    "crumbs": [
      "Modules",
      "03 - G√©n√©ralit√©s",
      "Biais/Variance"
    ]
  },
  {
    "objectID": "contents/generalities/04-bias-variance.html#comment-mesurer-la-qualit√©-dun-estimateur",
    "href": "contents/generalities/04-bias-variance.html#comment-mesurer-la-qualit√©-dun-estimateur",
    "title": "Biais/Variance",
    "section": "Comment mesurer la qualit√© d‚Äôun estimateur ?",
    "text": "Comment mesurer la qualit√© d‚Äôun estimateur ?\nUne fois que nous disposons d‚Äôun estimateur \\(\\widehat{f}\\) de la fonction \\(f\\), obtenu √† partir de \\(n\\) observations \\((y_1, x_1), \\dots, (y_n, x_n)\\), nous cherchons √† √©valuer la pr√©cision des pr√©dictions \\(\\widehat{Y} = \\widehat{f}(X)\\). L‚Äôid√©e est de v√©rifier dans quelle mesure \\(\\widehat{Y}\\) est proche de la vraie valeur de \\(Y\\).\n\n\n\n\n\n\nAvertissementD√©finition : Erreur quadratique moyenne\n\n\n\nLorsque \\(Y\\) est une variable quantitative, une mesure classique de la qualit√© de \\(\\widehat{f}\\) est l‚Äôerreur quadratique moyenne (mean square error, MSE) : \\[MSE(Y, \\widehat{Y}) = \\frac{1}{n} \\sum_{i = 1}^{n} \\left( y_i - \\widehat{y}_i\\right)^2 = \\frac{1}{n} \\sum_{i = 1}^{n} \\left( y_i - \\widehat{f}(x_i) \\right)^2,\\] o√π \\(\\widehat{y}_i = \\widehat{f}(x_i)\\) est la pr√©diction que \\(\\widehat{f}\\) donne pour l‚Äôobservation \\(x_i\\).\n\n\nUne MSE faible indique que les pr√©dictions sont proches des observations. Nous pouvons aussi l‚Äôinterpr√©ter comme la distance moyenne entre les valeurs observ√©es et les valeurs pr√©dites. Nous cherchons donc √† avoir une distance moyenne faible.\nDans le cas o√π \\(Y\\) est une variable qualitative, e.g.¬†une classe ou un label, on utilise une autre mesure : le taux d‚Äôerreur.\n\n\n\n\n\n\nAvertissementD√©finition: Taux d‚Äôerreur\n\n\n\nLorsque \\(Y\\) est une variable qualitative, une mesure classique de la qualit√© de \\(\\widehat{f}\\) est le taux d‚Äôerreur (error rate, ER) : \\[ER(Y, \\widehat{Y}) = \\frac{1}{n} \\sum_{i = 1}^{n} \\mathbb{1}(y_i \\neq \\widehat{y}_i) = \\frac{1}{n} \\sum_{i = 1}^{n} \\mathbb{1}(y_i \\neq \\widehat{f}(x_i)).\\] o√π \\(\\widehat{y}_i = \\widehat{f}(x_i)\\) est la pr√©diction que \\(\\widehat{f}\\) donne pour l‚Äôobservation \\(x_i\\).\n\n\nLe taux d‚Äôerreur mesure la proportion de mauvaises pr√©dictions. Il s‚Äôagit, l√† encore, d‚Äôune mesure de la distance moyenne entre \\(Y\\) et \\(\\widehat{Y}\\), adapt√©e aux variables qualititatives.",
    "crumbs": [
      "Modules",
      "03 - G√©n√©ralit√©s",
      "Biais/Variance"
    ]
  },
  {
    "objectID": "contents/generalities/04-bias-variance.html#le-compromis-biaisvariance",
    "href": "contents/generalities/04-bias-variance.html#le-compromis-biaisvariance",
    "title": "Biais/Variance",
    "section": "Le compromis biais/variance",
    "text": "Le compromis biais/variance\nNotre objectif est souvent de minimiser l‚Äôerreur de pr√©diction, non seulement sur les donn√©es observ√©es, mais surtout sur de nouvelles donn√©es (r√©cup√©r√©e apr√®s avoir estimer le mod√®le). Pour cela, nous nous int√©ressons √† l‚Äôerreur de pr√©diction : \\[\\mathbb{E}\\left[ \\left( Y - \\widehat{Y} \\right)^2 \\right] = \\mathbb{E}\\left[ \\left( Y - \\widehat{f}(X) \\right)^2 \\right].\\]\nCette erreur peut se d√©composer en trois composantes :\n\nLe biais : l‚Äôerreur due √† une approximation syst√©matique, e.g.¬†si on impose un mod√®le lin√©aire alors que la relation est non lin√©aire.\nLa variance : la sensibilit√© de l‚Äôestimateur aux fluctuations de l‚Äô√©chantillon d‚Äôapprentissage.\nL‚Äôerreur irr√©ductible : la variance intrins√®que du bruit \\(\\varepsilon\\), not√©e \\(\\sigma^2\\).\n\n\n\n\n\n\n\nImportantD√©composition biais/variance\n\n\n\nOn a : \\[\\mathbb{E}\\left[ (Y - \\widehat{Y})^2 \\right] = \\mathbb{E}\\left[ (Y - \\widehat{f}(X))^2 \\right] = \\mathrm{Biais}(\\widehat{f}(X))^2 + \\mathrm{Var}(\\widehat{f}(X)) + \\sigma^2.\\]\n\n\n\n\n\n\n\n\nNotePreuve\n\n\n\n\n\nTout d‚Äôabord, montrons que l‚Äôesp√©rance de l‚Äôerreur de l‚Äôestimateur se d√©compose en une partie r√©ductible et en une partie irr√©ductible.\n\\[\\begin{align*}\n\\mathbb{E}\\left[ \\left( Y - \\widehat{Y} \\right)^2 \\right]\n&= \\mathbb{E}\\left[ \\left( Y - \\widehat{f}(X) \\right)^2 \\right] \\\\\n&= \\mathbb{E}\\left[ \\left( f(X) + \\varepsilon - \\widehat{f}(X) \\right)^2 \\right] \\\\\n&= \\mathbb{E}\\left[ \\left( f(X) - \\widehat{f}(X) \\right)^2 \\right] + 2\\mathbb{E}\\left[ \\left( f(X) - \\widehat{f}(X) \\right)\\varepsilon \\right] + \\mathbb{E}[\\varepsilon^2] \\\\\n&= \\mathbb{E}\\left[ \\left( f(X) - \\widehat{f}(X) \\right)^2 \\right] + 2\\mathbb{E}\\left[ \\left( f(X) - \\widehat{f}(X) \\right) \\right] \\underbrace{\\mathbb{E}\\left[ \\varepsilon \\right]}_{= 0} + \\sigma^2 \\\\\n&= \\underbrace{\\mathbb{E}\\left[ \\left( f(X) - \\widehat{f}(X) \\right)^2 \\right]}_{\\text{r√©ductible}} + \\underbrace{\\sigma^2}_{\\text{irr√©ductible}}.\n\\end{align*}\\]\nOn utilise la lin√©arit√© de l‚Äôesp√©rance et le fait que \\(X\\) et \\(\\varepsilon\\) soient ind√©pendants. On s‚Äôint√©resse maintenant √† la partie ‚Äúr√©ductible‚Äù. L‚Äôastuce est de faire appara√Ætre \\(\\mathbb{E}\\left[ \\widehat{f}(X) \\right]\\).\n\\[\\begin{align*}\n\\mathbb{E}\\left[ \\left( f(X) - \\widehat{f}(X) \\right)^2 \\right]\n  &= \\mathbb{E}\\left[ \\left( f(X) - \\mathbb{E}\\left[ \\widehat{f}(X) \\right] + \\mathbb{E}\\left[ \\widehat{f}(X) \\right] - \\widehat{f}(X) \\right)^2 \\right] \\\\\n  &= \\underbrace{\\mathbb{E}\\left[ \\left( f(X) - \\mathbb{E}\\left[ \\widehat{f}(X) \\right] \\right)^2 \\right]}_{\\text{A}} \\\\\n  &\\quad - 2 \\underbrace{\\mathbb{E}\\left[ \\left( f(X) - \\mathbb{E}\\left[ \\widehat{f}(X) \\right] \\right) \\left( \\widehat{f}(X) - \\mathbb{E}\\left[ \\widehat{f}(X) \\right] \\right) \\right]}_{\\text{B}} \\\\\n  &\\quad + \\underbrace{\\mathbb{E}\\left[ \\left( \\widehat{f}(X) - \\mathbb{E}\\left[ \\widehat{f}(X) \\right] \\right)^2 \\right]}_{\\text{C}}.\n\\end{align*}\\]\nA. La fonction \\(f(X)\\) n‚Äô√©tant pas al√©atoire, on a \\(\\mathbb{E}\\left[ f(X) \\right] = f(X)\\) et donc\n\\[\\begin{align*}\n\\mathbb{E}\\left[ \\left( f(X) - \\mathbb{E}\\left[ \\widehat{f}(X) \\right] \\right)^2 \\right]\n  &= \\mathbb{E}\\left[ \\left( \\mathbb{E}\\left[ f(X) - \\widehat{f}(X) \\right] \\right)^2 \\right] \\\\\n  &= \\mathbb{E}\\left[ f(X) - \\widehat{f}(X) \\right]^2 \\\\\n  &= \\text{Biais}(\\widehat{f}(X))^2.\n\\end{align*}\\]\nB. En d√©veloppant l‚Äôexpression et en utilisant l‚Äôind√©pendance des variables, on trouve que \\(B = 0\\).\nC. En utilisant la d√©finition de la variance,\n\\[\\mathbb{E}\\left[ \\left( \\widehat{f}(X) - \\mathbb{E}\\left[ \\widehat{f}(X) \\right] \\right)^2 \\right] = \\mathrm{Var}(\\widehat{f}).\\]\nFinalement, on a\n\\[\\mathbb{E}\\left[ \\left( f(X) - \\widehat{f}(X) \\right)^2 \\right] = \\text{Biais}(\\widehat{f}(X))^2 + \\mathrm{Var}(\\widehat{f}(X)).\\]\nD‚Äôo√π le r√©sultat.\n\n\n\nCette d√©composition met en avant un compromis fondamental en analyse de donn√©es :\n\nSi on choisit un mod√®le peu flexible, le biais sera √©lev√©, mais la variance sera faible.\nSi on choisit un mod√®le flexible, le biais sera faible, mais la variance peut √™tre tr√®s √©lev√©e.\n\nNotre objectif est donc de trouver un juste √©quilibre entre biais et variance, i.e.¬†un mod√®le qui pr√©dit correctement, tout en √©tant g√©n√©ralisable √† de nouvelles donn√©es. La Figure¬†2 (a) pr√©sente un jeu de donn√©es et diff√©rents estimateurs \\(\\widehat{f}\\). En faisant varier le param√®tre \\(\\lambda\\), on obtient des mod√®les plus ou moins flexible (lorsque \\(\\lambda = 0.15\\), le mod√®le est flexible et lorsque \\(\\lambda = 1\\), le mod√®le est rigide). La Figure¬†2 (b) montre la valeur du biais, de la variance et de la MSE pour les mod√®les estim√©s pour la Figure¬†2 (a). On remarque que plus \\(\\lambda\\) est petit, plus la variance est grande, mais le biais est petit (le mod√®le est flexible). Inversement, plus \\(\\lambda\\) est grand, plus le biais est grand et la variance petite (le mod√®le est rigide). La courbe de MSE en fonction du param√®tre est une courbe en U. Comme on cherche √† minimiser la MSE, i.e.¬†√† faire un compromis entre le biais et la variance, on peut prendre \\(\\lambda = 0.5\\).\n\n\nCode\n# Load packages\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\n\nset.seed(42)\n\n# 1. Simulate a single dataset\nn &lt;- 100\nsigma2 &lt;- 0.1\nx &lt;- sort(runif(n, 0.05, 1))\ny &lt;- 4 * x * (1 - x) * log(x) + 2 + rnorm(n, 0, sqrt(sigma2))\ndf &lt;- data.frame(x = x, y = y, span = 0)\n\n# 2. Define grid and spans to compare\nx_grid &lt;- seq(0.05, 1, length.out = 300)\nspans_to_plot &lt;- seq(0.1, 1, by = 0.1)\n\n# 3. Compute loess fits for each span\nfits &lt;- lapply(spans_to_plot, function(s) {\n  loess_model &lt;- loess(y ~ x, data = df, span = s)\n  y_hat &lt;- predict(loess_model, newdata = data.frame(x = x_grid))\n  data.frame(x = x_grid, y = y_hat, span = s)\n})\n\nfit_df &lt;- bind_rows(fits)\nfit_df &lt;- fit_df |&gt; add_row(df)  # Add data points\n \nwrite.csv(fit_df, './data_fit.csv')\n\n\n# Parameters\nn_sim &lt;- 100          # number of simulated datasets\nspans &lt;- seq(0.1, 1, by = 0.05)  # LOESS smoothing parameters\nx_grid &lt;- seq(0.1, 1, length.out = 200)\nf_true &lt;- 4 * x_grid * (1 - x_grid) * log(x_grid) + 2\n\n# Storage for predictions\nresults &lt;- list()\n\nfor (s in spans) {\n  pred_matrix &lt;- matrix(NA, nrow = length(x_grid), ncol = n_sim)\n  \n  for (sim in 1:n_sim) {\n    x &lt;- sort(runif(n, 0.01, 1.1))\n    y &lt;- 4 * x * (1 - x) * log(x) + 2 + rnorm(n, 0, sqrt(sigma2))\n    df &lt;- data.frame(x = x, y = y)\n    \n    # Fit loess model with span = s\n    model &lt;- loess(y ~ x, data = df, span = s, degree = 2)\n    pred &lt;- predict(model, newdata = data.frame(x = x_grid), )\n    \n    pred_matrix[, sim] &lt;- pred\n  }\n  \n  # For each point in x_grid, compute bias¬≤, variance, MSE\n  mean_pred &lt;- rowMeans(pred_matrix, na.rm = TRUE)\n  bias2 &lt;- (mean_pred - f_true)^2\n  var_pred &lt;- apply(pred_matrix, 1, var, na.rm = TRUE)\n  mse &lt;- bias2 + var_pred\n  \n  results[[as.character(s)]] &lt;- data.frame(\n    span = s,\n    Biais2 = mean(bias2),\n    Variance = mean(var_pred),\n    MSE = mean(mse)\n  )\n}\n\n# Combine and reshape results\nresults_df &lt;- bind_rows(results)\nresults_long &lt;- pivot_longer(\n  results_df,\n  cols = c(\"Biais2\", \"Variance\", \"MSE\"),\n  names_to = \"component\", values_to = \"value\"\n)\n\nwrite.csv(results_long, './data_mse.csv')\n\n\n\n\nCode\ndata_fit = FileAttachment(\"../../include/data/bias-variance/data_fit.csv\").csv({ typed: true })\ndata_mse = FileAttachment(\"../../include/data/bias-variance/data_mse.csv\").csv({ typed: true })\n\nviewof span = Inputs.range(\n  [0.1, 1], \n  {value: 0.5, step: 0.1, label: tex`\\lambda`}\n)\n\nfiltered_fit = data_fit.filter(function(df) {\n  return df.span == span;\n})\n\npoints = data_fit.filter(function(df) {\n  return df.span == 0;\n})\n\ncurrent_mse = data_mse.filter(function(df) {\n  return df.span == span;\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nPlot.plot({\n  grid: true,\n  x: {\n    domain: [0, 1],\n    label: \"X\",\n  },\n  y: {\n    domain: [0, 3],\n    label: \"Y\",\n  },\n  marks: [\n    Plot.dot(\n      points,\n      {x: \"x\", y: \"y\", fill: \"#444444\", r: 3}\n    ),\n    Plot.line(\n      filtered_fit, \n      {x: \"x\", y: \"y\", stroke: \"#B0E1FA\", strokeWidth: 5}\n    ),\n\n  ]\n})\n\n\nCode\nPlot.plot({\n  grid: true,\n  color: {\n    legend: true,\n  },\n  x: {\n    domain: [0, 1],\n    label: \"Œª\",\n  },\n  y: {\n    domain: [0, 0.04],\n    label: \"Y\",\n  },\n  marks: [\n    Plot.line(\n      data_mse, \n      {x: \"span\", y: \"value\", stroke: \"component\", strokeWidth: 5}\n    ),\n    Plot.dot(\n      current_mse,\n      {x: \"span\", y: \"value\", fill: \"component\", r: 10}\n    ),\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Diff√©rents estimateurs \\(\\widehat{f}\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Diff√©rentes parties de l‚Äôerreur.\n\n\n\n\n\n\nFigure¬†2: Illustration du compromis biais/variance. Le param√®tre \\(\\lambda\\) contr√¥le la flexibilit√© du mod√®le, plus \\(\\lambda\\) est petit, plus le mod√®le est flexible.\n\n\n\n\nDe mani√®re g√©n√©rale, lorsque la flexibilit√© augmente, la diminution du biais est plus importante que l‚Äôaugmentation de la variance, ce qui fait d√©cro√Ætre l‚Äôerreur de pr√©diction. Cependant, √† partir d‚Äôun certain niveau de flexibilit√©, le biais devient n√©gligeable, et toute baisse suppl√©mentaire est compens√©e par l‚Äôaugmentation rapide de la variance. L‚Äôerreur de pr√©diction commence donc √† cro√Ætre. Il en r√©sulte une courbe en U de l‚Äôerreur de pr√©diction en fonction de la flexibilit√© du mod√®le : un mod√®le trop rigide engendre un fort biais, tandis qu‚Äôun mod√®le trop flexible conduit √† une trop grande variance.\n\n\n\n\n\n\nAstuceRemarque : Pourquoi un compromis ?\n\n\n\nIl est toujours possible de construire un mod√®le tr√®s flexible avec un biais nul, e.g.¬†un mod√®le qui passe par tous les points d‚Äôobservations, mais qui aura une variance √©norme. √Ä l‚Äôoppos√©, un mod√®le trop rigide, e.g.¬†une constante, aura un biais tr√®s important mais une variance presque nulle. Le compromis biais/variance consiste √† choisir un mod√®le qui contr√¥le ces deux quantit√©s.",
    "crumbs": [
      "Modules",
      "03 - G√©n√©ralit√©s",
      "Biais/Variance"
    ]
  },
  {
    "objectID": "contents/generalities/05-model-evaluation.html",
    "href": "contents/generalities/05-model-evaluation.html",
    "title": "√âvaluation de mod√®les",
    "section": "",
    "text": "Cette section est bas√©e sur James et al. (2021), chapitre 5.",
    "crumbs": [
      "Modules",
      "03 - G√©n√©ralit√©s",
      "√âvaluation de mod√®les"
    ]
  },
  {
    "objectID": "contents/generalities/05-model-evaluation.html#√©valuer-la-performance-dun-mod√®le-pr√©dictif",
    "href": "contents/generalities/05-model-evaluation.html#√©valuer-la-performance-dun-mod√®le-pr√©dictif",
    "title": "√âvaluation de mod√®les",
    "section": "√âvaluer la performance d‚Äôun mod√®le pr√©dictif",
    "text": "√âvaluer la performance d‚Äôun mod√®le pr√©dictif\nDans la section pr√©c√©dente, nous avons introduit des outils pour mesurer la qualit√© d‚Äôun estimateur : l‚Äôerreur quadratique moyenne (MSE) pour les variables quantitatives et le taux d‚Äôerreur (ER) pour les variables qualitatives. Ces mesures comparent les valeurs pr√©dites \\(\\widehat{Y} = \\widehat{f}(X)\\) aux valeurs observ√©es \\(Y\\). Cependant, si l‚Äôon calcule ces erreurs uniquement √† partir des donn√©es qui ont servi √† entra√Æner le mod√®le, on risque de sous-estimer la v√©ritable erreur de pr√©diction. Pourquoi ? Parce que l‚Äôestimateur \\(\\widehat{f}\\) a √©t√© ajust√© pour minimiser l‚Äôerreur sur ces m√™mes donn√©es. Il s‚Äôy adapte donc bien, et g√©n√©ralement, trop bien ! Cela peut conduire √† l‚Äôilllusion que notre mod√®le est performant. En effet, un mod√®le tr√®s flexible peut avoir une erreur faible sur les donn√©es d‚Äôentra√Ænement simplement parce qu‚Äôil capture le bruit plut√¥t que le signal. Mais si le mod√®le s‚Äôadapte trop aux donn√©es d‚Äôentra√Ænement, il risque de mal g√©n√©raliser √† de nouvelles donn√©es, i.e.¬†des donn√©es qu‚Äôil n‚Äôa jamais vues. Ce ph√©nom√®ne s‚Äôappelle le sur-ajustement (overfitting).\n\n\n\n\n\n\nAstuceRemarque: Sur-ajustement et sous-ajustement\n\n\n\nUn mod√®le trop flexible peut s‚Äôadapter parfaitement aux donn√©es d‚Äôentra√Ænement, y compris au bruit al√©atoire. Il aura une erreur faible sur ces donn√©es mais une erreur √©lev√©e sur de nouvelles observations. On dira qu‚Äôil y a sur-ajustement (overfitting) du mod√®le. √Ä l‚Äôinverse, un mod√®le trop rigide (par exemple, une droite constante) ne pourra pas capturer la structure des donn√©es, m√™me sur l‚Äôensemble d‚Äôentra√Ænement. On dira qu‚Äôil y a sous-ajustement (underfitting) du mod√®le.\nL‚Äôobjectif est de trouver le bon compromis entre flexibilit√© et capacit√© de g√©n√©ralisation.\n\n\nPour √©valuer objectivement un mod√®le, l‚Äôid√©al serait de le tester sur des donn√©es compl√®tement ind√©pendantes de celles utilis√© pour l‚Äôapprentissage. On distingue donc deux ensembles : un jeu d‚Äôentra√Ænement, utilis√© pour ajuster le mod√®le et un jeu de test, utilis√© pour √©valuer la performance pr√©dictive du mod√®le. En practique, nous n‚Äôavons g√©n√©ralement pas acc√®s √† un jeu de test pour faire cette √©valuation. Dans cette section, nous allons deux approches permettant de contourner ce probl√®me.",
    "crumbs": [
      "Modules",
      "03 - G√©n√©ralit√©s",
      "√âvaluation de mod√®les"
    ]
  },
  {
    "objectID": "contents/generalities/05-model-evaluation.html#jeu-de-donn√©es-de-validation",
    "href": "contents/generalities/05-model-evaluation.html#jeu-de-donn√©es-de-validation",
    "title": "√âvaluation de mod√®les",
    "section": "Jeu de donn√©es de validation",
    "text": "Jeu de donn√©es de validation\nQuand on ne dispose que d‚Äôun seul jeu de donn√©es, une solution simple consiste √† le diviser al√©atoirement en deux sous-ensembles : un jeu d‚Äôentra√Ænement pour ajuster le mod√®le et un jeu de validation pour estimer l‚Äôerreur de pr√©diction. On parle alors d‚Äôapproche par jeu de validation. La Figure¬†1 pr√©sente un sch√©ma de cette approche et la Figure¬†2 pr√©sente plusieurs combinaisons de jeux de donn√©es d‚Äôentra√Ænement et de validation, ainsi que les MSE associ√©es.\n\n\n\n\n\n\nFigure¬†1: Sch√©ma de l‚Äôapproche par jeu de validation.\n\n\n\n\n\nCode\n# Load packages\nlibrary(tidyverse)\nlibrary(dplyr)\n\n# Set seed for reproducibility\nset.seed(42)\n\n# Function definition\nf &lt;- function(x) {\n  4 * x * (1 - x) * log(x) + 2\n}\n\ngenerate_splits &lt;- function(data, train_prop = 0.7, K = 10) {\n  rows &lt;- list()\n  \n  for (k in 1:K) {\n    idx_train &lt;- sample(nrow(data), size = floor(train_prop * nrow(data)))\n    train &lt;- data[idx_train, ]\n    validation &lt;- data[-idx_train, ]\n    \n    train_tbl &lt;- tibble(\n      split = k,\n      set = \"train\",\n      x = train$x,\n      y = train$y\n    )\n    \n    validation_tbl &lt;- tibble(\n      split = k,\n      set = \"validation\",\n      x = validation$x,\n      y = validation$y\n    )\n    \n    rows[[k]] &lt;- bind_rows(train_tbl, validation_tbl)\n  }\n  \n  bind_rows(rows)\n}\n\n# Generate N data points in the interval (0, 1]\nN &lt;- 200\nspans &lt;- seq(0.1, 1, by = 0.05)  # LOESS smoothing parameters\nx_all &lt;- runif(N, min = 0.01, max = 1)  # avoid 0 due to log(x)\ny_all &lt;- f(x_all) + rnorm(N, mean = 0, sd = 0.1)  # add Gaussian noise\n\n# Put everything in a data.frame\ndata &lt;- data.frame(x = x_all, y = y_all)\n\n# Generate 10 training/validation splits\nsplits &lt;- generate_splits(data, train_prop = 0.7, K = 10)\nwrite.csv(splits, './splits.csv')\n\n\n# Fit a model with different smoothing parameters on each split\nsplits_number &lt;- unique(splits$split)\n\nresults &lt;- list()\nfor (n_split in splits_number) {\n  \n  df_split &lt;- splits |&gt; dplyr::filter(split == n_split)\n  train &lt;- df_split |&gt; \n    dplyr::filter(set == 'train') |&gt; \n    dplyr::select(x, y)\n  validation &lt;- df_split |&gt; \n    dplyr::filter(set == 'validation') |&gt; \n    dplyr::select(x, y)\n  \n  results_splits &lt;- list()\n  for (s in spans) {\n\n    # Fit loess model with span = s on train and predict on validation\n    model &lt;- loess(y ~ x, data = train, span = s, degree = 2)\n    pred &lt;- predict(model, newdata = validation$x)\n      \n    # For each point in x_grid, compute bias¬≤, variance, MSE\n    bias2 &lt;- mean((pred - validation$y)^2, na.rm = TRUE)\n    var_pred &lt;- var(pred, na.rm = TRUE)\n    mse &lt;- bias2 + var_pred\n    \n    results_splits[[as.character(s)]] &lt;- data.frame(\n      split = n_split,\n      span = s,\n      MSE = mse\n    )\n  }\n  results_splits_df &lt;- bind_rows(results_splits)\n  results[[as.character(n_split)]] &lt;- results_splits_df\n}\n\nresults &lt;- results |&gt; bind_rows()\nwrite.csv(results, './splits_mse.csv')\n\n\n\n\nCode\ndata = FileAttachment(\"../../include/data/model-evaluation/splits.csv\").csv({ typed: true })\ndata_mse = FileAttachment(\"../../include/data/model-evaluation/splits_mse.csv\").csv({ typed: true })\n\nviewof split = Inputs.range(\n  [1, 10], \n  {value: 1, step: 1, label: \"Autre jeu de validation\"}\n)\n\nfiltered = data.filter(function(df) {\n  return df.split == split;\n})\n\ncurrent_mse = data_mse.filter(function(df) {\n  return df.split == split;\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nPlot.plot({\n  grid: true,\n  x: {\n    domain: [0, 1],\n    label: \"X\",\n  },\n  y: {\n    domain: [0, 3],\n    label: \"Y\",\n  },\n  color: {\n    domain: [\"train\", \"validation\"],\n    range: [\"#f7a941\", \"#e30513\"],\n    legend: true\n  },\n  marks: [\n    Plot.dot(\n      filtered,\n      {x: \"x\", y: \"y\", fill: \"set\", r: 5}\n    ),\n  ]\n})\n\n\nCode\nPlot.plot({\n  grid: true,\n  color: {\n    legend: true,\n  },\n  x: {\n    domain: [0, 1],\n    label: \"Œª\",\n  },\n  y: {\n    domain: [0.1, 0.2],\n    label: \"Y\",\n  },\n  marks: [\n    Plot.line(\n      data_mse, \n      {\n        x: \"span\", y: \"MSE\", z: \"split\",\n        stroke: \"#AAAAAA\", strokeWidth: 2\n      }\n    ),\n    Plot.line(\n      current_mse,\n      {x: \"span\", y: \"MSE\", stroke: \"#e30513\", strokeWidth: 5}\n    ),\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Diff√©rents jeux d‚Äôentra√Ænement et validation.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) MSE pour diff√©rents jeux de validation.\n\n\n\n\n\n\nFigure¬†2: Illustration de l‚Äôapproche par jeu de donn√©es de validation.\n\n\n\n\n\n\n\n\n\n\nAstuceComment choisir la taille des sous-ensembles ?\n\n\n\nEn g√©n√©ral :\n\nSi l‚Äôon dispose d‚Äôun grand nombre d‚Äôobservations (disons plusieurs milliers), on peut faire une division \\(50-50\\).\nSi l‚Äôon dispose de moins d‚Äôobservations, on pr√©f√©rera garder plus d‚Äôobservations pour l‚Äôentra√Ænement. On peut, par exemple, faire une division \\(70-30\\) ou \\(80-20\\).\n\nCependant, il n‚Äôexiste pas de r√®gle universelle. Le bon choix d√©pend du contexte, de la complexit√© du mod√®le et de la quantit√© de donn√©es disponibles.\n\n\nLa m√©thode a cependant deux inconv√©nients. Le premier est que l‚Äôestimation de l‚Äôerreur est instable. En effet, la valeur de l‚Äôerreur de pr√©diction d√©pend des observations qui sont dans le jeu de validation. Un autre jeu de validation peut donner un r√©sultat diff√©rent. Le deuxi√®me est qu‚Äôil y a moins de donn√©es pour ajuster le mod√®le. Comme une partie des donn√©es est r√©serv√©e √† la validation, le mod√®le est appris sur un ensemble plus petit, et cela peut donc surestimer son erreur r√©elle par rapport √† s‚Äôil avait √©t√© appris sur l‚Äôensemble de donn√©es complet.",
    "crumbs": [
      "Modules",
      "03 - G√©n√©ralit√©s",
      "√âvaluation de mod√®les"
    ]
  },
  {
    "objectID": "contents/generalities/05-model-evaluation.html#validation-crois√©e",
    "href": "contents/generalities/05-model-evaluation.html#validation-crois√©e",
    "title": "√âvaluation de mod√®les",
    "section": "Validation crois√©e",
    "text": "Validation crois√©e\nPour contourner les limites de l‚Äôapproche pr√©c√©dente, on utilise souvent la validation crois√©e (cross-validation). Cette m√©thode est plus robuste et plus stable. Le principe est de r√©p√©ter l‚Äôapproche par jeu de validation plusieurs fois sur diff√©rents sous-ensembles du jeu de donn√©es.\nL‚Äôapproche consiste √† d√©couper al√©atoirement l‚Äôensemble des observations en \\(K\\) sous-ensembles de taille √©quivalentes (appel√©s folds). Le premier fold est utilis√© comme jeu de donn√©es de validation et le mod√®le est ajust√© sur les \\(K - 1\\) folds restant. L‚Äôerreur de pr√©diction est calcul√© sur le premier fold. Cette proc√©dure est r√©p√®t√© \\(K\\) fois; √† chaque fois, un diff√©rent fold est utilis√© comme jeu de donn√©es de validation. √Ä la fin, on a donc \\(K\\) valeurs pour l‚Äôerreur de pr√©diction. On calcule enfin la moyenne des \\(K\\) valeurs de pr√©diction. La Figure¬†3 pr√©sente un sch√©ma de cette approche.\n\n\n\n\n\n\nFigure¬†3: Sch√©ma de l‚Äôapproche par validation crois√©e\n\n\n\n\n\n\n\n\n\nAstuceComment choisir le nombre de sous-ensembles \\(K\\) ?\n\n\n\nLe choix du nombre de sous-ensembles \\(K\\) a un impact sur la qualit√© de l‚Äôestimation de l‚Äôerreur de pr√©diction, ainsi que sur le co√ªt computationnel de la proc√©dure. En pratique, on utilise souvent \\(K = 5\\) ou \\(K = 10\\). Ce choix repose sur un compromis entre la pr√©cision de l‚Äôestimation de l‚Äôerreur et le temps de calcul n√©cessaire. En effet, pour chaque valeur de \\(K\\), le mod√®le est ajust√© \\(K\\) fois. Par cons√©quent, plus \\(K\\) est grand, plus le co√ªt computationnel augmente.\nDans le cas limite o√π \\(K = n\\), i.e.¬†\\(K\\) est √©gal au nombre d‚Äôobservations dans le jeu de donn√©es, on parle de validation crois√©e leave-one-out (LOOCV). Dans ce cas, chaque observation sert une fois de validation et le mod√®le est entra√Æner sur les \\(n - 1\\) autres observations.\nCe choix de \\(K = n\\) minimise le biais dans l‚Äôestimation de l‚Äôerreur de pr√©diction, car √† chaque it√©ration, le mod√®le est ajust√© sur presque toutes les observations du jeu de donn√©es. Cependant, cela se fait au prix d‚Äôune forte variance. En effet, comme les ensembles d‚Äôentra√Ænement sont presque identiques, les erreurs de pr√©diction sont tr√®s corr√©l√©es entre elles, ce qui rend l‚Äôestimation globale de l‚Äôerreur instable.\nInversement, des valeurs plus faibles de \\(K\\) introduisent un l√©ger biais dans l‚Äôestimation de l‚Äôerreur (car les mod√®les sont ajust√©s sur des ensembles contenant moins d‚Äôobservations), mais r√©duisent la variance de cette estimation. Ce compromis biais/variance, coupl√© √† une r√©duction significative du temps de calcul, explique pourquo \\(K = 5\\) ou \\(K = 10\\) sont des choix standards en pratique.\n\n\nPour finir, la validation crois√©e est une m√©thode g√©n√©rale qui peut √™tre appliqu√©e avec la plupart des mod√®les.",
    "crumbs": [
      "Modules",
      "03 - G√©n√©ralit√©s",
      "√âvaluation de mod√®les"
    ]
  },
  {
    "objectID": "contents/remainders/01-linear-algebra.html",
    "href": "contents/remainders/01-linear-algebra.html",
    "title": "Alg√®bre lin√©aire",
    "section": "",
    "text": "Dans cette partie, on pr√©sente quelques r√©sultats d‚Äôalg√®bre lin√©aire utiles dans le cadre de ce cours. Pour plus d‚Äôinformation, vous pouvez vous r√©f√©rer au cours MAT-1200, √† Deisenroth, Faisal, et Ong (2020) (en anglais) et √† Grifone (2024) (en fran√ßais).",
    "crumbs": [
      "Modules",
      "02 - R√©visions",
      "Alg√®bre lin√©aire"
    ]
  },
  {
    "objectID": "contents/remainders/01-linear-algebra.html#quelques-propri√©t√©s-matricielles",
    "href": "contents/remainders/01-linear-algebra.html#quelques-propri√©t√©s-matricielles",
    "title": "Alg√®bre lin√©aire",
    "section": "Quelques propri√©t√©s matricielles",
    "text": "Quelques propri√©t√©s matricielles\nNotons \\(M_{n, m}(\\mathbb{R})\\), l‚Äôensemble des matrices √† \\(n\\) lignes et \\(m\\) colonnes dont les entr√©es appartiennent √† \\(\\mathbb{R}\\). Notons \\(M_{n}(\\mathbb{R})\\), l‚Äôensemble des matrices carr√©es de taille \\(n\\), i.e.¬†√† \\(n\\) lignes et \\(n\\) colonnes dont les entr√©es appartiennent √† \\(\\mathbb{R}\\). Soient \\(M\\), \\(N\\) et \\(P\\) des matrices appartenant √† \\(M_{n, m}(\\mathbb{R})\\). Soient \\(A\\) et \\(B\\) des matrices appartenant √† \\(M_{n}(\\mathbb{R})\\). Notons \\(I_n\\) la matrice identit√© de taille \\(n\\), i.e.¬†qui contient des \\(1\\) sur le diagonale et des \\(0\\) sur les √©l√©ments hors de la diagonale. Soient \\(u\\) et \\(v\\) appartenant √† \\(\\mathbb{R}^n\\), i.e.¬†des vecteurs colonnes de taille \\(n\\).\n\n\n\n\n\n\nImportantPropri√©t√©s de l‚Äôinverse de matrices\n\n\n\nSupposons que les matrices \\(A\\) et \\(B\\) soient inversibles. Alors le produit matriciel \\(AB\\) est inversible et est donn√© par:\n\\[(AB)^{-1} = B^{-1} A^{-1}.\\]\n\n\n\n\n\n\n\n\nNotePreuve\n\n\n\n\n\nPosons \\(C = AB\\) et \\(D = B^{-1} A^{-1}\\). Alors\n\\[\\begin{align*}\n  CD &= A B B^{-1} A^{-1} \\\\\n     &= A A^{-1} \\\\\n     &= I_n\n\\end{align*}\\]\nDe la m√™me fa√ßon, on trouve que \\(DC = I_n\\). Ainsi, \\(AB\\) est inversible et son inverse est donn√© par \\(B^{-1} A^{-1}\\).\n\n\n\n\n\n\n\n\n\nImportantPropri√©t√©s du d√©terminant de matrices\n\n\n\nConsid√©rant les matrices d√©finies en d√©but de section, on a :\n\n\\(\\text{det}(A^\\top) = \\text{det}(A)\\),\n\\(\\text{det}(AB) = \\text{det}(A)\\text{det}(B)\\),\n\\(\\text{det}(A^{-1}) = 1 / \\text{det}(A)\\).\n\n\n\n\n\n\n\n\n\nNotePreuve\n\n\n\n\n\nLes preuves des propri√©t√©s \\(1\\) et \\(2\\) sont techniques et sont omises, mais peuvent √™tre trouv√©es, par exemple, ici. Pour ce qui est de la troisi√®me propri√©t√©, par d√©finition, on a \\(A A^{-1} = I_n\\). Le d√©terminant de \\(I_n\\) est √©gale √† \\(1\\) (produit des √©l√©ments sur la diagonale). Donc \\(\\text{det}(A A^{-1}) = 1\\). Or, d‚Äôapr√®s la deuxi√®me propri√©t√©, \\(\\text{det}(A A^{-1}) = \\text{det}(A)\\text{det}(A^{-1})\\). On a donc bien \\(\\text{det}(A^{-1}) = 1 / \\text{det}(A)\\).\n\n\n\n\n\n\n\n\n\nImportantPropri√©t√©s de la trace de matrices\n\n\n\nConsid√©rant les matrices d√©finies en d√©but de section, on a :\n\n\\(\\text{tr}(A) = \\text{tr}(A^{\\top})\\),\n\\(\\text{tr}(A + B) = \\text{tr}(A) + \\text{tr}(B)\\),\n\\(\\text{tr}(MN^{\\top}) = \\text{tr}(N^{\\top}M)\\).\n\n\n\n\n\n\n\n\n\nNotePreuve\n\n\n\n\n\nPour une matrice carr√© \\(A\\), notons \\(a_{ij}\\), l‚Äô√©l√©ment de la matrice \\(A\\) √† la ligne \\(i\\) et √† la colonne \\(j\\). La trace de \\(A\\) est donn√©e par la somme des √©l√©ments diagonaux, i.e.¬†\\(\\text{tr}(A) = \\sum_{i = 1}^{n} a_{ii}\\).\n\nLa transposition ne changeant pas les √©l√©ments diagonaux, le r√©sultat est direct.\nNotons \\(C = A + B\\). Comme \\(A\\) et \\(B\\) sont des matrices carr√©es, \\(C\\) est une matrice carr√©e. On a \\(c_{ij} = a_{ij} + b_{ij}\\) pour tout \\(i, j = 1, \\dots, n\\). Donc \\[\\text{tr}(A + B) = \\text{tr}(C) = \\sum_{i = 1}^{n} c_{ii} = \\sum_{i = 1}^{n} a_{ii} + b_{ii} = \\sum_{i = 1}^{n} a_{ii} + \\sum_{i = 1}^{n} b_{ii} = \\text{tr}(A) + \\text{tr}(B).\\]\nLes matrices \\(M N^{\\top}\\) et \\(N^{\\top} M\\) sont carr√©es, de dimension respectives \\(n \\times n\\) et \\(m \\times m\\), on peut donc bien calculer leur trace. Notons \\(C = M N^{\\top}\\) et \\(D = N^{\\top} M\\). \\[\\text{tr}(M N^{\\top}) = \\text{tr}(C) = \\sum_{i = 1}^{n} c_{ii} = \\sum_{i = 1}^{n} \\sum_{j = 1}^{m} m_{ij} n_{ji} = \\sum_{j = 1}^{m} \\sum_{i = 1}^{n} n_{ji} m_{ij} = \\sum_{j = 1}^{m} d_{jj} = \\text{tr}(D)  = \\text{tr}(N^{\\top} M).\\]\n\n\n\n\n\n\n\n\n\n\nAvertissementD√©finition\n\n\n\n\nSoit \\(A\\) une matrice sym√©trique appartenant √† \\(M_n(\\mathbb{R})\\). \\(A\\) est d√©finie positive si \\(u^\\top A u &gt; 0\\) pour tout \\(u \\in \\mathbb{R}^n\\) tel que \\(u \\neq 0\\).\nSoit \\(A\\) appartenant √† \\(M_n(\\mathbb{R})\\). \\(A\\) est orthogonal si \\(A^\\top A = A A^\\top = I_n\\).",
    "crumbs": [
      "Modules",
      "02 - R√©visions",
      "Alg√®bre lin√©aire"
    ]
  },
  {
    "objectID": "contents/remainders/01-linear-algebra.html#valeurs-et-vecteurs-propres",
    "href": "contents/remainders/01-linear-algebra.html#valeurs-et-vecteurs-propres",
    "title": "Alg√®bre lin√©aire",
    "section": "Valeurs et vecteurs propres",
    "text": "Valeurs et vecteurs propres\n\n\n\n\n\n\nAvertissementD√©finition\n\n\n\nSoit \\(A\\) appartenant √† \\(M_n(\\mathbb{R})\\). On dit que \\(\\lambda \\in \\mathbb{R}\\) est une valeur propre de \\(A\\) s‚Äôil existe un vecteur \\(u \\in \\mathbb{R}^n\\) non nul tel que \\[Au = \\lambda u. \\tag{1}\\] Le vecteur \\(u\\) est appel√© vecteur propre de \\(A\\) correspondant √† la valeur propre \\(\\lambda\\).\nL‚Äôensemble des nombres r√©els \\(\\lambda\\) satisfaisant √âquation¬†1 est appel√© spectre de la matrice \\(A\\) et not√© \\(\\text{sp}(A)\\).\n\n\n\n\n\n\n\n\nImportantPropri√©t√© des vecteurs propres\n\n\n\n\nSi \\(u\\) est un vecteur propre de \\(A\\) correspondant √† une valeur propre \\(\\lambda\\), alors le vecteur \\(cu\\), \\(c \\in \\mathbb{R}^\\star\\) est √©galement un vecteur propre de \\(A\\) correspondant √† \\(\\lambda\\).\nSi \\(A\\) est sym√©trique et \\(u_{1}\\) et \\(u_{2}\\) sont des vecteurs propres correspondant √† des valeurs propres diff√©rentes de \\(A\\), alors \\(u_{1}\\) et \\(u_{2}\\) sont orthogonaux, i.e.¬†\\(u_{1}^\\top u_{2} = 0\\).\n\n\n\n\n\n\n\n\n\nNotePreuve\n\n\n\n\n\n\nSoit \\(c \\in \\mathbb{R}^\\star\\) et \\(u\\) un vecteur propre de \\(A\\) associ√© √† la valeur propre \\(\\lambda\\). On a : \\[A(cu) = cAu = c \\lambda u = \\lambda (cu).\\] Donc, le vecteur \\(cu\\) est aussi vecteur propre de \\(A\\) associ√© √† la valeur propre \\(\\lambda\\).\nSoient \\(\\lambda_{1}\\) et \\(\\lambda_{2}\\), les valeurs propres associ√©es √† \\(u_{1}\\) et \\(u_{2}\\), tel que \\(\\lambda_{1} \\neq \\lambda_{2}\\). On a \\(A u_{1} = \\lambda_{1} u_{1}\\) et \\(A u_{2} = \\lambda_{2} u_{2}\\). Ensuite \\[\\lambda_{1} u_{1}^{\\top} u_{2} = u_{1}^\\top A u_{2} = \\lambda_{2} u_{1}^\\top u_{2}.\\] Cela implique que \\((\\lambda_{1} - \\lambda_{2})u_{1}^\\top u_{2} = 0\\). Or, \\(\\lambda_{1} \\neq \\lambda_{2}\\). Donc, n√©cessairement, \\(u_{1}^\\top u_{2} = 0\\).\n\n\n\n\nCette deuxi√®me propri√©t√© nous sera utile lorque l‚Äôon s‚Äôint√©ressera √† la r√©duction de dimension et, en particulier, √† l‚Äôanalyse en composantes principales.\n\n\n\n\n\n\nImportantCaract√©risation de matrices avec ses √©l√©ments propres\n\n\n\n\nSi \\(A\\) est sym√©trique, alors toutes ses valeurs propres sont r√©elles.\nSi \\(A\\) est d√©finie positive, alors toutes ses valeurs propres sont strictement positives.\n\n\n\n\n\n\n\n\n\nNotePreuve\n\n\n\n\n\n\nConsid√©rons le cas plus g√©n√©ral o√π \\(A\\) est une matrice hermitenne. La matrice \\(A\\) est √©gale la transpos√© de son conjugu√©, not√© \\(A^*\\). Notons \\(\\lambda\\) une valeur propre associ√©e √† un vecteur propre \\(u\\), √©ventuellement complexe. On a : \\[\\begin{align}\n\\overline{u}^{\\top} A u &= \\overline{u}^\\top \\lambda u = \\lambda \\overline{u}^{\\top} u, \\\\\n\\overline{u}^\\top A u &= \\overline{u}^\\top A^* u = \\overline{Au}^\\top u = \\overline{\\lambda} \\overline{u}^\\top u.\n\\end{align}\\] Cela implique que \\((\\lambda - \\overline{\\lambda}) \\overline{u}^{\\top} u = 0\\). Comme \\(u \\neq 0\\), on a \\(\\lambda = \\overline{\\lambda}\\). Donc \\(\\lambda \\in \\mathbb{R}\\).\nConsid√©rons \\(u\\), vecteur propre de \\(A\\) associ√© √† la valeur propre \\(\\lambda\\). On a que \\(u^{\\top} A u = \\lambda u^{\\top} u\\). Or, comme \\(u \\neq 0\\), \\(u^{\\top}u \\neq 0\\). Donc \\[\\lambda = \\frac{u^{\\top} A u}{u^{\\top} u}.\\] Comme \\(A\\) est d√©finie postive, \\(u^{\\top} A u &gt; 0\\) pour tout vecteur \\(u\\) non nul. On en d√©duit que \\(\\lambda &gt; 0\\).",
    "crumbs": [
      "Modules",
      "02 - R√©visions",
      "Alg√®bre lin√©aire"
    ]
  },
  {
    "objectID": "contents/remainders/01-linear-algebra.html#diagonalisation-de-matrices",
    "href": "contents/remainders/01-linear-algebra.html#diagonalisation-de-matrices",
    "title": "Alg√®bre lin√©aire",
    "section": "Diagonalisation de matrices",
    "text": "Diagonalisation de matrices\n\n\n\n\n\n\nAvertissementD√©finition\n\n\n\nSoit \\(A\\) appartenant √† \\(M_n(\\mathbb{R})\\). On dit que \\(A\\) est diagonalisable s‚Äôil existe une matrice \\(P\\) appartenant √† \\(M_n(\\mathbb{R})\\) non-singuli√®re et une matrice diagonale \\(D\\) appartenant √† \\(M_n(\\mathbb{R})\\) telles que \\[P^{-1} A P = D \\Longleftrightarrow A = P D P^{-1}.\\]\n\n\n\n\n\n\n\n\nMise en gardeTh√©or√®me de d√©composition spectrale\n\n\n\nSoit \\(A\\) une matrice symm√©trique appartenant √† \\(M_n(\\mathbb{R})\\) et \\(\\lambda_{1}, \\dots, \\lambda_n\\), ses \\(n\\) valeurs propres. Alors, il existe une matrice orthogonal \\(P\\) appartenant √† \\(M_n(\\mathbb{R})\\) telle que \\[A = P \\Lambda P^\\top, \\quad\\text{o√π}\\quad \\Lambda = \\text{diag}(\\lambda_1, \\dots, \\lambda_n).\\]\n\n\nSi \\(A\\) admet \\(n\\) valeurs propres positives distinctes, alors on peut prendre \\(P\\) comme √©tant la matrice dont la \\(k\\)-i√®me colonne est le vecteur propre norm√© correspondant √† la \\(k\\)-i√®me valeur propre \\(\\lambda_k\\) de \\(A\\).\nSoit deux matrices sym√©triques, \\(A\\) et \\(B\\), comment d√©terminer le vecteur \\(u\\) tel que \\(u^{\\top} A u\\) soit maximal, sachant que \\(u^{\\top} B u = 1\\) ? Il suffit de prendre \\(u\\) comme le vecteur propre de \\(B^{-1}A\\) associ√© √† \\(\\lambda\\) la valeur propre maximale de \\(B^{-1}A\\). On obtient ainsi \\[u^{\\top} A u = u^{\\top}\\lambda M u = \\lambda U^{\\top} M u = \\lambda.\\]\n\n\n\n\n\n\nImportantCaract√©risation du d√©terminant et de la trace de matrices avec ses √©l√©ments propres\n\n\n\nSi \\(A\\) a comme valeurs propres (r√©elles, mais pas forc√©ment distinctes) \\(\\lambda_{1}, \\dots, \\lambda_{n}\\), alors\n\n\\(\\text{det}(A) = \\prod_{i = 1}^{n} \\lambda_i\\)\n\\(\\text{tr}(A) = \\sum_{i = 1}^{n} \\lambda_i.\\)\n\n\n\n\n\n\n\n\n\nNotePreuve\n\n\n\n\n\nEn utilisant le th√©or√®me de d√©composition spectrale, il existe une matrice \\(P\\) inversible tel que \\(A = P \\Lambda P^{-1}\\), o√π \\(\\Lambda\\) est une matrice diagonale contenant les valeurs propres. On a donc, pour le d√©terminant,\n\\[\\text{det}(A) = \\text{det}(P \\Lambda P^{-1}) = \\text{det}(P)\\text{det}(\\Lambda)\\text{det}(P^{-1}) = \\text{det}(P)\\text{det}(\\Lambda)\\text{det}(P)^{-1} = \\text{det}(\\lambda) = \\prod_{i = 1}^{n} \\lambda_i, \\]\net, pour la trace,\n\\[\\text{tr}(A) = \\text{tr}(P \\Lambda P^{-1}) = \\text{tr}(P^{-1} P \\Lambda) = \\text{tr}(\\Lambda) = \\sum_{i = 1}^{n} \\lambda_i.\\]",
    "crumbs": [
      "Modules",
      "02 - R√©visions",
      "Alg√®bre lin√©aire"
    ]
  },
  {
    "objectID": "contents/remainders/03-programming.html",
    "href": "contents/remainders/03-programming.html",
    "title": "Programmation",
    "section": "",
    "text": "Dans cette partie, on pr√©sente quelques r√©f√©rences sur les bases de la programmation dans diff√©rents langages. Ce cours ne sp√©cifie pas de langage √† utiliser, vous √™tes donc libre de choisir celui qui vous convient le mieux pour faire les diff√©rents exercices, ainsi que votre projet. Dans tous les cas, il y a un ensemble de bonnes pratiques qui permet d‚Äôavoir un code lisible, compr√©hensible et r√©utilisable. Vous trouvez un (rapide) guide des bonnes pratiques en programmation √† ce lien.",
    "crumbs": [
      "Modules",
      "02 - R√©visions",
      "Programmation"
    ]
  },
  {
    "objectID": "contents/remainders/03-programming.html#r",
    "href": "contents/remainders/03-programming.html#r",
    "title": "Programmation",
    "section": "R",
    "text": "R\nR est un langage de programmation sp√©cialis√© dans l‚Äôanalyse statistique et la visualisation de donn√©es. Il est gratuit, open-source et disponible sur Windows, macOS et Linux. R√©guli√®rement mis √† jour, de nouveaux packages sont disponibles chaque jour (cf.¬†CRAN).\nVoici un guide qui pr√©sente les bases de R: Apprendre R en Y minutes et un guide de bonnes pratiques en R. Enfin, un point important du langage R est l‚Äôop√©rateur pipe. Celui-ci permet de rendre le code plus clair. Vous pouvez trouver une explication de cette op√©rateur ici.",
    "crumbs": [
      "Modules",
      "02 - R√©visions",
      "Programmation"
    ]
  },
  {
    "objectID": "contents/remainders/03-programming.html#python",
    "href": "contents/remainders/03-programming.html#python",
    "title": "Programmation",
    "section": "Python",
    "text": "Python\nPython est un langage de programmation g√©n√©raliste. Il est gratuit, open-source et disponible sur Windows, macOS et Linux. Bien que g√©n√©raliste, il y a une importante communaut√© autour du l‚Äôanalyse de donn√©es, machine learning en Python. Ainsi, les m√©thodes usuelles ont d√©j√† √©t√© implement√©es, e.g.¬†dans le package sklearn, mais si besoin, il est possible d‚Äôaller voir le code pour le modifier.\nVoici un guide qui pr√©sente les bases de Python: Apprendre Python en Y minutes et un guide de bonnes pratiques en Python.",
    "crumbs": [
      "Modules",
      "02 - R√©visions",
      "Programmation"
    ]
  },
  {
    "objectID": "contents/remainders/03-programming.html#julia",
    "href": "contents/remainders/03-programming.html#julia",
    "title": "Programmation",
    "section": "Julia",
    "text": "Julia\nJulia est aussi un langage de programmation g√©n√©raliste. C‚Äôest un langage bien plus r√©cent que ses deux comp√®res, R et Python (d√©but des ann√©es 2010 pour Julia vs.¬†d√©but des ann√©es 1990 pour R et Python). Il peut donc sembler moins mature sur certains points, mais la plupart des m√©thodes d‚Äôanalyse de donn√©es classiques ont √©t√© impl√©ment√©es. De m√™me que R et Python, ce langage est gratuit, open-source et disponible sur les principaux syst√®mes d‚Äôexploitation. Julia a l‚Äôavantage d‚Äô√™tre plus rapide que R et Python. Il inclut aussi un support natif d‚Äôappel √† des librairies en C ou en Fortran, et un support non-natif d‚Äôappel √† des librairies en R et Python.\nVoici un guide qui pr√©sente les bases de Julia: Learn Julia in Y minutes (il n‚Äôy a pas de version fran√ßaise pour l‚Äôinstant) et un guide de bonnes pratiques en Julia (en anglais).",
    "crumbs": [
      "Modules",
      "02 - R√©visions",
      "Programmation"
    ]
  },
  {
    "objectID": "contents/remainders/03-programming.html#sas",
    "href": "contents/remainders/03-programming.html#sas",
    "title": "Programmation",
    "section": "SAS",
    "text": "SAS\nSAS est un langage de programmation propri√©taire sp√©cialis√© dans l‚Äôanalyse de donn√©es. Bien qu‚Äôutilis√© dans certaines industries, e.g.¬†pharmaceutique, son utilisation tend √† diminuer. De plus, la license √† renouveler tous les ans, l‚Äôabsence de communaut√© en ligne (pour avoir de l‚Äôaide, √† part la documentation officielle, il n‚Äôy a pas grand chose) et l‚Äôimpossibilit√© de voir le code des diff√©rentes proc√©dures font que je d√©conseille l‚Äôutilisation de SAS dans le cadre de ce cours. Cependant, si vous le voulez, l‚Äôuniversit√© peut fournir des licenses moyennant paiement.",
    "crumbs": [
      "Modules",
      "02 - R√©visions",
      "Programmation"
    ]
  },
  {
    "objectID": "contents/04-dimension.html",
    "href": "contents/04-dimension.html",
    "title": "Dimension",
    "section": "",
    "text": "Slides:\n\nAnalyse en composantes principales\nAnalyse factorielle des correspondances\nAnalyse des correspondances multiples\n\nTD: link\nTP: link",
    "crumbs": [
      "Modules",
      "04 - Dimension"
    ]
  },
  {
    "objectID": "contents/04-dimension.html#sommaire",
    "href": "contents/04-dimension.html#sommaire",
    "title": "Dimension",
    "section": "Sommaire",
    "text": "Sommaire\n\nAnalyse en composantes principales\nAnalyse factorielle des correspondances\nAnalyse des correspondances multiples\n\n\n\n\nFlatland (xkcd:721).",
    "crumbs": [
      "Modules",
      "04 - Dimension"
    ]
  },
  {
    "objectID": "contents/03-generalities.html",
    "href": "contents/03-generalities.html",
    "title": "G√©n√©ralit√©s",
    "section": "",
    "text": "Slides:\n\nProjet d‚Äôanalyse de donn√©es\nEspaces\nDistances - annot√©\nBiais / Variance - annot√©\n√âvaluation de mod√®les\n\nTD: link\nTP: link",
    "crumbs": [
      "Modules",
      "03 - G√©n√©ralit√©s"
    ]
  },
  {
    "objectID": "contents/03-generalities.html#sommaire",
    "href": "contents/03-generalities.html#sommaire",
    "title": "G√©n√©ralit√©s",
    "section": "Sommaire",
    "text": "Sommaire\n\nProjet d‚Äôanalyse de donn√©es\nEspaces\nDistances\nBiais / Variance\n√âvaluation de mod√®les\n\n\n\n\nCorrelation (xkcd:552).",
    "crumbs": [
      "Modules",
      "03 - G√©n√©ralit√©s"
    ]
  },
  {
    "objectID": "contents/misc/pipe.html",
    "href": "contents/misc/pipe.html",
    "title": "Op√©rateur pipe en R",
    "section": "",
    "text": "Cette page est bas√© sur un document qu‚ÄôAur√©lien Nicosia (ULaval) a cr√©√© en 2023 appel√© ‚ÄúOp√©rateur pipe‚Äù. Celui-ci a √©t√© mis √† jour.\nDepuis la version 4.1.0 de R, sorti en mai 2021, le langage a introduit l‚Äôop√©rateur pipe |&gt; en s‚Äôinspirant de ce que faisait le package magrittr. √Ä partir de R 4.3.0, le guide de style du tidyverse recommande l‚Äôutilisation de l‚Äôop√©rateur pipe de base, et non celui de magrittr.\n\n\n\n\n\n\nNoteRaccourcis clavier\n\n\n\nDans RStudio, le raccourci clavier pour ins√©rer l‚Äôop√©rateur pipe est:\n\nSous Windows : Ctrl + Shift + M\nSous macOS : ‚åò + ‚áß + M\n\n\n\nCet op√©rateur introduit une fa√ßon d‚Äôencha√Æner les intructions et de passer des arguments √† des fonctions de mani√®re plus lisible que la mani√®re classique.\nPour r√©sumer le fonctionnement de cette op√©rateur, voici comment il transforme quelques appels de fonctions:\n\nf(x) devient x |&gt; f();\nf(x, y) devient x |&gt; f(y);\nh(g(f(x))) devient x |&gt; f() |&gt; g() |&gt; h().\n\nCette op√©rateur permet de mettre en avant la s√©quence d‚Äôactions et non l‚Äôobjet sur lequel la s√©quence d‚Äôactions est faite. Cela rend le code plus lisible (et avoir un code lisible est une bonne pratique). En lisant de gauche √† droite l‚Äôinstruction h(g(f(x))), nous voyons d‚Äôabord l‚Äôappel √† la fonction h, puis l‚Äôappel √† la fonction g et finalement l‚Äôappel √† la focntion f. Pourtant, l‚Äô√©valuation de cette instruction se fait dans le sens inverse. En effet, R va d‚Äôabord :\n\n√©valuer f(x);\npuis, il passera le r√©sultat √† la fonction g et retournera le r√©sultat;\nqui sera pass√© √† la fonction h et le r√©sultat final sera retourn√©.\n\nSi nous voulons √©crire un code qui refl√®te l‚Äôordre des √©valuations correctement, nous pourrions √©crire :\n\nres1 &lt;- f(x)\nres2 &lt;- g(res1)\nh(res2)\n\nCe code a cependant le d√©faut de cr√©er des objects que nous souhaitons pas n√©cessairement conserver. L‚Äôop√©rateur |&gt; n‚Äôa pas ce d√©faut ! En effet, une instruction √©crite en utilisant l‚Äôop√©rateur |&gt; permet de suivre l‚Äôordre des √©valuations, sans cr√©er d‚Äôobjets inutilement en m√©moire.\nPour encore plus de clart√©, il est possible d‚Äô√©tendre sur plusieurs lignes un instruction contenant plusieurs op√©rateur |&gt; de fa√ßon √† avoir une fonction par ligne :\n\nx |&gt; \n  f() |&gt; \n  g() |&gt; \n  h()\n\nSi l‚Äôargument que nous souhaitons passer avec l‚Äôop√©rateur |&gt; n‚Äôest pas celui en premi√®re position, il faut utiliser _ comme suit avec un param√®tre nomm√©: f(y, z = x) devient x |&gt; f(y, z = _).\nPrenons un exemple pour illustrer l‚Äôutilisation de l‚Äôop√©rateur |&gt;. Supposons que nous avons la cha√Æne de caract√®res suivantes :\n\ntext &lt;- \"Ceci est un example\"\n\net que nous souhaitons la corriger‚Äîremplacer ‚Äúexample‚Äù par ‚Äúexemple‚Äù et ajouter un point √† la fin‚Äîavec l‚Äôintruction suivante :\n\npaste0(gsub(pattern = \"example\", replacement = \"exemple\", x = text), \".\")\n\n[1] \"Ceci est un exemple.\"\n\n\nCette instruction est un peu difficile √† lire en raison de l‚Äôappel √† la fonction gsub imbriqu√© dans un appel de fontion paste0. Nous pourrions la r√©√©crire comme suit avec l‚Äôop√©rateur |&gt; :\n\ntext |&gt; \n  gsub(pattern = \"example\", replacement = \"exemple\", x = _) |&gt; \n  paste0(\".\")\n\n[1] \"Ceci est un exemple.\"\n\n\nPrenons un autre exemple num√©rique. On souhaite faire le calcul suivant : \\[\\frac{(2 + 4) \\times 8}{2}.\\]\nPour cela, nous avons besoin de quelques fonctions math√©matiques.\n\nadd &lt;- function(x, y) {\n  x + y\n}\n\nmul &lt;- function(x, y) {\n  x * y\n}\n\ndiv &lt;- function(x, y) {\n  x / y\n}\n\nOn peut faire le calcul de trois mani√®res diff√©rentes:\n\n# En cr√©ant diff√©rents objets \nres1 &lt;- add(2, 4)\nres2 &lt;- mul(res1, 8)\nres3 &lt;- div(res2, 2)\nprint(res3)\n\n[1] 24\n\n# En imbriquant les fonctions\nres &lt;- div(mul(add(2, 4), 8), 2)\nprint(res)\n\n[1] 24\n\n# Avec l'op√©rateur pipe \nres &lt;- 2 |&gt;\n  add(4) |&gt; \n  mul(8) |&gt; \n  div(2)\nprint(res)\n\n[1] 24"
  },
  {
    "objectID": "contents/misc/good-practices-R.html",
    "href": "contents/misc/good-practices-R.html",
    "title": "Bonnes pratiques en R",
    "section": "",
    "text": "Cette page est bas√©e sur un document qu‚ÄôAur√©lien Nicosia (ULaval) a cr√©√© en 2023 appel√© ‚ÄúBonnes pratiques de programmation en R‚Äù. Celui-ci a √©t√© mis √† jour.\nR √©tant ‚Äújuste‚Äù un langage de programmation, il est techniquement possible d‚Äôutiliser un simple √©diteur de texte pour √©crire du R et un terminal pour lancer le code. Il est cependant bien plus commode‚Äîen particulier, pour les d√©butants‚Äîd‚Äôutiliser un √©diteur de code comme RStudio pour lancer son code. En effet, celui-ci permet de lancer son code de fa√ßon int√©ractive, de pouvoir voir son environnement de travail, de tester son code, ‚Ä¶\nDe plus, il vaut mieux s‚Äôassurer de travailler avec la derni√®re version de R et des packages dont nous avons besoin. Ainsi, nous risquons moins de rencontrer des bogues et nous pouvons profiter des derni√®res fonctionnalit√©s. Un point d‚Äôattention, cependant, je vous d√©conseillerais de changer la version de R ou des packages une fois que vous avez commenc√© √† travailler sur quelquechose. En effet, ce changement pourrais introduire des bogues au milieu de l‚Äôanalyse (il faut donc toujours reporter les versions des packages utilis√©s pour la reproducibilit√©). Il est possible d‚Äôavoir un r√©sum√© de l‚Äôenvironnement de travail avec les fonctions systemInfo() et packageVersion(pkg).\nConcernant la documentation, le package roxygen permet de convertir les commentaires de vos fonctions et packages en une documentation propre.\nUne syntaxe uniforme rend un code beaucoup plus facile √† lire et √† comprendre. Un guide de style √©nonce des normes pour avoir une syntaxe uniforme. Le guide de style du tidyverse est recommand√© en R. Avant de pr√©senter quelques conventions de style, notons qu‚Äôil est possible de modifier rapidement la mise en forme d‚Äôun bout de code R dans RStudio dans le menu ‚ÄúCode -&gt; Reformat Selection‚Äù.\nRetour √† la ligne et indentation\nUne fa√ßon simple de rendre som code plus lisible est d‚Äôy ins√©rer des retours √† la ligne et des indentations appropri√©s. Par exemple, supposons que nous avons la cha√Æne de caract√®re suivante :\n\ntext &lt;- \"Ceci est un example\"\n\nNous souhaitons corriger deux fautes dans cette phrase : le mot ‚Äúexample‚Äù √©crit en anglais plut√¥t qu‚Äôen fran√ßais et le point manquant √† la fin de la phrase. Ceci peut se faire avec l‚Äôinstruction :\n\npaste0(gsub(pattern = \"example\", replacement = \"exemple\", x = text), \".\")\n\nCette instruction comporte un appel de fonction imbriqu√© dans un autre. Elle est bien plus facile √† lire comme suit :\n\npaste0(\n  gsub(\n    pattern = \"example\",\n    replacement = \"exemple\",\n    x = text),\n  \".\"\n)\n\nOp√©rateur d‚Äôassignation\nEn R, on utilise &lt;- pour assigner une valeur √† une variable et on utilise = pour passer des valeurs √† des arguments dans un appel de fonctions.\nConventions de noms\nLe guide de style du tydiverse pr√©conise l‚Äôutilisation de lettres minuscules, de nombres et de l‚Äôunderscore _ pour nommer variables et fonctions. Les underscores sont utilis√©s pour s√©parer les mots dans un nom. Bien que l‚Äôon puisse trouver d‚Äôautres conventions, celles-ci sont √† √©viter. Dans tous les cas, il est important de choisir une convention et de la respecter. De plus, il est pr√©f√©rable d‚Äô√©viter les accents dans les noms de variables.\nOrganisation du code\nLorsque le code commence √† devenir long, il devient avantageux de le s√©parer en plusieurs fichiers. Par exemple, on peut avoir un fichier par partie de l‚Äôanalyse (un pour le nettoyage des donn√©es, un pour l‚Äôanalyse, un pour la visualisation, ‚Ä¶). De plus, une analyse de donn√©es n‚Äôest g√©n√©ralement pas constitu√© uniquement de code R, e.g.¬†fichiers de code C++, fichiers de donn√©es, fichiers du configuration, etc. Il est donc recommand√© des cr√©er des sous-dosssiers regroupant les fichiers du m√™me type. Les projets RStudio sont parfaits pour rassembler au m√™me endroit tous les fichiers relatifs au projet. De plus, ils permettent de faciliter le travail sur plusieurs projets simultan√©ment en g√©rant le passage d‚Äôun r√©pertoire de travail √† un autre.\nQuelques trucs √† faire\n\nR√©diger son code dans un script et l‚Äôenregistrer fr√©quemment. Cela permet d‚Äô√©viter de perdre la trace de certaines instructions importantes parce qu‚Äôelles ont √©t√© √©crites directement dans la console.\nIl est pr√©f√©rable de d√©buter toute session de travail en R avec un environnement de travail vide. Pour ce faire, il faut d√©sactiver la restauration automatique d‚Äôune image de session dans les param√®tres. Cela permet d‚Äô√™tre conscient de la pr√©sence des diff√©rents objets dans l‚Äôenvironnement de travail.\nNe pas utiliser la fonction load lorsque l‚Äôenvironnement de travail n‚Äôest pas vide. Cela permet de ne pas modifier un objet de l‚Äôenvironnement de travail en l‚Äô√©crasant.\nNe pas utiliser la fonction attach. Cela permet de ne pas modifier le chemin de recherche des fichiers.\nSauvegarder les options et param√®tres graphiques avant de les modifier.\nNe pas utiliser T et F √† la place de TRUE et FALSE."
  },
  {
    "objectID": "contents/supervised/03-ensemble.html",
    "href": "contents/supervised/03-ensemble.html",
    "title": "M√©thodes ensemblistes",
    "section": "",
    "text": "Lorsqu‚Äôon travaille avec de grands jeux de donn√©es, il est souvent plus efficace de combiner les pr√©dictions de plusieurs mod√®les simples, appel√©s classificateurs faibles, que d‚Äôessayer de construire un unique mod√®le tr√®s complexe. Cette id√©e est √† la base des m√©thodes d‚Äôensemble, qui visent √† produire une pr√©diction unique √† partir des pr√©dictions de plusieurs mod√®les. En regroupant ainsi plusieurs mod√®les, on peut g√©n√©ralement obtenir une meilleure performance globale et souvent une meilleure robustesse par rapport au bruit et au sur-ajustement.\nPlusieurs strat√©gies peuvent √™tre employ√©es pour combiner les r√©sultats de plusieurs mod√®les. Une premi√®re approche consiste √† faire la moyenne des probabilit√©s pr√©dites par chaque mod√®le. Une autre m√©thode courante est le vote de majorit√©, o√π la classe la plus souvent pr√©dite est retenue. On peut √©galement adopter une strat√©gie inspir√©e de la s√©lection de variables utilis√©e dans le mod√®le de r√©gression. Au lieu de choisir les ‚Äúmeilleures‚Äù variables, on s√©lectionne les mod√®les les plus pertinents pour l‚Äôensemble. Enfin, des techniques plus avanc√©es comme le bagging, le boosting et les for√™ts al√©atoires ont √©t√© d√©velopp√©es pour automatiser et optimiser cette combinaison de mod√®les.",
    "crumbs": [
      "Modules",
      "05 - Supervis√©e",
      "M√©thodes ensemblistes"
    ]
  },
  {
    "objectID": "contents/supervised/03-ensemble.html#bagging",
    "href": "contents/supervised/03-ensemble.html#bagging",
    "title": "M√©thodes ensemblistes",
    "section": "Bagging",
    "text": "Bagging\nLa m√©thode du bagging (bootstrap aggregating) consiste √† construire plusieurs versions d‚Äôun m√™me mod√®le √† partir d‚Äô√©chantillons diff√©rents du jeu de donn√©es d‚Äôorigine. Plus pr√©cisement, on commence par g√©n√©rer \\(B\\) √©chantillons bootstrap (tirages al√©atoires avec remise) √† partir des donn√©es d‚Äôapprentissage. Un arbre de classification est ensuite construit pour chacun de ces \\(B\\) √©chantillons. On obtient donc \\(B\\) arbres de classification. On pr√©dit la classe d‚Äôune nouvelle observation pour chacun des \\(B\\) arbres et on retient la classe pr√©dite le plus souvent, ce qui correspond √† un vote de majorit√©. Il est √©galement possible de calculer la moyenne des probabilit√©s de classe fournies par chaque arbre pour obtenir un score plus nuanc√©.\nLe principal avantage du bagging est qu‚Äôil permet de r√©duire la variance et d‚Äôaugmenter la stabilit√© des pr√©dictions. Cependant, cette m√©thode rend plus difficile l‚Äôinterpr√©tation du r√¥le jou√© par chaque variable dans la classification. Pour √©valuer l‚Äôimportance d‚Äôune variable, on peut additionner les r√©ductions de l‚Äôindice de Gini observ√©es √† chaque division bas√©e sur cette variable, puis faire la moyenne de ces r√©ductions sur tous les arbres. On exprime enfin ces moyennes sous forme de pourcentage relatif √† la r√©duction maximale observ√©e parmi toutes les variables.",
    "crumbs": [
      "Modules",
      "05 - Supervis√©e",
      "M√©thodes ensemblistes"
    ]
  },
  {
    "objectID": "contents/supervised/03-ensemble.html#for√™ts-al√©atoires",
    "href": "contents/supervised/03-ensemble.html#for√™ts-al√©atoires",
    "title": "M√©thodes ensemblistes",
    "section": "For√™ts al√©atoires",
    "text": "For√™ts al√©atoires\nLes for√™ts al√©atoires reprennent le principe du bagging, mais introduisent une source suppl√©mentaire d‚Äôal√©a qui permet de d√©corr√©ler les arbres construits. En effet, les arbres construits par l‚Äôalgorithme de bagging tendent √† se ressembler car les classes les plus fr√©quentes dominent. Pour r√©soudre ce probl√®me, la m√©thode des for√™ts al√©atoires impose qu‚Äô√† chaque division dans un arbre, on s√©lectionne al√©atoirement un sous-ensemble de \\(m\\) variables parmi les \\(p\\) disponibles et que seules ces \\(m\\) variables soient consid√©r√©es pour choisir la meilleure division. Un choix usuel est \\(m \\approx \\sqrt{p}\\). Cette diversification am√©liore la performance globale de l‚Äôalgorithme, ainsi que sa robustesse.",
    "crumbs": [
      "Modules",
      "05 - Supervis√©e",
      "M√©thodes ensemblistes"
    ]
  },
  {
    "objectID": "contents/supervised/03-ensemble.html#boosting",
    "href": "contents/supervised/03-ensemble.html#boosting",
    "title": "M√©thodes ensemblistes",
    "section": "Boosting",
    "text": "Boosting\n√Ä la diff√©rence du bagging, le boosting adopte une approche s√©quentielle. L‚Äôid√©e est de construire une s√©rie de mod√®les simples en accordant progressivement plus d‚Äôimportance aux observations mal class√©es par les mod√®les pr√©c√©dents. Chaque nouveau mod√®le est entra√Æn√© pour corriger les erreurs du pr√©c√©dent, ce qui permet d‚Äôatteindre une tr√®s bonne pr√©cision globale. Pour fonctionner efficacement, le boosting doit utiliser des mod√®les faibles, laissant au processus le soin de combiner leurs pr√©dictions de mani√®re optimale.\nIl existe plusieurs variantes du boosting. L‚Äôune des plus connues est AdaBoost (boosting adaptatif), qui ajuste les poids des observations √† chaque it√©ration. Une autre approche tr√®s utilis√©e aujourd‚Äôhui est le gradient boosting, qui repose sur la minimisation d‚Äôune fonction de perte √† l‚Äôaide d‚Äôune proc√©dure it√©rative inspir√©e de la descente du gradient.",
    "crumbs": [
      "Modules",
      "05 - Supervis√©e",
      "M√©thodes ensemblistes"
    ]
  },
  {
    "objectID": "contents/supervised/03-ensemble.html#optimisation-des-hyper-param√®tres",
    "href": "contents/supervised/03-ensemble.html#optimisation-des-hyper-param√®tres",
    "title": "M√©thodes ensemblistes",
    "section": "Optimisation des hyper-param√®tres",
    "text": "Optimisation des hyper-param√®tres\nComme toute m√©thode de classification, les mod√®les d‚Äôensembles n√©cessitent une optimisation de leur hyper-param√®tres pour atteindre un bon compromis entre performance et robustesse. On observe d‚Äôailleurs empiriquement que plus une m√©thode est puissante, plus l‚Äôoptimisation des hyper-param√®tres est complexe et sensible.\nVoici quelques exemples de param√®tres √† ajuster selon les m√©thodes :\n\nBagging : nombre d‚Äô√©chantillons bootstrap \\(B\\);\nFor√™ts al√©atoires : nombre d‚Äô√©chantillons bootstrap \\(B\\) et nombre de variables \\(m\\) √† consid√©rer √† chaque division;\nBoosting: taille des arbres, fraction du jeu de donn√©es √† √©chantillonner √† chaque √©tape, nombre d‚Äôit√©rations de l‚Äôalgorithme, ‚Ä¶\n\nPour optimiser ces hyper-param√®tres et √©viter le sur-ajustement, une validation crois√©e pour chacun des param√®tres est g√©n√©ralement n√©cessaire.",
    "crumbs": [
      "Modules",
      "05 - Supervis√©e",
      "M√©thodes ensemblistes"
    ]
  },
  {
    "objectID": "contents/supervised/02-tree.html",
    "href": "contents/supervised/02-tree.html",
    "title": "Arbres",
    "section": "",
    "text": "Les arbres de classification et de r√©gression (Classification And Regression Trees, CART) font parties des m√©thodes d‚Äôapprentissage supervis√©e. Ils permettent de pr√©dire une variable r√©ponse √† partir de plusieurs variables explicatives \\(X_1, \\dots, X_p\\). On dispose donc de \\(n\\) observations appartenant √† \\(\\mathbb{R}^p\\). Un arbre de classification cherche √† assigner une observation \\(i\\) √† l‚Äôun des \\(K\\) groupes sur la base de \\(X_{i1}, \\dots, X_{ip}\\), les valeurs des variables pour cette observation \\(i\\). Tandis qu‚Äôun arbre de r√©gression cherche √† pr√©dire la valeur d‚Äôune variable num√©rique continue \\(Y\\) √† partir des observations de \\(X_{1}, \\dots, X_p\\).",
    "crumbs": [
      "Modules",
      "05 - Supervis√©e",
      "Arbres"
    ]
  },
  {
    "objectID": "contents/supervised/02-tree.html#objectif",
    "href": "contents/supervised/02-tree.html#objectif",
    "title": "Arbres",
    "section": "Objectif",
    "text": "Objectif\nL‚Äôobjectif de l‚Äôalgorithme CART est de partionner l‚Äôespace des observations, i.e.¬†\\(\\mathbb{R}^p\\), en sous-ensembles homog√®nes √† l‚Äôaide de d√©coupages successifs. Chaque sous-ensemble est repr√©sent√© par une feuille de l‚Äôarbre, √† laquelle est associ√©e une pr√©diction.\nDe fa√ßon plus pr√©cise, l‚Äôalgorithme CART est un algorithme r√©cursif de d√©coupage binaire. Il proc√®de de la fa√ßon suivante :\n\nLe d√©coupage de l‚Äôespace \\(\\mathbb{R}^p\\) en sous-r√©gions rectangulaires (que l‚Äôon appelle hyper-rectangles), en coupant selon une variable \\(X_j, j = 1, \\dots, p\\) √† une certaine valeur seuil \\(s\\).\n√Ä chaque √©tape, on choisit la variable \\(X_j, j = 1, \\dots, p\\) et la valeur seuil \\(s\\) qui offrent la meilleure am√©lioration d‚Äôun crit√®re d‚Äôhomog√©n√©it√© (cf.¬†la section ‚ÄúCrit√®res d‚Äôhomog√©n√©it√©‚Äù).\nOn r√©p√®te ce d√©coupage tant qu‚Äôun crit√®re d‚Äôarr√™t n‚Äôest pas atteint (cf.¬†la section ‚ÄúChoix de la complexit√©‚Äù).\nChaque feuille contient des observations similaires et fournit une pr√©diction unique : la classe majoritaire pour la classification et la moyenne des \\(Y\\) pour la r√©gression. Ainsi, toutes les observations dans une m√™me feuille re√ßoivent la m√™me pr√©diction.\n\n\n\n\n\n\n\nAstuceRemarque\n\n\n\nL‚Äôalgorithme CART est un algortihme glouton : √† chaque √©tape, cette algorithme choisit localement la meilleure coupure. Mais cela ne garantit pas l‚Äôoptimum global !\nIl serait th√©oriquement pr√©f√©rable de chercher l‚Äôarbre √† \\(K\\) feuilles qui minimise une fonction de co√ªt globale : \\[\\min_{R_1, \\dots, R_K} \\sum_{k = 1}^{K} c_k,\\] o√π \\(c_k\\) mesure l‚Äôerreur dans le r√©gion \\(R_k\\). Cependant, il y aurait trop de combinaisons possibles pour r√©soudre ce probl√®me. C‚Äôest un probl√®me intraitable (intractable).",
    "crumbs": [
      "Modules",
      "05 - Supervis√©e",
      "Arbres"
    ]
  },
  {
    "objectID": "contents/supervised/02-tree.html#algorithme",
    "href": "contents/supervised/02-tree.html#algorithme",
    "title": "Arbres",
    "section": "Algorithme",
    "text": "Algorithme\nL‚Äôalgorithme se d√©roule comme suit, pour chaque noeud :\n\nPour chaque variable \\(X_j, j = 1, \\dots, p\\) et chaque seuil (valeur de coupure) possible \\(s\\), on calcule la r√©duction du crit√®re d‚Äôhomog√©n√©it√© entre les deux sous-ensembles.\nOn s√©lectionne le couple \\((X_j, s)\\) qui maximise la r√©duction du crit√®re d‚Äôhomog√©n√©it√©, i.e.¬†qui offre le gain maximal.\nOn divise le noeud en deux sous-noeuds (sous-ensembles) selon la r√®gle \\(X_j \\leq s\\) vs.¬†\\(X_j &gt; s\\).\nOn r√©p√®te ce processus jusqu‚Äô√† ce qu‚Äôun nombre maximal de feuilles soit atteint ou bien que le nombre d‚Äôobservations dans chaque feuille soit suffisant.\n\nLe noeud de d√©part consiste en l‚Äôensemble des observations.",
    "crumbs": [
      "Modules",
      "05 - Supervis√©e",
      "Arbres"
    ]
  },
  {
    "objectID": "contents/supervised/02-tree.html#crit√®res-dhomog√©n√©it√©-pour-la-classification",
    "href": "contents/supervised/02-tree.html#crit√®res-dhomog√©n√©it√©-pour-la-classification",
    "title": "Arbres",
    "section": "Crit√®res d‚Äôhomog√©n√©it√© pour la classification",
    "text": "Crit√®res d‚Äôhomog√©n√©it√© pour la classification\nSoit \\(\\widehat{p}_{jk}\\), la proportion d‚Äôobservations de la r√©gion \\(R_j\\) de l‚Äôensemble \\(\\mathbb{R}^p\\) qui appartiennent √† la classe \\(k\\). Trois crit√®res sont habituellement utilis√©s pour d√©terminer les divisions optimales √† chaque √©tape de la construction de l‚Äôarbre:\n\nLe Taux d‚Äôerreur de classification correspond √† la proportion d‚Äôobservations mal class√©es dans une feuille. Il est d√©fini comme \\[E_j = 1 - \\max_k \\widehat{p}_{jk}.\\] Ce crit√®re est facile √† interpr√©ter, mais il est peu sensible et donc il est rarement utilis√© pour la construction de l‚Äôarbre. Il est plut√¥t emply√© pour √©valuer la performance du mod√®le une fois l‚Äôarbre construit.\nL‚ÄôIndice de Gini est un des crit√®res les plus utilis√©s. Il est d√©fini comme \\[G_j = \\sum_{k = 1}^{K} \\widehat{p}_{jk} (1 - \\widehat{p}_{i j}).\\] Ce crit√®re est minimale et √©gale √† \\(0\\) lorsque tous les individus d‚Äôune r√©gion appartiennent √† une unique classe, ce qui refl√®te une homog√©n√©it√© parfaite. On cherche donc √† avoir un indice de Gini faible.\nL‚ÄôEntropie crois√©e est un autre crit√®re couramment utilis√©. Il est bas√© sur la th√©orie de l‚Äôinformation de Shannon et est d√©fini comme \\[D_j = - \\sum_{k = 1}^{K}  \\widehat{p}_{jk} \\log(\\widehat{p}_{j k}).\\] Ce crit√®re atteint aussi sa valeur minimale lorsque la classe est parfaitement pr√©dite. On cherche donc aussi √† avoir une entropie crois√©e faible.\n\nLors de la construction de l‚Äôarbre, on √©value pour chaque division possible la r√©duction d‚Äôh√©t√©rog√©n√©it√© qu‚Äôelle entra√Æne, appel√©e gain d‚Äôinformation. Par exemple, pour l‚Äôindice de Gini, le gain d‚Äôinformation s‚Äô√©crit comme la diff√©rence entre l‚Äôindice avant la division et une moyenne pond√©r√©e des indices dans les sous-groupes obtenus : \\[\\Delta G = G_{\\text{avant la division}} - \\left( \\frac{n_1}{n} G_1 + \\frac{n_2}{n} G_2 \\right),\\] o√π \\(n_1\\) et \\(n_2\\) repr√©sente le nombre d‚Äôobservations dans les deux sous-ensembles et \\(n = n_{1} + n_{2}\\). On choisit alors la division qui maximise ce gain.",
    "crumbs": [
      "Modules",
      "05 - Supervis√©e",
      "Arbres"
    ]
  },
  {
    "objectID": "contents/supervised/02-tree.html#choix-de-la-complexit√©-√©lagage-de-larbre",
    "href": "contents/supervised/02-tree.html#choix-de-la-complexit√©-√©lagage-de-larbre",
    "title": "Arbres",
    "section": "Choix de la complexit√© : √©lagage de l‚Äôarbre",
    "text": "Choix de la complexit√© : √©lagage de l‚Äôarbre\nPlus l‚Äôarbre devient profond, i.e plus le nombre de feuilles augmente, plus il s‚Äôadapte fid√®lement aux donn√©es d‚Äôapprentissage. Toutefois, cette grande adaptation conduit au ph√©nom√®ne de sur-ajustement (cf.¬†Section sur l‚Äô√©valuation de mod√®les). Ainsi, dans ce cas, le mod√®le ne sera plus capable de g√©n√©raliser √† des donn√©es inconnues. Cependant, si l‚Äôarbre n‚Äôa pas assez de feuilles, il y a un manque d‚Äôajustement aux donn√©es car la classification est trop simpliste.\nPour √©viter ce ph√©nom√®ne, on recourt √† une strat√©gie d‚Äô√©lagage (purning). Dans un premier temps, on fait cro√Ætre l‚Äôarbre jusqu‚Äô√† un d√©veloppement complet, i.e.¬†jusqu‚Äô√† ce que toutes les feuilles soient pures (ne contiennent qu‚Äôune seule observation) ou qu‚Äôaucune division ne soit pertinente. Dans un second temps, on coupe l‚Äôarbre en se d√©barassant des branches qui n‚Äôapportent que peu d‚Äôam√©lioration en termes de performance.\nL‚Äô√©lagage peut √™tre r√©alis√© √† l‚Äôaide d‚Äôun crit√®re appel√© co√ªt-complexit√©. On cherche alors √† minimiser une fonction combinant l‚Äôerreur de pr√©diction et la complexit√© de l‚Äôarbre. Cette fonction prend la forme suivante : \\[\\sum_{j = 1}^{|T|} c_j + \\alpha |T|,\\] o√π \\(|T|\\) est le nombre de feuille de l‚Äôarbre, \\(c_j\\) le co√ªt de classification ou de r√©gression dans la feuille \\(j\\) et \\(\\alpha\\) un param√®tre de r√©gularisation contr√¥lant la p√©nalit√© associ√©e √† la taille de l‚Äôarbre. Plus le param√®tre \\(\\alpha\\) est √©lev√©, plus on favorise les arbres simples. Le choix optimal de \\(\\alpha\\) peut se faire par validation crois√©e, en comparant les performances sur des jeux de validation pour diff√©rentes valeurs de ce param√®tre.",
    "crumbs": [
      "Modules",
      "05 - Supervis√©e",
      "Arbres"
    ]
  },
  {
    "objectID": "contents/supervised/02-tree.html#quelques-commentaires",
    "href": "contents/supervised/02-tree.html#quelques-commentaires",
    "title": "Arbres",
    "section": "Quelques commentaires",
    "text": "Quelques commentaires\nLes arbres pr√©sentent quelques avantages int√©ressants. Ils sont d‚Äôabord tr√®s interpr√©tables. En effet, chaque pr√©diction peut √™tre expliqu√©e √† l‚Äôaide d‚Äôun chemin dans l‚Äôarbre, ce qui les rend particuli√®rement adapt√©s √† une utilisation dans un contexte appliqu√©. Ils sont √©galement robustes aux valeurs extr√™mes et aux transformations monotones des variables explicatives. De plus, ils g√®rent naturellement les interactions entre les variables sans qu‚Äôil soit n√©cessaire de les expliciter. Ils peuvent √™tre utilis√©s √† la fois avec des variables quantitatives et quantitatives, et ne reposent sur aucune hypoth√®se de distribution des donn√©es. Ils effectuent automatiquement une s√©lection des variables au cours de leur construction. Les variables importantes auront tendance √† √™tre choisies en premi√®res pour la d√©coupe des noeuds. Enfin, ils constituent la base de nombreuses m√©thodes modernes d‚Äôapprentissage statistique, comme les for√™ts al√©atoires ou bien le gradient boosting (cf.¬†Section ‚ÄúM√©thodes ensemblistes‚Äù).\nMalgr√© leurs nombreux avantages, les arbres pr√©sentent aussi certaines limites. Ils sont instables. En effet, de l√©g√®res modifications dans les donn√©es peuvent entra√Æner des arbres tr√®s diff√©rents. Ainsi, lorsqu‚Äôils sont utilis√©s seuls, ils peuvent √™tre moins performants que d‚Äôautres m√©thodes, notemment lorsque les donn√©es sont bruit√©es. La forme des divisions (en rectangles align√©s sur les axes) peut √™tre inadapt√©e si les fronti√®res entre les classes sont non-lin√©aires. Par ailleurs, sans √©lagage, les arbres ont tendance √† surajuster les donn√©es. Ils sont √©galement sensibles au d√©s√©quilibre des classes, i.e.¬†lorsque certaines classes sont fortement sous-repr√©sent√©es dans les donn√©es. Ainsi, on leur pr√©f√©rera g√©n√©ralement une forme agr√©g√©e (e.g.¬†for√™ts al√©atoires, boosting) pour am√©liorer leur stabilit√© et leur performance.",
    "crumbs": [
      "Modules",
      "05 - Supervis√©e",
      "Arbres"
    ]
  },
  {
    "objectID": "informations/materials.html",
    "href": "informations/materials.html",
    "title": "Mat√©riel",
    "section": "",
    "text": "Cette page regroupe diff√©rentes ressources utiles pour le cours.\nThe Elements of Statistical Learning: Data Mining, Inference and Prediction, 2nd Edition\nAuteurs: Trevor Hastie, Robert Tibshirani et Jerome Friedman\n√âditeur: Springer (New York, 2009)\nISBN: 0-387-84857-0\nLien: site web des authors\nAn Introduction to Statistical Learning: with application in R, 2nd Edition\nAuteurs: Gareth James, Daniela Witten, Trevor Hastie et Robert Tibshirani\n√âditeur: Springer (New York, 2021)\nISBN: 1-0716-1417-4\nLien: site web des authors\nAn Introduction to Statistical Learning: with application in Python\nAuteurs: Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani et Jonathan Taylor\n√âditeur: Springer (Suisse, 2023)\nISBN: 3-031-38746-3\nLien: site web des authors",
    "crumbs": [
      "Informations",
      "Mat√©riel"
    ]
  },
  {
    "objectID": "informations/evaluations.html",
    "href": "informations/evaluations.html",
    "title": "√âvaluations",
    "section": "",
    "text": "Voici les diff√©rents examens pr√©vus pour ce module.\n\n\n\n\n\n\n\n\n\nTitre\nDate\nMode de travail\nPond√©ration\n\n\n\n\nExamen 1\n24 oct. 2025\nIndividuel\n30%\n\n\nExamen 2\n12 d√©c. 2025\nIndividuel\n50%\n\n\nProjet d‚Äôanalyse de donn√©es\n02 d√©c. 2025\nEn √©quipe\n20%\n\n\n\n\nIdentification. Lors d‚Äôun examen, une carte d‚Äôidentit√© avec photo admissible doit √™tre d√©pos√©e sur le coin de votre table. Les cartes admissibles sont la carte de l‚ÄôUniversit√© Laval en plastique, un permis de conduire canadien, une carte d‚Äôassurance-maladie avec photo √©mise par une province canadienne ou un passeport canadien ou √©tranger.",
    "crumbs": [
      "Informations",
      "√âvaluations"
    ]
  },
  {
    "objectID": "informations/evaluations.html#modalit√©s-d√©valuations",
    "href": "informations/evaluations.html#modalit√©s-d√©valuations",
    "title": "√âvaluations",
    "section": "",
    "text": "Voici les diff√©rents examens pr√©vus pour ce module.\n\n\n\n\n\n\n\n\n\nTitre\nDate\nMode de travail\nPond√©ration\n\n\n\n\nExamen 1\n24 oct. 2025\nIndividuel\n30%\n\n\nExamen 2\n12 d√©c. 2025\nIndividuel\n50%\n\n\nProjet d‚Äôanalyse de donn√©es\n02 d√©c. 2025\nEn √©quipe\n20%\n\n\n\n\nIdentification. Lors d‚Äôun examen, une carte d‚Äôidentit√© avec photo admissible doit √™tre d√©pos√©e sur le coin de votre table. Les cartes admissibles sont la carte de l‚ÄôUniversit√© Laval en plastique, un permis de conduire canadien, une carte d‚Äôassurance-maladie avec photo √©mise par une province canadienne ou un passeport canadien ou √©tranger.",
    "crumbs": [
      "Informations",
      "√âvaluations"
    ]
  },
  {
    "objectID": "informations/evaluations.html#informations-d√©taill√©es-sur-les-√©valuations",
    "href": "informations/evaluations.html#informations-d√©taill√©es-sur-les-√©valuations",
    "title": "√âvaluations",
    "section": "Informations d√©taill√©es sur les √©valuations",
    "text": "Informations d√©taill√©es sur les √©valuations\nExamen 1\nDate et lieu: 24 octobre 2025 de 08h30 √† 09h30\nMode de travail: Individuel\nPond√©ration: 30%\nRemise de l‚Äô√©valuation: Remise en main propre √† l‚Äôenseignant\nDirectives de l‚Äô√©valuation: Tout le contenu du cours (diapos, pr√©sentations en classe, lectures, ‚Ä¶) est susceptible d‚Äô√™tre √† l‚Äôexamen.\nMat√©riel autoris√©: Tout mat√©riel non-connect√© √† internet est autoris√©. Calculatrice autoris√©e. L‚Äôutilisation d‚Äôoutils d‚ÄôIA n‚Äôest PAS autoris√©e pour cette examen.\nInformation supplementaire: lien.\nExamen 2\nDate et lieu: 12 d√©cembre 2025 de 08h30 √† 10h20\nMode de travail: Individuel\nPond√©ration: 50%\nRemise de l‚Äô√©valuation: Remise en main propre √† l‚Äôenseignant\nDirectives de l‚Äô√©valuation: Tout le contenu du cours (diapos, pr√©sentations en classe, lectures, ‚Ä¶) est susceptible d‚Äô√™tre √† l‚Äôexamen.\nMat√©riel autoris√©: Tout mat√©riel non-connect√© √† internet est autoris√©. Calculatrice autoris√©e. L‚Äôutilisation d‚Äôoutils d‚ÄôIA n‚Äôest PAS autoris√©e pour cette examen.\nProjet d‚Äôanalyse de donn√©es\nDate et lieu: d√ª le 02 d√©cembre 2025 √† 17h00\nMode de travail: En √©quipe\nPond√©ration: 20%\nRemise de l‚Äô√©valuation: Via monPortail\nDirectives de l‚Äô√©valuation: Tout retard sur la remise du projet entrainera la note de 0 pour cette partie. Il y aura une pr√©sentation des posters le 9 d√©cembre entre 10h30 et 12h30. Les modalit√©s exactes des pr√©sentations restent √† d√©finir. Dans le cas o√π l‚Äôaffichage des posters ne puissent pas se faire au pavillon VCH, les pr√©sentations se feront dans la salle de classe PLT-3928. Le projet d‚Äôanalyse de donn√©es consiste en la cr√©ation d‚Äôun poster pr√©sentant une analyse de donn√©es. Le choix du sujet et de la probl√©matique est laiss√© √† votre discr√©tion. Il est recommand√© d‚Äôutiliser les m√©thodes pr√©sent√©es en cours, mais ce n‚Äôest pas obligatoire. Dans tous les cas, attendez vous √† avoir des questions sur les m√©thodes que vous pr√©sentez. Je suis bien entendu √† votre disposition pour r√©pondre √† toutes questions (o√π trouver des donn√©es, comment faire un poster, ‚Ä¶). Merci de d√©poser un dossier .zip connant votre projet. Ce dossier doit contenir votre poster au format .pdf, votre code, ainsi que vos donn√©es. Si la taille du fichier de vos donn√©es est trop importante, merci de m‚Äôenvoyer un email. Je m‚Äôoccuperai de l‚Äôimpression des posters.\nMat√©riel autoris√© : L‚Äôutilisation d‚Äôoutils d‚ÄôIA est autoris√©e pour cette examen. Il convient cependant de proprement documenter son utilisation.",
    "crumbs": [
      "Informations",
      "√âvaluations"
    ]
  },
  {
    "objectID": "informations/evaluations.html#√©chelle-des-cotes",
    "href": "informations/evaluations.html#√©chelle-des-cotes",
    "title": "√âvaluations",
    "section": "√âchelle des cotes",
    "text": "√âchelle des cotes\n\n\n\nCote\n% minimum\n% maximum\n\n\n\n\nA+\n92\n100\n\n\nA\n88\n91.99\n\n\nA-\n84\n87.99\n\n\nB+\n80\n83.99\n\n\nB\n75\n79.99\n\n\nB-\n70\n74.99\n\n\nC+\n65\n69.99\n\n\nC\n60\n64.99\n\n\nC-\n55\n59.99\n\n\nD+\n52\n54.99\n\n\nD\n50\n51.99\n\n\nE\n0\n49.99",
    "crumbs": [
      "Informations",
      "√âvaluations"
    ]
  },
  {
    "objectID": "informations/evaluations.html#d√©tails-sur-les-modalit√©s-d√©valuation",
    "href": "informations/evaluations.html#d√©tails-sur-les-modalit√©s-d√©valuation",
    "title": "√âvaluations",
    "section": "D√©tails sur les modalit√©s d‚Äô√©valuation",
    "text": "D√©tails sur les modalit√©s d‚Äô√©valuation\nConform√©ment √† la politique du D√©partement de math√©matiques et de statistique en mati√®re d‚Äôam√©lioration et de consolidation de la connaissance du fran√ßais, la qualit√© de l‚Äô√©crit sera sanctionn√©e dans tous les travaux et examens. Un maximum de 10% des points pourra √™tre enlev√© pour la qualit√© de la langue et de la r√©daction.\nAucun retard n‚Äôest accept√© pour la remise des travaux. Un retard entra√Æne automatiquement une note de z√©ro.\nToute reprise d‚Äô√©valuation accord√©e par la politique de reprise d‚Äô√©valuation du D√©partement aura lieu lors des dates de reprises officielles de la facult√©. Pour toute demande de r√©vision de note, vous devez suivre la proc√©dure du chapitre 4 du R√©glement des √©tudes. Vous trouverez un formulaire √† remplir √† l‚Äôadresse suivante: lien.\nLes s√©ances d‚Äôexamen de reprise FSG pour la session d‚Äôautomne 2025 auront lieu :\n\nLe jeudi 30 octobre 2025, local √† d√©terminer, de 9h √† 12h et/ou de 13h30 √† 16h30.\nLe jeudi 18 d√©cembre 2025, local √† d√©terminer, de 9h √† 12h et/ou de 13h30 √† 16h30.",
    "crumbs": [
      "Informations",
      "√âvaluations"
    ]
  },
  {
    "objectID": "informations/evaluations.html#absence-√†-une-activit√©-obligatoire",
    "href": "informations/evaluations.html#absence-√†-une-activit√©-obligatoire",
    "title": "√âvaluations",
    "section": "Absence √† une activit√© obligatoire",
    "text": "Absence √† une activit√© obligatoire\nCes modalit√©s s‚Äôappliquent en vertu des articles 4.41 et 4.42 du R√©glement des √©tudes de l‚ÄôUniversit√© Laval:\n\nArticle 4.41: Tout d√©faut de se soumettre √† une activit√© d‚Äô√©valuation entra√Æne la note de z√©ro pour cette activit√© d‚Äô√©valuation, √† moins que l‚Äô√©tudiante ou l‚Äô√©tudiant ne d√©montre que cette omission est attribuable √† des motifs s√©rieux.\nArticle 4.42: La reprise d‚Äôune √©valuation est possible pour des motifs s√©rieux. Elle se fait selon les modalit√©s pr√©vues par l‚Äôunit√© responsable de l‚Äôactivit√© de formation.\n\nLa reprise d‚Äôune √©valuation peut donc execptionnellement √™tre autoris√©e pour des motifs jug√©s s√©rieux, dans la mesure o√π la proc√©dure d√©crite ci-dessous est respect√©e.\nMotifs d‚Äôabsence jug√©s s√©rieux\nLes motifs suivants sont jug√©s s√©rieux et donc acceptables pour demander une reprise d‚Äô√©valuation:\n\nmaladie ou accident emp√™chant de se d√©placer;\nhospitalisation;\nmaladie grave ou d√©c√®s d‚Äôun proche;\nparticipation √† une activit√© sportive de haut niveau;\nconvocation en cour de justice.\n\nProc√©dure √† suivre\nD√®s que possible et au plus tard cinq (5) jours ouvrables apr√®s la date de l‚Äô√©valuation (ou dans certains cas, avant la date de l‚Äô√©valuation, d√®s que le motif sera connu), l‚Äô√©tudiante ou l‚Äô√©tudiant qui veut faire une demande de reprise d‚Äô√©valuation doit remplir et soumettre le formulaire √©lectronique ‚ÄúDemande de reprise d‚Äôune √©valuation‚Äù en prenant soin d‚Äôy joindre les pi√®ces justificatives requises.\nPour avoir plus de d√©tail sur les proc√©dures √† suivre et les motifs s√©rieux pouvant donner droit √† une reprise d‚Äô√©valuation, consulter le document ‚ÄúModalit√©s et proc√©dure de reprise d‚Äôune √©valuation sommative √† la Facult√© des sciences et de g√©nie‚Äù disponible sur le site web de la FSG.\nDans certains cas, la ou le responsable du cours pourrait adopter une proc√©dure simplifi√©e de gestion des demandes de reprises d‚Äô√©valuation, tout en respectant les crit√®res d√©crits dans cette politique. Dans ces cas, des explications particuli√®res seront donn√©es √† cet effet dans le plan de cours et pr√©sent√©es lors de la premi√®re s√©ance.",
    "crumbs": [
      "Informations",
      "√âvaluations"
    ]
  },
  {
    "objectID": "informations/evaluations.html#politique-sur-la-s√©curit√©-dans-les-laboratoires-informatiques",
    "href": "informations/evaluations.html#politique-sur-la-s√©curit√©-dans-les-laboratoires-informatiques",
    "title": "√âvaluations",
    "section": "Politique sur la s√©curit√© dans les laboratoires informatiques",
    "text": "Politique sur la s√©curit√© dans les laboratoires informatiques\nPour utiliser les laboratoires informatiques, il faut avoir pris connaissance de la Politique de s√©curit√© et vous engagez √† vous y conformer : lien.\nPour toute assistance technique, veuillez √©crire √† [aide@fsg.ulaval.ca]. En cas d‚Äôurgence, composez le 418-656-5555, poste 555 ou utilisez un t√©l√©phone rouge situ√© √† proximit√©.",
    "crumbs": [
      "Informations",
      "√âvaluations"
    ]
  },
  {
    "objectID": "informations/evaluations.html#politique-sur-lutilisation-dappareils-√©lectroniques",
    "href": "informations/evaluations.html#politique-sur-lutilisation-dappareils-√©lectroniques",
    "title": "√âvaluations",
    "section": "Politique sur l‚Äôutilisation d‚Äôappareils √©lectroniques",
    "text": "Politique sur l‚Äôutilisation d‚Äôappareils √©lectroniques\nLa politique sur l‚Äôutilisation d‚Äôappareils √©lectroniques de la Facult√© des Sciences et de G√©nie peut √™tre consult√©e √† l‚Äôadresse: lien.",
    "crumbs": [
      "Informations",
      "√âvaluations"
    ]
  },
  {
    "objectID": "informations/evaluations.html#politique-sur-le-plagiat-et-la-fraude-acad√©mique",
    "href": "informations/evaluations.html#politique-sur-le-plagiat-et-la-fraude-acad√©mique",
    "title": "√âvaluations",
    "section": "Politique sur le plagiat et la fraude acad√©mique",
    "text": "Politique sur le plagiat et la fraude acad√©mique\nR√®gles disciplinaires\nTout √©tudiant qui commet une infraction au R√®glement disciplinaire √† l‚Äôintention des √©tudiants de l‚ÄôUniversit√© Laval dans le cadre du pr√©sent cours, notamment en mati√®re de plagiat, est passible des sanctions qui sont pr√©vues dans ce r√®glement. Il est tr√®s important pour tout √©tudiant de prendre connaissance des articles 23 √† 46 du R√®glement disciplinaire. Celui-ci peut √™tre consult√© √† l‚Äôadresse suivante: lien.\nPlagiat\nTout √©tudiant est tenu de respecter les r√®gles relatives au plagiat. Constitue notamment du plagiat le fait de:\n\ncopier textuellement un ou plusieurs passages provenant d‚Äôun ouvrage sous format papier ou √©lectronique sans mettre ces passages entre guillemets et sans en mentionner la source;\nr√©sumer l‚Äôid√©e originale d‚Äôun auteur en l‚Äôexprimant dans ses propres mots (paraphraser) sans en mentionner la source;\ntraduire partiellement ou totalement un texte sans en mentionner la provenance;\nremettre un travail copi√© d‚Äôun autre √©tudiant (avec ou sans l‚Äôaccord de cet autre √©tudiant);\nremettre un travail t√©l√©charg√© d‚Äôun site d‚Äôachat ou d‚Äô√©change de travaux scolaires.\n\nL‚ÄôUniversit√© Laval √©tant abonn√©e √† un service de d√©tection de plagiat, il est possible que l‚Äôenseignant soumette vos travaux pour analyse.",
    "crumbs": [
      "Informations",
      "√âvaluations"
    ]
  },
  {
    "objectID": "informations/evaluations.html#√©tudiants-ayant-une-situation-de-handicap-li√©e-√†-une-limitation-fonctionnelle",
    "href": "informations/evaluations.html#√©tudiants-ayant-une-situation-de-handicap-li√©e-√†-une-limitation-fonctionnelle",
    "title": "√âvaluations",
    "section": "√âtudiants ayant une situation de handicap li√©e √† une limitation fonctionnelle",
    "text": "√âtudiants ayant une situation de handicap li√©e √† une limitation fonctionnelle\nAfin de b√©n√©ficier de mesures d‚Äôaccommodement pour les cours ou les examens, un rendez-vous avec une conseill√®re ou un conseiller du Centre d‚Äôaide aux √©tudiants travaillant en Accueil et soutien aux √©tudiants en situation de handicap (ACSESH) est n√©cessaire. Pour ce faire, les √©tudiants pr√©sentant une situation de handicap li√©e √† une limitation fonctionnelle permanente doivent visiter le site ‚ÄúAccommodement‚Äù et prendre un rendez-vous, le plus t√¥t possible.\nAu cours de la semaine qui suit l‚Äôautorisation des mesures, leur activation doit √™tre effectu√©e dans ‚ÄúAccommodement‚Äù pour assurer leur mise en place.\nLes √©tudiants ayant d√©j√† obtenu des mesures d‚Äôaccommodements scolaires doivent proc√©der √† l‚Äôactivation de leurs mesures pour les cours et/ou les examens dans ‚ÄúAccommodement‚Äù afin que celles-ci puissent √™tre mises en place. Il est √† noter que l‚Äôactivation doit s‚Äôeffectuer au cours de deux premi√®res semaines de cours.\nLes √©tudiants concern√©s recevront par la suite des facult√©s et d√©partements responsables de leurs cours les informations d√©taill√©es sur les modalit√©s permettant d‚Äôappliquer les mesures d‚Äôaccommodement identifi√©es.",
    "crumbs": [
      "Informations",
      "√âvaluations"
    ]
  },
  {
    "objectID": "informations/description.html",
    "href": "informations/description.html",
    "title": "Description du cours",
    "section": "",
    "text": "Dans ce cours, nous visons √† introduire des m√©thodes qui permettront aux √©tudiantes et √©tudiants d‚Äô√©tudier un jeu de donn√©es de ‚Äúhaute dimension‚Äù (ici, ‚Äúhaute‚Äù est pris dans le sens o√π l‚Äôon ne peut pas faire un simple graphique de l‚Äôensemble des variables pour toutes les observations) sans avoir recours √† un mod√®le probabiliste. Les techniques que l‚Äôon y enseigne servent √† r√©duire la dimension des donn√©es, identifier certains liens entre les variables, visualiser les donn√©es ou √† diviser le jeu de donn√©es en groupes/classes.\nSans n√©gliger la th√©orie, l‚Äôaccent sera mis sur l‚Äôaspect pratique de l‚Äôanalyse des donn√©es et l‚Äôutilisation d‚Äôun langage de programmation, que ce soit R, Python ou autre.",
    "crumbs": [
      "Informations",
      "Description"
    ]
  },
  {
    "objectID": "informations/description.html#objectifs",
    "href": "informations/description.html#objectifs",
    "title": "Description du cours",
    "section": "",
    "text": "Dans ce cours, nous visons √† introduire des m√©thodes qui permettront aux √©tudiantes et √©tudiants d‚Äô√©tudier un jeu de donn√©es de ‚Äúhaute dimension‚Äù (ici, ‚Äúhaute‚Äù est pris dans le sens o√π l‚Äôon ne peut pas faire un simple graphique de l‚Äôensemble des variables pour toutes les observations) sans avoir recours √† un mod√®le probabiliste. Les techniques que l‚Äôon y enseigne servent √† r√©duire la dimension des donn√©es, identifier certains liens entre les variables, visualiser les donn√©es ou √† diviser le jeu de donn√©es en groupes/classes.\nSans n√©gliger la th√©orie, l‚Äôaccent sera mis sur l‚Äôaspect pratique de l‚Äôanalyse des donn√©es et l‚Äôutilisation d‚Äôun langage de programmation, que ce soit R, Python ou autre.",
    "crumbs": [
      "Informations",
      "Description"
    ]
  },
  {
    "objectID": "informations/description.html#place-du-cours-dans-le-programme",
    "href": "informations/description.html#place-du-cours-dans-le-programme",
    "title": "Description du cours",
    "section": "Place du cours dans le programme",
    "text": "Place du cours dans le programme\nCe cours est g√©n√©ralement suivi par des √©tudiants du baccalaur√©at en statistique lors de leur deuxi√®me ann√©e de formation. C‚Äôest aussi un cours √† option en actuariat, en math√©matique ainsi que dans certains programmes de g√©nie et d‚Äôadministration.\nTous les √©tudiants doivent s‚Äôassurer d‚Äôavoir suivi au minimum un cours d‚Äôalg√®bre (ex. MAT-1200) et un cours de statistique de base (ex. STT-1000), car la majorit√© des m√©thodes d‚Äôanalyse de donn√©es s‚Äôappuient sur ces notions.\nLes √©tudiants devraient aussi avoir une certaine familiarit√© avec les concepts algorithmiques de base, ainsi qu‚Äôavec (au moins) un langage de programmation.",
    "crumbs": [
      "Informations",
      "Description"
    ]
  },
  {
    "objectID": "informations/description.html#objectifs-sp√©cifiques",
    "href": "informations/description.html#objectifs-sp√©cifiques",
    "title": "Description du cours",
    "section": "Objectifs sp√©cifiques",
    "text": "Objectifs sp√©cifiques\n√Ä la fin du cours, l‚Äô√©tudiant ou l‚Äô√©tudiante devrait √™tre capable :\n\nde comprendre et d√©crire les fondements th√©oriques des m√©thodes d‚Äôanalyse de donn√©es √©tudi√©es;\nd‚Äôidentifier correctement les situations o√π l‚Äôemploi de ces m√©thodes est indiqu√©;\nd‚Äôutiliser efficacement un langage de programmation pour mettre en oeuvre ces m√©thodes;\nd‚Äôanalyser et d‚Äôinterpr√©ter judicieusement les r√©sultats d√©coulant de l‚Äôanalyse;\nde formuler par √©crit les conclusions de l‚Äôanalyse, dans le respect des limites de la m√©thodologie.",
    "crumbs": [
      "Informations",
      "Description"
    ]
  },
  {
    "objectID": "informations/description.html#√©quipement-informatique",
    "href": "informations/description.html#√©quipement-informatique",
    "title": "Description du cours",
    "section": "√âquipement informatique",
    "text": "√âquipement informatique\nVous pourriez avoir besoin d‚Äôun ordinateur, de haut-parleurs ou d‚Äôun casque d‚Äô√©coute, d‚Äôun microphone, d‚Äôune webcam et d‚Äôune connexion Internet avec fil √† large bande ou sans fil. Pour v√©rifier les param√®tres de configuration minimaux selon le syst√®me d‚Äôexploitation, nous vous invitons √† visiter cette page.\nDe plus, ce cours peut n√©cessiter des besoins logiciels particuliers qui seront alors d√©crits dans d‚Äôautres sections du plan de cours, le cas √©ch√©ant.",
    "crumbs": [
      "Informations",
      "Description"
    ]
  },
  {
    "objectID": "informations/description.html#fonctionnement",
    "href": "informations/description.html#fonctionnement",
    "title": "Description du cours",
    "section": "Fonctionnement",
    "text": "Fonctionnement\nLes cours et examens auront lieu en pr√©sentiel les mardi et vendredi matin, mais le mat√©riel sera disponible en ligne. De fa√ßon g√©n√©rale, la s√©ance du vendredi sera une s√©ance magistrale et la s√©ance du mardi sera d√©di√©e √† des exercices pratiques r√©alis√©s de fa√ßon autonome par les √©tudiants avec le soutien de l‚Äôenseignant.",
    "crumbs": [
      "Informations",
      "Description"
    ]
  },
  {
    "objectID": "informations/description.html#approches-p√©dagogiques",
    "href": "informations/description.html#approches-p√©dagogiques",
    "title": "Description du cours",
    "section": "Approches p√©dagogiques",
    "text": "Approches p√©dagogiques\nL‚Äôapproche p√©dagogique privil√©gi√©e est l‚Äôexpos√© interactif (vendredi) en alternance avec des p√©riodes de laboratoires (mardi). L‚Äôapproche p√©dagogique est tr√®s ax√©e sur l‚Äôapprentissage actif et exige un engagement soutenu des √©tudiants tout au long de la session.",
    "crumbs": [
      "Informations",
      "Description"
    ]
  },
  {
    "objectID": "informations/mcq.html",
    "href": "informations/mcq.html",
    "title": "QCM",
    "section": "",
    "text": "Le premier examen est un questionaire √† choix multiples avec quantification de l‚Äôincertitude.\nL‚Äô utilisation de degr√©s de certitude permet √† l‚Äô√©tudiant et √† l‚Äô√©tudiante d‚Äô√©valuer le niveau de connaissance de la r√©ponse donn√©e. La bar√®me des notes, bas√© sur la th√©orie des d√©cision (Leclercq et al. 1993), peut sembler bizarre mais il a √©t√© fait de tel mani√®re que:\n\ndire la v√©rit√© soit la strat√©gie qui rapporte le plus de points;\nceux qui s‚Äôauto-√©valuent bien gagnent plus de points que si l‚Äôon appliquait un bar√®me correctif tenant compte des probabilit√©s d‚Äôavoir la r√©ponse correcte al√©atoirement.\n\n\n\n\nSi vous condid√©rez que votre r√©ponse a une probabilit√© d‚Äô√™tre correcte comprise entre‚Ä¶\n\n\nChoisissez le degr√© de certitude‚Ä¶\n\n\nVous obtiendrez les points suivants en case de r√©ponse‚Ä¶\n\n\n\n\n\n\n\n\ncorrecte\n\n\nincorrecte\n\n\n\n\n0% et 25%\n\n\n0\n\n\n+13\n\n\n+4\n\n\n\n\n25% et 50%\n\n\n1\n\n\n+16\n\n\n+3\n\n\n\n\n50% et 70%\n\n\n2\n\n\n+17\n\n\n+2\n\n\n\n\n70% et 85%\n\n\n3\n\n\n+18\n\n\n0\n\n\n\n\n85% et 95%\n\n\n4\n\n\n+19\n\n\n-6\n\n\n\n\n95% et 100%\n\n\n5\n\n\n+20\n\n\n-10\n\n\n\nG√©n√©ralement, les √©tudiants s‚Äôauto-estiment avec r√©alisme, et sont avantag√©s par les degr√©s de certitude. Leur score obtenu est meilleur que s‚Äôil avait √©t√© calcul√© uniquement sur la base du nombre de r√©ponses justes.",
    "crumbs": [
      "Informations",
      "√âvaluations",
      "QCM"
    ]
  },
  {
    "objectID": "informations/mcq.html#information-sur-le-premier-examen",
    "href": "informations/mcq.html#information-sur-le-premier-examen",
    "title": "QCM",
    "section": "",
    "text": "Le premier examen est un questionaire √† choix multiples avec quantification de l‚Äôincertitude.\nL‚Äô utilisation de degr√©s de certitude permet √† l‚Äô√©tudiant et √† l‚Äô√©tudiante d‚Äô√©valuer le niveau de connaissance de la r√©ponse donn√©e. La bar√®me des notes, bas√© sur la th√©orie des d√©cision (Leclercq et al. 1993), peut sembler bizarre mais il a √©t√© fait de tel mani√®re que:\n\ndire la v√©rit√© soit la strat√©gie qui rapporte le plus de points;\nceux qui s‚Äôauto-√©valuent bien gagnent plus de points que si l‚Äôon appliquait un bar√®me correctif tenant compte des probabilit√©s d‚Äôavoir la r√©ponse correcte al√©atoirement.\n\n\n\n\nSi vous condid√©rez que votre r√©ponse a une probabilit√© d‚Äô√™tre correcte comprise entre‚Ä¶\n\n\nChoisissez le degr√© de certitude‚Ä¶\n\n\nVous obtiendrez les points suivants en case de r√©ponse‚Ä¶\n\n\n\n\n\n\n\n\ncorrecte\n\n\nincorrecte\n\n\n\n\n0% et 25%\n\n\n0\n\n\n+13\n\n\n+4\n\n\n\n\n25% et 50%\n\n\n1\n\n\n+16\n\n\n+3\n\n\n\n\n50% et 70%\n\n\n2\n\n\n+17\n\n\n+2\n\n\n\n\n70% et 85%\n\n\n3\n\n\n+18\n\n\n0\n\n\n\n\n85% et 95%\n\n\n4\n\n\n+19\n\n\n-6\n\n\n\n\n95% et 100%\n\n\n5\n\n\n+20\n\n\n-10\n\n\n\nG√©n√©ralement, les √©tudiants s‚Äôauto-estiment avec r√©alisme, et sont avantag√©s par les degr√©s de certitude. Leur score obtenu est meilleur que s‚Äôil avait √©t√© calcul√© uniquement sur la base du nombre de r√©ponses justes.",
    "crumbs": [
      "Informations",
      "√âvaluations",
      "QCM"
    ]
  },
  {
    "objectID": "informations/schedule.html",
    "href": "informations/schedule.html",
    "title": "Plan",
    "section": "",
    "text": "Le plan est donn√© √† titre indicatif.\n\n\n\nSemaine\nPlan\n√âvaluations\n\n\n\n\n1er septembre\nPr√©sentation du cours et rappels\n\n\n\n8 septembre\nRappels\n\n\n\n15 septembre\nG√©n√©ralit√©s\n\n\n\n22 septembre\nG√©n√©ralit√©s\n\n\n\n29 septembre\nACP\n\n\n\n6 octobre\nAFC\n\n\n\n13 octobre\nACM\n\n\n\n20 octobre\nAnalyse discriminante\nExamen intra\n\n\n27 octobre\nSemaine de lecture\nExamen de reprise\n\n\n3 novembre\nArbres\n\n\n\n10 novembre\nMod√®les ensemblistes\n\n\n\n17 novembre\n\\(k\\)-moyennes\n\n\n\n24 novembre\nM√©thodes hi√©rarchiques + Mod√®les de m√©langes\n\n\n\n1er d√©cembre\nR√©vision + F√™te de l‚Äôuniversit√©\nRemise du poster\n\n\n8 d√©cembre\n\nPr√©sentation du poster + Examen final\n\n\n15 d√©cembre\n\nExamen de reprise",
    "crumbs": [
      "Informations",
      "Plan"
    ]
  },
  {
    "objectID": "informations/cours.html",
    "href": "informations/cours.html",
    "title": "Cours",
    "section": "",
    "text": "Voici les enregistrements des cours pour le semestre Automne 2025:\n\nCours du 02 septembre 2025: lien.",
    "crumbs": [
      "Informations",
      "Cours"
    ]
  },
  {
    "objectID": "informations/general.html",
    "href": "informations/general.html",
    "title": "Informations g√©n√©rales",
    "section": "",
    "text": "Facult√© des sciences et de g√©nie\nD√©partement de math√©matiques et de statistique",
    "crumbs": [
      "Informations",
      "G√©n√©rales"
    ]
  },
  {
    "objectID": "informations/general.html#stt-2200-analyse-de-donn√©es",
    "href": "informations/general.html#stt-2200-analyse-de-donn√©es",
    "title": "Informations g√©n√©rales",
    "section": "STT-2200: Analyse de donn√©es",
    "text": "STT-2200: Analyse de donn√©es\nFormule d‚Äôenseignement: Pr√©sentiel\nTemps consacr√©:\n\nCours: 2h\nLaboratoire: 1h\nTravail personnel: 6h\nTotal: 9h\n\nCr√©dits: 3\nPr√©alables:\n\nACT-2000 ou STT-1000 ou STT-1300 ou STT-1900\nMAT-1200 ou ACT-2002\nSTT-1100 ou IFT-4902 ou ECN-2090 ou GLO-1901 ou IFT-1004\n\nPlage horaire:\n\n\n\nType\nJour\nHoraire\nSalle\n\n\n\n\nLabo\nMardi\n10h30-12h20\nPLT-3928\n\n\nClasse\nVendredi\n08h30-10h20\nVCH-3830",
    "crumbs": [
      "Informations",
      "G√©n√©rales"
    ]
  },
  {
    "objectID": "informations/general.html#coordonn√©es-et-disponibilit√©s",
    "href": "informations/general.html#coordonn√©es-et-disponibilit√©s",
    "title": "Informations g√©n√©rales",
    "section": "Coordonn√©es et disponibilit√©s",
    "text": "Coordonn√©es et disponibilit√©s\nEnseignant: Steven Golovkine\nLocal: VCH-2209\nE-mail: steven.golovkine@mat.ulaval.ca\nDisponibilit√©s: Je suis disponible pour vous rencontrer sur rendez-vous par couriel ou sans rendez-vous du lundi au jeudi entre 8h et 10h.\n\nL‚ÄôUniversit√© reconna√Æt le droit √† la d√©connexion des professeures et professeurs, des personnes charg√©es de cours et des autres membres du personnel enseignant. Cela signifie que ces personnes ne sont pas tenues de consulter les messages qui leur sont envoy√©s (courriel, bo√Æte vocale, message dans un forum, etc.) pendant les soirs, fins de semaine, jours f√©ri√©s et vacances. La personne qui aura envoy√© un message durant ces p√©riodes devra donc s‚Äôattendre √† recevoir une r√©ponse dans un d√©lai raisonnable, calcul√© √† partir de la reprise des heures normales de travail.",
    "crumbs": [
      "Informations",
      "G√©n√©rales"
    ]
  },
  {
    "objectID": "contents/05-supervisee.html",
    "href": "contents/05-supervisee.html",
    "title": "Supervis√©e",
    "section": "",
    "text": "Slides: link\nTD: link\nTP: link",
    "crumbs": [
      "Modules",
      "05 - Supervis√©e"
    ]
  },
  {
    "objectID": "contents/05-supervisee.html#sommaire",
    "href": "contents/05-supervisee.html#sommaire",
    "title": "Supervis√©e",
    "section": "Sommaire",
    "text": "Sommaire\n\nAnalyse discriminante\nArbres\nMod√®les ensemblistes\n\n\n\n\nExtrapolating (xkcd:605).",
    "crumbs": [
      "Modules",
      "05 - Supervis√©e"
    ]
  },
  {
    "objectID": "contents/supervised/01-discriminant.html",
    "href": "contents/supervised/01-discriminant.html",
    "title": "Analyse discriminante",
    "section": "",
    "text": "L‚Äôanalyse discriminante vise √† classer des individus dans diff√©rents groupes √† partir de variables explicatives continues. C‚Äôest une m√©thode supervis√©e : les groupes sont connus dans les donn√©es d‚Äôapprentissage et l‚Äôobjectif est d‚Äôapprendre une r√®gle de classification optimale.",
    "crumbs": [
      "Modules",
      "05 - Supervis√©e",
      "Analyse discriminante"
    ]
  },
  {
    "objectID": "contents/supervised/01-discriminant.html#notation",
    "href": "contents/supervised/01-discriminant.html#notation",
    "title": "Analyse discriminante",
    "section": "Notation",
    "text": "Notation\nSoit \\(X = (X_{i j}) \\in \\mathbb{R}^{n \\times p}\\) un matrice de donn√©es, o√π \\(n\\) est le nombre d‚Äôindividus dans l‚Äô√©chantillon, \\(p\\) est le nombre de variables et \\(X_{i j}\\) est la valeur de la \\(j\\)e variable pour le \\(i\\)e individus.\nOn suppose qu‚Äôil y a \\(K\\) groupes dans la population et que chaque individu appartient √† l‚Äôun des \\(K\\) groupes. Pour \\(k \\in \\{ 1, \\dots, K \\}\\), on note \\(I_k\\) l‚Äôensemble des individus du groupe \\(k\\), \\(n_k\\) le nombre d‚Äôobservations dans \\(I_k\\). On a donc \\(\\sum_{k = 1}^{K} n_k = n\\).",
    "crumbs": [
      "Modules",
      "05 - Supervis√©e",
      "Analyse discriminante"
    ]
  },
  {
    "objectID": "contents/supervised/01-discriminant.html#objectif",
    "href": "contents/supervised/01-discriminant.html#objectif",
    "title": "Analyse discriminante",
    "section": "Objectif",
    "text": "Objectif\nNos observations sont dans \\(\\mathbb{R}^p\\). Pour faire de la classification √† partir de \\(X_{1}, \\dots, X_p\\), on doit partionner \\(\\mathbb{R}^p\\) en \\(K\\) sous-ensembles de sorte que chacun des \\(K\\) sous-ensembles soit associ√© √† l‚Äôun des \\(K\\) groupes. Dans le cas de l‚Äôanalyse discriminante, l‚Äôid√©e est de passer de \\(\\mathbb{R}^p\\) √† \\(\\mathbb{R}\\) en calculant, pour chaque observation, un score \\(f(X_{1}, \\dots, X_p) \\in \\mathbb{R}\\) et ensuite utiliser ce score pour d√©terminer le groupe d‚Äôappartenance (et donc de partionner \\(\\mathbb{R}\\)). Le score propos√© par Fisher est une combinaison lin√©aire des variables, i.e. \\[f(X_{1}, \\dots, X_p) = a^{\\top} X + b = a_{1} X_{1} + \\cdots + a_p X_p + b.\\] Cette fonction de score nous permet de regrouper les individus d‚Äôun m√™me groupe dans des zones proches du score et de s√©parer autant que possible les diff√©rents groupes selon ce score. On en d√©duira \\(K\\) intervalles de d√©cision \\(S_{1}, \\dots, S_K\\) associ√©s aux groupes.\n\n\n\n\n\n\nAstuceRemarque\n\n\n\nSans perte de g√©n√©ralit√©, on peut choisir \\[-b = a_{1} \\overline{X}_1 + \\dots + a_p \\overline{X}_p = a^{\\top} \\overline{X}\\] ce qui permet de centrer les variables en enlevant le vecteur de moyenne \\[\\overline{X} = \\left( \\overline{X}_1, \\dots, \\overline{X}_p \\right).\\]\nIl ne reste plus qu‚Äô√† choisir le vecteur \\(a = (a_{1}, \\dots, a_p)\\).",
    "crumbs": [
      "Modules",
      "05 - Supervis√©e",
      "Analyse discriminante"
    ]
  },
  {
    "objectID": "contents/supervised/01-discriminant.html#crit√®re-de-fisher",
    "href": "contents/supervised/01-discriminant.html#crit√®re-de-fisher",
    "title": "Analyse discriminante",
    "section": "Crit√®re de Fisher",
    "text": "Crit√®re de Fisher\nL‚Äôid√©e centrale de Fisher est d‚Äôoptimiser le rapport entre la variabilit√© inter-groupes et la variabilit√© intra-groupe en fonction du vectuer \\(a\\). Dit autrement, on voudrait choisir le vecteur \\(a\\) de sorte que les scores soient, √† la fois, tr√®s diff√©rents entre les groupes et tr√®s similaires √† l‚Äôint√©rieur d‚Äôun groupe. On s‚Äôint√©resse donc √† la variabilit√© des scores √† l‚Äôint√©rieur des groupes et entre les groupes.\nNotons\n\n\\(S\\), la matrice de variance-covariance totale donn√©e par\n\n\\[S = \\sum_{i = 1}^{n} (X_i - \\overline{X})(X_i - \\overline{X})^{\\top} \\quad \\text{o√π}\\quad \\overline{X} = \\frac{1}{n} \\sum_{i = 1}^{n} X_i;\\]\n\n\\(W\\), la matrice de variance-covariance intra-groupe donn√©e par\n\n\\[W = \\sum_{k = 1}^{K} \\sum_{i \\in I_k} (X_i - \\overline{X}_k)(X_i - \\overline{X}_k)^{\\top} \\quad \\text{o√π} \\quad \\overline{X}_k = \\frac{1}{n_k} \\sum_{i \\in I_k} X_i;\\]\n\n\\(B\\), la matrice de variance-covariance inter-groupe donn√©e par\n\n\\[B = \\sum_{k = 1}^{K} n_k (\\overline{X}_k - \\overline{X})(\\overline{X}_k - \\overline{X})^{\\top}.\\]\n√âtant donn√© que \\(a \\in \\mathbb{R}^p\\), on a: \\[\\mathrm{Var}(f(X_{1}, \\dots, X_p)) = \\mathrm{Var}(a^{\\top} X) = a^{\\top} \\mathrm{Var}(X) a.\\] Comme \\(S\\) est un estimateur de la variance totale de \\(X\\), nous pouvons estimer cette variance par \\[\\widehat{\\mathrm{Var}}(f(X_{1}, \\dots, X_p)) = \\frac{1}{n} a^{\\top} S a.\\]\nLa base de l‚Äôanalyse discriminante repose sur le fait que \\[S = W + B.\\]\n\n\n\n\n\n\nAstucePreuve que \\(S = W + B\\)\n\n\n\n\n\nOn peut prouver ce r√©sultat en consid√©rant la d√©finition des matrices \\(S, W\\) et \\(B\\). La moyenne de la variable \\(j\\) pour tous les individus de l‚Äô√©chantillon est \\[\\overline{X}_j = \\frac{1}{n}\\sum_{i = 1}^{n} X_{i j}.\\]\nLa moyenne de la variable \\(j\\) pour les individus du groupe \\(k\\) est \\[\\overline{X}_{k j} = \\frac{1}{n_{k}} \\sum_{i \\in I_k} X_{i j}.\\]\nLa somme des carr√©s totale est \\[s_{j j^\\prime} \\sum_{i = 1}^{n} (X_{i j} - \\overline{X}_j)(X_{i j^\\prime} - \\overline{X}_{j^\\prime}).\\]\nOn tirerait de la matrice \\(S\\) une estimation de \\(\\mathrm{Cov}(X_j, X_{j^\\prime})\\) si toutes les observations provenaient d‚Äôun m√™me groupe. On d√©finit \\(s_{j j^\\prime}\\) comme √©tant \\[s_{j j^\\prime} = w_{j j^\\prime} + b_{j j^\\prime},\\] o√π \\[w_{j j^\\prime} = \\sum_{k = 1}^{q} \\sum_{i \\in I_k} (X_{i j} - \\overline{X}_{k j})(X_{i j^\\prime} - \\overline{X}_{k j^\\prime}),\\]\n\\[b_{j j^\\prime} = \\sum_{k = 1}^{q} n_k (\\overline{X}_{k j} - \\overline{X}_j)(\\overline{X}_{k j^\\prime} - \\overline{X}_{j^\\prime}).\\]\nPreuve:\n\nPoser \\(X_{ij} - \\overline{X}_j = X_{ij} - \\overline{X}_{kj} + \\overline{X}_{kj} - \\overline{X}_j\\), dans la d√©finition de \\(s_{j j^\\prime}\\), iden pour \\(X_{i j^\\prime}\\).\nD√©velopper les produits.\nRemplacer \\(\\sum_{i = 1}^{n}\\) par \\(\\sum_{k = 1}^{q} \\sum_{i \\in I_k}\\).\nFaire les simplications appropri√©es.\n\n\n\n\nAinsi, en rempla√ßant \\(S\\) par \\(W + B\\), on obtient \\[\\widehat{\\mathrm{Var}}(a^{\\top} X) = \\frac{1}{n} a^{\\top} S a = \\frac{1}{n} \\left( a^{\\top} W a + a^{\\top} B a \\right).\\] Le crit√®re de Fisher s‚Äô√©crit donc, pour \\(a \\in \\mathbb{R}^p\\), \\[J(a) = \\frac{a^{\\top} B a}{a^{\\top} W a}.\\] Celui-ci peut se r√©√©crire de fa√ßon √©quivalente comme \\[J(a) = \\frac{a^{\\top} B a}{a^{\\top} S a}.\\] On cherche √† maximiser \\(J(a)\\). Ce probl√®me peut √™tre reformul√© des fa√ßons suivantes, toutes √©quivalentes:\n\nMaximiser \\(J(a)\\) sous la contrainte que \\(a^{\\top} a = 1\\).\nMaximiser \\(a^{\\top} B a\\) sous la contrainte que \\(a^{\\top} S a = 1\\).\nMaximiser \\(c^{\\top} S^{-1/2} B S^{-1/2} c\\) sous la contrainte que \\(c^{\\top} c = 1\\), o√π \\(c = S^{1/2} a\\).\n\nEn r√©√©crivant la troisi√®me formulation, on obtient \\[c^{\\top} \\left( S^{-1/2} B S^{-1/2} \\right) c \\quad\\text{s.c.}\\quad c^{\\top} c = 1,\\] on peut prendre \\(a = S^{-1/2} c\\), o√π \\(c\\) est un vecteur propre norm√© associ√© √† \\(\\lambda_{1}\\), la premi√®re valeur propre de \\(S^{-1/2} B S^{-1/2}\\). De fa√ßon √©quivalente, de la deuxi√®me formulation, on peut prendre \\(a\\), un vecteur propre norm√© associ√© √† \\(\\lambda_{1}\\) la premi√®re valeur propre de \\(S^{-1} B\\). Notons que comme, pour \\(\\lambda\\) une valeur propre quelconque de \\(S^{-1/2} B S^{-1/2}\\), \\[S^{-1/2} B S^{-1/2} c = \\lambda c \\quad\\text{et}\\quad a = S^{-1/2} c,\\] alors \\[S^{-1/2} B a = \\lambda S^{1/2} a \\Rightarrow S^{-1} B a = \\lambda a.\\] Les valeurs propres de \\(S^{-1} B\\) et de \\(S^{-1/2} B S^{-1/2}\\) sont donc les m√™mes et on peut facilement passer des vecteurs propres de \\(S^{-1} B\\) √† ceux de \\(S^{-1/2} B S^{-1/2}\\).",
    "crumbs": [
      "Modules",
      "05 - Supervis√©e",
      "Analyse discriminante"
    ]
  },
  {
    "objectID": "contents/supervised/01-discriminant.html#fonction-discriminante",
    "href": "contents/supervised/01-discriminant.html#fonction-discriminante",
    "title": "Analyse discriminante",
    "section": "Fonction discriminante",
    "text": "Fonction discriminante\nLa fonction discriminante de Fisher est donc \\[f(x) = a^{\\top} (x - \\overline{X}),\\] o√π \\(a\\) est le vecteur propre norm√© associ√© √† la plus grande valeur propre de \\(S^{-1} B\\). Les scores, i.e.¬†la repr√©sentation des observations dans \\(\\mathbb{R}\\), sont donn√©es par \\(U_i = a^{\\top} (X_{i} - \\overline{X})\\). Ces scores sont des combinaisons lin√©aires des variables et ils maximisent le rapport variance intra-groupe / variance inter-groupes.\n\n\n\n\n\n\nNoteRemarque\n\n\n\nOn peut aussi prendre \\(U_i = a^{\\top} X_i\\), car ajouter la m√™me constante √† toutes les observations \\(i = 1, \\dots, n\\) ne fait que d√©caler la repr√©sentation dans \\(\\mathbb{R}\\).\n\n\n\nPouvoir discriminant\nPuisque la matrice \\(S^{-1/2} B S^{-1/2}\\) est sym√©trique et d√©finie positive, ses valeurs propres sont toutes r√©elles et positives. De plus, on a que \\(S^{-1} B a = \\lambda_{1} a\\). Ainsi, \\[B a = \\lambda_{1} S a \\Rightarrow a^{\\top} B a = \\lambda_{1} a^{\\top} S a \\Rightarrow \\lambda_{1} = \\frac{a^{\\top} B a}{a^{\\top} S a}.\\]\nOn a donc \\(0 \\leq \\lambda_{1} \\leq 1\\). La valeur propre \\(\\lambda_{1}\\) peut donc √™tre vue comme le pouvoir discriminant de \\(f\\):\n\n\\(\\lambda_{1} = 1 \\Rightarrow a^{\\top} B a = a^{\\top} S a\\), donc \\(100\\%\\) de la variabilit√© est entre les groupes et il n‚Äôy a aucune variabilit√© √† l‚Äôint√©rieur des groupes. Cela repr√©sente le cas id√©al.\n\\(\\lambda_{1} = 0 \\Rightarrow a^{\\top} B a = 0\\), donc il n‚Äôy a aucune variabilit√© entre les gorupes et toute la variabilit√© est concentr√©e √† l‚Äôint√©rieur des groupes. Cela repr√©sente le cas o√π l‚Äôanalyse discriminante est inutile.\n\n\n\nR√®gle de classification\nApr√®s avoir estimer la fonction discriminante \\(f(x)\\), on peut calculer le score moyen de chaque groupe \\(k\\) d√©fini comme √©tant \\[m_k = a^{\\top} \\left( \\overline{X}_{k 1}, \\dots, \\overline{X}_{k p} \\right)^{\\top},\\] o√π \\(\\overline{X}_{k j}, j = 1, \\dots, p\\) est la moyenne de la \\(j\\)e variable pour les individus appartenant au \\(k\\)e groupe.\nConsid√©rons maintenant une nouvelle observation \\(X_{0} \\in \\mathbb{R}^p\\) qui n‚Äô√©tait pas dans le jeu de donn√©es lors de l‚Äôestimation de \\(f(x)\\). Pour classer ce nouvel individu dans un groupe de la population, on calcule son score \\(f(X_{0}) = a^{\\top} X_{0}\\). Ensuite, on l‚Äôassigne au groupe \\(k^\\star\\) dont le score moyen \\(m_k\\) est le plus proche de \\(f(X_0)\\), i.e.¬†le groupe tel que \\[\\left| a^{\\top} X_{0} - m_{k_0} \\right| = \\min_{1 \\leq k \\leq K} \\left| a^{\\top} X_{0} - m_k \\right|.\\] Autrement dit, on a \\[k^\\star = \\arg\\min_{k} \\left| a^{\\top} X_0 - m_k \\right|.\\] En applicant cette r√®gle √† l‚Äô√©chantillon \\(X_{1}, \\dots, X_n\\), on peut estimer les risques de mauvaise classification avec la matrice de confusion.\n\n\nCas particulier de la classification binaire\nDans le cas o√π \\(K = 2\\), il est possible de calculer explicitement la valeur de vecteur propre \\(a\\). Soit \\[C =  \\sqrt{\\frac{n_{1} n_{2}}{n}} (\\overline{X}_1 - \\widetilde{X}_2),\\] \\(\\overline{X}_i, i = 1, 2\\) sont les vecteurs moyens des caract√©riques dans chaque groupe, alors \\[B = C C^{\\top}, \\quad a = S^{-1} C.\\]\nSupposons que \\[ m_{1} = a^{\\top} \\overline{X}_1 &gt; a^{\\top} \\overline{X}_2 = m_{2}.\\] Alors, on classe un individu \\(X_0\\) dans le premier groupe si \\[a^{\\top} X_0 &gt; \\overline{m} = \\frac{m_{1} + m_{2}}{2} = a^{\\top} \\left( \\frac{\\overline{X}_1 + \\overline{X}_2}{2} \\right),\\] et dans le second groupe sinon. Cette r√®gle de classification est √©quivalente √† \\[(\\overline{X}_1 - \\overline{X}_2)^\\top S^{-1} X_0 &gt; \\frac{1}{2}(\\overline{X}_1 - \\overline{X}_2)^\\top S^{-1} (\\overline{X}_1 + \\overline{X}_2).\\]",
    "crumbs": [
      "Modules",
      "05 - Supervis√©e",
      "Analyse discriminante"
    ]
  },
  {
    "objectID": "contents/06-nonsupervisee.html",
    "href": "contents/06-nonsupervisee.html",
    "title": "Non-supervis√©e",
    "section": "",
    "text": "Slides:\n\n\\(k\\)-moyennes\nM√©thodes hi√©rarchiques\nM√©langes de gaussiennes\n\nTD: link\nTP: link",
    "crumbs": [
      "Modules",
      "06 - Non-supervis√©e"
    ]
  },
  {
    "objectID": "contents/06-nonsupervisee.html#sommaire",
    "href": "contents/06-nonsupervisee.html#sommaire",
    "title": "Non-supervis√©e",
    "section": "Sommaire",
    "text": "Sommaire\n\n\\(k\\)-moyennes\nM√©thodes hi√©rarchiques\nM√©langes de gaussiennes\n\n\n\n\n\\(k\\)-means (xkcd:2731).",
    "crumbs": [
      "Modules",
      "06 - Non-supervis√©e"
    ]
  },
  {
    "objectID": "contents/misc/good-practices.html",
    "href": "contents/misc/good-practices.html",
    "title": "Bonnes pratiques de programmation",
    "section": "",
    "text": "Cette page est bas√© sur un document qu‚ÄôAur√©lien Nicosia (ULaval) a cr√©√© en 2023 appel√© ‚ÄúBonnes pratiques de programmation en R‚Äù. Celui-ci a √©t√© mis √† jour et √©crit dans un contexte plus g√©n√©ral.\nPeu importe le langage informatique, employer de bonnes pratiques de programmation signifie respecter certaines normes afin de cr√©er du ‚Äúbon‚Äù code. On peut donc se demander ce qu‚Äôest un ‚Äúbon‚Äù code. Pour moi, un ‚Äúbon‚Äù code est un code qui remplit trois objectifs:\nPourquoi est-ce souhaitable d‚Äôadopter de bonnes pratiques ? Cela permet que le code soit compris et utilis√© par n‚Äôimporte qui (et en particulier, soi-m√™me dans le futur). √Ä long terme, les bonnes pratiques apportent une augmentation de notre productivit√© en √©vitant les r√©p√©titions inutiles."
  },
  {
    "objectID": "contents/misc/good-practices.html#objectif-1-produire-les-r√©sultats-escompt√©s",
    "href": "contents/misc/good-practices.html#objectif-1-produire-les-r√©sultats-escompt√©s",
    "title": "Bonnes pratiques de programmation",
    "section": "Objectif 1 : Produire les r√©sultats escompt√©s",
    "text": "Objectif 1 : Produire les r√©sultats escompt√©s\nLa priorit√© lors du d√©veloppement de tout code informatique est certainement l‚Äô√©criture d‚Äôun code qui r√©alise bien ce qu‚Äôil doit r√©aliser. Donc, un ‚Äúbon‚Äù‚Äù code doit produire les bons r√©sultats. Pour y arriver, le code doit d‚Äôabord √™tre fonctionnel, c‚Äôest-√†-dire ne pas contenir de bogues. Pour s‚Äôassurer d‚Äô√©crire du code qui fonctionne correctement, il faut simplement le tester. Il vaut mieux tester fr√©quemment, √† chaque petit ajout, plut√¥t que de produire beaucoup de code avant de le tester. Ainsi, il y a beaucoup moins de d√©bogage √† faire. Un courant de pens√©e en informatique pr√¥ne m√™me l‚Äô√©criture des tests avant l‚Äô√©criture du code (test driven development). Cependant, une meilleure pratique est de formaliser les tests afin de pouvoir facilement les lancer √† nouveau lors de modifications futures apport√©es au code."
  },
  {
    "objectID": "contents/misc/good-practices.html#objectif-2-code-facile-√†-maintenir",
    "href": "contents/misc/good-practices.html#objectif-2-code-facile-√†-maintenir",
    "title": "Bonnes pratiques de programmation",
    "section": "Objectif 2 : Code facile √† maintenir",
    "text": "Objectif 2 : Code facile √† maintenir\nMaintenir un code informatique signifie de s‚Äôassurer qu‚Äôil continue de fonctionner correctement dans le futur, malgr√© les modifications qui lui sont apport√©es. Un code utilis√© fr√©quemment est un code appel√© √† √™tre mis √† jour, soit pour y ajouter des fonctionnalit√©s, soit pour corriger des bogues non d√©tect√©s par les tests, mais d√©couverts par des utilisateurs. Reprendre un code √©crit par quelqu‚Äôun d‚Äôautre, ou √©crit par nous-m√™mes quelques mois auparavant, n‚Äôest pas toujours une t√¢che facile. Cependant, s‚Äôil s‚Äôagit d‚Äôun code correctement √©crit, il ne devrait pas √™tre trop difficile √† comprendre et √† modifier.\nLa maintenance d‚Äôun code est bas√© sur trois principes: son versionnage, sa compr√©hensibilit√© et sa r√©utilisabilit√©.\nLe principe du versionnage est d‚Äôutiliser un logiciel qui enregistre les diff√©rentes modifications faites sur le code. Le plus connu est Git. Il permet de naviguer entre les diff√©rentes versions de votre code, de cr√©er plusieurs versions (appel√©e branches), de collaborer √† plusieurs sur un m√™me code. C‚Äôest vraiment un indispensable √† avoir dans sa panoplie. Le versionnage peut ensuite √™tre enregistr√© sur Github qui g√®re la machinerie sous-jacente. √Ä titre d‚Äôexemple, ce site utilise Git et Github pour son versionnage et vous pouvez voir les diff√©rentes versions ici.\nUn code compr√©hensible est clair et se lit bien (presque comme du texte). Il comporte souvent des instructions qui parlent d‚Äôelles-m√™mes. Ces instructions sont typiquement succinctes, car une instruction trop longue effectue souvent plusieurs t√¢ches difficilement discernables. Si la lecture d‚Äôune instruction ne permet pas √† un programmeur initi√© dans le langage informatique employ√© de comprendre ce qu‚Äôelle r√©alise, il est alors recommand√© d‚Äôins√©rer un commentaire dans le code pour expliquer √† quoi sert l‚Äôinstruction. En plus de commentaires pour expliquer certains instructions, toutes fonctions devraient √™tre document√©es. La documentation d‚Äôune fonction devrait contenir: un texte explicatif de ce que fait la fonction, une description des arguments accept√©s en entr√©e, une description des r√©sultats produits et un exemple d‚Äôutilisation. Lorsque que l‚Äôon programme, il est aussi de bon ton de suivre un guide de style. Un guide de style est un ensemble de r√®gles sur lesquelles les d√©veloppeurs se sont mis d‚Äôaccord et qui permet d‚Äôavoir une syntaxe similaire √† travers les diff√©rents projets. En R, vous pouvez utiliser le guide de style du tidyverse. En Python, vous pouvez utiliser le guide de style PEP8 √©crit par le cr√©ateur de Python. Et en Julia, vous pouvez utiliser le guide de style fournit avec le manuel du langage. Vous pouvez utiliser un linter, un outil d‚Äôanalyse de code statique pour vous aidez √† respecter ces guides de styles. √Ä noter que ces guides de style sont des recommendations et il n‚Äôy a rien d‚Äôobligatoire √† les suivre. Certaines r√®gles se contredisant, je vous recommanderai m√™me de ne pas en suivre certaines.\nLa fa√ßon la plus commune d‚Äôavoir un code facile √† r√©utiliser est d‚Äôen faire des fonctions que l‚Äôon peut ensuite partager √† travers un package."
  },
  {
    "objectID": "contents/misc/good-practices.html#objectif-3-code-suffisamment-rapide",
    "href": "contents/misc/good-practices.html#objectif-3-code-suffisamment-rapide",
    "title": "Bonnes pratiques de programmation",
    "section": "Objectif 3 : Code suffisamment rapide",
    "text": "Objectif 3 : Code suffisamment rapide\nApr√®s nous √™tre assur√©s que notre code fonctionne correctement et qu‚Äôil est facilement maintenable, nous pouvons nous pr√©occuper de son temps d‚Äôex√©cution. Bien qu‚Äôil ne s‚Äôagisse pas du crit√®re le plus important pour d√©finir ce qu‚Äôest du ‚Äúbon‚Äù code, c‚Äôest tout de m√™me un crit√®re √† ne pas n√©gliger, car un code trop lent risque de ne pas √™tre utilis√©. Pour produire du code computationnellement efficace, il faut :\n\nmettre en pratique quelques trucs simples, i.e.¬†utiliser les syntaxes optimis√©es des diff√©rents langages;\ncomparer le temps d‚Äôex√©cution de diff√©rentes fa√ßons de programmer une t√¢che;\nparfois faire du calcul en parall√®le;\nparfois programmer des bouts de code dans un autre langage plus bas niveau."
  },
  {
    "objectID": "contents/misc/good-practices.html#en-r√©sum√©",
    "href": "contents/misc/good-practices.html#en-r√©sum√©",
    "title": "Bonnes pratiques de programmation",
    "section": "En r√©sum√©",
    "text": "En r√©sum√©\nEn r√©sumer, pour adopter de bonnes pratiques de programmation, il faut :\n\nTester son code fr√©quemment son code.\nUtiliser un logiciel de gestion de versions.\nDocumenter son code.\nRespecter un guide de style.\nFactoriser son code en cr√©ant des fonctions et des packages.\nOptimiser le temps d‚Äôex√©cution."
  },
  {
    "objectID": "contents/misc/good-practices-python.html",
    "href": "contents/misc/good-practices-python.html",
    "title": "Bonnes pratiques en Python",
    "section": "",
    "text": "Cette page est inspir√©e sur ce document.\nDe m√™me que R, Python est ‚Äújuste‚Äù un langage de programmation, il est donc possible d‚Äô√©crire du code Python dans un √©diteur de texte et de lancer ce code via un terminal. Bien que ce soit faisable, il est plus simple d‚Äôutilser un √©diteur de code. Bien qu‚Äôil soit techniquement possible d‚Äôutiliser RStudio comme √©diteur pour Python, je vous recommanderais dans utiliser sp√©cialiser comme PyCharm. Le format notebook est aussi tr√®s populaire en Python. Dans ce cas, je vous conseillerais la librarie marimo qui permet de transformer ses fichiers .py en notebook.\nDe plus, il vaut mieux s‚Äôassurer de travailler avec la derni√®re version stable de Python comptatible avec les packages les utilis√©s (e.g.¬†numpy, matplotlib, sklearn). En effet, tr√®s souvent, il y a un d√©lai pour que les derni√®res versions des packages soient comptatibles avec la derni√®re version de Python. Il est donc important de partager la version des packages utilis√©s avec l‚Äôanalyse.\nConcernant la documentation, le standard en Python est de suivre les recommendations du PEP 257. La libraries Sphinx permet de g√©n√©rer une documentation √† partir des docstring.\nUne syntaxe uniforme rend un code beaucoup plus facile √† lire et √† comprendre. Un guide de style √©nonce des normes pour avoir une syntaxe uniforme. Le guide de style PEP 8 est recommand√© en Python.\nRetour √† la ligne et indentation\nL‚Äôindentation fait partie de Python, i.e.¬†les blocks if...else et for sont d√©finis gr√¢ce aux indentations. Il est possible d‚Äôutiliser un tab ou des espaces. Le choix vous appartient mais il est important d‚Äô√™tre consistant dans votre code.\nConvention de noms\nL‚Äôutilisation des noms de variables avec un seul caract√®re est √† √©viter. Sinon, les variables, fonctions, m√©thodes, packages et modules sont nomm√©s avec des lettres minuscules, des nombres et l‚Äôunderscore _. Les classes et exceptions doivent √™tre nomm√©s avec des majuscules pour s√©parer les mots (UneClasse). Les constants sont en majuscules.\nOrganisation du code\nDe m√™me que pour R, lorsque le code commence √† devenir long, il devient avantageux de le s√©parer en plusieurs fichiers. Par exemple, on peut avoir un fichier par partie de l‚Äôanalyse (un pour le nettoyage des donn√©es, un pour l‚Äôanalyse, un pour la visualisation, ‚Ä¶). On peut aussi cr√©er plusieurs sous-dosssiers.\nQuelques trucs √† faire\n\nNe pas faire de comparaison √† True, False or None.\n\n\nif attr:\n    print(\"True!\")\n\nif not attr:\n    print(\"False!\")\n\nif attr is None:\n    print(\"None!\")\n\n\nUtiliser la compr√©hension de liste lorsque cela est possible.\n\n\na = [3, 4, 5]\nb = [i for i in a if i &gt; 4]\n\n\nCharger un fichier avec with. Cela permet d‚Äô√™tre sur que la connexion avec le fichier est ferm√© une fois qu‚Äôil a √©t√© lu.\n\n\nwith open(\"file.txt\") as f:\n    read(f)\n\n\nUtiliser un maximum de 80 caract√®res par ligne.\nUtiliser des parenth√®ses pour aller √† la ligne dans les longues cha√Ænes de caract√®res."
  },
  {
    "objectID": "contents/02-revisions.html",
    "href": "contents/02-revisions.html",
    "title": "R√©visions",
    "section": "",
    "text": "Slides:\n\nAlg√®bre lin√©aire - annot√©\nProbabilit√©s et statistiques - annot√©\nAlgorithmique / Programmation\n\nTD: link\nTP: link",
    "crumbs": [
      "Modules",
      "02 - R√©visions"
    ]
  },
  {
    "objectID": "contents/02-revisions.html#sommaire",
    "href": "contents/02-revisions.html#sommaire",
    "title": "R√©visions",
    "section": "Sommaire",
    "text": "Sommaire\n\nAlg√®bre lin√©aire\nProbabilit√©s et statistiques\nAlgorithmique / Programmation\n\n\n\n\nPrediction (xkcd:2370).",
    "crumbs": [
      "Modules",
      "02 - R√©visions"
    ]
  },
  {
    "objectID": "contents/08-conclusion.html",
    "href": "contents/08-conclusion.html",
    "title": "Conclusion",
    "section": "",
    "text": "Pour conclure, il n‚Äôexiste aucun algorithme universellement meilleur permettant de r√©soudre tous les probl√®mes de classification ou de pr√©diction. On appelle ce ph√©nom√®ne le no free lunch theorem. Chaque jeu de donn√©es, chaque contexte d‚Äôanalyse, chaque objectif de mod√©lisation est unique. Il n‚Äôy a donc pas de solution pr√©-√©tablie : c‚Äôest donc au statisticien ou au data scientist de faire preuve de discernement, d‚Äôesprit critique, et de cr√©ativit√© pour adapter les m√©thodes aux donn√©es et √† la question pos√©e.\nCela dit, on peut d√©gager certains principes qui peuvent nous guider dans une d√©marche rigoureuse :\n\nCommencer par une exploration descriptive des donn√©es, en √©tudiant les distributions, les relations entre variables, les valeurs manquantes, etc.\nTirer parti de l‚Äôexpertise m√©tier, en consultant les personnes qui connaissent le contexte des donn√©es (experts du domaine, utilisateurs finaux, etc.).\nS‚Äôinspirer des m√©thodes ayant d√©j√† donn√© de bons r√©sultats dans des situations similaires, en gardant toutefois un regard critique.\n\nUne des principales difficult√©s en pratique est souvent li√©e √† la dimensionnalit√© du probl√®me : le nombre de mod√®les ou d‚Äôapproches possibles augmente rapidement avec le nombre de variables disponibles. Il devient alors crucial de simplifier l‚Äôespace de recherche, e.g.¬†r√©duire la dimension avec une ACP, qui permettent de r√©sumer l‚Äôinformation tout en limitant le bruit. Une approche fr√©quente consiste √† appliquer des m√©thodes d‚Äôanalyse √† un sous-ensemble de variables soigneusement choisies, ou √† utiliser les composantes principales comme nouvelles variables explicatives. Mais l√† encore, aucune r√®gle g√©n√©rale ne permet d‚Äôidentifier automatiquement le ‚Äúbon‚Äù sous-ensemble‚Ä¶\nEnfin, on peut noter que plusieurs questions centrales en analyse de donn√©es ont √©t√© laiss√©es de c√¥t√© dans ce cours. Pourtant, celles-ci sont omnipr√©sentes en pratique. Parmi ces questions :\n\nComment d√©finir les variables √† utiliser comme pr√©dicteurs (feature engineering) ?\nComment rep√©rer et g√©rer les donn√©es aberrantes (*outliers) ?\nComment scinder les donn√©es entre apprentissage, validation et test ?\nQue faire en pr√©sence de donn√©es manquantes ?\nLes donn√©es analys√©es sont-elles repr√©sentatives de la population cible ?\n\nCes questions soulignent que l‚Äôanalyse de donn√©es est autant un art qu‚Äôune science. Il ne suffit pas d‚Äôappliquer m√©caniquement des algorithmes : il faut formuler des hypoth√®ses claires, √©valuer leurs limites, et rester attentif aux enjeux √©thiques, sociaux et pratiques.",
    "crumbs": [
      "Modules",
      "08 - Conclusion"
    ]
  },
  {
    "objectID": "contents/remainders/02-probabilities.html",
    "href": "contents/remainders/02-probabilities.html",
    "title": "Probabilit√©s et Statistiques",
    "section": "",
    "text": "Dans cette partie, on pr√©sente quelques r√©sultats en probabilit√©s et statistiques dans le cadre de ce cours. Pour plus d‚Äôinformation, vous pouvez vous r√©f√©rer au cours STT-1000, √† Wasserman (2010) (en anglais) et √† Delmas (2013) (en fran√ßais).",
    "crumbs": [
      "Modules",
      "02 - R√©visions",
      "Probabilit√©s et Statistiques"
    ]
  },
  {
    "objectID": "contents/remainders/02-probabilities.html#mod√©liser-le-hasard",
    "href": "contents/remainders/02-probabilities.html#mod√©liser-le-hasard",
    "title": "Probabilit√©s et Statistiques",
    "section": "Mod√©liser le hasard",
    "text": "Mod√©liser le hasard\nBeaucoup de ph√©nom√®nes r√©els ne sont pas pr√©visibles et g√©n√©ralement, leurs r√©sultats contiennent une certaine variabilit√©. Cette variabilit√© est prise en compte gr√¢ce √† une mesure de l‚Äôincertitude que l‚Äôon appelle mesure de probabilit√©s.\n\n\n\n\n\n\nAvertissementD√©finition\n\n\n\nL‚Äôespace d‚Äô√©v√®nements \\(S\\) est l‚Äôensemble de tous les r√©sultats possibles d‚Äôun ph√©nom√®ne. Un √©v√®nement est un sous-ensemble de l‚Äôespace d‚Äô√©v√®nements \\(S\\).\n\n\n\n\n\n\n\n\nNoteExemples\n\n\n\n\nSi l‚Äôexp√©rience consiste √† lancer un pi√®ce, \\(S = \\{0, 1\\}\\). Le r√©sultat de cette exp√©rience ne peut pas √™tre connu √† l‚Äôavance. Par exemple, \\(E = \\{1\\}\\) est un √©v√®nement de \\(S\\).\nSi on s‚Äôint√©resse √† la dur√©e de vie d‚Äôun t√©l√©phone, \\(S = \\mathbb{R}_{+}\\). On peut aussi choisir \\(S = [0, M]\\), car cette dur√©e de vie n‚Äôest probablement pas infini ! L‚Äô√©v√®nement \\(E = [10, \\infty)\\) repr√©sente l‚Äô√©v√®nement ‚Äúune dur√©e de vie de plus de 10 unit√©s de temps‚Äù.\nPour le nombre de jours sans neige √† Qu√©bec dans l‚Äôann√©e, on peut choisir \\(S = \\mathbb{N}\\). L‚Äô√©v√®nement \\(E = (0, 5]\\) repr√©sente l‚Äô√©v√®nement ‚Äúmoins de 5 jours sans neige √† Qu√©bec dans l‚Äôann√©e‚Äù.\n\n\n\n\n\n\n\n\n\nAvertissementD√©finition\n\n\n\nUne mesure de probabilit√©s \\(\\mathbb{P}\\) sur \\(S\\) est une application (fonction) d√©finie sur l‚Äôespace d‚Äô√©v√®nements et satisfaisant les propri√©t√©s suivantes :\n\nPour chaque √©v√®nement \\(E\\), \\(\\mathbb{P}(E) \\in [0, 1]\\).\n\\(\\mathbb{P}(S) = 1\\).\nSoient \\(E_{1}, E_{2}, \\dots\\), une s√©quence d‚Äô√©v√®nements (finie ou infinie) mutuellement exclusive, i.e.¬†\\(\\forall i \\neq j, E_{i} \\cap E_{j} = \\varnothing\\). On a \\[\\mathbb{P}(\\bigcup_{n = 1}^{\\infty} E_n) = \\sum_{n = 1}^{\\infty} \\mathbb{P}(E_n).\\]\n\nOn appelle \\(\\mathbb{P}(E)\\), la probabilit√© de l‚Äô√©v√®nement \\(E\\).\n\n\nLa d√©finition de mesures de probabilit√©s peut √™tre subjective et li√© √† l‚Äôexp√©rience du statisticien. En reprenant l‚Äôexemple 3 sur le nombre de jours sans neige √† Qu√©bec dans l‚Äôann√©e. Une personne venant d‚Äôarriver au Canada peut vouloir donner la m√™me probabilit√© √† chacun des jours, alors qu‚Äôun Qu√©b√©cois aura plus d‚Äôinformation et pourras faire varier les probabilit√©s en fonction de cette connaissance.\n\n\n\n\n\n\nAvertissementD√©finition\n\n\n\nDeux √©v√®nements \\(E\\) et \\(F\\) sont dits ind√©pendants si \\(\\mathbb{P}(E \\cap F) = \\mathbb{P}(E) \\times \\mathbb{P}(F)\\).\n\n\n\n\n\n\n\n\nAvertissementD√©finition\n\n\n\nSoient \\(E\\) et \\(F\\), deux √©v√®nements, la probabilit√© conditionelle que \\(E\\) se r√©alise sachant que \\(F\\) s‚Äôest r√©alis√© est d√©finie par : \\[\\mathbb{P}(E \\mid F) = \\frac{\\mathbb{P}(E \\cap F)}{\\mathbb{P}(F)}.\\]\n\n\nDe fa√ßon intuitive, deux √©v√®nements sont ind√©pendants si la connaissance de l‚Äôun ne donne aucune information sur la r√©alisation de l‚Äôautre. On a aussi \\(\\mathbb{P}(E \\mid F) = \\mathbb{P}(E)\\).",
    "crumbs": [
      "Modules",
      "02 - R√©visions",
      "Probabilit√©s et Statistiques"
    ]
  },
  {
    "objectID": "contents/remainders/02-probabilities.html#variables-al√©atoires",
    "href": "contents/remainders/02-probabilities.html#variables-al√©atoires",
    "title": "Probabilit√©s et Statistiques",
    "section": "Variables al√©atoires",
    "text": "Variables al√©atoires\nEn probabilit√©, la convention est d‚Äôexprimer le r√©sultat d‚Äôexp√©riences comme la valeur d‚Äôune fonction appel√© variable al√©atoire. Cette caract√©risation est toujours possible.\n\n\n\n\n\n\nAvertissementD√©finition\n\n\n\nSoit une variable al√©atoire \\(X\\). La distribution de cette variable al√©atoire est d√©finie par l‚Äôapplication \\(A \\mapsto \\mathbb{P}(X \\in A)\\).\n\n\n\n\n\n\n\n\nAvertissementD√©finition\n\n\n\nSoit une variable al√©atoire \\(X\\). Cette variable al√©atoire est discr√®te si elle prend, au plus, un nombre d√©nombrable de valeurs. Dans ce cas, la distribution de \\(X\\) est donn√©e par les probabilit√©s \\(\\mathbb{P}(X = x)\\) pour tout r√©sultat \\(x\\).\n\n\n\n\n\n\n\n\nAvertissementD√©finition\n\n\n\nSoit une variable al√©atoire \\(X\\). Cette variable al√©atoire est continue si les probabilit√©s \\(\\mathbb{P}(X \\in A)\\) sont donn√©es par des int√©grales de la forme \\(\\int_{A} f(x) dx\\) o√π \\(f: \\mathbb{R}^d \\to \\mathbb{R}_+\\) est une fonction int√©grable tel que \\(\\int_{\\mathbb{R}^d} f(x) dx = 1\\). Notons que, pour un r√©sultat \\(x\\) fix√©, \\(\\mathbb{P}(X = x) = 0\\).\n\n\n\n\n\n\n\n\nAvertissementD√©finition\n\n\n\nSoit une variable al√©atoire \\(X\\). L‚Äôesp√©rance math√©matique \\(\\mathbb{E}(X)\\) de \\(X\\) est la valeur moyenne du r√©sultat de \\(X\\) par rapport √† sa distribution de probabilit√©. L‚Äôesp√©rance est g√©n√©ralement not√© \\(\\mu\\).\n\n\nSoit \\(F\\) un ensemble d√©nombrable. Une variable al√©atoire discr√®te \\(X\\) a pour esp√©rance \\(\\mathbb{E}(X) = \\sum_{x \\in F} x \\mathbb{P}(X = x)\\). Soit une variable al√©atoire continue \\(X\\) ayant pour densit√© \\(f\\), son esp√©rance est donn√©e par \\(\\mathbb{E}(X) = \\int_{\\mathbb{R}^d} x f(x) dx\\).\n\n\n\n\n\n\nMise en gardeTh√©or√®me de transfert\n\n\n\nSoit une variable al√©atoire \\(X\\). Soit \\(g: \\mathbb{R}^d \\mapsto \\mathbb{R}\\) une fonction telle que \\(\\mathbb{E}\\left[ g(X) \\right]\\) existe. On a :\n\nSi \\(X\\) est une variable al√©atoire discr√®te, \\(\\mathbb{E}\\left[ g(X) \\right] = \\sum_{x \\in F} g(x) \\mathbb{P}(X = x)\\);\nSi \\(X\\) est une variable al√©atoire continue de densit√© \\(f\\), \\(\\mathbb{E}\\left[ g(X) \\right] = \\int_{\\mathbb{R}^d} g(x)f(x) dx\\).\n\n\n\n\n\n\n\n\n\nImportantPropri√©t√©s: Lin√©arit√© de l‚Äôesp√©rance\n\n\n\nSoient \\(X\\) et \\(Y\\), deux variables al√©atoires, dont les esp√©rances sont d√©finies et soit \\(\\lambda \\in R\\). On a :\n\n\\(\\mathbb{E}(X + Y) = \\mathbb{E}(X) + \\mathbb{E}(Y)\\);\n\\(\\mathbb{E}(\\lambda X) = \\lambda \\mathbb{E}(X)\\).\n\n\n\n\n\n\n\n\n\nNotePreuve\n\n\n\n\n\nLa preuve se d√©duit du th√©or√®me de transfert et de la lin√©arit√© de l‚Äôaddition et de l‚Äôint√©gration.\n\n\n\n\n\n\n\n\n\nAvertissementD√©finition\n\n\n\nSoit \\(X\\) une variable al√©atoire telle que l‚Äôesp√©rance de son carr√© existe. La variance de \\(X\\) est d√©finie par \\[\\mathrm{Var}(X) = \\mathbb{E}\\left[ \\left( X - \\mathbb{E}(X) \\right)^2 \\right] = \\mathbb{E}\\left[ X^2 \\right] - \\mathbb{E}\\left[ X \\right]^2.\\]\n\n\nLa variance mesure la dispersion d‚Äôune variable al√©atoire autour de sa moyenne. On peut aussi s‚Äôint√©resser √† l‚Äô√©cart-type, d√©fini comme la racine carr√©e de la variance : \\(\\sigma(X) = \\sqrt{\\mathrm{Var}(X)}\\).\n\n\n\n\n\n\nAvertissementD√©finition\n\n\n\nSoient \\(X\\) et \\(Y\\), deux variables al√©atoires et \\(A\\) et \\(B\\), deux √©venements. Si les √©venements \\(\\left\\{ X \\in A \\right\\}\\) et \\(\\left\\{ Y \\in B \\right\\}\\) sont ind√©pendants, alors on dit que les variables al√©atoires \\(X\\) et \\(Y\\) sont ind√©pendantes.\n\n\nDe cette d√©finition, on en d√©duit que :\n\npour des fonctions \\(f\\) et \\(g\\), les variables al√©atoires \\(f(X)\\) et \\(g(Y)\\) sont ind√©pendantes;\nsi les variables al√©atoires \\(X\\) et \\(Y\\) sont √† valeurs r√©elles et que leur esp√©rance existe, alors l‚Äôesp√©rance du produit \\(XY\\) existe et \\(\\mathbb{E}(XY) = \\mathbb{E}(X) \\times \\mathbb{E}(Y)\\).\n\n\n\n\n\n\n\nAvertissementD√©finition\n\n\n\nSoit \\(X\\) une variable al√©atoire. La fonction de r√©partition \\(F: \\mathbb{R} \\mapsto [0, 1]\\) de \\(X\\) est d√©finie par \\[F(t) = \\mathbb{P}(X \\leq t), \\quad t \\in \\mathbb{R}.\\]",
    "crumbs": [
      "Modules",
      "02 - R√©visions",
      "Probabilit√©s et Statistiques"
    ]
  },
  {
    "objectID": "contents/remainders/02-probabilities.html#vecteurs-al√©atoires",
    "href": "contents/remainders/02-probabilities.html#vecteurs-al√©atoires",
    "title": "Probabilit√©s et Statistiques",
    "section": "Vecteurs al√©atoires",
    "text": "Vecteurs al√©atoires\nSupposons que \\(X = (X_{1}, X_{2})\\) est une variable al√©atoire de dimension \\(2\\) de densit√© \\(f_{X}\\). On appelle g√©n√©ralement les variables al√©atoires de dimension sup√©rieure √† \\(1\\), des vecteurs al√©atoires. Les densit√©s de \\(X_{1}\\) et \\(X_{2}\\) sont appel√©es les densit√©s marginales. Lorsque \\(X_{1}\\) et \\(X_{2}\\) sont ind√©pendantes, on a : \\[f_X(x, y) = f_{X_{1}}(x) \\cdot f_{X_{2}}(y), \\quad (x, y) \\in \\mathbb{R}^2.\\]\n\n\n\n\n\n\nNoteExemple de la loi normale multidimensionnelle\n\n\n\nOn dit qu‚Äôun vecteur al√©atoire \\(X\\) de dimension \\(p\\) suit une loi normale multidimensionnelle de moyenne \\(\\mu\\) et de variance \\(\\Sigma\\), si sa densit√© est donn√©e par \\[f_X(x) = \\frac{1}{(2 \\pi)^{p /2}} \\cdot \\frac{1}{(\\text{det} \\Sigma)^{1/2}} \\cdot \\exp\\left\\{ -\\frac{1}{2}\\left( x - \\mu \\right)^\\top \\Sigma^{-1} \\left( x - \\mu \\right) \\right\\}, \\quad x \\in \\mathbb{R}^p.\\]\nOn note \\(X \\sim \\mathcal{N}_{p}(\\mu, \\Sigma)\\).\n\n\nEn statistiques, une quantit√© importante √† mesurer est la d√©pendance lin√©aire entre \\(X_{1}\\) et \\(X_{2}\\). Pour cela, on peut utiliser la covariance ou la correlation.\n\n\n\n\n\n\nAvertissementD√©finition\n\n\n\nSoit \\(X = (X_{1}, X_{2})\\) un vecteur al√©atoire tel que l‚Äôesp√©rance du carr√© de \\(X_{1}\\) et de \\(X_{2}\\) existe. La covariance entre \\(X_{1}\\) et \\(X_{2}\\) est donn√©e par \\[\\mathrm{Cov}(X_{1}, X_{2}) = \\mathbb{E}\\left[ (X_{1} - \\mathbb{E}(X_{1})) (X_{2} - \\mathbb{E}(X_{2}))\\right].\\]\nLa corr√©lation entre \\(X_{1}\\) et \\(X_{2}\\) est une version de la covariance normalis√©e par l‚Äô√©cart-type des variables al√©atoires. Elle est donn√©e par \\[\\mathrm{Corr}(X_{1}, X_{2}) = \\frac{\\mathrm{Cov}(X_{1}, X_{2})}{\\sigma(X_{1}) \\sigma(X_{2})}.\\]\n\n\nOn peut interpr√©ter le signe de la covariance et de la corr√©lation. Si elles sont strictement positives, \\(X_{1}\\) et \\(X_{2}\\) ont tendance √† aller dans la m√™me direction. Si \\(X_{1}\\) augmente, alors \\(X_{2}\\) aussi, et inversement. Si elles sont strictement n√©gatives, \\(X_{1}\\) et \\(X_{2}\\) ont tendance √† aller dans des directions oppos√©es. Si \\(X_{1}\\) augmente, alors \\(X_{2}\\) diminue, et inversement. Si la covariance est √©gales √† \\(0\\), il n‚Äôy a pas de r√®gles et \\(X_{1}\\) et \\(X_{2}\\) sont dites orthogonales.\n\n\n\n\n\n\nImportantPropri√©t√©s\n\n\n\nSoit \\(X = (X_{1}, X_{2})\\) un vecteur al√©atoire. On a\n\n\\(\\mathrm{Cov}(X_{1}, X_{2}) = \\mathbb{E}(X_{1}X_{2}) - \\mathbb{E}(X_{1})\\mathbb{E}(X_{2})\\);\n\\(\\mathrm{Cov}(X_{1}, X_{2}) = \\mathrm{Cov}(X_{2}, X_{1})\\);\n\\(\\mathrm{Cov}(X_{1} + \\lambda Y_{1}, X_{2}) = \\mathrm{Cov}(X_{1}, X_{2}) + \\lambda \\mathrm{Cov}(Y_{1}, X_{2})\\).\n\n\n\n\n\n\n\n\n\nNotePreuve\n\n\n\n\nOn trouve le r√©sultat en d√©veloppement le produit dans la d√©finition de la covariance.\nEn utilisant le point 1. et la commutativit√© de la multiplication.\nEn utilisant le point 1. et la lin√©arit√© de l‚Äôesp√©rance.",
    "crumbs": [
      "Modules",
      "02 - R√©visions",
      "Probabilit√©s et Statistiques"
    ]
  },
  {
    "objectID": "contents/remainders/02-probabilities.html#estimation",
    "href": "contents/remainders/02-probabilities.html#estimation",
    "title": "Probabilit√©s et Statistiques",
    "section": "Estimation",
    "text": "Estimation\nEn practique, nous n‚Äôavons pas une connaissance parfaite de nos vecteurs al√©atoires, mais seulement des r√©alisations de ceux-ci (que l‚Äôon appelle √©chantillon). Notons \\(x_{1}, \\dots, x_{n}\\), \\(n\\) r√©alisations ind√©pendantes d‚Äôun vecteur al√©atoire \\(X\\) de moyenne \\(\\mu\\) et de variance \\(\\Sigma\\).\nL‚Äôestimateur de la moyenne \\(\\mu\\) est donn√© par \\[\\widehat{\\mu} = \\overline{X} \\coloneqq \\frac{1}{n} \\sum_{i = 1}^{n} x_i.\\]\nL‚Äôestimateur de la variance \\(\\Sigma\\) est donn√© par \\[\\widehat{\\Sigma} \\coloneqq \\frac{1}{n - 1}\\sum_{i = 1}^{n} (x_i - \\widehat{\\mu})(x_i - \\widehat{\\mu})^\\top.\\]\nPourquoi divise t-on cette somme par \\(n -1\\) et non par \\(n\\) pour estimer la variance ? Si l‚Äôon divise par \\(n\\), \\(\\widehat{\\Sigma}\\) est un estimateur biais√© de la variance. En effet, il faut prendre en compte que l‚Äôon utilise un estimateur biais√© de la moyenne dans l‚Äôestimateur de la variance et donc corriger pour cette estimation.\nNotons \\(D = \\{\\text{diag}(\\widehat{\\Sigma})\\}^{1/2}\\), la matrice des √©carts-types calcul√©s sur l‚Äô√©chantillon. On peut estimer la matrice des corr√©lations sur l‚Äô√©chantillon par \\[\\widehat{R} = D^{-1} \\widehat{\\Sigma} D^{-1}.\\]",
    "crumbs": [
      "Modules",
      "02 - R√©visions",
      "Probabilit√©s et Statistiques"
    ]
  },
  {
    "objectID": "contents/07-further.html",
    "href": "contents/07-further.html",
    "title": "Aspects √©thiques",
    "section": "",
    "text": "L‚Äôanalyse de donn√©es est aujourd‚Äôhui utilis√© dans de nombreux domaines : sant√©, √©ducation, politique publique, etc. Les donn√©es jouent m√™me un r√¥le central dans la plupart des domaines. Les analyses permettent de prendre des d√©cisions mieux inform√©es et parfois automatis√©es. Dans certains domaines, ces d√©cisions sont incons√©quentes, mais pour d‚Äôautres domaines, cela soul√®ve des questions √©thiques majeures. Par exemple, est-ce que mes donn√©es repr√©sentent fid√©lement ma population ? quelles d√©cisions sont prises √† partir de ces analyses ? quelles sont les cons√©quences pour les individus concern√©s ?\nLes donn√©es ne sont pas neutres, elles sont le reflet d‚Äôun contexte social, institutionnel et technique. Ainsi, les mod√®les qui en sont issus peuvent renforcer les in√©galit√©s, reproduire des biais historiques, ou √©ventuellement porter atteinte √† la vie priv√©e des individus. Il est donc essentiel, lorsque l‚Äôon fait une analyse de donn√©es, d‚Äôavoir une certaine vigilance √©thique, en r√©flechissant aux impacts sociaux, aux limites m√©thodologiques et aux responsabilit√©s associ√©es √† ses choix.",
    "crumbs": [
      "Modules",
      "07 - √âthique"
    ]
  },
  {
    "objectID": "contents/07-further.html#confidentialit√©-des-donn√©es",
    "href": "contents/07-further.html#confidentialit√©-des-donn√©es",
    "title": "Aspects √©thiques",
    "section": "Confidentialit√© des donn√©es",
    "text": "Confidentialit√© des donn√©es\nLorsqu‚Äôon manipule des donn√©es individuelles, il est essentiel de prot√©ger la vie priv√©e des personnes concern√©es. Plusieurs approches existent pour minimiser les risques de r√©-identification ou d‚Äôexploitation abusive.\nL‚Äôanonymisation des donn√©es consiste √† supprimer ou √† transformer les identifiants directs (e.g.¬†les noms et adresses) et indirects (e.g.¬†les dates de naissance et les codes postaux) susceptibles de permettre l‚Äôidentification d‚Äôun individu. Il convient de faire particuli√®rement attention √† ce cas, dans certaines situations, c‚Äôest le croisement de plusieurs variables qui permet l‚Äôidentification d‚Äôun individu et non une unique variable. Par exemple, deux chercheurs de l‚Äôuniversit√© du Texas ont √©t√© capable d‚Äôidentifier des utilisateurs de Netflix en utilisant des notes sur IMDb (c.f. lien). On peut aussi r√©duire la granularit√© des informations en limitant les d√©tails fournis (e.g.¬†en regroupant les √¢ges en tranches et en n‚Äôutilisant pas l‚Äô√¢ge exact) pour r√©duire le risque de r√©-identification.\nLorsque certaines modalit√©s d‚Äôune variable cat√©gorielle sont associ√©es √† tr√®s peu d‚Äôindividus, on peut les regrouper pour √©viter qu‚Äôune combinaison unique de caract√©ristiques ne permette de retrouver une personne. On peut aussi ajouter du bruit aux donn√©es ou aux r√©sultats. Cela permet de pr√©server des tendances globales tout en rendant les identification individuelles plus difficiles. Il faut faire attention √† ne pas ajouter trop de bruit, sinon on risque de compromettre la validit√© des analyses. Enfin, on peut appliquer une approche de confidentialit√© diff√©rentielle consistant √† garantir qu‚Äôun individu ne peut pas √™tre identifi√©, m√™me en connaissant toutes les autres donn√©es du jeu.",
    "crumbs": [
      "Modules",
      "07 - √âthique"
    ]
  },
  {
    "objectID": "contents/07-further.html#implications-sociales",
    "href": "contents/07-further.html#implications-sociales",
    "title": "Aspects √©thiques",
    "section": "Implications sociales",
    "text": "Implications sociales\nL‚Äôutilisation des r√©sultats d‚Äôanalyses statistiques dans des contextes sociaux ou d√©cisionnels soul√®ve d‚Äôimportants enjeux √©thiques. En effet, plusieurs sources de discrimination peuvent intervenir dans un processus apparemment rigoureux.\nIl peut y avoir un biais dans l‚Äô√©chantillonnage si le jeu de donn√©es ne refl√®te pas fid√®lement la population cible (e.g.¬†la sous-repr√©sentation de certaines communaut√©s). Dans ce cas, les mod√®les entra√Æn√©s risquent d‚Äô√™tre in√©quitables. Il peut aussi y avoir un biais dans la variable √† expliquer. Par exemple, pour un mod√®le bas√© sur des d√©cisions judiciaires pass√©es, il est possible qu‚Äôil perp√©tue les biais historiques des jugements.\nOn peut aussi se trouver avec une validit√© variable selon les groupes. Un m√™me mod√®le peut tr√®s bien fonctionner pour un sous-groupe de la population, mais √™tre tr√®s mauvais pour un autre, menant √† des erreurs syst√©matiques. Enfin, si certaines classes sont mal repr√©sent√©es, e.g.¬†des maladies rares ou des groupes d√©mographiques minoritaires, et donc peu pr√©sentes dans les donn√©es, le mod√®le peut mal les pr√©dire ou m√™me les ignorer.",
    "crumbs": [
      "Modules",
      "07 - √âthique"
    ]
  },
  {
    "objectID": "contents/07-further.html#d√©construire-quelques-mythes",
    "href": "contents/07-further.html#d√©construire-quelques-mythes",
    "title": "Aspects √©thiques",
    "section": "D√©construire quelques mythes",
    "text": "D√©construire quelques mythes\n\n‚ÄúLa machine apprend toute seule.‚Äù\n\nEn r√©alit√©, le choix des donn√©es, des variables, de la variable √† expliquer et de l‚Äôalgorithme est faite par des √™tres humains. L‚Äô‚Äúapprentissage‚Äù d√©pend enti√®rement des d√©cisions humaines en amont.\n\n‚ÄúC‚Äôest objectif, c‚Äôest bas√© sur des donn√©es.‚Äù\n\nLes donn√©es ne sont jamais neutres. Elles sont le produit d‚Äôun contexte social, institutionnel et m√©thodologique. Les mod√®les entra√Æn√©s sur ces donn√©es h√©ritent de leurs biais.\n\n‚ÄúMon mod√®le ne peut pas √™tre sexiste car je n‚Äôai pas utilis√© le genre pour le construire.‚Äù\n\nM√™me sans inclure explicitement une variable comme le genre, un mod√®le peut apprendre√† en inf√©rer √† partir d‚Äôautres variables corr√©l√©es comme le profession ou le parcours scolaire. L‚Äôexclusion explicite d‚Äôune variable sensible ne garantit pas l‚Äôabsence de biais.",
    "crumbs": [
      "Modules",
      "07 - √âthique"
    ]
  },
  {
    "objectID": "contents/07-further.html#que-faire-pour-r√©duire-ces-biais",
    "href": "contents/07-further.html#que-faire-pour-r√©duire-ces-biais",
    "title": "Aspects √©thiques",
    "section": "Que faire pour r√©duire ces biais ?",
    "text": "Que faire pour r√©duire ces biais ?\nPlusieurs approches peuvent √™tre mises en oeuvre √† diff√©rentes √©tapes du processus :\n\nAgir en amont sur les donn√©es : r√©√©quilibrer l‚Äô√©chantillon, sur-repr√©senter certains groupes ou encore corriger les biais connus dans la variable √† expliquer.\nModifier les mod√®les a posteriori : appliquer des corrections sur les r√©sultats produits comme le recalibrage des probabilit√©s ou l‚Äôajustement des seuils de d√©cision pour certaines classes.\nModifier la fonction de perte utilis√©e pour l‚Äôapprentissage : int√©grer des p√©nalit√©s visant √† r√©duire les in√©galit√©s de performance entre les groupes. On peut, par exemple, imposer une √©quit√© en terme de faux positifs pour les diff√©rents groupes dans la population.",
    "crumbs": [
      "Modules",
      "07 - √âthique"
    ]
  },
  {
    "objectID": "contents/generalities/02-spaces.html",
    "href": "contents/generalities/02-spaces.html",
    "title": "Espaces",
    "section": "",
    "text": "Avant de pouvoir mod√©liser ou analyser des donn√©es, il est fondamental de bien comprendre la nature des variables que l‚Äôon manipule. En effet, le type de variables d√©termine :\nDans cette section, nous pr√©sentons les types de variables les plus courants, ainsi que les espaces associ√©s.",
    "crumbs": [
      "Modules",
      "03 - G√©n√©ralit√©s",
      "Espaces"
    ]
  },
  {
    "objectID": "contents/generalities/02-spaces.html#unit√©-statistique",
    "href": "contents/generalities/02-spaces.html#unit√©-statistique",
    "title": "Espaces",
    "section": "Unit√© statistique",
    "text": "Unit√© statistique\nUne unit√© statistique est l‚Äô√©l√©ment de base sur lequel une observation est effectu√©. Moralement, c‚Äôest le ‚Äúporteur‚Äù de l‚Äôinformation qui est utilis√© pour d√©terminer le niveau d‚Äôagr√©gation de l‚Äôanalyse. L‚Äôunit√© statistique est un choix du mod√©lisateur.\n\n\n\n\n\n\nNoteExemples\n\n\n\n\nDans le cas d‚Äôune enqu√™te sur les revenus, on peut choisir l‚Äôindividu comme unit√©.\nDans le cas d‚Äôune √©tude sur les classes d‚Äôun lyc√©e, on peut choisir la classe comme unit√©.\nDans le cas d‚Äôune base de donn√©es d‚Äôimagerie m√©dicale, on choisir l‚Äôimage comme unit√©.\n\n\n\n\n\n\n\n\n\nMise en gardeParfois, une m√™me base de donn√©es peut √™tre analys√©e √† plusieurs niveaux.\n\n\n\nUne image est constitu√©e de pixels, chacun pouvant √™tre d√©crit par des variables num√©riques (e.g.¬†valeurs RVB, opacit√©, ‚Ä¶). On peut choisir d‚Äôanalyser chaque pixel, et donc prendre le pixel pour unit√©, ou bien analyser chaque image comme un tout, et donc prendre l‚Äôimage comme unit√©.",
    "crumbs": [
      "Modules",
      "03 - G√©n√©ralit√©s",
      "Espaces"
    ]
  },
  {
    "objectID": "contents/generalities/02-spaces.html#types-de-variables",
    "href": "contents/generalities/02-spaces.html#types-de-variables",
    "title": "Espaces",
    "section": "Types de variables",
    "text": "Types de variables\nOn distingue g√©n√©ralement quatre types de variables, que l‚Äôon identifie au niveau de la plus petite unit√© statistique du jeu de donn√©es.\n\n\n\n\n\n\nAvertissementVariable num√©rique (ou quantitative)\n\n\n\nUne variable num√©rique (ou quantitative) est une variable dont les valeurs sont des nombres repr√©sentant une quantit√© mesurable.\nExemples: revenu en dollars, masse, √¢ge, ‚Ä¶\n\n\n\n\n\n\n\n\nAvertissementVariable ordinale\n\n\n\nUne variable ordinale est une variable qualitative (ou cat√©gorielle) dont les modalit√©s peuvent √™tre ordonn√©es naturellement, sans que l‚Äô√©cart entre les modalit√©s soit quantifiable.\nExemples: niveau de revenu (faible, moyen ou √©lev√©), niveau de satisfaction (‚Äútout-√†-fait en d√©saccord‚Äù, ‚Äúen d√©saccord‚Äù, ‚Äúpas d‚Äôavis‚Äù, ‚Äúd‚Äôaccord‚Äù, ‚Äútout-√†-fait d‚Äôaccord‚Äù), ‚Ä¶\n\n\n\n\n\n\n\n\nAvertissementVariable nominale sym√©trique\n\n\n\nUne variable nominale sym√©trique est une variable qualitative (ou cat√©gorielle) dont toutes les modalit√©s sont aussi informatives l‚Äôune que l‚Äôautre.\nExemples: nationalit√©, fili√®re de formation, ‚Ä¶\n\n\n\n\n\n\n\n\nAvertissementVariable nominale asym√©trique\n\n\n\nUne variable nominale asym√©trique est une variable qualitative (ou cat√©gorielle) dont l‚Äôune des modalit√©s a un statut particulier, souvent plus fr√©quente ou consid√©r√©e comme la valeur ‚Äúpar d√©faut‚Äù. Ainsi, avoir deux observations avec la valeur ‚Äúpar d√©faut‚Äù de cette variable nominale asym√©trique ne nous apprend pas grand chose sur celles-ci; alors que on peut retirer beaucoup plus d‚Äôinformation de deux observations qui n‚Äôont pas la valeur ‚Äúpar d√©faut‚Äù.\nExemples: pr√©sence ou absence d‚Äôun sympt√¥me, transaction frauduleuse ou non, ‚Ä¶\n\n\nBien que ces types de variables soient les plus communs, on peut trouver beaucoup d‚Äôautres types de variables. Par exemple, on peut s‚Äôint√©resser √† de la comparaison de courbes, de textes, d‚Äôimages, de r√©seaux, etc. Dans ces situations, le choix de la repr√©sentation d√©pend du niveau auquel on souhaite se placer, et donc de l‚Äôunit√© statistique.",
    "crumbs": [
      "Modules",
      "03 - G√©n√©ralit√©s",
      "Espaces"
    ]
  },
  {
    "objectID": "contents/generalities/02-spaces.html#espaces-associ√©s",
    "href": "contents/generalities/02-spaces.html#espaces-associ√©s",
    "title": "Espaces",
    "section": "Espaces associ√©s",
    "text": "Espaces associ√©s\nUne fois que nos donn√©es ont √©t√© collect√©s, la premi√®re √©tape d‚Äôune analyse statistique consiste √† choisir un espace math√©matique dans lequel travailler. Cette espace, que l‚Äôon appelle parfois espace d‚Äôobservation et que l‚Äôon note \\(\\mathcal{X}\\), d√©pend du type de donn√©es observ√©es. Il constitue le cadre formel dans lequel nos variables prennent leurs valeurs, et il guide les choix m√©thodologiques qui suivront.\n\n\n\n\n\n\nNoteCas d‚Äôune variable num√©rique\n\n\n\nLorsque l‚Äôon observe un variable num√©rique (e.g.¬†la temp√©rature d‚Äôun pays), l‚Äôespace naturel dans lequel travailler est l‚Äôensemble des r√©els, \\(\\mathcal{X} = \\mathbb{R}\\). Dans certains cas, on peut restreindre cet espace √† un intervalle sp√©cifique. Par exemple, si on s‚Äôint√©resse √† la taille d‚Äôune personne, on peut prendre \\(\\mathcal{X} = [0, +\\infty)\\) car la variable consid√©r√©e ne peut pas √™tre n√©gative.\n\n\n\n\n\n\n\n\nNoteCas d‚Äôune variable nominale (ou qualitative, ou cat√©gorielle)\n\n\n\nPour une variable nominal, l‚Äôespace est un ensemble fini de modalit√©, l‚Äôensemble des modalit√©s prises par la variable. Par exemple, si on √©tudie les r√©sultats d‚Äôun lancer de d√©s, la variable peut prendre les valeurs \\(1\\) √† \\(6\\), et l‚Äôespace associ√© sera donc \\(\\mathcal{X} = \\{ 1, 2, 3, 4, 5, 6 \\}\\).\n\n\nLorsque les donn√©es sont plus conplexes, il faut choisir des espaces plus adapt√©s. Pour de l‚Äôanalyse de courbes ou de signaux, on peut travailler dans un espace de fonctions. Par exemple, on peut consid√©rer l‚Äôespace des fonctions continues sur un intervalle ferm√© \\([a, b]\\), not√© \\(\\mathcal{X} = \\mathcal{C}([a, b])\\). Pour de l‚Äôanalyse de texte (vu comme une s√©quence de caract√®res), l‚Äôespace de travail peut √™tre un alphabet. Par exemple, on peut consid√©rer \\(\\mathcal{X} = \\{ \\text{A}, \\text{B}, \\dots, \\text{Z} \\}\\).\nSouvent, on observe plusieurs variables en m√™me temps, e.g.¬†la taille, le poids et le sexe d‚Äôun individus. Dans ce cas, l‚Äôespace d‚Äôobservation sera le produit cart√©sien (aussi appel√© ensemble produit) des espaces associ√©s √† chaque variable : \\[\\mathcal{X} = \\mathcal{X}_1 \\times \\mathcal{X}_2 \\times \\dots \\mathcal{X}_p,\\] o√π \\(p\\) est le nombre de variables. Dans le cas o√π on observe \\(p\\) variables num√©rique, on notera plus simplement \\(\\mathcal{X} = \\mathbb{R}^p\\)",
    "crumbs": [
      "Modules",
      "03 - G√©n√©ralit√©s",
      "Espaces"
    ]
  },
  {
    "objectID": "contents/generalities/03-distance.html",
    "href": "contents/generalities/03-distance.html",
    "title": "Distances",
    "section": "",
    "text": "Dans tout projet d‚Äôanalyse de donn√©es, il est n√©cessaire de pouvoir quantifier la ressemblance (ou la dissemblance) entre deux observations. Pour cela, on utilise la notion de distance (ou similarit√©) entre les observations. Le choix de cette distance influence directement les r√©sultats des algorithmes d‚Äôapprentissage, de regroupement et de visualisations.",
    "crumbs": [
      "Modules",
      "03 - G√©n√©ralit√©s",
      "Distances"
    ]
  },
  {
    "objectID": "contents/generalities/03-distance.html#notion-de-distance",
    "href": "contents/generalities/03-distance.html#notion-de-distance",
    "title": "Distances",
    "section": "Notion de distance",
    "text": "Notion de distance\nUne distance est une fonction math√©matique mesurant √† quel point deux objets sont √©loign√© l‚Äôun de l‚Äôautre dans un espace donn√©e. Plus la distance est grande, plus les observations somt √©loign√©.\n\n\n\n\n\n\nAvertissementD√©finition de mesure de distance\n\n\n\nUne fonction \\(d: \\mathcal{X} \\times \\mathcal{X} \\to \\mathbb{R}\\) est une distance sur un ensemble \\(\\mathcal{X}\\) si, pour tout \\(x, y, z \\in \\mathcal{X}\\), les conditions suivantes sont v√©rifi√©es :\n\nnon-n√©gativit√©: \\(d(x, y) \\geq 0\\);\ns√©paration: \\(d(x, y) = 0 \\Leftrightarrow x = y\\);\nsym√©trie: \\(d(x, y) = d(y, x)\\);\nin√©galit√© triangulaire: \\(d(x, y) \\leq d(x, z) + d(y, z)\\).\n\n\n\n\n\n\n\n\n\nNoteLa distance euclidienne\n\n\n\nLorsque les observations sont repr√©sent√©es par des vecteurs num√©riques dans \\(\\mathbb{R}^p\\) de m√™me ordre de grandeur, la distance euclidienne est souvent un bon choix.\nSoit \\(x, y \\in \\mathbb{R}^p\\), la distance euclidienne est donn√©es par : \\[d(x, y) = \\left\\| x - y \\right\\|_2 = \\left( \\sum_{i = 1}^{p} (x_i - y_i)^2 \\right)^{1/2}.\\]\n\n\n\n\n\n\n\n\nNoteLa distance \\(L_q\\) (ou de Minkowski)\n\n\n\nSoit \\(x, y \\in \\mathbb{R}^p\\), la distance \\(L_q\\) est donn√©e, pour \\(q &gt; 0\\), par : \\[d(x, y) = \\left\\| x - y \\right\\|_q = \\left( \\sum_{i = 1}^{p} |x_i - y_i|^q \\right)^{1 / q}.\\]\nCas particuliers:\n\nPour \\(q = 1\\), on obtient la distance de Manhattan : \\[d(x, y) = \\left\\| x - y \\right\\|_1 = \\sum_{i = 1}^{p} |x_i - y_i|.\\]\nPour \\(q = 2\\), on obtient la distance euclidienne.\n\n\n\n\n\n\n\n\n\nNoteExemple\n\n\n\nConsid√©rons le jeu de donn√©es suivant :\n\nTaille et poids moyens au Canada (Source: Statistique Canada, Enqu√™te sur la sant√© dans les collectivit√©s canadiennes (2008)).\n\n\nNom\nTaille\nPoids\n\n\n\n\nAlice\n162.1\n66.8\n\n\nBob\n175.8\n81.6\n\n\n\nLa distance euclidienne entre Alice et Bob est \\[d(\\text{Alice}, \\text{Bob}) = \\sqrt{(162.1 - 175.8)^2 + (66.8 - 81.6)^2} = 20.16.\\]\nLa distance de Manhattan entre Alice et Bob est \\[d(\\text{Alice}, \\text{Bob}) = |162.1 - 175.8| + |66.8 - 81.6| = 28.5.\\]\n\n\nLa distance \\(L_q\\) n‚Äôest pas invariante aux changements d‚Äô√©chelle. Par exemple, si on multiplie toutes les composantes d‚Äôun vecteur par un facteur \\(\\lambda\\), la distance entre deux vecteurs change du facteur \\(\\lambda\\).\nEn practique, on pr√©f√®re travailler avec des variables standardis√©es. Ainsi, en notant, \\(\\mu_i\\), la moyenne, et \\(\\sigma_i\\), l‚Äô√©cart-type de la variable \\(i\\), la distance euclidienne avec des variables standardis√©es est donn√©e par : \\[d(x, y) = \\sum_{i = 1}^{p} \\left\\{ \\frac{x_i - \\mu_i}{\\sigma_i} - \\frac{y_i - \\mu_i}{\\sigma_i} \\right\\}^2 = \\sum_{i = 1}^{p} \\left( \\frac{x_i - y_i}{\\sigma_i} \\right)^2.\\]\n\n\n\n\n\n\nImportantPropri√©t√©\n\n\n\nLa distance euclidienne avec des variables standardis√©es est invariante par changement d‚Äô√©chelle.\n\n\n\n\n\n\n\n\nNotePreuve\n\n\n\n\n\nSoit \\(\\lambda \\neq 0\\) et soit \\(X\\) une variable al√©atoire. On a \\(\\mathbb{E}(\\lambda X) = \\lambda \\mathbb{E}(X)\\) et \\(\\mathrm{Var}(\\lambda X) = \\lambda^2 \\mathrm{Var}(X)\\). Donc\n\\[d(\\lambda x, \\lambda y) = \\sum_{i = 1}^{p} \\left\\{ \\frac{\\lambda x_i - \\lambda \\mu_i}{\\lambda \\sigma_i} - \\frac{\\lambda y_i - \\lambda \\mu_i}{\\lambda \\sigma_i} \\right\\}^2 = \\sum_{i = 1}^{p} \\left( \\frac{x_i - y_i}{\\sigma_i} \\right)^2 = d(x, y).\\]",
    "crumbs": [
      "Modules",
      "03 - G√©n√©ralit√©s",
      "Distances"
    ]
  },
  {
    "objectID": "contents/generalities/03-distance.html#notion-de-similarit√©",
    "href": "contents/generalities/03-distance.html#notion-de-similarit√©",
    "title": "Distances",
    "section": "Notion de similarit√©",
    "text": "Notion de similarit√©\n√Ä l‚Äôoppos√© de la notion de distance, une mesure de similarit√© quantifie √† quel point deux observations sont proches dans un espace donn√©. Ainsi, plus la similarit√© est grande, plus les observations sont proches.\n\n\n\n\n\n\nAvertissementD√©finition de mesure de similarit√©\n\n\n\nUne fonction \\(s: \\mathcal{X} \\times \\mathcal{X} \\to \\mathbb{R}\\) est une mesure de similarit√© sur un ensemble \\(\\mathcal{X}\\) si, pour tout \\(x, y \\in \\mathcal{X}\\), les conditions suivantes sont v√©rifi√©es :\n\n\\(s(x, y) \\geq 0\\);\n\\(s(x, y) = s(y, x)\\);\n\\(s(x, x) = 1 \\geq s(x, y)\\).\n\n\n\nUne distance peut se transformer en similarit√© en posant \\[s(x, y) = \\frac{1}{1 + d(x, y)}.\\]\nCette transformation garantit que plus la distance est grande, plus la similarit√© est faible. Toutefois, l‚Äôinverse n‚Äôest pas toujours possible car une mesure de similarit√© ne respecte pas forc√©ment l‚Äôin√©galit√© triangulaire. On peut aussi d√©finir la dissemblance entre deux objets: \\[d^\\star(x, y) = 1 - s(x, y).\\]",
    "crumbs": [
      "Modules",
      "03 - G√©n√©ralit√©s",
      "Distances"
    ]
  },
  {
    "objectID": "contents/generalities/03-distance.html#cas-de-variables-qualitatives",
    "href": "contents/generalities/03-distance.html#cas-de-variables-qualitatives",
    "title": "Distances",
    "section": "Cas de variables qualitatives",
    "text": "Cas de variables qualitatives\nLorsque l‚Äôon travaille avec des variables qualitatives, les distances num√©riques habituelles (comme les distances \\(L_p\\)) n‚Äôont g√©n√©ralement pas de sens. Par exemple, si une variable prend ses valeurs dans l‚Äôensemble \\[\\mathcal{X} = \\{ \\text{Rouge}, \\text{Vert}, \\text{Bleu} \\},\\] alors il n‚Äôy a pas de sens √† calculer la diff√©rence entre \\(\\text{Bleu}\\) et \\(\\text{Rouge}\\), car ces modalit√©s ne portent aucune structure num√©rique intrins√®que et n‚Äôont aucune notion d‚Äôordre ou d‚Äô√©cart.\nUne mauvaise pratique consisterait √† attribuer arbitrairement des valeurs num√©riques aux momdalit√©s (e.g.¬†\\(\\text{Rouge} = 1\\), \\(\\text{Vert} = 2\\), \\(\\text{Bleu} = 3\\)) ce qui introduirait un ordre artificiel entre elles. Cela risquerait de biaiser fortement les analyses.\n\nEncodage \\(1\\) parmi \\(K\\)\nLorsque l‚Äôon veut utiliser un mod√®le se basant sur une notion de distance entre les observations (i.e.¬†la plupart des mod√®les), on doit utiliser un encodage adapt√©. L‚Äôencodage \\(1\\) parmi \\(K\\) (one-hot encoding) consiste √† encoder une variable qualitative √† \\(K\\) modalit√©s sous la forme d‚Äôun vecteur binaire de dimension \\(K\\), dans lequel une seule entr√©e est √† \\(1\\), les autres √† \\(0\\). Ainsi, pour l‚Äôexemple de \\(\\mathcal{X} = \\{ \\text{Rouge, Vert, Bleu} \\}\\), on obtiendra l‚Äôencodage suivant: ‚ÄúRouge‚Äù donne \\((1, 0, 0)\\), ‚ÄúVert‚Äù donne \\((0, 1, 0)\\) et ‚ÄúBleu‚Äù donne \\((0, 0, 1)\\).\nCette m√©thode d‚Äôencodage a l‚Äôavantage de ne pas introduire d‚Äôordre artificiel entre les modalit√©s. Cependant, si la variable a beaucoup de modalit√©s, l‚Äôespace de repr√©sentation sera de grande dimension, ce qui peut nuire √† l‚Äôefficacit√© de certaines m√©thodes d‚Äôanalyse.\n\n\nD√©finir une distance adapt√©e\nUne fois les modalit√©s encod√©es, on peut d√©finir une distance entre deux observations de variables qualitatives.\n\n\n\n\n\n\nNoteLa distance discr√®te (ou distance de Hamming)\n\n\n\nSoit \\(\\mathcal{X}\\) un ensemble discret et soient \\(x\\) et \\(y\\) deux observations de \\(X\\), la distance discr√®te est donn√©e par \\[d(x, y) = \\begin{cases}\n  0, & \\text{si } x = y,\\\\\n  1, & \\text{si } x \\neq y\n\\end{cases}.\\]\n\n\nPour des vecteurs de variables qualitatives (e.g.¬†la comparaison de plusieurs individus d√©crits par plusieurs caract√©ristiques), la distance discr√®te est la somme des d√©saccords entre les composantes : \\[d(x, y) = \\sum_{i = 1}^{p} \\mathbb{1}(x_i \\neq y_i),\\] o√π \\(p\\) est le nombre de variables.\n\n\n\n\n\n\nNoteExemple\n\n\n\nPrenons les caract√©ristiques de trois personnes.\n\n\n\nTable¬†1: Caract√©ristiques de trois personnes.\n\n\n\n\n\nNom\nCouleur\nYeux\nCheveux\n\n\n\n\nAlice\nRouge\nBleu\nBlond\n\n\nBob\nVert\nBleu\nRoux\n\n\nChris\nRouge\nVert\nBlond\n\n\n\n\n\n\nOn calcule la distance entre deux personnes comme le nombre de caract√©ristiques diff√©rentes. Ainsi,\n\\[d(\\text{Alice}, \\text{Bob}) = 1 + 0 + 1 = 2,\\]\n\\[d(\\text{Alice}, \\text{Chris}) = 0 + 1 + 0 = 1,\\]\n\\[d(\\text{Bob}, \\text{Chris}) = 1 + 1 + 1 = 3.\\]\n\n\nPlut√¥t que de compter les diff√©rences, on peut aussi compter les accords et les normaliser :\n\\[s(x, y) = \\frac{1}{p}\\sum_{i = 1}^{p} \\mathbb{1}(x_i = y_i),\\]\nce qui donne une mesure de similarit√© comprise entre \\(0\\) (aucun accord) et \\(1\\) (identique).\n\n\n\n\n\n\nNoteExemple\n\n\n\nEn reprenant l‚Äôexemple pr√©c√©dent (cf. Table¬†1), on trouve les similarit√©s suivantes :\n\\[s(\\text{Alice}, \\text{Bob}) = \\frac{0 + 1 + 0}{3} = \\frac{1}{3},\\]\n\\[s(\\text{Alice}, \\text{Chris}) = \\frac{1 + 0 + 1}{3} = \\frac{2}{3},\\]\n\\[s(\\text{Bob}, \\text{Chris}) = \\frac{0 + 0 + 0}{3} = 0.\\]\n\n\n\n\nDistance de Jaccard\nLorsque le nombre de variables binaires est grand (e.g.¬†dans le cas o√π un encodage \\(1\\) parmi \\(K\\) a √©t√© fait sur \\(p\\) variables qualitatives), la distance discr√®te n‚Äôest pas forc√©ment tr√®s adapt√©e car le nombre d‚Äôaccords risque d‚Äô√™tre petit par rapport au nombre total de variables (\\(K \\times p\\) dans l‚Äôexemple pr√©c√©dent), ce qui va donner de petites distances dans tous les cas. Une des solutions est de se concentrer uniquement sur les attributs qui valent \\(1\\), car g√©n√©ralement, une variable binaire √† \\(0\\) n‚Äôapporte pas sp√©cialement d‚Äôinformation (en tout cas, moins qu‚Äôune variable binaire √† \\(1\\)). L‚Äôindice de Jaccard (Intersection over Union, IoU) a √©t√© introduit pour prendre cela en compte.\n\n\n\n\n\n\nAvertissementD√©finition : Indice de Jaccard\n\n\n\nConsid√©rons deux observations \\(x\\) et \\(y\\) de \\(K\\) variables binaires. Toutes les variables peuvent prendre les valeurs \\(0\\) et \\(1\\).\nD√©finissons les quantit√©s suivantes :\n\n\\(M_{11}\\), le nombre de variables √† \\(1\\) pour \\(x\\) et \\(y\\);\n\\(M_{10}\\), le nombre de variables √† \\(1\\) pour \\(x\\) et \\(0\\) pour \\(y\\);\n\\(M_{01}\\), le nombre de variables √† \\(0\\) pour \\(x\\) et \\(1\\) pour \\(y\\);\n\\(M_{00}\\), le nombre de variables √† \\(0\\) pour \\(x\\) et \\(y\\).\n\nChaque variable binaire √©tant forc√©ment compt√©e, soit dans \\(M_{11}\\), soit dans \\(M_{10}\\), soit dans \\(M_{01}\\), soit dans \\(M_{00}\\), leur somme est donc √©gale √† \\(K\\).\nL‚Äôindice de Jaccard est d√©fini comme \\[J(x, y) = \\frac{M_{11}}{M_{10} + M_{01} + M_{11}} = \\frac{M_{11}}{K - M_{00}}.\\]\n\n\nEn faisant attention au cas o√π les deux observations ne sont constitu√©es que de \\(0\\) (on prend \\(J(x, y) = 1\\) dans ce cas), l‚Äôindice de Jaccard est une mesure de similarit√©.\n\n\n\n\n\n\nImportantPropri√©t√© : Distance de Jaccard\n\n\n\nPour deux observations \\(x\\) et \\(y\\) de \\(K\\) variables binaires, la distance de Jaccard est donn√©e par \\[d(x, y) = 1 - J(x, y).\\]\n\n\n\n\n\n\n\n\nNotePreuve\n\n\n\n\n\nPour montrer que la distance de Jaccard est bien une distance, on doit montrer les quatres propri√©t√©s des distances. Notons d‚Äôabord que la distance de Jaccard peut se r√©√©crire comme \\[d(x, y) = \\frac{M_{10} + M_{01}}{M_{01} + M_{10} + M_{11}}.\\]\n\nTous les termes au num√©rateur et au d√©nominateur sont positifs, donc \\(d(x, y) \\geq 0\\).\nMontrons que \\(d(x, y) = 0 \\Leftrightarrow x = y\\).\n\nSupposons que \\(d(x, y) = 0\\). Alors \\(M_{01} + M_{10} = 0\\). Donc, il n‚Äôy a pas de variables qui valent \\(0\\) pour \\(x\\) et \\(1\\) pour \\(y\\) et inversement. Comme \\(M_{01} + M_{10} + M_{11} &gt; 0\\), on a \\(x = y\\).\nMaintenant, supposons que \\(x = y\\). Alors \\(M_{01} = M_{10} = 0\\). Donc \\(d(x, y) = 0\\).\n\nOn a que \\(d(x, y) = d(y, x)\\) car l‚Äôindice de Jaccard est sym√©trique.\nLa preuve de l‚Äôin√©galit√© triangulaire sera faite en exercice (cf.¬†TD)).\n\n\n\n\n\n\n\n\n\n\nNoteExemple\n\n\n\nPrenons un questionnaire de \\(5\\) questions ferm√©es. Supposons que la r√©ponse ‚ÄúOui‚Äù soit encod√©e par \\(1\\) et la r√©ponse ‚ÄúNon‚Äù soit encod√©e par \\(0\\).\n\n\n\nTable¬†2: Caract√©ristiques de deux personnes.\n\n\n\n\n\nNom\nQ1\nQ2\nQ3\nQ4\nQ5\n\n\n\n\nAlice\n1\n0\n1\n0\n0\n\n\nBob\n1\n0\n0\n1\n0\n\n\n\n\n\n\nPour la distance entre Alice et Bob, on a \\(M_{11} = 1\\), \\(M_{10} = 1\\), \\(M_{01} = 1\\) et \\(M_{00} = 2\\). Donc, la similarit√© de Jaccard est donn√©e par \\(J(\\text{Alice}, \\text{Bob}) = \\frac{1}{3}\\). Ainsi, la distance de Jaccard est \\(d(\\text{Alice}, \\text{Bob}) = 1 - J(x, y) = \\frac{2}{3}\\).",
    "crumbs": [
      "Modules",
      "03 - G√©n√©ralit√©s",
      "Distances"
    ]
  },
  {
    "objectID": "contents/unsupervised/01-kmeans.html",
    "href": "contents/unsupervised/01-kmeans.html",
    "title": "\\(k\\)-means",
    "section": "",
    "text": "On dispose de \\(n\\) observations \\(X_{1}, \\dots, X_n\\) d√©crites par \\(p\\) variables num√©riques. Les variables sont g√©n√©ralement standardis√©es pour √©viter qu‚Äôune variable domine les autres √† cause de son √©chelle. Notre objectif est de regrouper ces \\(n\\) observations en \\(K\\) groupes (ou classes), de sorte que :\n\nLes observations au sein d‚Äôun m√™me groupe soient les plus similaires possible (dans un certain sens).\nLes observations appartenant √† des groupes diff√©rents soient les moins similaires possible.\n\nAutrement dit, nous cherchons une fonction de classification : \\[C: \\{ 1, \\dots, n \\} \\to \\{ 1, \\dots, K \\}\\] qui, √† chaque observation \\(i \\in \\{ 1, \\dots, n \\}\\) associe une √©tiquette de groupe \\(C(i) \\in \\{ 1, \\dots, K \\}\\).\n\n\n\n\n\n\nAvertissementD√©finition : fonction de co√ªt\n\n\n\nLa qualit√© d‚Äôune partition \\(C\\) est mesur√©e √† l‚Äôaide d‚Äôune fonction de co√ªt \\(W\\), qui √©value la somme des distances intra-groupes : \\[W(C) = \\sum_{k = 1}^{K} \\sum_{i : C(i) = k} \\sum_{j : C(j) = k} d(X_i, X_j),\\] o√π \\(d(X_i, X_j)\\) est une mesure de la dissimilarit√© entre les observations \\(X_i\\) et \\(X_j\\). Par exemple, on peut utiliser la distance euclidienne.\n\n\nPlus \\(W(C)\\) est faible, meilleure est la qualit√© de la partition au sens de la coh√©sion intra-groupe. Le probl√®me de la classication non-supervis√©e est donc un probl√®me d‚Äôoptimisation combinatoire. Il s‚Äôagit de trouver la fonction \\(C\\) qui minimise \\(W(C)\\).\nCependant, ce probl√®me est tr√®s difficile (voir impossible) √† r√©sourdre exactement. En effet, il existe \\(K^n\\) combinaisons possibles (chaque observation pouvant appartenir √† un des \\(K\\) groupes). Il est donc compliqu√© d‚Äôexplorer toutes les solutions possibles m√™me pour de petites valeurs de \\(n\\) et de \\(K\\).\nPour r√©soudre ce probl√®me, on utilise des algorithmes gloutons (greedy algorithm) qui proc√®dent de mani√®re it√©rative. L‚Äôid√©e est de, premi√®rement, explorer un sous-ensemble restreint de l‚Äôespace des partitions. Ensuite, l‚Äôalgorithme am√©liore progressivement la solution de mani√®re it√©rative. Ceux-ci ne garantissent pas d‚Äôatteindre le minimum global, mais plut√¥t un minimum local qui est souvent une bonne solution en pratique.\n\n\n\n\n\n\nAstuceHypoth√®ses\n\n\n\n\nLes \\(p\\) variables sont num√©riques, cat√©gorielles ou ordinales, souvent cent√©es-r√©duites.\nLe nombre de groueps \\(K\\) est fix√© avant de lancer l‚Äôalgorithme (choisi par l‚Äôutilisateur).",
    "crumbs": [
      "Modules",
      "06 - Non-supervis√©e",
      "$k$-means"
    ]
  },
  {
    "objectID": "contents/unsupervised/01-kmeans.html#objectif-de-la-classication-non-supervis√©e",
    "href": "contents/unsupervised/01-kmeans.html#objectif-de-la-classication-non-supervis√©e",
    "title": "\\(k\\)-means",
    "section": "",
    "text": "On dispose de \\(n\\) observations \\(X_{1}, \\dots, X_n\\) d√©crites par \\(p\\) variables num√©riques. Les variables sont g√©n√©ralement standardis√©es pour √©viter qu‚Äôune variable domine les autres √† cause de son √©chelle. Notre objectif est de regrouper ces \\(n\\) observations en \\(K\\) groupes (ou classes), de sorte que :\n\nLes observations au sein d‚Äôun m√™me groupe soient les plus similaires possible (dans un certain sens).\nLes observations appartenant √† des groupes diff√©rents soient les moins similaires possible.\n\nAutrement dit, nous cherchons une fonction de classification : \\[C: \\{ 1, \\dots, n \\} \\to \\{ 1, \\dots, K \\}\\] qui, √† chaque observation \\(i \\in \\{ 1, \\dots, n \\}\\) associe une √©tiquette de groupe \\(C(i) \\in \\{ 1, \\dots, K \\}\\).\n\n\n\n\n\n\nAvertissementD√©finition : fonction de co√ªt\n\n\n\nLa qualit√© d‚Äôune partition \\(C\\) est mesur√©e √† l‚Äôaide d‚Äôune fonction de co√ªt \\(W\\), qui √©value la somme des distances intra-groupes : \\[W(C) = \\sum_{k = 1}^{K} \\sum_{i : C(i) = k} \\sum_{j : C(j) = k} d(X_i, X_j),\\] o√π \\(d(X_i, X_j)\\) est une mesure de la dissimilarit√© entre les observations \\(X_i\\) et \\(X_j\\). Par exemple, on peut utiliser la distance euclidienne.\n\n\nPlus \\(W(C)\\) est faible, meilleure est la qualit√© de la partition au sens de la coh√©sion intra-groupe. Le probl√®me de la classication non-supervis√©e est donc un probl√®me d‚Äôoptimisation combinatoire. Il s‚Äôagit de trouver la fonction \\(C\\) qui minimise \\(W(C)\\).\nCependant, ce probl√®me est tr√®s difficile (voir impossible) √† r√©sourdre exactement. En effet, il existe \\(K^n\\) combinaisons possibles (chaque observation pouvant appartenir √† un des \\(K\\) groupes). Il est donc compliqu√© d‚Äôexplorer toutes les solutions possibles m√™me pour de petites valeurs de \\(n\\) et de \\(K\\).\nPour r√©soudre ce probl√®me, on utilise des algorithmes gloutons (greedy algorithm) qui proc√®dent de mani√®re it√©rative. L‚Äôid√©e est de, premi√®rement, explorer un sous-ensemble restreint de l‚Äôespace des partitions. Ensuite, l‚Äôalgorithme am√©liore progressivement la solution de mani√®re it√©rative. Ceux-ci ne garantissent pas d‚Äôatteindre le minimum global, mais plut√¥t un minimum local qui est souvent une bonne solution en pratique.\n\n\n\n\n\n\nAstuceHypoth√®ses\n\n\n\n\nLes \\(p\\) variables sont num√©riques, cat√©gorielles ou ordinales, souvent cent√©es-r√©duites.\nLe nombre de groueps \\(K\\) est fix√© avant de lancer l‚Äôalgorithme (choisi par l‚Äôutilisateur).",
    "crumbs": [
      "Modules",
      "06 - Non-supervis√©e",
      "$k$-means"
    ]
  },
  {
    "objectID": "contents/unsupervised/01-kmeans.html#algorithme-k-moyennes",
    "href": "contents/unsupervised/01-kmeans.html#algorithme-k-moyennes",
    "title": "\\(k\\)-means",
    "section": "Algorithme \\(k\\)-moyennes",
    "text": "Algorithme \\(k\\)-moyennes\nL‚Äôalgorithme des \\(k\\)-moyennes (\\(k\\)-means) est une m√©thode classique de classication non-supervis√©e. L‚Äôobjectif est de regrouper \\(n\\) observations en \\(K\\) groupes homog√®nes, i.e.¬†tels que les observations au sein d‚Äôun m√™me groupe soient aussi proches que possible, tandis que celles appartenant √† des groupes diff√©rents soient aussi √©loign√©es que possible. Cette m√©thode repose sur une mesure de dissimilarit√©, g√©n√©ralement la distance euclienne pour des donn√©es quantitatives.\n\n\n\n\n\n\nAstuceAlgorithme\n\n\n\nVoici les principales √©tapes de l‚Äôalgorithme des \\(k\\)-moyennes.\n\nChoix du nombre de groupes \\(K\\) : Le nombre de classes doit √™tre choisi √† l‚Äôavance.\nInitialisation : On partitionne al√©atoirement les \\(n\\) observations en \\(K\\) groupes (ou on choisit al√©atoirement \\(K\\) observations comme centres initiaux).\nCalcul des centro√Ødes : Pour chaque groupe \\(k\\), on calcule leur centre de gravit√©\n\n\\[\\mu_k = \\frac{1}{N_k} \\sum_{i: C(i) = k} X_i, \\quad \\text{pour}\\quad k = 1, \\dots, K\\] o√π \\(N_k\\) est le nombre d‚Äôobservations dans le groupe \\(k\\).\n\nR√©affectation : Chaque observation est affect√©e au centre le plus proche, i.e.¬†au groupe dont le centro√Øde minimise la distance √† l‚Äôobservation.\nIt√©ration : On r√©p√®te les √©tapes 3 et 4 jusqu‚Äô‚Äô√† la statibilisation des groupes, i.e.¬†jusqu‚Äô√† ce qu‚Äôaucune observation ne change de groupe.\n\n\n\nLa convergence de l‚Äôalgorithme est garantie en un nombre fini d‚Äôit√©rations, car chaque √©tape r√©duit l‚Äôinertie intra-groupe, i.e.¬†la somme des distances des observations √† leur centro√Ødes respectif. En revanche, rien ne garantit que l‚Äôalgorithme atteigne un minimum global de \\(W\\), il peut converger vers un minimum locale.\nExemple avec une petite visualisation.\nCette algorithme pr√©sente plusieurs limites. Tout d‚Äôabord, il est sensible √† l‚Äôinitialisation des groupes. Les r√©sultats peuvent varier d‚Äôun essai √† l‚Äôautre en fonction du choix initial des centro√Ødes. Ce probl√®me peut √™tre r√©solu en utilisant l‚Äôalgorithme kmean++ (cf.¬†link). Deuxi√®mement, l‚Äôalgorithme n√©cessite de conna√Ætre √† l‚Äôavance le nombre de groupes \\(K\\), ce qui n‚Äôest pas toujours √©vident. Il existe plusieurs crit√®res pour guider ce choix, tels que le coefficient de silhouette, la m√©thode du coude (elbow method) ou encore des crit√®res d‚Äôinformation comme le BIC ou l‚ÄôAIC dans un cadre probabiliste. Troisi√®mement, √† chaque it√©ration, l‚Äôalgorithme requiert le recalcul de toutes les distances entre les observations et les centro√Ødes. Cela peut devenir co√ªteux en temps de calcul lorsque le nombre d‚Äôobservations \\(n\\) ou le nombre de variables \\(p\\) est √©lev√©. Enfin, l‚Äôalgorithme est assez sensible aux valeurs extr√™mes. La moyenne est en effet influenc√©e par les observations atypiques, ce qui peut fausser le calcul des centro√Ødes et engendrer des regroupements incoh√©rents.",
    "crumbs": [
      "Modules",
      "06 - Non-supervis√©e",
      "$k$-means"
    ]
  },
  {
    "objectID": "contents/unsupervised/01-kmeans.html#algorithme-k-m√©doides",
    "href": "contents/unsupervised/01-kmeans.html#algorithme-k-m√©doides",
    "title": "\\(k\\)-means",
    "section": "Algorithme \\(k\\)-m√©doides",
    "text": "Algorithme \\(k\\)-m√©doides\nPour rem√©dier √† certaines des limites des \\(k\\)-moyennes, on peut utiliser une variante plus robuste : l‚Äôalgorithme des \\(k\\)-m√©do√Ødes. Contrairement aux \\(k\\)-moyennes, les \\(k\\)-m√©do√Ødes utilisent des observations r√©elles comme repr√©sentants. Plus pr√©cis√©ment, dans chaque groupe, le m√©do√Øde est l‚Äôobservation qui minimise la somme des distances aux autres observations du m√™me groupe.\nCette m√©thode pr√©sente plusieurs avantages par rapport aux \\(k\\)-moyennes. D‚Äôabord, elle est plus robuste aux valeurs extr√™mes, car les m√©do√Ødes sont moins influenc√©s par les observations atypiques que les moyennes. Ensuite, elle est compatible avec des variables ordinales ou cat√©gorielles, √† condition d‚Äôutiliser une mesure de dissimilarit√© appropri√©e. On peut aussi sp√©cifier une matrice de dissimilarit√© sur mesure, adapt√©e √† la nature des donn√©es.\nEn revanche, l‚Äôalgorithme des \\(k\\)-m√©do√Ødes partage certains inconv√©nients avec celui des \\(k\\)-moyennes. Il faut aussi sp√©cifier √† l‚Äôavance le nombre de groupes \\(K\\). De plus, son co√ªt computationnel est plus √©lev√©, en particulier si l‚Äôon utilise des distances non euclidiennes ou si l‚Äôon travaille avec un grand nombre d‚Äôobservations.\nExemple avec une comparison avec le \\(k\\)-means.",
    "crumbs": [
      "Modules",
      "06 - Non-supervis√©e",
      "$k$-means"
    ]
  },
  {
    "objectID": "contents/dimension/01-pca.html",
    "href": "contents/dimension/01-pca.html",
    "title": "Analyse en composantes principales",
    "section": "",
    "text": "Travailler avec un grand nombre de variables peut poser plusieurs probl√®mes pratiques et th√©orique :\n\nVisualisation compliqu√©e : il est impossible de repr√©senter visuellement des donn√©es au-del√† de 3 dimensions.\nS√©paration des classes difficile : dans des probl√®mes de classification, la s√©paration entre les groupes peut √™tre cach√©e dans une combinaison de variables plut√¥t que dans les variables prises individuellement.\nCo√ªt computationnel √©l√©v√© : des mod√®les complexes peuvent devenir difficiles √† ajuster lorsque le nombre de variables est grand.\nCorr√©lations fortes : des variables redondantes rendent les mod√®les instables ou difficiles √† interpr√©ter.\n\nLa question naturelle √† se poser est donc : peut-on r√©duire la dimension du jeu de donn√©es sans perdre trop d‚Äôinformation ?\nR√©duire la dimension, ce n‚Äôest pas simplement la suppression de variables. En effet, cela risquerait de faire dispara√Ætre de l‚Äôinformation pouvant √™tre utile au mod√®le. Une meilleure approche consiste √† construire de nouvelles variables, obtenues comme combinaisons lin√©aires des variables initiales, qui r√©sument l‚Äôinformation essentielle du jeu de donn√©es. Une m√©thode possible pour cela est l‚ÄôAnalyse en Composantes Principales (ACP).",
    "crumbs": [
      "Modules",
      "04 - Dimension",
      "Analyse en composantes principales"
    ]
  },
  {
    "objectID": "contents/dimension/01-pca.html#pourquoi-changer-de-dimension",
    "href": "contents/dimension/01-pca.html#pourquoi-changer-de-dimension",
    "title": "Analyse en composantes principales",
    "section": "",
    "text": "Travailler avec un grand nombre de variables peut poser plusieurs probl√®mes pratiques et th√©orique :\n\nVisualisation compliqu√©e : il est impossible de repr√©senter visuellement des donn√©es au-del√† de 3 dimensions.\nS√©paration des classes difficile : dans des probl√®mes de classification, la s√©paration entre les groupes peut √™tre cach√©e dans une combinaison de variables plut√¥t que dans les variables prises individuellement.\nCo√ªt computationnel √©l√©v√© : des mod√®les complexes peuvent devenir difficiles √† ajuster lorsque le nombre de variables est grand.\nCorr√©lations fortes : des variables redondantes rendent les mod√®les instables ou difficiles √† interpr√©ter.\n\nLa question naturelle √† se poser est donc : peut-on r√©duire la dimension du jeu de donn√©es sans perdre trop d‚Äôinformation ?\nR√©duire la dimension, ce n‚Äôest pas simplement la suppression de variables. En effet, cela risquerait de faire dispara√Ætre de l‚Äôinformation pouvant √™tre utile au mod√®le. Une meilleure approche consiste √† construire de nouvelles variables, obtenues comme combinaisons lin√©aires des variables initiales, qui r√©sument l‚Äôinformation essentielle du jeu de donn√©es. Une m√©thode possible pour cela est l‚ÄôAnalyse en Composantes Principales (ACP).",
    "crumbs": [
      "Modules",
      "04 - Dimension",
      "Analyse en composantes principales"
    ]
  },
  {
    "objectID": "contents/dimension/01-pca.html#analyse-en-composantes-principales",
    "href": "contents/dimension/01-pca.html#analyse-en-composantes-principales",
    "title": "Analyse en composantes principales",
    "section": "Analyse en composantes principales",
    "text": "Analyse en composantes principales\nL‚ÄôACP est une m√©thode non-supervis√©e (sans variables √† expliquer) permettant de r√©duire la dimension d‚Äôun jeu de donn√©es tout en conservant le plus d‚Äôinformation possible. Cette m√©thode est utilis√©e lorsque l‚Äôon dispose de \\(n\\) observations de \\(p\\) variables num√©riques continues avec \\(p\\) trop ‚Äúgrand‚Äù pour permettre une mod√©lisation ou une visualisation efficace. La m√©thode a √©t√© introduite par dans Hotelling (1933).\n\n\n\n\n\n\nAstuceApplications courantes\n\n\n\n\nVisualisation d‚Äôun jeu de donn√©es multidimensionnelles.\nR√©duction du nombre de variables de \\(p\\) √† \\(k \\ll p\\) pour faciliter la construction de mod√®le.\nCompression d‚Äôimages ou de signaux.\nExploration de donn√©es biologiques, textuelles ou environnementales.\n\n\n\n\n\n\n\n\n\nNoteExemples\n\n\n\n\nComparer des √©quipes de hockey sur la base de six statistiques de fin de saison.\nR√©sumer la criminalit√© entre les provinces canadiennes sur la base des taux de sept types de crimes diff√©rents.\nCompresser des images form√©es de \\(1084 \\times 1084\\) pixels en quelques variables.\nIdentifier le nombre de variantes d‚Äôun type de tumeur √† partir du degr√© d‚Äôexpression de millions de g√®nes.\n\n\n\n\nFormulation math√©matique\nSoit un vecteur al√©atoire compos√© de \\(p\\) variables \\(X = \\left( X_{1}, \\dots, X_p \\right)^{\\top}\\), centr√© et ayant comme matrice de variance-covariance \\(\\Sigma\\). Notons \\(\\alpha_{1} = \\left( \\alpha_{11}, \\dots, \\alpha_{1p} \\right)^{\\top}\\), un vecteur de coefficients. On cherche une combinaison lin√©aire\n\\[Y_{1} = \\alpha_{1}^{\\top} X = \\sum_{k = 1}^{p} \\alpha_{1k}X_k,\\]\ntelle que la variance de \\(Y_{1}\\) soit maximale. L‚Äôid√©e est simple: on d√©sire combiner \\(p\\) variables en une seule, mais en ‚Äúcapturant‚Äù la plus grande partie possible de la variabilit√©.\nIl faut d‚Äôabord ajouter une contrainte sur \\(\\alpha_{1}\\), puisque sinon on n‚Äôaurait qu‚Äô√† prendre \\(\\alpha_{1k} = \\pm \\infty\\) et on aurait \\(\\mathrm{Var}(Y_{1}) = +\\infty\\) ce qui est d√©finitivement maximal ! On contraint donc \\(\\alpha_{1}\\) de sorte qu‚Äôil ait une norme √©gale √† \\(1\\).\nCela revient √† calculer : \\[\\max_{\\alpha_1^\\top \\alpha_1 = 1} \\mathrm{Var}(Y_1) = \\max_{\\alpha_1^\\top \\alpha_1 = 1} \\alpha_1^\\top \\Sigma \\alpha_{1}.\\]\nCe probl√®me se r√©sout par les multiplicateurs de Lagrange. Il conduit √† l‚Äô√©quation \\[\\Sigma \\alpha_1 = \\lambda_{1} \\alpha_{1},\\] o√π \\(\\lambda_{1}\\) est la plus grande valeur propre de \\(\\Sigma\\) et \\(\\alpha_{1}\\) le vecteur propre associ√©.\nOn d√©finit ainsi la premi√®re composante principale. On construit les suivantes en imposant des conditions d‚Äôorthogonalit√© (ind√©pendance lin√©aire) avec les pr√©c√©dentes, ce qui revient √† chercher les vecteurs propres suivants : \\[\\Sigma \\alpha_k = \\lambda_k \\alpha_k, \\quad \\text{avec}\\quad \\lambda_{1} \\geq \\lambda_2 \\geq \\dots \\lambda_p.\\] Les composantes principales sont donc donn√©es par \\[Y_k = \\alpha_k^\\top X, \\quad\\text{avec } \\alpha_k \\text{ vecteur propre associ√© √† } \\lambda_k.\\]\n\n\n\n\n\n\nAstuceRemarque\n\n\n\nSi \\(\\lambda_{1} &gt; \\dots &gt; \\lambda_{p}\\), alors les composantes principales sont uniques, √† un signe pr√®s.\n\n\n\n\n\n\n\n\nNotePreuve\n\n\n\n\n\nOn cherche √† calculer \\[\\max_{\\alpha_1^\\top \\alpha_1 = 1} \\mathrm{Var}(Y_1) = \\max_{\\alpha_1^\\top \\alpha_1 = 1} \\alpha_1^\\top \\Sigma \\alpha_{1}.\\]\nLe probl√®me est donc de maximiser \\[F(\\alpha_{1}) = \\alpha_{1}^\\top \\Sigma \\alpha_1, \\quad\\text{s.c.}\\quad \\alpha_1^\\top \\alpha_1 = 1.\\]\nOn peut r√©crire ce probl√®me √† l‚Äôaide des multiplicateurs de Lagrange, soit maximiser \\[F(\\alpha_1, \\lambda) = \\alpha_1^\\top \\Sigma \\alpha_1 - \\lambda (\\alpha_1^\\top \\alpha_1 -1 ),\\] o√π \\(\\lambda\\) est un multiplicateur de Lagrange.\nPour solutionner ce probl√®me, on d√©rive \\(F\\) par rapport √† \\(\\alpha_{1}\\) et √† \\(\\lambda\\).\n\\[\\begin{cases}\n\\frac{\\partial F(\\alpha_{1}, \\lambda)}{\\partial \\alpha_{1}} = 2 \\Sigma \\alpha_{1} - 2 \\lambda \\alpha_{1},\\\\\n\\frac{\\partial F(\\alpha_{1}, \\lambda)}{\\partial \\lambda} = 1 - \\alpha_{1}^\\top \\alpha_{1}.\n\\end{cases}\n\\]\nEnsuite, on √©galise √† \\(0\\), ce qui donne: \\[\\begin{cases}\n\\Sigma \\alpha_{1} = \\lambda \\alpha_{1} \\\\\n\\alpha_{1}^\\top \\alpha_{1} = 1\n\\end{cases}\n.\\] La seconde √©quation est bien entendue notre contrainte. La premi√®re √©quation est celle qui nous int√©resse. En utilisant cette √©quation et la d√©finition des √©l√©ments propres, on d√©duit que\n\n\\(\\alpha_{1}\\) est un vecteur propre (norm√©) de \\(\\Sigma\\);\n\\(\\lambda\\) est la valeur propre correspondante.\n\nOn a donc que \\[\\mathrm{Var}(Y_{1}) = \\alpha_{1}^\\top \\Sigma \\alpha_{1} = \\lambda \\alpha_{1}^\\top \\alpha_1 = \\lambda.\\] Puisque l‚Äôon veut maximiser cette quantit√©, on conclut que:\n\n\\(\\lambda = \\lambda_{1}\\), la plus grande valeur propre de \\(\\Sigma\\);\n\\(\\alpha_{1}\\), le vecteur propre norm√© correspondant.\n\nOn continue ensuite avec le calcul de la deuxi√®me composante. Ici, on poursuit simultan√©ment deux objectifs:\n\nConserver le maximum de variation pr√©sente dans \\(X\\);\nSimplifier la structure de d√©pendance pour faciliter l‚Äôinterpr√©tation et assurer la stabilit√© num√©rique d‚Äô√©ventuelles m√©thodes qui utiliseront les composantes principales obtenues.\n\n√âtant donn√© \\(Y_{1}\\), la deuxi√®me composante principale \\(Y_{2} = \\alpha_{2}^\\top X\\) est d√©finie telle que\n\n\\(\\mathrm{Var}(Y_{2}) = \\alpha_{2}^\\top \\Sigma \\alpha_{2}\\) est maximale;\n\\(\\alpha_{2}^\\top \\alpha_{2} = 1\\);\n\\(\\mathrm{Cov}(Y_{1}, Y_{2}) = 0\\).\n\nOr, on a que \\[\\mathrm{Cov}(Y_{1}, Y_{2}) = \\mathrm{Cov}(\\alpha_{1}^\\top X, \\alpha_{2}^\\top X) = \\alpha_{1}^\\top \\Sigma \\alpha_{2} = \\alpha_{2}^\\top \\Sigma \\alpha_{1} = \\lambda_{1} \\alpha_{2}^\\top \\alpha_{1}.\\]\nOn cherche donc le vecteur \\(\\alpha_{2}\\) qui maximise: \\[F(\\alpha_{2}, \\lambda, \\kappa) = \\alpha_{2}^\\top \\Sigma \\alpha_{2} - \\lambda (\\alpha_{2}^\\top \\alpha_{2} - 1) - \\kappa (\\alpha_{2}^\\top \\alpha_{1} - 0).\\]\nDe m√™me que pour la premi√®re composante, on d√©rive \\(F\\) par rapport √† \\(\\alpha_{2}\\), \\(\\lambda\\) et \\(\\kappa\\).\n\\[\\begin{cases}\n\\frac{\\partial F(\\alpha_{2}, \\lambda, \\kappa)}{\\partial \\alpha_{2}} = 2 \\Sigma \\alpha_{2} - 2 \\lambda \\alpha_{2} - \\kappa \\alpha_1 \\\\\n\\frac{\\partial F(\\alpha_{2}, \\lambda, \\kappa)}{\\partial \\lambda} = 1 - \\alpha_{2}^\\top \\alpha_{2} \\\\\n\\frac{\\partial F(\\alpha_{2}, \\lambda, \\kappa)}{\\partial \\kappa} = - \\alpha_2^\\top \\alpha_1\n\\end{cases}\n\\]\nEn √©galisant les √©quations √† \\(0\\), on retrouve les deux √©quations des contraintes, ainsi que\n\\[2 \\Sigma \\alpha_{2} - 2 \\lambda \\alpha_{2} - \\kappa \\alpha_{1} = 0.\\]\nEn multipliant cette √©quation √† gauche et √† droite par \\(\\alpha_{1}^\\top\\), on trouve\n\\[2 \\alpha_{1}^\\top \\Sigma \\alpha_{2} - 2 \\alpha_{1}^\\top \\lambda \\alpha_{2} - \\kappa \\alpha_{1}^\\top \\alpha_{1} = 0.\\]\nOr \\(\\alpha_{1}^\\top \\Sigma = \\lambda_{1} \\alpha_{1}^\\top\\), et \\(\\alpha_{1}^\\top \\alpha_{1} = 1\\), donc \\[2 \\alpha_{1}^\\top \\lambda \\alpha_{2} - 2 \\alpha_{1}^\\top \\lambda \\alpha_{2} - \\kappa \\alpha_{1}^\\top \\alpha_{1} = 0 \\implies - \\kappa = 0.\\]\nEn substituant ce r√©sulat, on obtient \\[\\Sigma \\alpha_{2} = \\lambda \\alpha_{2}.\\]\net donc \\(\\lambda\\) est une autre valeur propre de \\(\\Sigma\\). Puisque \\[\\mathrm{Var}(Y_{2}) = \\alpha_{2}^\\top \\Sigma \\alpha_{2} = \\alpha_{2}^\\top \\lambda \\alpha_{2} = \\lambda,\\] on a que cette variance est maximale si \\(\\lambda = \\lambda_{2}\\), la deuxi√®me plus grande valeur propre de \\(\\Sigma\\), et cons√©quemment \\(\\alpha_{2}\\) est le vecteur propre norm√© correspondant.\nOn peut g√©n√©raliser ce r√©sultat en utilisant des maximisations successives. On en conclut que \\[Y_k = \\alpha_k^\\top X,\\] o√π \\(\\alpha_k\\) est le vecteur propre norm√© associ√© √† \\(\\lambda_k\\), la \\(k\\)e plus grande valeur propre de \\(\\Sigma\\).\n\n\n\nIl est possible d‚Äôavoir une repr√©sentation plus compacte de l‚ÄôACP √† l‚Äôaide de matrices. Soit \\(A = \\left( \\alpha_{1}, \\dots, \\alpha_p \\right) \\in \\mathbb{R}^{p \\times p}\\), la matrice dont les colonnes sont les vecteurs propres. On a \\(Y = AX\\) et la covariance des composantes principales s‚Äô√©crit \\[\\mathrm{Var}(Y) = A^\\top \\Sigma A.\\]\n\n\n\n\n\n\nImportantPropri√©t√©s de \\(A\\)\n\n\n\n\n\\(A^{\\top} A = A A^{\\top} = I_p\\);\n\\(A^{\\top} = A^{-1}\\);\n\\(\\Sigma A = A \\Lambda\\), o√π \\(\\Lambda = \\text{diag}(\\lambda_{1}, \\dots, \\lambda_p)\\);\n\\(\\mathrm{Var}(Y) = A^{\\top} \\Sigma A = \\Lambda \\implies \\mathrm{Cov}(Y_k, Y_l) = 0\\) si \\(k \\neq l\\) et \\(\\mathrm{Var}(Y_k) = \\lambda_k \\geq \\mathrm{Var}(Y_l) = \\lambda_l\\) si et seulement si \\(k \\geq l\\).\n\n\n\n\n\n\n\n\n\nNotePreuves\n\n\n\n\n\n\nPar construction, les colonnes de \\(A\\) sont orthogonaux deux √† deux et de norme \\(1\\) (cf.¬†contraintes sur les vecteurs propres). La matrice \\(A\\) est donc une matrice orthogonale. Et donc \\(A^{\\top} A = A A^{\\top} = I_p\\).\nDe m√™me, comme \\(A\\) est orthogonal, on a \\(A^{\\top} = A^{-1}\\).\nLe r√©sultat est imm√©diat en faisant le produit de matrices.\nOn a \\(\\mathrm{Var}(Y) = A^{\\top} \\Sigma A = A^{\\top} A \\Lambda = \\Lambda\\). Comme \\(\\Lambda\\) est une matrice diagonale, \\(\\mathrm{Cov}(Y_k, Y_l) = 0\\) si \\(k \\neq l\\) (car pas sur la diagonale) et comme \\(\\lambda_{1} \\geq \\dots \\geq \\lambda_p\\), on a \\(\\mathrm{Var}(Y_1) \\geq \\dots \\geq \\mathrm{Var}(Y_p)\\).\n\n\n\n\nUne mesure globale de la variation presente dans les donn√©es est donn√©e par la trace de la matrice \\(\\Sigma\\): \\[\\text{tr}(\\Sigma) = \\text{tr}(\\Lambda) = \\sum_{i = 1}^{p} \\lambda_i = \\sum_{k = 1}^{p} \\mathrm{Var}(Y_k).\\]\nLa proportion de variation expliqu√©e par la composante principale \\(Y_k\\) est donc donn√©e par le ratio entre la valeur propre \\(k\\) et la somme des valeurs propres : \\[\\frac{\\lambda_k}{\\lambda_{1} + \\cdots + \\lambda_p} = \\frac{\\mathrm{Var}(Y_k)}{\\text{tr}(\\Sigma)}.\\]\nDe fa√ßon similaire, les \\(m\\) premi√®res composantes expliquent \\[100\\% \\times \\frac{\\sum_{k = 1}^{m} \\lambda_k}{\\sum_{k = 1}^{p} \\lambda_k} = 100\\% \\times \\frac{\\sum_{k = 1}^{m} \\mathrm{Var}(Y_k)}{\\sum_{k = 1}^{p} \\mathrm{Var}(Y_k)}\\] de la variabilit√© dans les variables.",
    "crumbs": [
      "Modules",
      "04 - Dimension",
      "Analyse en composantes principales"
    ]
  },
  {
    "objectID": "contents/dimension/01-pca.html#pratique-de-lacp",
    "href": "contents/dimension/01-pca.html#pratique-de-lacp",
    "title": "Analyse en composantes principales",
    "section": "Pratique de l‚ÄôACP",
    "text": "Pratique de l‚ÄôACP\n\nEstimation de la matrice de variance-covariance\nEn pratique, la matrice de variance-covariance \\(\\Sigma\\) est inconnue. Pour faire une ACP, il est n√©cessaire d‚Äôestimer \\(\\Sigma\\) √† partir d‚Äôun √©chantillon al√©atoire \\(X_{1}, \\dots, X_n\\) de r√©alisation ind√©pendante de \\(X\\). Un estimateur (sans biais) de \\(\\Sigma\\) est donn√© par \\[\\widehat{\\Sigma} = \\frac{1}{n - 1} \\sum_{i = 1}^{n} \\left( X_i - \\overline{X} \\right)\\left( X_i - \\widehat{X} \\right)^{\\top},\\] o√π \\(\\overline{X}\\) est la moyenne empirique de l‚Äô√©chantillon.\nLa matrice \\(\\widehat{\\Sigma}\\) ainsi obtenue est sym√©trique √† coefficients r√©els donc diagonalisable. Elle admet une d√©composition spectrale de la forme \\[\\widehat{\\Sigma} = \\widehat{A} \\widehat{\\Lambda} \\widehat{A}^{\\top},\\] o√π \\(\\widehat{A}\\) est une matrice orthogonale dont les colonnes sont les estimateurs des vecteurs propres de \\(\\Sigma\\) et \\(\\widehat{\\Lambda}\\) est une matrice diagonale contenant les estimateurs des valeurs propres de \\(\\Sigma\\), suppos√©es ordonn√©es de fa√ßon d√©croissante.\nLes composantes principales sont obtenues en projetant les observations \\(X_i\\) dans la base des vecteurs propres : \\[Y_i = \\widehat{A}^{\\top} X_i.\\]\n\n\nQuelques remarques\n\n\n\n\n\n\nAstuceSensibilit√© √† l‚Äô√©chelle de \\(X_1, \\dots, X_p\\)\n\n\n\nPuisque l‚Äôon cherche une combinaison lin√©aire de \\(X_{1}, \\dots, X_p\\) qui maximise la variance, une variable \\(X_k\\) ayant une grande variance aura un poids d√©mesur√© dans les composantes principales, ce qui peut fausser l‚Äôinterpr√©tation. On peut, par exemple, penser √† la mesure de distance. En effet, exprimer une distance entre m√®tres plut√¥t qu‚Äôen kilom√®tres multiplierait la variance de cette variable par \\(1\\) million (\\((10^3)^2\\)). Cette variable aurait donc un poids majeur dans toutes les composantes.\nAinsi, si les variables sont exprim√©es dans des unit√©s diff√©rentes ou pr√©sentent des ordres de grandeurs tr√®s vari√©s, il est recommand√© de standardiser les variables avant d‚Äôeffectuer une ACP. Cela revient √† effectuer une ACP sur la matrice des corr√©lations.\n\n\n\n\n\n\n\n\nAstucePremi√®re √©tape dans une analyse pr√©dictive\n\n\n\nIl arrive que l‚ÄôACP soit effectu√©e parce que l‚Äôon veuille pr√©dire la valeur de variable \\(Z\\) √† partir des valeurs de variables \\(X_{1}, \\dots, X_p\\) mais que \\(p\\) soit simplement trop grand. Dans ce cas, on applique l‚ÄôACP pour obtenir les \\(k \\ll p\\) premi√®res composantes principales \\(Y_1, \\dots, Y_k\\) et on les utilise pour pr√©dire \\(Z\\).\nPuisque les composantes principales retiennent la majeure partie de l‚Äôinformation contenue dans les variables originales, c‚Äôest g√©n√©ralement une fa√ßon raisonnable de faire.\n\n\n\n\n\n\n\n\nAstuceRotation des axes et qualit√© de repr√©sentation\n\n\n\nPuisque la matrice \\(A\\) est orthogonale, le produit matriciel \\(Y = A^{\\top}X\\) repr√©sente une rotation de l‚Äôespace des variables. Les nouveaux axes correspondent aux directions orthogonales de variation maximale successives, en supposant que \\(\\lambda_{1} &gt; \\dots &gt; \\lambda_p\\). Ainsi, \\(Y_i = A^{\\top} X_i\\) donne les coordonn√©es de l‚Äôobservation \\(X_i\\) dans le nouveau syst√®me d‚Äôaxes. On appelle \\(Y_{ik}\\) le score de l‚Äôobservation \\(X_i\\) sur l‚Äôaxe principal \\(k\\) et se calcule comme \\[Y_{ik} = \\alpha_k^\\top X_i = \\sum_{l = 1}^{p} \\alpha_{kl}X_{il}.\\]\nLa qualit√© de la repr√©sentation de l‚Äôobservation \\(i\\) sur l‚Äôaxe \\(k\\) est donn√©e par \\[Q_{ik} = \\frac{Y_{ik}^2}{d^2(0, Y_i)} = \\frac{Y_{ik}^2}{\\sqrt{Y_{i1}^2 + \\dots + Y_{i p}^2}}.\\]\n\n\n\n\nChoix du nombre de composantes\nUn enjeu central en ACP est de choisir combien de composantes principales retenir. En conserver trop ne r√©duit pas la dimension et en conserver trop peu peut faire perdre de l‚Äôinformation pertinente. Voici les principales r√®gles empiriques utilis√©es :\n\nR√®gle des \\(80\\%\\) : Retenir le nombre minimal de composantes n√©cessaires pour expliquer au moins \\(80\\%\\) de la variance totale. Ce seuil est arbitraire, mais il donne souvent une bonne intuition.\nR√®gle de Kaiser : Si l‚ÄôACP est faite √† partir de la matrice des corr√©lations, alors la moyenne des valeurs propres vaut \\(1\\). On recommande de ne garder que les composantes ayant une valeur propre sup√©rieure √† la moyenne des valeurs propres, soit \\(1\\).\nR√®gle de Joliffe : Variante plus stricte de la r√®gle de Kaiser, qui sugg√®re de conserver les composantes avec une valeur propre sup√©rieure √† \\(0.7\\) pour une ACP faite avec la matrice des corr√©lations.\nR√®gle de Cattell (ou du coude) : On trace les valeurs propres \\(\\lambda_k\\) en fonction de leur rang \\(k\\) et on cherche un point de rupture dans la d√©croissance. Au-del√† de ce point, les composantes suppl√©mentaires expliquent peu de variance suppl√©mentaire.\n\nCes r√®gles sont des outils d‚Äôaide √† la d√©cision, mais le choix du nombre de composantes d√©pend aussi du contexte, des objectifs de l‚Äôanalyse, et de la facilit√© d‚Äôinterpr√©tation.",
    "crumbs": [
      "Modules",
      "04 - Dimension",
      "Analyse en composantes principales"
    ]
  },
  {
    "objectID": "contents/01-introduction.html",
    "href": "contents/01-introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "Slides: link",
    "crumbs": [
      "Modules",
      "01 - Introduction"
    ]
  },
  {
    "objectID": "contents/01-introduction.html#quest-ce-que-lanalyse-de-donn√©es",
    "href": "contents/01-introduction.html#quest-ce-que-lanalyse-de-donn√©es",
    "title": "Introduction",
    "section": "Qu‚Äôest-ce que l‚Äôanalyse de donn√©es ?",
    "text": "Qu‚Äôest-ce que l‚Äôanalyse de donn√©es ?\nL‚Äôanalyse de donn√©es est un ensemble de m√©thodes permettant de retirer de l‚Äôinformation d‚Äôun jeu de donn√©es. On parle aussi d‚Äôapprentissage statistique (statistical learning). L‚Äôid√©e est d‚Äôutiliser des mod√®les statistiques pour comprendre comment les donn√©es sont structur√©es et comment elles int√©ragissent l‚Äôune avec l‚Äôautre.\n\n\n\n\n\n\nNoteExemple\n\n\n\nImaginons que vous √™tes employ√© par l‚ÄôOrganisation des Nations Unies (ONU). Votre mission est d‚Äôanalyser l‚Äôesp√©rance de vie √† travers le monde. Pour cela, vous disposez d‚Äôune mesure de l‚Äôesp√©rance de vie dans chaque pays membre de l‚ÄôONU, bien s√ªr, mais aussi le PIB par habitant, les montants des d√©penses li√©s √† la sant√©, le taux de fertilit√©, le taux d‚Äôurbanisation, le niveau d‚Äô√©ducation du pays, etc. Le but de l‚Äôanalyse de donn√©es est de trouver des liens entre ses diff√©rentes variables et la variable d‚Äôint√©r√™t, l‚Äôesp√©rance de vie, de visualiser ces donn√©es, et √©ventuellement de pr√©dire l‚Äôesp√©rance de vie √† partir des autres variables.",
    "crumbs": [
      "Modules",
      "01 - Introduction"
    ]
  },
  {
    "objectID": "contents/01-introduction.html#objectifs-du-cours",
    "href": "contents/01-introduction.html#objectifs-du-cours",
    "title": "Introduction",
    "section": "Objectifs du cours",
    "text": "Objectifs du cours\nDans ce cours, on cherche √† introduire des m√©thodes qui permettent une √©tude d‚Äôun jeu de donn√©es de ‚Äúhaute dimension‚Äù (dans le sens o√π l‚Äôon ne peut pas faire un simple graphique de l‚Äôensemble des variables pour chaque observation) sans avoir recours √† un mod√®le probabiliste. Les diff√©rentes techniques que l‚Äôon va voir peuvent servir √†:\n\nvisualiser les donn√©es;\nr√©duire la dimension des donn√©es;\nidentifier certains liens entre les variables;\ndiviser le jeu de donn√©es en groupes/classes.\n\nCe cours n‚Äôa pas vocation √† √™tre exhaustif, dans le sens de pr√©senter toutes les m√©thodes possibles. Ce cours n‚Äôa pas non plus vocation √† √™tre √† l‚Äô√©tat de l‚Äôart, dans le sens o√π on ne s‚Äôint√©ressera pas aux derniers d√©veloppements en apprentissage machine. Ce cours n‚Äôest pas non plus un cours de programmation.\nPour finir cette indroduction, voici un passage de Statistical Rethinking de Richard McElreath (McElreath 2020) trouvant r√©sonnance dans ce cours.\n\nStatistics courses [‚Ä¶] tend to resemble horoscopes. There are two senses to this resemblance. First, in order to remain plausibly correct, they must remain tremendously vague. This is because the targets of the advice, for both horoscopes and statistical advices, are diverse. But only the most general advice applies to all cases. A horoscope uses only the basic facts of birth to forecast life events, and a [‚Ä¶] statistical guide uses only the basic facts of measurement and design to dictate a model. It is easy to do better, once more detail is available. In the case of statistical analysis, it is typically only the scientist who can provide that detail, not the statistician. Second, there are strong incentives for both astrologers and statisticians to exaggerate the power and importance of their advice. No one likes an astrologer who forecasts doom, and few want a statistician who admits the answers as desired are not in the data as collected. Scientists desire results, and they will buy and attend to statisticians and statistical procedures that promise them. What we end up with is too often horoscopic: vague and optimistic, but still claiming critical importance.\n\n\n\n\nMachine learning (xkcd:1838).",
    "crumbs": [
      "Modules",
      "01 - Introduction"
    ]
  },
  {
    "objectID": "td/02-revision-td.html",
    "href": "td/02-revision-td.html",
    "title": "TD: R√©vision",
    "section": "",
    "text": "Dans ce probl√®me, on cherche √† mod√©liser la r√©partition des v√©los en libre service dans les diff√©rentes stations de l‚Äôuniversit√© Laval. Le campus comporte cinq stations pouvant acceuillir un total de \\(136\\) v√©los (lien): Abitibi-Prince (ABP), Alphone-Desjardins (ADJ), Charles-De Koninck (DKN), Ferdinand-Vandry (VND) et PEPS.\nSupposons que tous les v√©los sont retourn√©s √† une des cinq stations √† la fin de la journ√©e, i.e.¬†pour chaque jour, il y a un moment dans la journ√©e (par exemple, √† minuit), o√π tous les v√©los sont √† une certaine station. Nous pouvons donc nous int√©resser √† ces stations √† ce moment de la journ√©e pour chaque jour. Nous cherchons √† mod√©liser les movements des v√©los de minuit pour un jour donn√© jusqu‚Äô√† minuit du jour suivant. Nous trouvons que :\n\npour les v√©los emprunt√©s √† ABP, \\(50\\%\\) y retourne, \\(10\\%\\) vont √† ADJ, \\(20\\%\\) vont √† DKN et \\(20\\%\\) vont √† VND.\npour les v√©los emprunt√©s √† ADJ, \\(30\\%\\) y retourne, \\(10\\%\\) vont √† ABP, \\(10\\%\\) vont √† DKN, \\(10\\%\\) vont √† VND et \\(40\\%\\) vont au PEPS.\npour les v√©los emprunt√©s √† DKN, \\(80\\%\\) y retourne, \\(5\\%\\) vont √† ADJ, et \\(15\\%\\) vont au PEPS.\npour les v√©los emprunt√©s √† VND, \\(50\\%\\) y retourne, \\(20\\%\\) vont √† ABP, \\(5\\%\\) vont √† ADJ, \\(15\\%\\) vont √† DKN et \\(10\\%\\) vont au PEPS.\npour les v√©los emprunt√©s au PEPS, \\(0\\%\\) y retourne, \\(25\\%\\) vont √† ABP, \\(25\\%\\) vont √† ADJ, \\(25\\%\\) vont √† DKN et \\(25\\%\\) vont √† VND.\n\n\nFaire un graphe de la situation.\n√âcrire la matrice \\(T\\) tel que chaque entr√©e \\(t_{ij}\\) corresponde √† la probabilit√© de passer de la station \\(i\\) √† la station \\(j\\), \\(i, j \\in \\{ \\text{ABP}, \\text{ADJ}, \\text{DKN}, \\text{VND}, \\text{PEPS} \\}\\). On appelle cette matrice, la matrice de transition.\nQuelle est la probabilit√© qu‚Äôun v√©lo soit √† la station PEPS au jour 2 sachant qu‚Äôil √©tait √† la station ADJ au d√©but ?\nSupposons qu‚Äôil y ait \\(20\\) v√©los √† ABP, \\(35\\) v√©los √† ADJ, \\(26\\) v√©los √† DKN, \\(45\\) √† VND et \\(10\\) au PEPS. En utilisant une diagonalisation de la matrice \\(T\\), donner la r√©partition des v√©los apr√®s \\(10\\) jours."
  },
  {
    "objectID": "td/02-revision-td.html#exercice-1-un-probl√®me-de-v√©lo",
    "href": "td/02-revision-td.html#exercice-1-un-probl√®me-de-v√©lo",
    "title": "TD: R√©vision",
    "section": "",
    "text": "Dans ce probl√®me, on cherche √† mod√©liser la r√©partition des v√©los en libre service dans les diff√©rentes stations de l‚Äôuniversit√© Laval. Le campus comporte cinq stations pouvant acceuillir un total de \\(136\\) v√©los (lien): Abitibi-Prince (ABP), Alphone-Desjardins (ADJ), Charles-De Koninck (DKN), Ferdinand-Vandry (VND) et PEPS.\nSupposons que tous les v√©los sont retourn√©s √† une des cinq stations √† la fin de la journ√©e, i.e.¬†pour chaque jour, il y a un moment dans la journ√©e (par exemple, √† minuit), o√π tous les v√©los sont √† une certaine station. Nous pouvons donc nous int√©resser √† ces stations √† ce moment de la journ√©e pour chaque jour. Nous cherchons √† mod√©liser les movements des v√©los de minuit pour un jour donn√© jusqu‚Äô√† minuit du jour suivant. Nous trouvons que :\n\npour les v√©los emprunt√©s √† ABP, \\(50\\%\\) y retourne, \\(10\\%\\) vont √† ADJ, \\(20\\%\\) vont √† DKN et \\(20\\%\\) vont √† VND.\npour les v√©los emprunt√©s √† ADJ, \\(30\\%\\) y retourne, \\(10\\%\\) vont √† ABP, \\(10\\%\\) vont √† DKN, \\(10\\%\\) vont √† VND et \\(40\\%\\) vont au PEPS.\npour les v√©los emprunt√©s √† DKN, \\(80\\%\\) y retourne, \\(5\\%\\) vont √† ADJ, et \\(15\\%\\) vont au PEPS.\npour les v√©los emprunt√©s √† VND, \\(50\\%\\) y retourne, \\(20\\%\\) vont √† ABP, \\(5\\%\\) vont √† ADJ, \\(15\\%\\) vont √† DKN et \\(10\\%\\) vont au PEPS.\npour les v√©los emprunt√©s au PEPS, \\(0\\%\\) y retourne, \\(25\\%\\) vont √† ABP, \\(25\\%\\) vont √† ADJ, \\(25\\%\\) vont √† DKN et \\(25\\%\\) vont √† VND.\n\n\nFaire un graphe de la situation.\n√âcrire la matrice \\(T\\) tel que chaque entr√©e \\(t_{ij}\\) corresponde √† la probabilit√© de passer de la station \\(i\\) √† la station \\(j\\), \\(i, j \\in \\{ \\text{ABP}, \\text{ADJ}, \\text{DKN}, \\text{VND}, \\text{PEPS} \\}\\). On appelle cette matrice, la matrice de transition.\nQuelle est la probabilit√© qu‚Äôun v√©lo soit √† la station PEPS au jour 2 sachant qu‚Äôil √©tait √† la station ADJ au d√©but ?\nSupposons qu‚Äôil y ait \\(20\\) v√©los √† ABP, \\(35\\) v√©los √† ADJ, \\(26\\) v√©los √† DKN, \\(45\\) √† VND et \\(10\\) au PEPS. En utilisant une diagonalisation de la matrice \\(T\\), donner la r√©partition des v√©los apr√®s \\(10\\) jours."
  },
  {
    "objectID": "td/02-revision-td.html#exercice-2-paradoxe-des-deux-enfants",
    "href": "td/02-revision-td.html#exercice-2-paradoxe-des-deux-enfants",
    "title": "TD: R√©vision",
    "section": "Exercice 2: Paradoxe des deux enfants",
    "text": "Exercice 2: Paradoxe des deux enfants\nOn cherche √† mod√©liser les probabilit√©s d‚Äôavoir des enfants d‚Äôun certain sexe. Dans toutes les questions, les enfants sont soit de sexe masculin, soit de sexe f√©minin, de fa√ßon √©quiprobable.\n\nM. Gagnon a deux enfants. L‚Äôenfant a√Æn√© est une fille. Quelle est la probabilit√© que son deuxi√®me enfant soit aussi une fille ?\nMme Tremblay a deux enfants. On lui pose la question suivante : ‚ÄúAvez-vous au moins un gar√ßon ?‚Äù et elle r√©pond : ‚ÄúOui‚Äù. Quelle est la probabilit√© que les deux enfants soient des gar√ßons ?\nEst-ce que la probabilit√© de Mme Tremblay d‚Äôavoir deux gar√ßons change si on lui pose la question suivante : ‚ÄúIndiquez moi le sexe de l‚Äôun de vos enfants.‚Äù et qu‚Äôelle r√©pond : ‚ÄúJ‚Äôai (au moins) un gar√ßon.‚Äù ?\nOn croise M. Gagnon dans la rue en train de se balader avec sa fille. On lui demande quel jour de la semaine elle est n√©e. Elle nous r√©pond: ‚ÄúVendredi‚Äù. Quelle est la probabilit√© que son deuxi√®me enfant soit aussi une fille ?"
  },
  {
    "objectID": "td/02-revision-td.html#exercice-3-la-loi-dohm",
    "href": "td/02-revision-td.html#exercice-3-la-loi-dohm",
    "title": "TD: R√©vision",
    "section": "Exercice 3: La loi d‚ÄôOhm",
    "text": "Exercice 3: La loi d‚ÄôOhm\nOn cherche √† calculer la valeur d‚Äôun r√©sistance. Pour cela, on lui envoie un courant √©lectrique (intensit√© en amp√®res, A) et on mesure la diff√©rence de potentiel entre les bornes de la r√©sistance (tension en volt, V). On trouve les valeurs suivantes :\n\n\n\nIntensit√© (A)\nVoltage (V)\n\n\n\n\n0.2\n4.0\n\n\n0.5\n10.4\n\n\n0.9\n18.7\n\n\n1.0\n21.1\n\n\n1.2\n25.1\n\n\n1.3\n27.4\n\n\n1.8\n37.8\n\n\n\n\nCalculer l‚Äôintensit√© moyenne et le voltage moyen.\nCalculer les variances de l‚Äôintensit√© et du voltage.\nCalculer la covariance entre l‚Äôintensit√© et le voltage.\nLa loi d‚ÄôOhm nous dit que l‚Äôintensit√© et le voltage sont proportionnelle suivant la relation \\(V = R A\\) o√π \\(R\\) est la valeur de la r√©sistance en Ohm. Cette relation est √©quivalente √† estimer un mod√®le lin√©aire entre l‚Äôintensit√© et le voltage. La r√©sistance \\(R\\) est donc donn√©e par le ratio entre la covariance entre l‚Äôintensit√© et le voltage et la variance de l‚Äôintensit√© (cf.¬†cours de r√©gression pour une preuve). Calculer la valeur de \\(R\\).\nEn reprenant les valeurs du tableau, v√©rifier la valeur de \\(R\\). Pourquoi est-ce que l‚Äôon ne retrouve pas exactement les valeurs donn√©es ?"
  },
  {
    "objectID": "tp/06-unsupervised-tp.html",
    "href": "tp/06-unsupervised-tp.html",
    "title": "TP: Dimension",
    "section": "",
    "text": "Vous pouvez faire les exercices dans le langage de votre choix."
  },
  {
    "objectID": "tp/05-supervised-tp.html",
    "href": "tp/05-supervised-tp.html",
    "title": "TP: Dimension",
    "section": "",
    "text": "Vous pouvez faire les exercices dans le langage de votre choix."
  },
  {
    "objectID": "slides/05-supervised-ensemble-slides.html#principe-fondamental",
    "href": "slides/05-supervised-ensemble-slides.html#principe-fondamental",
    "title": "Dimension",
    "section": "Principe fondamental",
    "text": "Principe fondamental\n\nId√©e centrale Combiner les pr√©dictions de plusieurs mod√®les simples (classificateurs faibles) plut√¥t qu‚Äôun unique mod√®le complexe\n\n\nMeilleure performance globale\nPlus grande robustesse au bruit\nR√©duction du sur-ajustement\nStabilit√© des pr√©dictions am√©lior√©e"
  },
  {
    "objectID": "slides/05-supervised-ensemble-slides.html#strat√©gies-de-combinaison",
    "href": "slides/05-supervised-ensemble-slides.html#strat√©gies-de-combinaison",
    "title": "Dimension",
    "section": "Strat√©gies de combinaison",
    "text": "Strat√©gies de combinaison\n\nMoyenne des probabilit√©s pr√©dites par chaque mod√®le\nVote de majorit√© : Classe la plus souvent pr√©dite\nS√©lection de mod√®les : Analogie avec la s√©lection de variables\nM√©thodes automatis√©es : Bagging, boosting, for√™ts al√©atoires"
  },
  {
    "objectID": "slides/05-supervised-ensemble-slides.html#bootstrap-aggregating",
    "href": "slides/05-supervised-ensemble-slides.html#bootstrap-aggregating",
    "title": "Dimension",
    "section": "Bootstrap Aggregating",
    "text": "Bootstrap Aggregating\n\nPrincipe Construire plusieurs versions d‚Äôun m√™me mod√®le sur des √©chantillons diff√©rents\n\n\nG√©n√©rer \\(B\\) √©chantillons bootstrap (tirage avec remise)\nConstruire un arbre pour chaque √©chantillon\nObtenir \\(B\\) arbres de classification\nPr√©dire par vote de majorit√© ou moyenne des probabilit√©s"
  },
  {
    "objectID": "slides/05-supervised-ensemble-slides.html#avantages-et-√©valuation",
    "href": "slides/05-supervised-ensemble-slides.html#avantages-et-√©valuation",
    "title": "Dimension",
    "section": "Avantages et √©valuation",
    "text": "Avantages et √©valuation\n\nAvantages principaux - R√©duction de la variance des pr√©dictions - Augmentation de la stabilit√©\n\n\nInconv√©nient Perte d‚Äôinterpr√©tabilit√© des variables individuelles\n\n\n√âvaluation de l‚Äôimportance des variables - Sommer les r√©ductions d‚Äôindice de Gini par variable - Moyenner sur tous les arbres - Exprimer en pourcentage relatif"
  },
  {
    "objectID": "slides/05-supervised-ensemble-slides.html#extension-du-bagging",
    "href": "slides/05-supervised-ensemble-slides.html#extension-du-bagging",
    "title": "Dimension",
    "section": "Extension du bagging",
    "text": "Extension du bagging\n\nProbl√®me du bagging Les arbres se ressemblent (dominance des classes fr√©quentes)\n\n\nSolution des for√™ts al√©atoires Introduction d‚Äôune source suppl√©mentaire d‚Äôal√©a\n\n\nM√©canisme √Ä chaque division, s√©lectionner al√©atoirement \\(m\\) variables parmi \\(p\\) disponibles\nChoix usuel: \\(m \\approx \\sqrt{p}\\)"
  },
  {
    "objectID": "slides/05-supervised-ensemble-slides.html#b√©n√©fices-de-la-randomisation",
    "href": "slides/05-supervised-ensemble-slides.html#b√©n√©fices-de-la-randomisation",
    "title": "Dimension",
    "section": "B√©n√©fices de la randomisation",
    "text": "B√©n√©fices de la randomisation\n\nD√©corr√©lation des arbres construits\nDiversification des mod√®les\nAm√©lioration de la performance globale\nRobustesse accrue\n\n\nR√©sultat Meilleure g√©n√©ralisation que le bagging simple"
  },
  {
    "objectID": "slides/05-supervised-ensemble-slides.html#approche-s√©quentielle",
    "href": "slides/05-supervised-ensemble-slides.html#approche-s√©quentielle",
    "title": "Dimension",
    "section": "Approche s√©quentielle",
    "text": "Approche s√©quentielle\n\nPhilosophie oppos√©e au bagging Construction s√©quentielle au lieu de parall√®le\n\n\nPrincipe 1. Construire une s√©rie de mod√®les simples 2. Accorder progressivement plus d‚Äôimportance aux erreurs 3. Corriger les erreurs du mod√®le pr√©c√©dent 4. Atteindre une tr√®s haute pr√©cision globale"
  },
  {
    "objectID": "slides/05-supervised-ensemble-slides.html#condition-de-succ√®s",
    "href": "slides/05-supervised-ensemble-slides.html#condition-de-succ√®s",
    "title": "Dimension",
    "section": "Condition de succ√®s",
    "text": "Condition de succ√®s\n\nExigence fondamentale Utiliser des mod√®les faibles (weak learners)\n\n\nRaison Laisser au processus de boosting le soin de combiner optimalement les pr√©dictions"
  },
  {
    "objectID": "slides/05-supervised-ensemble-slides.html#variantes-principales",
    "href": "slides/05-supervised-ensemble-slides.html#variantes-principales",
    "title": "Dimension",
    "section": "Variantes principales",
    "text": "Variantes principales\n\n1. AdaBoost (Adaptive Boosting) - Ajuste les poids des observations √† chaque it√©ration - M√©thode historique et bien √©tablie\n\n\n2. Gradient Boosting - Minimisation d‚Äôune fonction de perte - Proc√©dure it√©rative inspir√©e de la descente du gradient - Tr√®s utilis√© aujourd‚Äôhui"
  },
  {
    "objectID": "slides/05-supervised-ensemble-slides.html#n√©cessit√©-de-loptimisation",
    "href": "slides/05-supervised-ensemble-slides.html#n√©cessit√©-de-loptimisation",
    "title": "Dimension",
    "section": "N√©cessit√© de l‚Äôoptimisation",
    "text": "N√©cessit√© de l‚Äôoptimisation\n\nObservation empirique Plus une m√©thode est puissante, plus l‚Äôoptimisation des hyper-param√®tres est complexe et sensible\n\n\nObjectif Atteindre un bon compromis entre performance et robustesse"
  },
  {
    "objectID": "slides/05-supervised-ensemble-slides.html#param√®tres-par-m√©thode",
    "href": "slides/05-supervised-ensemble-slides.html#param√®tres-par-m√©thode",
    "title": "Dimension",
    "section": "Param√®tres par m√©thode",
    "text": "Param√®tres par m√©thode\n\nBagging - Nombre d‚Äô√©chantillons bootstrap \\(B\\)\n\n\nFor√™ts al√©atoires - Nombre d‚Äô√©chantillons bootstrap \\(B\\) - Nombre de variables \\(m\\) √† consid√©rer √† chaque division\n\n\nBoosting - Taille des arbres - Fraction du jeu de donn√©es √©chantillonn√© - Nombre d‚Äôit√©rations - Param√®tres de r√©gularisation"
  },
  {
    "objectID": "slides/05-supervised-ensemble-slides.html#m√©thode-doptimisation",
    "href": "slides/05-supervised-ensemble-slides.html#m√©thode-doptimisation",
    "title": "Dimension",
    "section": "M√©thode d‚Äôoptimisation",
    "text": "M√©thode d‚Äôoptimisation\n\nValidation crois√©e N√©cessaire pour chaque param√®tre afin d‚Äô√©viter le sur-ajustement\n\n\nProcessus 1. D√©finir une grille de valeurs pour chaque param√®tre 2. √âvaluer par validation crois√©e 3. S√©lectionner la combinaison optimale 4. Tester sur un jeu de donn√©es ind√©pendant"
  },
  {
    "objectID": "slides/05-supervised-ensemble-slides.html#r√©capitulatif-des-m√©thodes",
    "href": "slides/05-supervised-ensemble-slides.html#r√©capitulatif-des-m√©thodes",
    "title": "Dimension",
    "section": "R√©capitulatif des m√©thodes",
    "text": "R√©capitulatif des m√©thodes\n\n\n\n\nM√©thode\nApproche\nAvantage principal\n\n\n\n\nBagging\nParall√®le\nR√©duction variance\n\n\nFor√™ts al√©atoires\nParall√®le + al√©a\nD√©corr√©lation\n\n\nBoosting\nS√©quentielle\nCorrection d‚Äôerreurs"
  },
  {
    "objectID": "slides/05-supervised-ensemble-slides.html#points-cl√©s-√†-retenir",
    "href": "slides/05-supervised-ensemble-slides.html#points-cl√©s-√†-retenir",
    "title": "Dimension",
    "section": "Points cl√©s √† retenir",
    "text": "Points cl√©s √† retenir\n\nEnsemble &gt; Individuel : Combinaison de mod√®les faibles\nDiversit√© : Cl√© du succ√®s des m√©thodes ensemblistes\nTrade-off : Performance vs Interpr√©tabilit√©\nOptimisation : Cruciale pour la performance finale\nApplications : Base de nombreux algorithmes modernes (XGBoost, LightGBM, etc.)"
  },
  {
    "objectID": "slides/05-supervised-ensemble-slides.html#merci-pour-votre-attention",
    "href": "slides/05-supervised-ensemble-slides.html#merci-pour-votre-attention",
    "title": "Dimension",
    "section": "Merci pour votre attention",
    "text": "Merci pour votre attention\nQuestions ?\n\nLes m√©thodes ensemblistes : quand l‚Äôunion fait la force"
  },
  {
    "objectID": "slides/03-generalities-bias-variance-slides-annotated.html#objectif-de-mod√©lisation",
    "href": "slides/03-generalities-bias-variance-slides-annotated.html#objectif-de-mod√©lisation",
    "title": "G√©n√©ralit√©s",
    "section": "Objectif de mod√©lisation",
    "text": "Objectif de mod√©lisation\n\nMod√©liser la relation entre :\n\nVariable r√©ponse \\(Y\\) (quantitative, qualitative, etc.).\nVariables explicatives \\(X = (X_1, \\ldots, X_p)\\)."
  },
  {
    "objectID": "slides/03-generalities-bias-variance-slides-annotated.html#mod√®le-g√©n√©ral",
    "href": "slides/03-generalities-bias-variance-slides-annotated.html#mod√®le-g√©n√©ral",
    "title": "G√©n√©ralit√©s",
    "section": "Mod√®le g√©n√©ral",
    "text": "Mod√®le g√©n√©ral\n\n\\[Y = f(X) + \\varepsilon\\]\n\n\\(f\\) : fonction d√©terministe (information syst√©matique).\n\\(\\varepsilon\\) : terme d‚Äôerreur al√©atoire."
  },
  {
    "objectID": "slides/03-generalities-bias-variance-slides-annotated.html#hypoth√®ses-du-mod√®le",
    "href": "slides/03-generalities-bias-variance-slides-annotated.html#hypoth√®ses-du-mod√®le",
    "title": "G√©n√©ralit√©s",
    "section": "Hypoth√®ses du mod√®le",
    "text": "Hypoth√®ses du mod√®le\n\nSur le terme d‚Äôerreur \\(\\varepsilon\\) :\n\nInd√©pendant des variables explicatives \\(X\\).\n\\(\\mathbb{E}[\\varepsilon] = 0\\) (esp√©rance nulle).\n\\(\\mathrm{Var}(\\varepsilon) = \\sigma^2\\) (variance constante).\n\n\n‚Üí Cadre g√©n√©ral pour toutes les m√©thodes du cours."
  },
  {
    "objectID": "slides/03-generalities-bias-variance-slides-annotated.html#plan",
    "href": "slides/03-generalities-bias-variance-slides-annotated.html#plan",
    "title": "G√©n√©ralit√©s",
    "section": "Plan",
    "text": "Plan\n\n\nMesures de qualit√© - MSE et taux d‚Äôerreur\nCompromis biais/variance - D√©composition fondamentale\nFlexibilit√© des mod√®les - Rigidit√© vs adaptabilit√©\nOptimisation pratique - Trouver l‚Äô√©quilibre"
  },
  {
    "objectID": "slides/03-generalities-bias-variance-slides-annotated.html#comment-mesurer-la-qualit√©",
    "href": "slides/03-generalities-bias-variance-slides-annotated.html#comment-mesurer-la-qualit√©",
    "title": "G√©n√©ralit√©s",
    "section": "Comment mesurer la qualit√© ?",
    "text": "Comment mesurer la qualit√© ?\n\nUne fois \\(\\hat{f}\\) estim√©, comment √©valuer \\(\\hat{Y} = \\hat{f}(X)\\) ?\n\n\nId√©e : Mesurer √† quel point \\(\\hat{Y}\\) est proche de la vraie valeur \\(Y\\)."
  },
  {
    "objectID": "slides/03-generalities-bias-variance-slides-annotated.html#variables-quantitatives-eqm-mse",
    "href": "slides/03-generalities-bias-variance-slides-annotated.html#variables-quantitatives-eqm-mse",
    "title": "G√©n√©ralit√©s",
    "section": "Variables quantitatives : EQM (MSE)",
    "text": "Variables quantitatives : EQM (MSE)\n\n\n\nErreur Quadratique Moyenne (MSE)\n\n\n\\[\\text{MSE}(Y, \\hat{Y}) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{f}(x_i))^2\\]\n\n\n\n\nInterpr√©tation ‚Üí Distance moyenne entre valeurs observ√©es et pr√©dites.\n\n\nMSE faible ‚Üí Pr√©dictions proches des observations."
  },
  {
    "objectID": "slides/03-generalities-bias-variance-slides-annotated.html#variables-qualitatives-taux-derreur-er",
    "href": "slides/03-generalities-bias-variance-slides-annotated.html#variables-qualitatives-taux-derreur-er",
    "title": "G√©n√©ralit√©s",
    "section": "Variables qualitatives : Taux d‚Äôerreur (ER)",
    "text": "Variables qualitatives : Taux d‚Äôerreur (ER)\n\n\n\nTaux d‚Äôerreur (ER)\n\n\n\\[\\text{ER}(Y, \\hat{Y}) = \\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{1}(y_i \\neq \\hat{f}(x_i))\\]\n\n\n\n\nInterpr√©tation ‚Üí Proportion de mauvaises pr√©dictions.\n\n\nER faible ‚Üí Pr√©dictions proches des observations."
  },
  {
    "objectID": "slides/03-generalities-bias-variance-slides-annotated.html#exactitude-vs-interpr√©tabilit√©",
    "href": "slides/03-generalities-bias-variance-slides-annotated.html#exactitude-vs-interpr√©tabilit√©",
    "title": "G√©n√©ralit√©s",
    "section": "Exactitude vs Interpr√©tabilit√©",
    "text": "Exactitude vs Interpr√©tabilit√©\n\nMod√®les simples (ex: r√©gression lin√©aire) :\n\nFaciles √† interpr√©ter\nRelations complexes mal capt√©es\n\nMod√®les flexibles (ex: for√™t al√©atoire) :\n\nMeilleures pr√©dictions\nDifficiles √† interpr√©ter\n\n\nLe choix d√©pend de l‚Äôobjectif : compr√©hension ou performance ?"
  },
  {
    "objectID": "slides/03-generalities-bias-variance-slides-annotated.html#no-free-lunch-in-statistics",
    "href": "slides/03-generalities-bias-variance-slides-annotated.html#no-free-lunch-in-statistics",
    "title": "G√©n√©ralit√©s",
    "section": "‚ÄúNo Free Lunch in Statistics‚Äù",
    "text": "‚ÄúNo Free Lunch in Statistics‚Äù\n\n\n\n\nIl n‚Äôexiste pas de m√©thode universellement optimale !\n\n\nUne m√©thode performante dans un contexte peut √©chouer ailleurs.\n‚Üí Toujours adapter l‚Äôapproche au probl√®me."
  },
  {
    "objectID": "slides/03-generalities-bias-variance-slides-annotated.html#erreur-de-pr√©diction",
    "href": "slides/03-generalities-bias-variance-slides-annotated.html#erreur-de-pr√©diction",
    "title": "G√©n√©ralit√©s",
    "section": "Erreur de pr√©diction",
    "text": "Erreur de pr√©diction\n\nNotre vrai objectif : Minimiser l‚Äôerreur sur de nouvelles donn√©es\n\\[\\mathbb{E}[(Y - \\hat{Y})^2] = \\mathbb{E}[(Y - \\hat{f}(X))^2]\\]\n\n\nCette erreur se d√©compose en 3 parties‚Ä¶"
  },
  {
    "objectID": "slides/03-generalities-bias-variance-slides-annotated.html#d√©composition-biaisvariance",
    "href": "slides/03-generalities-bias-variance-slides-annotated.html#d√©composition-biaisvariance",
    "title": "G√©n√©ralit√©s",
    "section": "D√©composition biais/variance",
    "text": "D√©composition biais/variance\n\n\n\nD√©composition biais/variance\n\n\n\\[\\mathbb{E}[(Y - \\hat{f}(X))^2] = \\text{Biais}(\\hat{f}(X))^2 + \\text{Var}(\\hat{f}(X)) + \\sigma^2\\]\n\n\n\nTrois composantes :\n\nBiais\\(^2\\) ‚Üí Erreur syst√©matique d‚Äôapproximation.\nVariance ‚Üí Sensibilit√© aux fluctuations d‚Äô√©chantillon.\n\\(\\sigma^2\\) ‚Üí Erreur irr√©ductible (bruit intrins√®que)."
  },
  {
    "objectID": "slides/03-generalities-bias-variance-slides-annotated.html#d√©composition-biaisvariance-2",
    "href": "slides/03-generalities-bias-variance-slides-annotated.html#d√©composition-biaisvariance-2",
    "title": "G√©n√©ralit√©s",
    "section": "D√©composition biais/variance",
    "text": "D√©composition biais/variance\n\nMod√®le peu flexible (ex: r√©gression lin√©aire)\n\n‚úÖ Variance faible\n‚ùå Biais √©lev√©\n\nMod√®le tr√®s flexible (ex: r√©gression avec beaucoup de polyn√¥mes)\n\n‚ùå Variance √©lev√©e\n‚úÖ Biais faible\n\n\n‚Üí Besoin d‚Äôun √©quilibre optimal !"
  },
  {
    "objectID": "slides/03-generalities-bias-variance-slides-annotated.html#d√©composition-biaisvariance-3",
    "href": "slides/03-generalities-bias-variance-slides-annotated.html#d√©composition-biaisvariance-3",
    "title": "G√©n√©ralit√©s",
    "section": "D√©composition biais/variance",
    "text": "D√©composition biais/variance\n\n\nPar Bigbossfarin - Own work, CC0, Link."
  },
  {
    "objectID": "slides/03-generalities-bias-variance-slides-annotated.html#strat√©gies-doptimisation",
    "href": "slides/03-generalities-bias-variance-slides-annotated.html#strat√©gies-doptimisation",
    "title": "G√©n√©ralit√©s",
    "section": "Strat√©gies d‚Äôoptimisation",
    "text": "Strat√©gies d‚Äôoptimisation\n\nComment trouver l‚Äô√©quilibre ?\n\nValidation crois√©e ‚Üí Estimation de l‚Äôerreur de pr√©diction.\nS√©lection de mod√®le ‚Üí Comparer diff√©rentes des mod√®les avec diff√©rentes flexibilit√©s.\nM√©thodes d‚Äôensembles ‚Üí Combiner plusieurs mod√®les.\nR√©gularisation ‚Üí Contr√¥ler la complexit√© du mod√®le."
  },
  {
    "objectID": "slides/03-generalities-bias-variance-slides-annotated.html#implications-pour-la-pratique",
    "href": "slides/03-generalities-bias-variance-slides-annotated.html#implications-pour-la-pratique",
    "title": "G√©n√©ralit√©s",
    "section": "Implications pour la pratique",
    "text": "Implications pour la pratique\n\nQuestions √† se poser :\n\nAi-je assez de donn√©es pour un mod√®le flexible ?\nMon objectif est-il la pr√©diction ou la compr√©hension ?\nQuelle est la complexit√© r√©elle du ph√©nom√®ne √©tudi√© ?\nComment √©valuer la performance sur nouvelles donn√©es ?"
  },
  {
    "objectID": "slides/03-generalities-bias-variance-slides-annotated.html#conclusion",
    "href": "slides/03-generalities-bias-variance-slides-annotated.html#conclusion",
    "title": "G√©n√©ralit√©s",
    "section": "Conclusion",
    "text": "Conclusion\n\nErreur totale = Biais\\(^2\\) + Variance + Bruit irr√©ductible.\nCompromis entre biais et variance.\nMod√®les rigides ‚Üí Fort biais, faible variance.\nMod√®les flexibles ‚Üí Faible biais, forte variance.\nOptimum ‚Üí √âquilibre minimisant l‚Äôerreur totale.\nPas de solution universelle ‚Üí Il faut s‚Äôadapter au contexte.\n\n\nProchaine √©tape ‚Üí Validation et s√©lection de mod√®les."
  },
  {
    "objectID": "slides/05-supervised-tree-slides.html#introduction",
    "href": "slides/05-supervised-tree-slides.html#introduction",
    "title": "Dimension",
    "section": "Introduction",
    "text": "Introduction\n\nCART: Classification And Regression Trees\nM√©thode d‚Äôapprentissage supervis√©e\nClassification: Pr√©dire l‚Äôappartenance √† l‚Äôun des \\(K\\) groupes\nR√©gression: Pr√©dire une variable num√©rique continue \\(Y\\)\nVariables: \\(X_1, \\ldots, X_p\\) avec \\(n\\) observations dans \\(\\mathbb{R}^p\\)"
  },
  {
    "objectID": "slides/05-supervised-tree-slides.html#objectif-principal",
    "href": "slides/05-supervised-tree-slides.html#objectif-principal",
    "title": "Dimension",
    "section": "Objectif principal",
    "text": "Objectif principal\n\nPartitionner l‚Äôespace des observations \\(\\mathbb{R}^p\\) en sous-ensembles homog√®nes\n\n\nAlgorithme r√©cursif de d√©coupage binaire 1. D√©couper selon une variable \\(X_j\\) √† un seuil \\(s\\) 2. Cr√©er des sous-r√©gions rectangulaires (hyper-rectangles) 3. Chaque sous-ensemble = une feuille avec une pr√©diction\n\n\nPr√©diction par feuille - Classification: Classe majoritaire - R√©gression: Moyenne des \\(Y\\)"
  },
  {
    "objectID": "slides/05-supervised-tree-slides.html#principe-de-lalgorithme",
    "href": "slides/05-supervised-tree-slides.html#principe-de-lalgorithme",
    "title": "Dimension",
    "section": "Principe de l‚Äôalgorithme",
    "text": "Principe de l‚Äôalgorithme\n\nD√©coupage selon \\(X_j \\leq s\\) vs \\(X_j &gt; s\\)\nChoix optimal de \\((X_j, s)\\) maximisant le crit√®re d‚Äôhomog√©n√©it√©\nR√©p√©tition jusqu‚Äôau crit√®re d‚Äôarr√™t\nPr√©diction unique par feuille pour toutes les observations\n\n\n‚ö†Ô∏è Algorithme glouton: Optimisation locale, pas globale !"
  },
  {
    "objectID": "slides/05-supervised-tree-slides.html#algorithme-d√©taill√©",
    "href": "slides/05-supervised-tree-slides.html#algorithme-d√©taill√©",
    "title": "Dimension",
    "section": "Algorithme d√©taill√©",
    "text": "Algorithme d√©taill√©\n\nPour chaque n≈ìud:\n\n\n√âvaluer tous les couples \\((X_j, s)\\) possibles\nCalculer la r√©duction du crit√®re d‚Äôhomog√©n√©it√©\nS√©lectionner le couple qui maximise le gain\nDiviser le n≈ìud selon \\(X_j \\leq s\\) vs \\(X_j &gt; s\\)\nR√©p√©ter jusqu‚Äôaux crit√®res d‚Äôarr√™t\n\n\nPoint de d√©part: L‚Äôensemble complet des observations"
  },
  {
    "objectID": "slides/05-supervised-tree-slides.html#crit√®res-dhomog√©n√©it√©---classification",
    "href": "slides/05-supervised-tree-slides.html#crit√®res-dhomog√©n√©it√©---classification",
    "title": "Dimension",
    "section": "Crit√®res d‚Äôhomog√©n√©it√© - Classification",
    "text": "Crit√®res d‚Äôhomog√©n√©it√© - Classification\nSoit \\(\\widehat{p}_{jk}\\) la proportion d‚Äôobservations de la r√©gion \\(R_j\\) appartenant √† la classe \\(k\\)\n\n1. Taux d‚Äôerreur de classification \\[E_j = 1 - \\max_k \\widehat{p}_{jk}\\] - Facile √† interpr√©ter - Peu sensible ‚Üí rarement utilis√© pour la construction"
  },
  {
    "objectID": "slides/05-supervised-tree-slides.html#crit√®res-dhomog√©n√©it√©-suite",
    "href": "slides/05-supervised-tree-slides.html#crit√®res-dhomog√©n√©it√©-suite",
    "title": "Dimension",
    "section": "Crit√®res d‚Äôhomog√©n√©it√© (suite)",
    "text": "Crit√®res d‚Äôhomog√©n√©it√© (suite)\n\n2. Indice de Gini (le plus utilis√©) \\[G_j = \\sum_{k=1}^{K} \\widehat{p}_{jk}(1 - \\widehat{p}_{jk})\\] - Minimal (\\(= 0\\)) quand homog√©n√©it√© parfaite - On cherche un indice faible\n\n\n3. Entropie crois√©e (th√©orie de l‚Äôinformation) \\[D_j = -\\sum_{k=1}^{K} \\widehat{p}_{jk} \\log(\\widehat{p}_{jk})\\] - Bas√© sur Shannon - Minimal pour classe parfaitement pr√©dite"
  },
  {
    "objectID": "slides/05-supervised-tree-slides.html#gain-dinformation",
    "href": "slides/05-supervised-tree-slides.html#gain-dinformation",
    "title": "Dimension",
    "section": "Gain d‚Äôinformation",
    "text": "Gain d‚Äôinformation\n\nMesure de l‚Äôam√©lioration apr√®s division\n\n\nExemple avec l‚Äôindice de Gini: \\[\\Delta G = G_{\\text{avant}} - \\left(\\frac{n_1}{n} G_1 + \\frac{n_2}{n} G_2\\right)\\]\no√π: - \\(n_1, n_2\\) : nombres d‚Äôobservations dans les sous-ensembles - \\(n = n_1 + n_2\\)\n\n\nObjectif: Maximiser le gain d‚Äôinformation"
  },
  {
    "objectID": "slides/05-supervised-tree-slides.html#probl√®me-du-sur-ajustement",
    "href": "slides/05-supervised-tree-slides.html#probl√®me-du-sur-ajustement",
    "title": "Dimension",
    "section": "Probl√®me du sur-ajustement",
    "text": "Probl√®me du sur-ajustement\n\nDilemme - Arbre trop profond ‚Üí Sur-ajustement aux donn√©es d‚Äôapprentissage - Arbre trop simple ‚Üí Sous-ajustement, classification trop simpliste\n\n\nCons√©quence Perte de capacit√© de g√©n√©ralisation √† de nouvelles donn√©es\n\n\nSolution: √âlagage (pruning)"
  },
  {
    "objectID": "slides/05-supervised-tree-slides.html#√©lagage-de-larbre",
    "href": "slides/05-supervised-tree-slides.html#√©lagage-de-larbre",
    "title": "Dimension",
    "section": "√âlagage de l‚Äôarbre",
    "text": "√âlagage de l‚Äôarbre\n\nStrat√©gie en deux temps: 1. Croissance compl√®te jusqu‚Äôaux feuilles pures 2. √âlagage des branches peu performantes\n\n\nCrit√®re co√ªt-complexit√© \\[\\sum_{j=1}^{|T|} c_j + \\alpha |T|\\]\no√π: - \\(|T|\\) : nombre de feuilles - \\(c_j\\) : co√ªt dans la feuille \\(j\\) - \\(\\alpha\\) : param√®tre de r√©gularisation"
  },
  {
    "objectID": "slides/05-supervised-tree-slides.html#choix-du-param√®tre-Œ±",
    "href": "slides/05-supervised-tree-slides.html#choix-du-param√®tre-Œ±",
    "title": "Dimension",
    "section": "Choix du param√®tre Œ±",
    "text": "Choix du param√®tre Œ±\n\nInterpr√©tation - \\(\\alpha\\) √©lev√© ‚Üí Favorise les arbres simples - \\(\\alpha\\) faible ‚Üí Arbres plus complexes\n\n\nM√©thode de s√©lection Validation crois√©e pour comparer les performances sur diff√©rentes valeurs de \\(\\alpha\\)"
  },
  {
    "objectID": "slides/05-supervised-tree-slides.html#avantages-des-arbres",
    "href": "slides/05-supervised-tree-slides.html#avantages-des-arbres",
    "title": "Dimension",
    "section": "Avantages des arbres",
    "text": "Avantages des arbres\n\nTr√®s interpr√©tables : Chemin explicite pour chaque pr√©diction\nRobustes aux valeurs extr√™mes et transformations monotones\nGestion naturelle des interactions entre variables\nVariables mixtes : Quantitatives et qualitatives\nAucune hypoth√®se sur la distribution des donn√©es\nS√©lection automatique des variables importantes\nBase de m√©thodes modernes (for√™ts al√©atoires, boosting)"
  },
  {
    "objectID": "slides/05-supervised-tree-slides.html#limites-des-arbres",
    "href": "slides/05-supervised-tree-slides.html#limites-des-arbres",
    "title": "Dimension",
    "section": "Limites des arbres",
    "text": "Limites des arbres\n\nInstabilit√© : Sensibles aux petites modifications des donn√©es\nPerformance limit√©e seuls, surtout avec donn√©es bruit√©es\nDivisions rectangulaires inadapt√©es aux fronti√®res non-lin√©aires\nTendance au sur-ajustement sans √©lagage\nSensibilit√© au d√©s√©quilibre des classes\nSolution : M√©thodes ensemblistes (for√™ts, boosting)"
  },
  {
    "objectID": "slides/05-supervised-tree-slides.html#r√©capitulatif",
    "href": "slides/05-supervised-tree-slides.html#r√©capitulatif",
    "title": "Dimension",
    "section": "R√©capitulatif",
    "text": "R√©capitulatif\n\nLes arbres CART - M√©thode de partitionnement r√©cursif - Algorithme glouton simple et intuitif - N√©cessitent un √©lagage pour √©viter le sur-ajustement\n\n\nUsage recommand√© - Excellents pour l‚Äôinterpr√©tabilit√© - Meilleurs en combinaison (m√©thodes ensemblistes) - Premi√®re approche pour comprendre les donn√©es"
  },
  {
    "objectID": "slides/05-supervised-tree-slides.html#merci-pour-votre-attention",
    "href": "slides/05-supervised-tree-slides.html#merci-pour-votre-attention",
    "title": "Dimension",
    "section": "Merci pour votre attention",
    "text": "Merci pour votre attention\nQuestions ?\n\nLes arbres de d√©cision : simplicit√©, interpr√©tabilit√© et fondement des m√©thodes ensemblistes"
  },
  {
    "objectID": "slides/06-unsupervised-hierarchy-slides.html#motivation",
    "href": "slides/06-unsupervised-hierarchy-slides.html#motivation",
    "title": "Non-supervis√©e",
    "section": "Motivation",
    "text": "Motivation\n\nLimites du \\(k\\)-means - N√©cessite les donn√©es originales - Probl√©matique avec matrice de similarit√©/distance uniquement\n\n\nSolution : Classification hi√©rarchique - S√©rie de partitions imbriqu√©es - De \\(n\\) groupes (chaque observation isol√©e) √† 1 groupe (toutes ensemble) - Repr√©sentation : Dendrogramme"
  },
  {
    "objectID": "slides/06-unsupervised-hierarchy-slides.html#types-dalgorithmes",
    "href": "slides/06-unsupervised-hierarchy-slides.html#types-dalgorithmes",
    "title": "Non-supervis√©e",
    "section": "Types d‚Äôalgorithmes",
    "text": "Types d‚Äôalgorithmes\n\nAlgorithmes ascendants\n\nD√©part : \\(n\\) groupes individuels\nProcessus : Fusions successives\n\nAlgorithmes descendants\n\nD√©part : 1 groupe de toutes les observations\nProcessus : Divisions successives\n\n\n\nR√©sultat commun : \\(n\\) partitions hi√©rarchiques de 1 √† \\(n\\) groupes"
  },
  {
    "objectID": "slides/06-unsupervised-hierarchy-slides.html#algorithmes-descendants",
    "href": "slides/06-unsupervised-hierarchy-slides.html#algorithmes-descendants",
    "title": "Non-supervis√©e",
    "section": "Algorithmes descendants",
    "text": "Algorithmes descendants\n\nPrincipe - D√©marrage avec toutes les observations dans un groupe - Division du groupe le moins homog√®ne √† chaque √©tape - Maximisation de la dissimilarit√© entre sous-groupes\n\n\nInconv√©nient majeur Co√ªt computationnel tr√®s √©lev√© (√©valuation de toutes les divisions possibles)\n\n\nUsage : Rarement utilis√© en pratique"
  },
  {
    "objectID": "slides/06-unsupervised-hierarchy-slides.html#algorithmes-ascendants",
    "href": "slides/06-unsupervised-hierarchy-slides.html#algorithmes-ascendants",
    "title": "Non-supervis√©e",
    "section": "Algorithmes ascendants",
    "text": "Algorithmes ascendants\n\nPrincipe - D√©marrage avec \\(n\\) groupes distincts - Fusion des deux groupes les plus similaires √† chaque √©tape - Continuation jusqu‚Äô√† un seul groupe\n\n\nAvantage : Plus efficace computationnellement\n\n\nEnjeu cl√© : D√©finir la distance entre groupes"
  },
  {
    "objectID": "slides/06-unsupervised-hierarchy-slides.html#plus-proche-voisin-single-linkage",
    "href": "slides/06-unsupervised-hierarchy-slides.html#plus-proche-voisin-single-linkage",
    "title": "Non-supervis√©e",
    "section": "Plus proche voisin (Single Linkage)",
    "text": "Plus proche voisin (Single Linkage)\n\nD√©finition \\[d(A, B) = \\min \\{d_{ij}: i \\in A, j \\in B\\}\\] Distance = plus petite distance entre individus des deux groupes\n\n\nAvantages - Bon avec variables de types diff√©rents - Groupes de formes irr√©guli√®res - Robuste aux donn√©es aberrantes - Bonnes propri√©t√©s th√©oriques\n\n\nInconv√©nients - Groupes d√©s√©quilibr√©s (effet ‚Äúcha√Æne‚Äù) - Moins performant pour groupes r√©guliers"
  },
  {
    "objectID": "slides/06-unsupervised-hierarchy-slides.html#voisin-le-plus-distant-complete-linkage",
    "href": "slides/06-unsupervised-hierarchy-slides.html#voisin-le-plus-distant-complete-linkage",
    "title": "Non-supervis√©e",
    "section": "Voisin le plus distant (Complete Linkage)",
    "text": "Voisin le plus distant (Complete Linkage)\n\nD√©finition \\[d(A, B) = \\max \\{d_{ij}: i \\in A, j \\in B\\}\\] Distance = plus grande distance entre individus\n\n\nAvantages - Groupes r√©guliers et homog√®nes en taille - Adapt√© aux variables de types diff√©rents\n\n\nInconv√©nients - Extr√™mement sensible aux donn√©es aberrantes - Force des groupes de m√™me taille"
  },
  {
    "objectID": "slides/06-unsupervised-hierarchy-slides.html#m√©thode-de-la-moyenne-average-linkage",
    "href": "slides/06-unsupervised-hierarchy-slides.html#m√©thode-de-la-moyenne-average-linkage",
    "title": "Non-supervis√©e",
    "section": "M√©thode de la moyenne (Average Linkage)",
    "text": "M√©thode de la moyenne (Average Linkage)\n\nD√©finition \\[d(A, B) = \\frac{1}{n_A n_B} \\sum_{i \\in A} \\sum_{j \\in B} d(X_i, X_j)\\] Distance = moyenne de toutes les distances inter-groupes\n\n\nCaract√©ristiques - Consid√®re toutes les interactions - Produit des groupes homog√®nes (variance faible) - Privil√©gie des groupes de variance similaire"
  },
  {
    "objectID": "slides/06-unsupervised-hierarchy-slides.html#m√©thode-du-centro√Øde",
    "href": "slides/06-unsupervised-hierarchy-slides.html#m√©thode-du-centro√Øde",
    "title": "Non-supervis√©e",
    "section": "M√©thode du centro√Øde",
    "text": "M√©thode du centro√Øde\n\nD√©finition \\[d(A, B) = d(\\overline{X}_A, \\overline{X}_B)\\] Distance entre les moyennes des groupes\n\n\nMise √† jour apr√®s fusion \\[\\overline{X}_{AB} = \\frac{n_A \\overline{X}_A + n_B \\overline{X}_B}{n_A + n_B}\\]\n\n\nPropri√©t√© : Assez robuste aux donn√©es aberrantes, mais peu performant sans elles"
  },
  {
    "objectID": "slides/06-unsupervised-hierarchy-slides.html#m√©thode-de-la-m√©diane",
    "href": "slides/06-unsupervised-hierarchy-slides.html#m√©thode-de-la-m√©diane",
    "title": "Non-supervis√©e",
    "section": "M√©thode de la m√©diane",
    "text": "M√©thode de la m√©diane\n\nPrincipe : Mise √† jour r√©cursive des distances\n\n\nFormule de mise √† jour \\[d(AB, C) = \\frac{d(A, C) + d(B, C)}{2} - \\frac{d(A, B)}{4}\\]\n\n\nCaract√©ristiques - Tr√®s robuste aux donn√©es aberrantes - Peu efficace sans valeurs extr√™mes"
  },
  {
    "objectID": "slides/06-unsupervised-hierarchy-slides.html#m√©thode-de-ward",
    "href": "slides/06-unsupervised-hierarchy-slides.html#m√©thode-de-ward",
    "title": "Non-supervis√©e",
    "section": "M√©thode de Ward",
    "text": "M√©thode de Ward\n\nPrincipe : Minimisation de l‚Äôaugmentation d‚Äôinertie intra-groupe\n\n\nCrit√®re d‚Äôoptimisation \\[I_{AB} = SC_{AB} - SC_A - SC_B = \\frac{d^2(\\overline{X}_A, \\overline{X}_B)}{\\frac{1}{n_A} + \\frac{1}{n_B}}\\]\n\n\nConditions optimales - Observations gaussiennes multivari√©es - M√™me matrice de variance-covariance - Moyennes diff√©rentes"
  },
  {
    "objectID": "slides/06-unsupervised-hierarchy-slides.html#ward-avantages-et-limites",
    "href": "slides/06-unsupervised-hierarchy-slides.html#ward-avantages-et-limites",
    "title": "Non-supervis√©e",
    "section": "Ward : Avantages et limites",
    "text": "Ward : Avantages et limites\n\nAvantages - Tr√®s efficace sous hypoth√®ses gaussiennes - Groupes homog√®nes et de taille comparable - M√©thode tr√®s populaire en pratique\n\n\nInconv√©nients - Sensible aux donn√©es aberrantes - Tend √† former des groupes de m√™me taille - Hypoth√®ses gaussiennes restrictives"
  },
  {
    "objectID": "slides/06-unsupervised-hierarchy-slides.html#m√©thode-flexible",
    "href": "slides/06-unsupervised-hierarchy-slides.html#m√©thode-flexible",
    "title": "Non-supervis√©e",
    "section": "M√©thode flexible",
    "text": "M√©thode flexible\n\nFormule g√©n√©rale \\[d(C, AB) = \\alpha_A d(C, A) + \\alpha_B d(C, B) + \\beta d(A, B) + \\gamma |d(C, A) - d(C, B)|\\]\n\n\nFlexibilit√© Selon \\((\\alpha_A, \\alpha_B, \\beta, \\gamma)\\), on retrouve toutes les m√©thodes pr√©c√©dentes\n\n\nParam√©trage standard - \\(\\alpha_A = \\alpha_B\\), \\(\\alpha_A + \\alpha_B + \\beta = 1\\), \\(\\gamma = 0\\) - Choix courant : \\(\\beta = -0.25\\) - Donn√©es aberrantes : \\(\\beta = -0.5\\)"
  },
  {
    "objectID": "slides/06-unsupervised-hierarchy-slides.html#le-probl√®me-du-choix",
    "href": "slides/06-unsupervised-hierarchy-slides.html#le-probl√®me-du-choix",
    "title": "Non-supervis√©e",
    "section": "Le probl√®me du choix",
    "text": "Le probl√®me du choix\n\nQuestion centrale Parmi les \\(n\\) partitions possibles, laquelle choisir ?\n\n\nCrit√®res pratiques - Interpr√©tabilit√© scientifique ou m√©tier - Pertinence op√©rationnelle - Nombre de groupes \\(K\\) d√©termin√© √† l‚Äôavance\n\n\nCrit√®res statistiques Quand les consid√©rations pratiques ne suffisent pas"
  },
  {
    "objectID": "slides/06-unsupervised-hierarchy-slides.html#outils-disponibles",
    "href": "slides/06-unsupervised-hierarchy-slides.html#outils-disponibles",
    "title": "Non-supervis√©e",
    "section": "Outils disponibles",
    "text": "Outils disponibles\n\nLibrairies sp√©cialis√©es - R NbClust : ~30 crit√®res - Python sklearn : ~10 crit√®res\n\n\nAdaptation Crit√®res surtout adapt√©s aux variables continues"
  },
  {
    "objectID": "slides/06-unsupervised-hierarchy-slides.html#d√©composition-de-linertie",
    "href": "slides/06-unsupervised-hierarchy-slides.html#d√©composition-de-linertie",
    "title": "Non-supervis√©e",
    "section": "D√©composition de l‚Äôinertie",
    "text": "D√©composition de l‚Äôinertie\n\nInertie totale \\[I_{\\text{tot}} = \\frac{1}{n}\\sum_{i=1}^{n} d(X_i, G)\\]\n\n\nD√©composition fondamentale \\[I_{\\text{tot}} = I_{\\text{intra-groupe}} + I_{\\text{inter-groupe}}\\]\n\n\nObjectif : Maximiser \\(I_{\\text{inter-groupe}}\\) (s√©paration des groupes)"
  },
  {
    "objectID": "slides/06-unsupervised-hierarchy-slides.html#inertie-inter-groupe-et-intra-groupe",
    "href": "slides/06-unsupervised-hierarchy-slides.html#inertie-inter-groupe-et-intra-groupe",
    "title": "Non-supervis√©e",
    "section": "Inertie inter-groupe et intra-groupe",
    "text": "Inertie inter-groupe et intra-groupe\n\nInertie inter-groupe (s√©paration) \\[I_{\\text{inter-groupe}} = \\frac{1}{n}\\sum_{k=1}^{K} n_k d(G_k, G)\\]\n\n\nInertie intra-groupe (compacit√©) \\[I_{\\text{intra-groupe}} = \\frac{1}{n}\\sum_{k=1}^{K} \\sum_{i \\in C_k} d(X_i, G_k)\\]\n\n\nPrincipe : Groupes compacts \\(\\Rightarrow\\) faible inertie intra-groupe"
  },
  {
    "objectID": "slides/06-unsupervised-hierarchy-slides.html#crit√®res-d√©riv√©s",
    "href": "slides/06-unsupervised-hierarchy-slides.html#crit√®res-d√©riv√©s",
    "title": "Non-supervis√©e",
    "section": "Crit√®res d√©riv√©s",
    "text": "Crit√®res d√©riv√©s\n\nPseudo-\\(R^2\\) \\[\\text{pseudo-}R^2 = \\frac{I_{\\text{inter-groupe}}}{I_{\\text{tot}}}\\] Proportion d‚Äôinertie expliqu√©e par la partition\n\n\nStatistique Calinski-Harabasz (CH) \\[\\text{CH} = \\frac{I_{\\text{inter-groupe}} / (K-1)}{I_{\\text{intra-groupe}} / (n-K)}\\] √âquilibre entre compacit√© et s√©paration"
  },
  {
    "objectID": "slides/06-unsupervised-hierarchy-slides.html#indice-de-dunn",
    "href": "slides/06-unsupervised-hierarchy-slides.html#indice-de-dunn",
    "title": "Non-supervis√©e",
    "section": "Indice de Dunn",
    "text": "Indice de Dunn\n\nD√©finition \\[D = \\frac{\\text{Distance minimale entre 2 groupes}}{\\text{Distance maximale dans un groupe}}\\]\n\n\nObjectif : Maximiser \\(D\\) - Groupes denses (faible distance interne) - Groupes bien s√©par√©s (grande distance externe)"
  },
  {
    "objectID": "slides/06-unsupervised-hierarchy-slides.html#indice-de-silhouette",
    "href": "slides/06-unsupervised-hierarchy-slides.html#indice-de-silhouette",
    "title": "Non-supervis√©e",
    "section": "Indice de silhouette",
    "text": "Indice de silhouette\n\nPour une observation \\(X_i\\) \\[S(X_i) = \\frac{b_i - a_i}{\\max(b_i, a_i)}\\] o√π : - \\(a_i\\) : distance moyenne aux observations du m√™me groupe - \\(b_i\\) : distance moyenne au groupe le plus proche\n\n\nInterpr√©tation - \\(S(X_i) \\approx 1\\) : Bien class√©e - \\(S(X_i) \\approx 0\\) : √Ä la fronti√®re - \\(S(X_i) \\approx -1\\) : Mal class√©e"
  },
  {
    "objectID": "slides/06-unsupervised-hierarchy-slides.html#utilisation-de-la-silhouette",
    "href": "slides/06-unsupervised-hierarchy-slides.html#utilisation-de-la-silhouette",
    "title": "Non-supervis√©e",
    "section": "Utilisation de la silhouette",
    "text": "Utilisation de la silhouette\n\nSilhouette moyenne Indicateur de coh√©rence globale de la partition\n\n\nAvantage : Intuitive et facilement interpr√©table\n\n\nUsage : Souvent utilis√©e pour choisir le nombre optimal de groupes"
  },
  {
    "objectID": "slides/06-unsupervised-hierarchy-slides.html#recommandations-pratiques",
    "href": "slides/06-unsupervised-hierarchy-slides.html#recommandations-pratiques",
    "title": "Non-supervis√©e",
    "section": "Recommandations pratiques",
    "text": "Recommandations pratiques\n\nCroiser les approches\n\nConnaissance m√©tier\nVisualisation (dendrogramme)\nCrit√®res statistiques multiples\n\nAucun crit√®re n‚Äôest parfait\n\nCompl√©mentarit√© des indicateurs\nD√©cision √©clair√©e et contextualis√©e\n\nValidation\n\nInterpr√©tabilit√© des groupes obtenus\nStabilit√© des r√©sultats"
  },
  {
    "objectID": "slides/06-unsupervised-hierarchy-slides.html#r√©capitulatif",
    "href": "slides/06-unsupervised-hierarchy-slides.html#r√©capitulatif",
    "title": "Non-supervis√©e",
    "section": "R√©capitulatif",
    "text": "R√©capitulatif\n\nClassification hi√©rarchique - Alternative robuste au \\(k\\)-means - Exploration compl√®te de la hi√©rarchie de groupes - Choix de m√©thode de distance crucial\n\n\nM√©thodes populaires - Ward (sous hypoth√®ses gaussiennes) - Average linkage (bon compromis) - Complete linkage (groupes r√©guliers)\n\n\nChoix du nombre de groupes Combinaison de crit√®res pratiques et statistiques"
  },
  {
    "objectID": "slides/06-unsupervised-hierarchy-slides.html#merci-pour-votre-attention",
    "href": "slides/06-unsupervised-hierarchy-slides.html#merci-pour-votre-attention",
    "title": "Non-supervis√©e",
    "section": "Merci pour votre attention",
    "text": "Merci pour votre attention\nQuestions ?\n\nClassification hi√©rarchique : explorer la structure naturelle des donn√©es"
  },
  {
    "objectID": "slides/06-unsupervised-kmeans-slides.html#objectif-de-la-classification-non-supervis√©e",
    "href": "slides/06-unsupervised-kmeans-slides.html#objectif-de-la-classification-non-supervis√©e",
    "title": "Non-supervis√©e",
    "section": "Objectif de la classification non-supervis√©e",
    "text": "Objectif de la classification non-supervis√©e\n\nDonn√©es \\(n\\) observations \\(X_1, \\ldots, X_n\\) d√©crites par \\(p\\) variables num√©riques\n\n\nPr√©paration Variables g√©n√©ralement standardis√©es pour √©viter les effets d‚Äô√©chelle\n\n\nObjectif Regrouper en \\(K\\) groupes tels que : 1. Intra-groupe : Observations similaires 2. Inter-groupe : Groupes distincts"
  },
  {
    "objectID": "slides/06-unsupervised-kmeans-slides.html#fonction-de-classification",
    "href": "slides/06-unsupervised-kmeans-slides.html#fonction-de-classification",
    "title": "Non-supervis√©e",
    "section": "Fonction de classification",
    "text": "Fonction de classification\n\nD√©finition \\[C: \\{1, \\ldots, n\\} \\to \\{1, \\ldots, K\\}\\] Associe √† chaque observation \\(i\\) une √©tiquette de groupe \\(C(i)\\)\n\n\nFonction de co√ªt \\[W(C) = \\sum_{k=1}^{K} \\sum_{i: C(i)=k} \\sum_{j: C(j)=k} d(X_i, X_j)\\] o√π \\(d(X_i, X_j)\\) mesure la dissimilarit√© entre observations\n\n\nObjectif : Minimiser \\(W(C)\\) (coh√©sion intra-groupe)"
  },
  {
    "objectID": "slides/06-unsupervised-kmeans-slides.html#complexit√©-du-probl√®me",
    "href": "slides/06-unsupervised-kmeans-slides.html#complexit√©-du-probl√®me",
    "title": "Non-supervis√©e",
    "section": "Complexit√© du probl√®me",
    "text": "Complexit√© du probl√®me\n\nProbl√®me d‚Äôoptimisation combinatoire - \\(K^n\\) combinaisons possibles - Exploration exhaustive impossible m√™me pour petites valeurs\n\n\nSolution pratique : Algorithmes gloutons - Exploration d‚Äôun sous-ensemble restreint - Am√©lioration it√©rative progressive - Convergence vers un minimum local\n\n\nHypoth√®ses 1. Variables num√©riques, souvent centr√©es-r√©duites 2. Nombre de groupes \\(K\\) fix√© √† l‚Äôavance"
  },
  {
    "objectID": "slides/06-unsupervised-kmeans-slides.html#principe-g√©n√©ral",
    "href": "slides/06-unsupervised-kmeans-slides.html#principe-g√©n√©ral",
    "title": "Non-supervis√©e",
    "section": "Principe g√©n√©ral",
    "text": "Principe g√©n√©ral\n\nObjectif Regrouper \\(n\\) observations en \\(K\\) groupes homog√®nes\n\n\nMesure de dissimilarit√© G√©n√©ralement la distance euclidienne pour donn√©es quantitatives\n\n\nStrat√©gie - Observations proches au sein d‚Äôun groupe - Groupes √©loign√©s entre eux"
  },
  {
    "objectID": "slides/06-unsupervised-kmeans-slides.html#algorithme-d√©taill√©",
    "href": "slides/06-unsupervised-kmeans-slides.html#algorithme-d√©taill√©",
    "title": "Non-supervis√©e",
    "section": "Algorithme d√©taill√©",
    "text": "Algorithme d√©taill√©\n\nChoix de \\(K\\) : Nombre de groupes fix√© √† l‚Äôavance\nInitialisation : Partition al√©atoire ou centres initiaux al√©atoires\nCalcul des centro√Ødes : \\[\\mu_k = \\frac{1}{N_k} \\sum_{i: C(i)=k} X_i\\]\nR√©affectation : Chaque observation au centre le plus proche\nIt√©ration : R√©p√©ter 3-4 jusqu‚Äô√† stabilisation"
  },
  {
    "objectID": "slides/06-unsupervised-kmeans-slides.html#convergence",
    "href": "slides/06-unsupervised-kmeans-slides.html#convergence",
    "title": "Non-supervis√©e",
    "section": "Convergence",
    "text": "Convergence\n\nGarantie de convergence En nombre fini d‚Äôit√©rations\n\n\nM√©canisme Chaque √©tape r√©duit l‚Äôinertie intra-groupe (somme des distances observations ‚ÜîÔ∏é centro√Ødes)\n\n\n‚ö†Ô∏è Limitation importante Convergence vers un minimum local, pas n√©cessairement global"
  },
  {
    "objectID": "slides/06-unsupervised-kmeans-slides.html#limites-des-k-moyennes",
    "href": "slides/06-unsupervised-kmeans-slides.html#limites-des-k-moyennes",
    "title": "Non-supervis√©e",
    "section": "Limites des \\(k\\)-moyennes",
    "text": "Limites des \\(k\\)-moyennes\n\nSensibilit√© √† l‚Äôinitialisation\n\nR√©sultats variables selon centres initiaux\nSolution : Algorithme \\(k\\)-means++\n\nChoix de \\(K\\) √† l‚Äôavance\n\nPas toujours √©vident\nCrit√®res : Silhouette, m√©thode du coude, BIC/AIC\n\nCo√ªt computationnel\n\nRecalcul de toutes les distances √† chaque it√©ration\nProbl√©matique pour grand \\(n\\) ou \\(p\\)"
  },
  {
    "objectID": "slides/06-unsupervised-kmeans-slides.html#limites-suite",
    "href": "slides/06-unsupervised-kmeans-slides.html#limites-suite",
    "title": "Non-supervis√©e",
    "section": "Limites (suite)",
    "text": "Limites (suite)\n\n\nSensibilit√© aux valeurs extr√™mes\n\nLa moyenne influenc√©e par observations atypiques\nCentro√Ødes fauss√©s\nRegroupements incoh√©rents\n\n\n\n\nCons√©quence N√©cessit√© de pr√©traiter les donn√©es (d√©tection d‚Äôoutliers)"
  },
  {
    "objectID": "slides/06-unsupervised-kmeans-slides.html#principe-des-k-m√©do√Ødes",
    "href": "slides/06-unsupervised-kmeans-slides.html#principe-des-k-m√©do√Ødes",
    "title": "Non-supervis√©e",
    "section": "Principe des \\(k\\)-m√©do√Ødes",
    "text": "Principe des \\(k\\)-m√©do√Ødes\n\nDiff√©rence fondamentale Utilise des observations r√©elles comme repr√©sentants (pas des moyennes)\n\n\nM√©do√Øde Observation qui minimise la somme des distances aux autres observations du m√™me groupe\n\n\nAvantage conceptuel Repr√©sentants concrets et interpr√©tables"
  },
  {
    "objectID": "slides/06-unsupervised-kmeans-slides.html#avantages-des-k-m√©do√Ødes",
    "href": "slides/06-unsupervised-kmeans-slides.html#avantages-des-k-m√©do√Ødes",
    "title": "Non-supervis√©e",
    "section": "Avantages des \\(k\\)-m√©do√Ødes",
    "text": "Avantages des \\(k\\)-m√©do√Ødes\n\nRobustesse aux valeurs extr√™mes\n\nM√©do√Ødes moins sensibles que moyennes\nMeilleure r√©sistance aux outliers\n\nCompatibilit√© √©tendue\n\nVariables ordinales ou cat√©gorielles\nMesures de dissimilarit√© personnalis√©es\n\nFlexibilit√©\n\nMatrice de dissimilarit√© sur mesure\nAdapt√©e √† la nature des donn√©es"
  },
  {
    "objectID": "slides/06-unsupervised-kmeans-slides.html#inconv√©nients-des-k-m√©do√Ødes",
    "href": "slides/06-unsupervised-kmeans-slides.html#inconv√©nients-des-k-m√©do√Ødes",
    "title": "Non-supervis√©e",
    "section": "Inconv√©nients des \\(k\\)-m√©do√Ødes",
    "text": "Inconv√©nients des \\(k\\)-m√©do√Ødes\n\nProbl√®mes partag√©s avec \\(k\\)-moyennes - Choix de \\(K\\) √† l‚Äôavance - Convergence vers minimum local\n\n\nCo√ªt computationnel plus √©lev√© - Calculs plus complexes pour identifier m√©do√Ødes - Particuli√®rement avec distances non-euclidiennes - Probl√©matique pour grands jeux de donn√©es"
  },
  {
    "objectID": "slides/06-unsupervised-kmeans-slides.html#comparaison-des-m√©thodes",
    "href": "slides/06-unsupervised-kmeans-slides.html#comparaison-des-m√©thodes",
    "title": "Non-supervis√©e",
    "section": "Comparaison des m√©thodes",
    "text": "Comparaison des m√©thodes\n\n\n\n\nAspect\n\\(k\\)-moyennes\n\\(k\\)-m√©do√Ødes\n\n\n\n\nRepr√©sentants\nCentro√Ødes (moyennes)\nObservations r√©elles\n\n\nRobustesse\nSensible aux outliers\nPlus robuste\n\n\nTypes de variables\nNum√©riques\nNum√©riques + ordinales\n\n\nCo√ªt computationnel\nMod√©r√©\nPlus √©lev√©\n\n\nInterpr√©tabilit√©\nAbstraite\nConcr√®te"
  },
  {
    "objectID": "slides/06-unsupervised-kmeans-slides.html#crit√®res-de-choix-de-k",
    "href": "slides/06-unsupervised-kmeans-slides.html#crit√®res-de-choix-de-k",
    "title": "Non-supervis√©e",
    "section": "Crit√®res de choix de \\(K\\)",
    "text": "Crit√®res de choix de \\(K\\)\n\nM√©thode du coude (Elbow method)\n\nGraphique inertie vs \\(K\\)\nRecherche du ‚Äúcoude‚Äù\n\nCoefficient de silhouette\n\nMesure qualit√© des groupes\nValeurs entre -1 et 1\n\nCrit√®res d‚Äôinformation\n\nBIC, AIC en cadre probabiliste\nP√©nalisent la complexit√©"
  },
  {
    "objectID": "slides/06-unsupervised-kmeans-slides.html#points-cl√©s-√†-retenir",
    "href": "slides/06-unsupervised-kmeans-slides.html#points-cl√©s-√†-retenir",
    "title": "Non-supervis√©e",
    "section": "Points cl√©s √† retenir",
    "text": "Points cl√©s √† retenir\n\nClassification non-supervis√©e : D√©couverte de structure sans √©tiquettes\n\\(k\\)-moyennes : M√©thode classique, efficace mais limitations\n\\(k\\)-m√©do√Ødes : Alternative robuste, co√ªt plus √©lev√©\nChoix de \\(K\\) : Crucial, plusieurs crit√®res disponibles\nInitialisation : Importante pour √©viter minima locaux\nPr√©traitement : Standardisation et gestion des outliers essentielles"
  },
  {
    "objectID": "slides/06-unsupervised-kmeans-slides.html#merci-pour-votre-attention",
    "href": "slides/06-unsupervised-kmeans-slides.html#merci-pour-votre-attention",
    "title": "Non-supervis√©e",
    "section": "Merci pour votre attention",
    "text": "Merci pour votre attention\nQuestions ?\n\nClassification non-supervis√©e : d√©couvrir la structure cach√©e des donn√©es"
  },
  {
    "objectID": "slides/03-generalities-distance-slides.html#pourquoi-les-distances",
    "href": "slides/03-generalities-distance-slides.html#pourquoi-les-distances",
    "title": "G√©n√©ralit√©s",
    "section": "Pourquoi les distances ?",
    "text": "Pourquoi les distances ?\n\nDans tout projet d‚Äôanalyse de donn√©es, il faut pouvoir quantifier la ressemblance ou la dissemblance entre observations.\n\nImpact direct sur :\n\nAlgorithmes d‚Äôapprentissage\nM√©thodes de regroupement\nTechniques de visualisation"
  },
  {
    "objectID": "slides/03-generalities-distance-slides.html#plan",
    "href": "slides/03-generalities-distance-slides.html#plan",
    "title": "G√©n√©ralit√©s",
    "section": "Plan",
    "text": "Plan\n\n\nNotion de distance - D√©finition math√©matique\nDistances num√©riques - \\(L_q\\), euclidienne, Manhattan\nNotion de similarit√© - Concept oppos√©\nVariables qualitatives - Encodages et distances adapt√©es\nDistance de Jaccard - Cas des variables binaires"
  },
  {
    "objectID": "slides/03-generalities-distance-slides.html#d√©finition-math√©matique",
    "href": "slides/03-generalities-distance-slides.html#d√©finition-math√©matique",
    "title": "G√©n√©ralit√©s",
    "section": "D√©finition math√©matique",
    "text": "D√©finition math√©matique\n\n\n\nDistance sur \\(\\mathcal{X}\\)\n\n\nUne fonction \\(d: \\mathcal{X} \\times \\mathcal{X} \\to \\mathbb{R}\\) est une distance si :\n\nNon-n√©gativit√© : \\(d(x, y) \\geq 0\\)\nS√©paration : \\(d(x, y) = 0 \\Leftrightarrow x = y\\)\nSym√©trie : \\(d(x, y) = d(y, x)\\)\nIn√©galit√© triangulaire : \\(d(x, y) \\leq d(x, z) + d(y, z)\\)"
  },
  {
    "objectID": "slides/03-generalities-distance-slides.html#distance-euclidienne",
    "href": "slides/03-generalities-distance-slides.html#distance-euclidienne",
    "title": "G√©n√©ralit√©s",
    "section": "Distance euclidienne",
    "text": "Distance euclidienne\n\n\n\nD√©finition\n\n\nPour \\(x, y \\in \\mathbb{R}^p\\) :\n\\[d(x, y) = \\left\\| x - y \\right\\|_2 = \\sqrt{\\sum_{i=1}^{p} (x_i - y_i)^2}\\]\nQuand l‚Äôutiliser : Variables num√©riques de m√™me ordre de grandeur."
  },
  {
    "objectID": "slides/03-generalities-distance-slides.html#distance-euclidienne-preuve",
    "href": "slides/03-generalities-distance-slides.html#distance-euclidienne-preuve",
    "title": "G√©n√©ralit√©s",
    "section": "Distance euclidienne (preuve)",
    "text": "Distance euclidienne (preuve)"
  },
  {
    "objectID": "slides/03-generalities-distance-slides.html#distance-l_q-minkowski",
    "href": "slides/03-generalities-distance-slides.html#distance-l_q-minkowski",
    "title": "G√©n√©ralit√©s",
    "section": "Distance \\(L_q\\) (Minkowski)",
    "text": "Distance \\(L_q\\) (Minkowski)\n\n\n\nFormule g√©n√©rale :\n\n\n\\[d(x, y) = \\left( \\sum_{i=1}^{p} |x_i - y_i|^q \\right)^{1/q}\\]\nCas particuliers :\n\n\\(q = 1\\) : Distance de Manhattan ‚Üí \\(d(x, y) = \\sum_{i=1}^{p} |x_i - y_i|\\)\n\\(q = 2\\) : Distance euclidienne"
  },
  {
    "objectID": "slides/03-generalities-distance-slides.html#exemple-concret-wooclap-kffpdj",
    "href": "slides/03-generalities-distance-slides.html#exemple-concret-wooclap-kffpdj",
    "title": "G√©n√©ralit√©s",
    "section": "Exemple concret (WOOCLAP: KFFPDJ)",
    "text": "Exemple concret (WOOCLAP: KFFPDJ)\n\n\n\nNom\nTaille (cm)\nPoids (kg)\n\n\n\n\nAlice\n165\n70\n\n\nBob\n177\n75"
  },
  {
    "objectID": "slides/03-generalities-distance-slides.html#probl√®me-d√©chelle",
    "href": "slides/03-generalities-distance-slides.html#probl√®me-d√©chelle",
    "title": "G√©n√©ralit√©s",
    "section": "Probl√®me d‚Äô√©chelle",
    "text": "Probl√®me d‚Äô√©chelle\n\n\n\nAttention !\n\n\nLes distances \\(L_q\\) ne sont pas invariantes aux changements d‚Äô√©chelle."
  },
  {
    "objectID": "slides/03-generalities-distance-slides.html#probl√®me-d√©chelle---solution",
    "href": "slides/03-generalities-distance-slides.html#probl√®me-d√©chelle---solution",
    "title": "G√©n√©ralit√©s",
    "section": "Probl√®me d‚Äô√©chelle - solution",
    "text": "Probl√®me d‚Äô√©chelle - solution\n\nStandardiser les variables !\n\nDistance euclidienne standardis√©e :\n\\[d(x, y) = \\sum_{i=1}^{p} \\left( \\frac{x_i - y_i}{\\sigma_i} \\right)^2\\]\no√π \\(\\sigma_i\\) = √©cart-type de la variable \\(i\\)."
  },
  {
    "objectID": "slides/03-generalities-distance-slides.html#notion-de-similarit√©",
    "href": "slides/03-generalities-distance-slides.html#notion-de-similarit√©",
    "title": "G√©n√©ralit√©s",
    "section": "Notion de similarit√©",
    "text": "Notion de similarit√©\n\n\n\n\nMesure de similarit√© sur \\(\\mathcal{X}\\)\n\n\nUne fonction \\(s: \\mathcal{X} \\times \\mathcal{X} \\to \\mathbb{R}\\) telle que :\n\n\\(s(x, y) \\geq 0\\)\n\\(s(x, y) = s(y, x)\\) (sym√©trie)\n\\(s(x, x) = 1 \\geq s(x, y)\\) (maximum pour soi-m√™me)"
  },
  {
    "objectID": "slides/03-generalities-distance-slides.html#conversion-distance-similarit√©",
    "href": "slides/03-generalities-distance-slides.html#conversion-distance-similarit√©",
    "title": "G√©n√©ralit√©s",
    "section": "Conversion distance ‚ÜîÔ∏é similarit√©",
    "text": "Conversion distance ‚ÜîÔ∏é similarit√©\n\nDistance ‚Üí Similarit√© :\n\\[s(x, y) = \\frac{1}{1 + d(x, y)}\\]\n\n‚ö†Ô∏è Une mesure de similarit√© ne respecte g√©n√©ralement pas l‚Äôin√©galit√© triangulaire.\n\n\nSimilarit√© ‚Üí Dissemblance :\n\\[d^*(x, y) = 1 - s(x, y)\\]"
  },
  {
    "objectID": "slides/03-generalities-distance-slides.html#variables-qualitatives",
    "href": "slides/03-generalities-distance-slides.html#variables-qualitatives",
    "title": "G√©n√©ralit√©s",
    "section": "Variables qualitatives",
    "text": "Variables qualitatives\n\nProbl√®me : pour \\(\\mathcal{X} = \\{\\text{Rouge}, \\text{Vert}, \\text{Bleu}\\}\\).\n\n\n‚ùå Mauvaise pratique :\nRouge = 1, Vert = 2, Bleu = 3 ‚Üí Introduit un ordre artificiel !\n\n\n\n‚úÖ Bonne pratique : Encodage adapt√©"
  },
  {
    "objectID": "slides/03-generalities-distance-slides.html#encodage-1-parmi-k-one-hot-encoding",
    "href": "slides/03-generalities-distance-slides.html#encodage-1-parmi-k-one-hot-encoding",
    "title": "G√©n√©ralit√©s",
    "section": "Encodage ‚Äú1 parmi K‚Äù (One-hot encoding)",
    "text": "Encodage ‚Äú1 parmi K‚Äù (One-hot encoding)\n\n\nRouge ‚Üí \\((1, 0, 0)\\)\nVert ‚Üí \\((0, 1, 0)\\)\nBleu ‚Üí \\((0, 0, 1)\\)\n\n\nAvantages : Pas d‚Äôordre artificiel\nInconv√©nients : Grande dimension si beaucoup de modalit√©s"
  },
  {
    "objectID": "slides/03-generalities-distance-slides.html#distance-discr√®te",
    "href": "slides/03-generalities-distance-slides.html#distance-discr√®te",
    "title": "G√©n√©ralit√©s",
    "section": "Distance discr√®te",
    "text": "Distance discr√®te\n\n\n\nDistance de Hamming\n\n\nPour des vecteurs :\n\\[d(x, y) = \\sum_{i=1}^{p} \\mathbb{1}(x_i \\neq y_i)\\]"
  },
  {
    "objectID": "slides/03-generalities-distance-slides.html#exemple-distance-discr√®te-wooclap-kffpdj",
    "href": "slides/03-generalities-distance-slides.html#exemple-distance-discr√®te-wooclap-kffpdj",
    "title": "G√©n√©ralit√©s",
    "section": "Exemple : Distance discr√®te (WOOCLAP: KFFPDJ)",
    "text": "Exemple : Distance discr√®te (WOOCLAP: KFFPDJ)\n\n\n\nNom\nCouleur\nYeux\nCheveux\n\n\n\n\nAlice\nRouge\nVert\nBlond\n\n\nBob\nVert\nBleu\nRoux\n\n\nChris\nRouge\nVert\nBlond"
  },
  {
    "objectID": "slides/03-generalities-distance-slides.html#similarit√©-par-accords",
    "href": "slides/03-generalities-distance-slides.html#similarit√©-par-accords",
    "title": "G√©n√©ralit√©s",
    "section": "Similarit√© par accords",
    "text": "Similarit√© par accords\n\nCompter les accords normalis√©s :\n\\[s(x, y) = \\frac{1}{p}\\sum_{i=1}^{p} \\mathbb{1}(x_i = y_i)\\]\n\nExemple pr√©c√©dent :\n\n\\(s(\\text{Alice}, \\text{Bob}) = 0\\)\n\\(s(\\text{Alice}, \\text{Chris}) = 1\\)\n\\(s(\\text{Bob}, \\text{Chris}) = 0\\)"
  },
  {
    "objectID": "slides/03-generalities-distance-slides.html#probl√®me-des-variables-binaires",
    "href": "slides/03-generalities-distance-slides.html#probl√®me-des-variables-binaires",
    "title": "G√©n√©ralit√©s",
    "section": "Probl√®me des variables binaires",
    "text": "Probl√®me des variables binaires\n\nAvec l‚Äôencodage ‚Äú1 parmi K‚Äù :\n\nBeaucoup de variables binaires\nBeaucoup de 0, peu de 1\nDistance discr√®te peu informative\n\n\nSolution : Se concentrer sur les attributs √† 1 ‚Üí Distance de Jaccard"
  },
  {
    "objectID": "slides/03-generalities-distance-slides.html#indice-de-jaccard-d√©finition",
    "href": "slides/03-generalities-distance-slides.html#indice-de-jaccard-d√©finition",
    "title": "G√©n√©ralit√©s",
    "section": "Indice de Jaccard : D√©finition",
    "text": "Indice de Jaccard : D√©finition\n\nPour deux observations de \\(K\\) variables binaires :\n\n\\(M_{11}\\) : nombre de variables √† 1 pour x et y\n\\(M_{10}\\) : nombre de variables √† 1 pour x, 0 pour y\n\\(M_{01}\\) : nombre de variables √† 0 pour x, 1 pour y\n\\(M_{00}\\) : nombre de variables √† 0 pour x et y\n\n\n\\[J(x, y) = \\frac{M_{11}}{M_{10} + M_{01} + M_{11}} = \\frac{M_{11}}{K - M_{00}}\\]"
  },
  {
    "objectID": "slides/03-generalities-distance-slides.html#distance-de-jaccard",
    "href": "slides/03-generalities-distance-slides.html#distance-de-jaccard",
    "title": "G√©n√©ralit√©s",
    "section": "Distance de Jaccard",
    "text": "Distance de Jaccard\n\n\n\n\nDistance de Jaccard :\n\n\n\\[d(x, y) = 1 - J(x, y) = \\frac{M_{10} + M_{01}}{M_{01} + M_{10} + M_{11}}\\]\n\n\n\nIntuition : Proportion de d√©saccords parmi les variables ‚Äúactives‚Äù."
  },
  {
    "objectID": "slides/03-generalities-distance-slides.html#exemple-distance-de-jaccard",
    "href": "slides/03-generalities-distance-slides.html#exemple-distance-de-jaccard",
    "title": "G√©n√©ralit√©s",
    "section": "Exemple : Distance de Jaccard",
    "text": "Exemple : Distance de Jaccard\n\n\n\nNom\nQ1\nQ2\nQ3\nQ4\nQ5\n\n\n\n\nAlice\n1\n0\n1\n0\n1\n\n\nBob\n1\n0\n0\n1\n0"
  },
  {
    "objectID": "slides/03-generalities-distance-slides.html#r√©capitulatif-des-distances",
    "href": "slides/03-generalities-distance-slides.html#r√©capitulatif-des-distances",
    "title": "G√©n√©ralit√©s",
    "section": "R√©capitulatif des distances",
    "text": "R√©capitulatif des distances\n\n\n\n\nType de donn√©es\nDistance utilisable\n\n\n\n\nNum√©riques\nEuclidienne (standardis√©e)\n\n\nQualitatives\nDiscr√®te (Hamming)\n\n\nBinaires nombreuses\nJaccard\n\n\nMixtes\nCombinaison adapt√©e"
  },
  {
    "objectID": "slides/03-generalities-distance-slides.html#choix-pratique-de-la-distance",
    "href": "slides/03-generalities-distance-slides.html#choix-pratique-de-la-distance",
    "title": "G√©n√©ralit√©s",
    "section": "Choix pratique de la distance",
    "text": "Choix pratique de la distance\n\nQuestions √† se poser :\n\nQuel type de variables ? (num√©riques/qualitatives)\nM√™me √©chelle ou standardisation n√©cessaire ?\nBeaucoup de variables binaires ?\nLes double-absences sont-elles informatives ?\n\n\n‚Üí Le choix de distance influence directement les r√©sultats !"
  },
  {
    "objectID": "slides/03-generalities-distance-slides.html#conclusion",
    "href": "slides/03-generalities-distance-slides.html#conclusion",
    "title": "G√©n√©ralit√©s",
    "section": "Conclusion",
    "text": "Conclusion\n\n\nDistance ‚Üí mesure de dissemblance (4 propri√©t√©s math√©matiques)\nSimilarit√© ‚Üí mesure de ressemblance (concept oppos√©)\nVariables num√©riques ‚Üí Distance euclidienne (standardis√©e)\nVariables qualitatives ‚Üí Encodage + distance adapt√©e\nVariables binaires ‚Üí Distance de Jaccard\n\n\nProchaine √©tape ‚Üí Compromis biais/variance"
  },
  {
    "objectID": "slides/05-supervised-discriminant-slides.html#introduction",
    "href": "slides/05-supervised-discriminant-slides.html#introduction",
    "title": "Supervis√©e",
    "section": "Introduction",
    "text": "Introduction\n\nObjectif: Classer des individus dans diff√©rents groupes\nM√©thode supervis√©e: Les groupes sont connus dans les donn√©es d‚Äôapprentissage\nVariables: Variables explicatives continues\nBut: Apprendre une r√®gle de classification optimale"
  },
  {
    "objectID": "slides/05-supervised-discriminant-slides.html#notation",
    "href": "slides/05-supervised-discriminant-slides.html#notation",
    "title": "Supervis√©e",
    "section": "Notation",
    "text": "Notation\n\nDonn√©es - \\(X = (X_{ij}) \\in \\mathbb{R}^{n \\times p}\\) : matrice de donn√©es - \\(n\\) : nombre d‚Äôindividus - \\(p\\) : nombre de variables\n\n\nGroupes - \\(K\\) groupes dans la population - \\(I_k\\) : ensemble des individus du groupe \\(k\\) - \\(n_k\\) : nombre d‚Äôobservations dans \\(I_k\\) - \\(\\sum_{k=1}^{K} n_k = n\\)"
  },
  {
    "objectID": "slides/05-supervised-discriminant-slides.html#objectif-principal",
    "href": "slides/05-supervised-discriminant-slides.html#objectif-principal",
    "title": "Supervis√©e",
    "section": "Objectif principal",
    "text": "Objectif principal\n\nPrincipe - Partitionner \\(\\mathbb{R}^p\\) en \\(K\\) sous-ensembles - Passer de \\(\\mathbb{R}^p\\) √† \\(\\mathbb{R}\\) via un score\n\n\nScore de Fisher \\[f(X_1, \\ldots, X_p) = a^{\\top} X + b = a_1 X_1 + \\cdots + a_p X_p + b\\]\n\n\nSimplification En centrant les variables: \\(-b = a^{\\top} \\overline{X}\\)\nIl reste √† d√©terminer le vecteur \\(a = (a_1, \\ldots, a_p)\\)"
  },
  {
    "objectID": "slides/05-supervised-discriminant-slides.html#crit√®re-de-fisher",
    "href": "slides/05-supervised-discriminant-slides.html#crit√®re-de-fisher",
    "title": "Supervis√©e",
    "section": "Crit√®re de Fisher",
    "text": "Crit√®re de Fisher\n\nId√©e centrale Optimiser le rapport entre: - Variabilit√© inter-groupes (maximiser) - Variabilit√© intra-groupe (minimiser)\n\n\nMatrices de variance-covariance\n\n\\(S\\) : matrice totale\n\\(W\\) : matrice intra-groupe\n\n\\(B\\) : matrice inter-groupe\n\nRelation fondamentale: \\(S = W + B\\)"
  },
  {
    "objectID": "slides/05-supervised-discriminant-slides.html#matrices-de-variance-covariance",
    "href": "slides/05-supervised-discriminant-slides.html#matrices-de-variance-covariance",
    "title": "Supervis√©e",
    "section": "Matrices de variance-covariance",
    "text": "Matrices de variance-covariance\nMatrice totale \\[S = \\sum_{i=1}^{n} (X_i - \\overline{X})(X_i - \\overline{X})^{\\top}\\]\nMatrice intra-groupe \\[W = \\sum_{k=1}^{K} \\sum_{i \\in I_k} (X_i - \\overline{X}_k)(X_i - \\overline{X}_k)^{\\top}\\]\nMatrice inter-groupe \\[B = \\sum_{k=1}^{K} n_k (\\overline{X}_k - \\overline{X})(\\overline{X}_k - \\overline{X})^{\\top}\\]"
  },
  {
    "objectID": "slides/05-supervised-discriminant-slides.html#crit√®re-doptimisation",
    "href": "slides/05-supervised-discriminant-slides.html#crit√®re-doptimisation",
    "title": "Supervis√©e",
    "section": "Crit√®re d‚Äôoptimisation",
    "text": "Crit√®re d‚Äôoptimisation\n\nCrit√®re de Fisher \\[J(a) = \\frac{a^{\\top} B a}{a^{\\top} W a} = \\frac{a^{\\top} B a}{a^{\\top} S a}\\]\n\n\nFormulations √©quivalentes 1. Maximiser \\(J(a)\\) sous contrainte \\(a^{\\top} a = 1\\) 2. Maximiser \\(a^{\\top} B a\\) sous contrainte \\(a^{\\top} S a = 1\\) 3. Maximiser \\(c^{\\top} S^{-1/2} B S^{-1/2} c\\) sous contrainte \\(c^{\\top} c = 1\\)"
  },
  {
    "objectID": "slides/05-supervised-discriminant-slides.html#solution-optimale",
    "href": "slides/05-supervised-discriminant-slides.html#solution-optimale",
    "title": "Supervis√©e",
    "section": "Solution optimale",
    "text": "Solution optimale\n\nR√©sultat \\(a\\) est un vecteur propre norm√© associ√© √† \\(\\lambda_1\\), la plus grande valeur propre de \\(S^{-1} B\\)\n\n\nFonction discriminante \\[f(x) = a^{\\top} (x - \\overline{X})\\]\n\n\nScores des observations \\[U_i = a^{\\top} (X_i - \\overline{X})\\] Ces scores maximisent le rapport variance inter-groupe / variance intra-groupe"
  },
  {
    "objectID": "slides/05-supervised-discriminant-slides.html#pouvoir-discriminant",
    "href": "slides/05-supervised-discriminant-slides.html#pouvoir-discriminant",
    "title": "Supervis√©e",
    "section": "Pouvoir discriminant",
    "text": "Pouvoir discriminant\n\nInterpr√©tation de \\(\\lambda_1\\) \\[\\lambda_1 = \\frac{a^{\\top} B a}{a^{\\top} S a} \\quad \\text{avec } 0 \\leq \\lambda_1 \\leq 1\\]\n\n\n\\(\\lambda_1 = 1\\) : Cas id√©al\n\n100% de variabilit√© entre les groupes\nAucune variabilit√© intra-groupe\n\n\\(\\lambda_1 = 0\\) : Analyse inutile\n\nAucune variabilit√© entre les groupes\nToute la variabilit√© est intra-groupe"
  },
  {
    "objectID": "slides/05-supervised-discriminant-slides.html#r√®gle-de-classification",
    "href": "slides/05-supervised-discriminant-slides.html#r√®gle-de-classification",
    "title": "Supervis√©e",
    "section": "R√®gle de classification",
    "text": "R√®gle de classification\n\nScore moyen par groupe \\[m_k = a^{\\top} (\\overline{X}_{k1}, \\ldots, \\overline{X}_{kp})^{\\top}\\]\n\n\nClassification d‚Äôune nouvelle observation \\(X_0\\) 1. Calculer le score: \\(f(X_0) = a^{\\top} X_0\\) 2. Assigner au groupe \\(k^*\\) tel que: \\[k^* = \\arg\\min_{k} |a^{\\top} X_0 - m_k|\\]"
  },
  {
    "objectID": "slides/05-supervised-discriminant-slides.html#cas-binaire-k-2",
    "href": "slides/05-supervised-discriminant-slides.html#cas-binaire-k-2",
    "title": "Supervis√©e",
    "section": "Cas binaire (\\(K = 2\\))",
    "text": "Cas binaire (\\(K = 2\\))\n\nSolution explicite \\[C = \\sqrt{\\frac{n_1 n_2}{n}} (\\overline{X}_1 - \\overline{X}_2)\\] \\[B = CC^{\\top}, \\quad a = S^{-1}C\\]\n\n\nR√®gle de classification Si \\(m_1 &gt; m_2\\), classer \\(X_0\\) dans le groupe 1 si: \\[a^{\\top} X_0 &gt; \\frac{m_1 + m_2}{2}\\]\n\n\nForme √©quivalente \\[(\\overline{X}_1 - \\overline{X}_2)^{\\top} S^{-1} X_0 &gt; \\frac{1}{2}(\\overline{X}_1 - \\overline{X}_2)^{\\top} S^{-1} (\\overline{X}_1 + \\overline{X}_2)\\]"
  },
  {
    "objectID": "slides/05-supervised-discriminant-slides.html#√©valuation-de-la-performance",
    "href": "slides/05-supervised-discriminant-slides.html#√©valuation-de-la-performance",
    "title": "Supervis√©e",
    "section": "√âvaluation de la performance",
    "text": "√âvaluation de la performance\n\nMatrice de confusion - Appliquer la r√®gle de classification √† l‚Äô√©chantillon d‚Äôapprentissage - Comparer les classifications pr√©dites aux vraies classes - Estimer les risques de mauvaise classification\n\n\nAvantages de l‚Äôanalyse discriminante - M√©thode lin√©aire simple et interpr√©table - Solution analytique explicite - R√©duction de dimension naturelle (\\(\\mathbb{R}^p \\to \\mathbb{R}\\))"
  },
  {
    "objectID": "slides/05-supervised-discriminant-slides.html#merci-pour-votre-attention",
    "href": "slides/05-supervised-discriminant-slides.html#merci-pour-votre-attention",
    "title": "Supervis√©e",
    "section": "Merci pour votre attention",
    "text": "Merci pour votre attention\nQuestions ?\n\nL‚Äôanalyse discriminante de Fisher : une m√©thode √©l√©gante pour la classification supervis√©e"
  },
  {
    "objectID": "slides/04-dimension-ca-slides.html#introduction-√†-lafc",
    "href": "slides/04-dimension-ca-slides.html#introduction-√†-lafc",
    "title": "Dimension",
    "section": "Introduction √† l‚ÄôAFC",
    "text": "Introduction √† l‚ÄôAFC\nL‚ÄôAnalyse Factorielle des Correspondances (AFC) est une m√©thode d‚Äôanalyse exploratoire pour repr√©senter graphiquement les relations entre les modalit√©s de deux variables qualitatives.\nObjectifs : - Repr√©senter simultan√©ment les profils-lignes et profils-colonnes - Pr√©server la distance du \\(\\chi^2\\) - Trouver une repr√©sentation en faible dimension (2D-3D) - R√©v√©ler les proximit√©s g√©om√©triques = similarit√©s entre modalit√©s"
  },
  {
    "objectID": "slides/04-dimension-ca-slides.html#afc-comme-double-acp",
    "href": "slides/04-dimension-ca-slides.html#afc-comme-double-acp",
    "title": "Dimension",
    "section": "AFC comme double ACP",
    "text": "AFC comme double ACP\n\n\n\n\n\n\nAstuce\n\n\nL‚ÄôAFC peut √™tre vue comme une double ACP pond√©r√©e : - ACP des profils-lignes dans \\(\\mathbb{R}^p\\) - ACP des profils-colonnes dans \\(\\mathbb{R}^n\\) - Avec m√©triques du \\(\\chi^2\\) adapt√©es √† chaque espace\n\n\n\nCette dualit√© permet une repr√©sentation coh√©rente des lignes et colonnes dans le m√™me espace r√©duit."
  },
  {
    "objectID": "slides/04-dimension-ca-slides.html#notation-de-base",
    "href": "slides/04-dimension-ca-slides.html#notation-de-base",
    "title": "Dimension",
    "section": "Notation de base",
    "text": "Notation de base\nTableau de contingence : \\(K = (k_{ij})\\) avec \\(k_{ij}\\) = effectif classe \\(i\\), cat√©gorie \\(j\\)\nTableau de fr√©quences relatives : \\(F = (f_{ij})\\) o√π \\[f_{ij} = \\frac{k_{ij}}{k_{\\bullet\\bullet}}\\]\nMarges : - Ligne : \\(f_{i\\bullet} = \\sum_{j=1}^p f_{ij}\\) - Colonne : \\(f_{\\bullet j} = \\sum_{i=1}^n f_{ij}\\) - Total : \\(f_{\\bullet\\bullet} = 1\\)"
  },
  {
    "objectID": "slides/04-dimension-ca-slides.html#exemple-concret",
    "href": "slides/04-dimension-ca-slides.html#exemple-concret",
    "title": "Dimension",
    "section": "Exemple concret",
    "text": "Exemple concret\n√âtudiants STT-2200 (Automne 2025) : Majeure √ó Type d‚Äôadmission\n\n\n\nMajeure\nColl√®ge\nULaval\nAutre univ.\nHors QC\nTotal\n\n\n\n\nActuariat\n0.08\n0\n0\n0.04\n0.12\n\n\nStatistique\n0.08\n0.16\n0.04\n0\n0.28\n\n\nBio-info\n0.16\n0.08\n0\n0.08\n0.32\n\n\nFinance\n0.08\n0\n0\n0\n0.08\n\n\nTotal\n0.52\n0.28\n0.04\n0.16\n1\n\n\n\n25 √©tudiants au total, convertis en fr√©quences relatives."
  },
  {
    "objectID": "slides/04-dimension-ca-slides.html#ind√©pendance-statistique",
    "href": "slides/04-dimension-ca-slides.html#ind√©pendance-statistique",
    "title": "Dimension",
    "section": "Ind√©pendance statistique",
    "text": "Ind√©pendance statistique\nTest d‚Äôind√©pendance : Si les variables sont ind√©pendantes, \\[f_{ij} \\approx f_{i\\bullet} f_{\\bullet j}\\]\nTest du \\(\\chi^2\\) : \\[T = \\sum_{i,j} \\frac{(k_{ij} - \\mathbb{E}(k_{ij}))^2}{\\mathbb{E}(k_{ij})} = \\sum_{i,j} \\frac{(k_{ij} - \\frac{k_{i\\bullet}k_{\\bullet j}}{k_{\\bullet\\bullet}})^2}{\\frac{k_{i\\bullet}k_{\\bullet j}}{k_{\\bullet\\bullet}}}\\]\nSi \\(T \\approx 0\\) ‚Üí ind√©pendance\nSi \\(T\\) grand ‚Üí d√©pendance (int√©ressant pour l‚ÄôAFC)"
  },
  {
    "objectID": "slides/04-dimension-ca-slides.html#profils-lignes-et-profils-colonnes",
    "href": "slides/04-dimension-ca-slides.html#profils-lignes-et-profils-colonnes",
    "title": "Dimension",
    "section": "Profils-lignes et profils-colonnes",
    "text": "Profils-lignes et profils-colonnes\nProfil-ligne \\(i\\) : R√©partition de la modalit√© \\(i\\) parmi les colonnes \\[L_i = \\left(\\frac{f_{i1}}{f_{i\\bullet}}, \\ldots, \\frac{f_{ip}}{f_{i\\bullet}}\\right)\\]\nProfil-colonne \\(j\\) : R√©partition de la modalit√© \\(j\\) parmi les lignes \\[C_j = \\left(\\frac{f_{1j}}{f_{\\bullet j}}, \\ldots, \\frac{f_{nj}}{f_{\\bullet j}}\\right)\\]\nLes profils sont des distributions conditionnelles."
  },
  {
    "objectID": "slides/04-dimension-ca-slides.html#profils-moyens",
    "href": "slides/04-dimension-ca-slides.html#profils-moyens",
    "title": "Dimension",
    "section": "Profils moyens",
    "text": "Profils moyens\nProfil-ligne moyen = fr√©quences marginales colonnes : \\[\\text{Profil moyen} = (f_{\\bullet 1}, \\ldots, f_{\\bullet p})\\]\nProfil-colonne moyen = fr√©quences marginales lignes : \\[\\text{Profil moyen} = (f_{1\\bullet}, \\ldots, f_{n\\bullet})\\]\nInd√©pendance ‚ü∫ Tous les profils = profils moyens"
  },
  {
    "objectID": "slides/04-dimension-ca-slides.html#distance-du-chi2",
    "href": "slides/04-dimension-ca-slides.html#distance-du-chi2",
    "title": "Dimension",
    "section": "Distance du \\(\\chi^2\\)",
    "text": "Distance du \\(\\chi^2\\)\nDistance entre profils-lignes : \\[d^2(L_i, L_{i'}) = \\sum_{j=1}^p \\frac{1}{f_{\\bullet j}} \\left(\\frac{f_{ij}}{f_{i\\bullet}} - \\frac{f_{i'j}}{f_{i'\\bullet}}\\right)^2\\]\nDistance entre profils-colonnes : \\[d^2(C_j, C_{j'}) = \\sum_{i=1}^n \\frac{1}{f_{i\\bullet}} \\left(\\frac{f_{ij}}{f_{\\bullet j}} - \\frac{f_{ij'}}{f_{\\bullet j'}}\\right)^2\\]\nPond√©ration par l‚Äôinverse des fr√©quences marginales."
  },
  {
    "objectID": "slides/04-dimension-ca-slides.html#formulation-matricielle",
    "href": "slides/04-dimension-ca-slides.html#formulation-matricielle",
    "title": "Dimension",
    "section": "Formulation matricielle",
    "text": "Formulation matricielle\nMatrices de poids : - \\(D_n = \\text{diag}(f_{i\\bullet})\\) (poids des lignes) - \\(D_p = \\text{diag}(f_{\\bullet j})\\) (poids des colonnes)\nDistance du \\(\\chi^2\\) matricielle : - Profils-lignes : \\(d^2(L_i, L_{i'}) = (L_i - L_{i'})^\\top D_p^{-1} (L_i - L_{i'})\\) - Profils-colonnes : \\(d^2(C_j, C_{j'}) = (C_j - C_{j'})^\\top D_n^{-1} (C_j - C_{j'})\\)"
  },
  {
    "objectID": "slides/04-dimension-ca-slides.html#analyse-directe-profils-lignes",
    "href": "slides/04-dimension-ca-slides.html#analyse-directe-profils-lignes",
    "title": "Dimension",
    "section": "Analyse directe (profils-lignes)",
    "text": "Analyse directe (profils-lignes)\nObjectif : Maximiser la variance projet√©e des profils-lignes\nProbl√®me d‚Äôoptimisation : \\[\\max_u u^\\top D_p^{-1} F^\\top D_n^{-1} F D_p^{-1} u \\quad \\text{s.c.} \\quad u^\\top D_p^{-1} u = 1\\]\nSolution : Vecteurs propres de la matrice \\[S = F^\\top D_n^{-1} F D_p^{-1}\\]\nPremier axe factoriel : \\(S u_1 = \\lambda_1 u_1\\)"
  },
  {
    "objectID": "slides/04-dimension-ca-slides.html#analyse-duale-profils-colonnes",
    "href": "slides/04-dimension-ca-slides.html#analyse-duale-profils-colonnes",
    "title": "Dimension",
    "section": "Analyse duale (profils-colonnes)",
    "text": "Analyse duale (profils-colonnes)\nObjectif : Maximiser la variance projet√©e des profils-colonnes\nProbl√®me d‚Äôoptimisation similaire avec matrice : \\[T = F D_p^{-1} F^\\top D_n^{-1}\\]\nPremier axe factoriel : \\(T v_1 = \\mu_1 v_1\\)\nPropri√©t√© fondamentale : \\(S\\) et \\(T\\) ont les m√™mes \\(r = \\min(n-1, p-1)\\) valeurs propres positives !"
  },
  {
    "objectID": "slides/04-dimension-ca-slides.html#relations-entre-analyses",
    "href": "slides/04-dimension-ca-slides.html#relations-entre-analyses",
    "title": "Dimension",
    "section": "Relations entre analyses",
    "text": "Relations entre analyses\nCoh√©rence des repr√©sentations : \\(\\lambda_k = \\mu_k\\) pour \\(k = 1, \\ldots, r\\)\nRelations entre vecteurs propres : \\[u_k = \\frac{1}{\\sqrt{\\lambda_k}} F^\\top D_n^{-1} v_k\\] \\[v_k = \\frac{1}{\\sqrt{\\lambda_k}} F D_p^{-1} u_k\\]\nCette propri√©t√© garantit une repr√©sentation coh√©rente dans le m√™me espace r√©duit."
  },
  {
    "objectID": "slides/04-dimension-ca-slides.html#coordonn√©es-factorielles",
    "href": "slides/04-dimension-ca-slides.html#coordonn√©es-factorielles",
    "title": "Dimension",
    "section": "Coordonn√©es factorielles",
    "text": "Coordonn√©es factorielles\nCoordonn√©es des profils-lignes sur l‚Äôaxe \\(k\\) : \\[\\Phi_k = D_n^{-1} F D_p^{-1} u_k\\]\nCoordonn√©es des profils-colonnes sur l‚Äôaxe \\(k\\) : \\[\\Psi_k = D_p^{-1} F^\\top D_n^{-1} v_k\\]\nRelations entre coordonn√©es : \\[\\Phi_k = \\frac{1}{\\sqrt{\\lambda_k}} D_n^{-1} F \\Psi_k\\]"
  },
  {
    "objectID": "slides/04-dimension-ca-slides.html#centrage-et-centre-de-gravit√©",
    "href": "slides/04-dimension-ca-slides.html#centrage-et-centre-de-gravit√©",
    "title": "Dimension",
    "section": "Centrage et centre de gravit√©",
    "text": "Centrage et centre de gravit√©\nConvention : Repr√©sentation centr√©e en \\((0,0)\\) dans les logiciels\nCentres de gravit√© : - Lignes : \\(G_L = (f_{\\bullet 1}, \\ldots, f_{\\bullet p})^\\top\\) - Colonnes : \\(G_C = (f_{1\\bullet}, \\ldots, f_{n\\bullet})^\\top\\)\nCentrage : Soustraire les profils moyens \\[\\frac{f_{ij}}{f_{i\\bullet}} - f_{\\bullet j} = \\frac{f_{ij} - f_{i\\bullet}f_{\\bullet j}}{f_{i\\bullet}}\\]"
  },
  {
    "objectID": "slides/04-dimension-ca-slides.html#inertie-totale",
    "href": "slides/04-dimension-ca-slides.html#inertie-totale",
    "title": "Dimension",
    "section": "Inertie totale",
    "text": "Inertie totale\nMatrice centr√©e : \\(S^* = (s_{jj'}^*)\\) avec \\[s_{jj'}^* = \\sum_{i=1}^n \\frac{(f_{ij} - f_{i\\bullet}f_{\\bullet j})(f_{ij'} - f_{i\\bullet}f_{\\bullet j'})}{f_{i\\bullet}f_{\\bullet j'}}\\]\nInertie totale : \\[\\text{tr}(S^*) = \\sum_{j=1}^p \\sum_{i=1}^n \\frac{(f_{ij} - f_{i\\bullet}f_{\\bullet j})^2}{f_{i\\bullet}f_{\\bullet j}}\\]\nCette inertie correspond √† la statistique du \\(\\chi^2\\) normalis√©e."
  },
  {
    "objectID": "slides/04-dimension-ca-slides.html#repr√©sentation-barycentrique",
    "href": "slides/04-dimension-ca-slides.html#repr√©sentation-barycentrique",
    "title": "Dimension",
    "section": "Repr√©sentation barycentrique",
    "text": "Repr√©sentation barycentrique\nPropri√©t√© remarquable : Sur chaque axe factoriel,\n\\[[\\Phi_k]_i = \\frac{1}{\\sqrt{\\lambda_k}} \\sum_{j=1}^p \\frac{f_{ij}}{f_{i\\bullet}} [\\Psi_k]_j\\]\n\\[[\\Psi_k]_j = \\frac{1}{\\sqrt{\\lambda_k}} \\sum_{i=1}^n \\frac{f_{ij}}{f_{\\bullet j}} [\\Phi_k]_i\\]\nInterpr√©tation : Chaque profil-ligne est au barycentre des profils-colonnes (et vice versa)."
  },
  {
    "objectID": "slides/04-dimension-ca-slides.html#double-repr√©sentation-barycentrique",
    "href": "slides/04-dimension-ca-slides.html#double-repr√©sentation-barycentrique",
    "title": "Dimension",
    "section": "Double repr√©sentation barycentrique",
    "text": "Double repr√©sentation barycentrique\n\n\n\n\n\n\nAstuce\n\n\nPropri√©t√© fondamentale de l‚ÄôAFC :\nSur les axes factoriels, chaque point d‚Äôun nuage est au barycentre des points de l‚Äôautre nuage, pond√©r√© par les contributions respectives.\n\n\n\nCette propri√©t√© permet une interpr√©tation conjointe des modalit√©s des deux variables dans le m√™me graphique."
  },
  {
    "objectID": "slides/04-dimension-ca-slides.html#applications-pratiques",
    "href": "slides/04-dimension-ca-slides.html#applications-pratiques",
    "title": "Dimension",
    "section": "Applications pratiques",
    "text": "Applications pratiques\nDomaines d‚Äôapplication : - Marketing : Pr√©f√©rences √ó Segments de client√®le - Sociologie : Cat√©gories sociales √ó Comportements\n- Biologie : Esp√®ces √ó Environnements - Linguistique : Mots √ó Contextes\nAvantages : - Visualisation intuitive des associations - Identification des profils similaires - D√©tection de structures cach√©es"
  },
  {
    "objectID": "slides/04-dimension-ca-slides.html#interpr√©tation-des-r√©sultats",
    "href": "slides/04-dimension-ca-slides.html#interpr√©tation-des-r√©sultats",
    "title": "Dimension",
    "section": "Interpr√©tation des r√©sultats",
    "text": "Interpr√©tation des r√©sultats\nProximit√© g√©om√©trique = Similarit√© des profils\nAxes factoriels : Directions de variation maximale\nContribution des modalit√©s : Importance dans la construction des axes\nInertie expliqu√©e : \\(\\frac{\\lambda_k}{\\sum_{j=1}^r \\lambda_j} \\times 100\\%\\)\nQualit√© de repr√©sentation : Proportion de l‚Äôinertie d‚Äôun point expliqu√©e par les axes retenus."
  },
  {
    "objectID": "slides/04-dimension-ca-slides.html#conclusion",
    "href": "slides/04-dimension-ca-slides.html#conclusion",
    "title": "Dimension",
    "section": "Conclusion",
    "text": "Conclusion\nL‚ÄôAFC est un outil puissant pour :\n\nExplorer les relations entre variables qualitatives\nVisualiser des structures complexes en 2D/3D\n\nIdentifier des associations non √©videntes\nInterpr√©ter conjointement lignes et colonnes\n\nLa dualit√© ligne-colonne et la repr√©sentation barycentrique en font une m√©thode unique pour l‚Äôanalyse de tableaux de contingence."
  },
  {
    "objectID": "slides/03-generalities-distance-slides-annotated.html#pourquoi-les-distances",
    "href": "slides/03-generalities-distance-slides-annotated.html#pourquoi-les-distances",
    "title": "G√©n√©ralit√©s",
    "section": "Pourquoi les distances ?",
    "text": "Pourquoi les distances ?\n\nDans tout projet d‚Äôanalyse de donn√©es, il faut pouvoir quantifier la ressemblance ou la dissemblance entre observations.\n\nImpact direct sur :\n\nAlgorithmes d‚Äôapprentissage\nM√©thodes de regroupement\nTechniques de visualisation"
  },
  {
    "objectID": "slides/03-generalities-distance-slides-annotated.html#plan",
    "href": "slides/03-generalities-distance-slides-annotated.html#plan",
    "title": "G√©n√©ralit√©s",
    "section": "Plan",
    "text": "Plan\n\n\nNotion de distance - D√©finition math√©matique\nDistances num√©riques - \\(L_q\\), euclidienne, Manhattan\nNotion de similarit√© - Concept oppos√©\nVariables qualitatives - Encodages et distances adapt√©es\nDistance de Jaccard - Cas des variables binaires"
  },
  {
    "objectID": "slides/03-generalities-distance-slides-annotated.html#d√©finition-math√©matique",
    "href": "slides/03-generalities-distance-slides-annotated.html#d√©finition-math√©matique",
    "title": "G√©n√©ralit√©s",
    "section": "D√©finition math√©matique",
    "text": "D√©finition math√©matique\n\n\n\nDistance sur \\(\\mathcal{X}\\)\n\n\nUne fonction \\(d: \\mathcal{X} \\times \\mathcal{X} \\to \\mathbb{R}\\) est une distance si :\n\nNon-n√©gativit√© : \\(d(x, y) \\geq 0\\)\nS√©paration : \\(d(x, y) = 0 \\Leftrightarrow x = y\\)\nSym√©trie : \\(d(x, y) = d(y, x)\\)\nIn√©galit√© triangulaire : \\(d(x, y) \\leq d(x, z) + d(y, z)\\)"
  },
  {
    "objectID": "slides/03-generalities-distance-slides-annotated.html#distance-euclidienne",
    "href": "slides/03-generalities-distance-slides-annotated.html#distance-euclidienne",
    "title": "G√©n√©ralit√©s",
    "section": "Distance euclidienne",
    "text": "Distance euclidienne\n\n\n\nD√©finition\n\n\nPour \\(x, y \\in \\mathbb{R}^p\\) :\n\\[d(x, y) = \\left\\| x - y \\right\\|_2 = \\sqrt{\\sum_{i=1}^{p} (x_i - y_i)^2}\\]\nQuand l‚Äôutiliser : Variables num√©riques de m√™me ordre de grandeur."
  },
  {
    "objectID": "slides/03-generalities-distance-slides-annotated.html#distance-euclidienne-preuve",
    "href": "slides/03-generalities-distance-slides-annotated.html#distance-euclidienne-preuve",
    "title": "G√©n√©ralit√©s",
    "section": "Distance euclidienne (preuve)",
    "text": "Distance euclidienne (preuve)"
  },
  {
    "objectID": "slides/03-generalities-distance-slides-annotated.html#distance-l_q-minkowski",
    "href": "slides/03-generalities-distance-slides-annotated.html#distance-l_q-minkowski",
    "title": "G√©n√©ralit√©s",
    "section": "Distance \\(L_q\\) (Minkowski)",
    "text": "Distance \\(L_q\\) (Minkowski)\n\n\n\nFormule g√©n√©rale :\n\n\n\\[d(x, y) = \\left( \\sum_{i=1}^{p} |x_i - y_i|^q \\right)^{1/q}\\]\nCas particuliers :\n\n\\(q = 1\\) : Distance de Manhattan ‚Üí \\(d(x, y) = \\sum_{i=1}^{p} |x_i - y_i|\\)\n\\(q = 2\\) : Distance euclidienne"
  },
  {
    "objectID": "slides/03-generalities-distance-slides-annotated.html#exemple-concret-wooclap-kffpdj",
    "href": "slides/03-generalities-distance-slides-annotated.html#exemple-concret-wooclap-kffpdj",
    "title": "G√©n√©ralit√©s",
    "section": "Exemple concret (WOOCLAP: KFFPDJ)",
    "text": "Exemple concret (WOOCLAP: KFFPDJ)\n\n\n\nNom\nTaille (cm)\nPoids (kg)\n\n\n\n\nAlice\n165\n70\n\n\nBob\n177\n75"
  },
  {
    "objectID": "slides/03-generalities-distance-slides-annotated.html#probl√®me-d√©chelle",
    "href": "slides/03-generalities-distance-slides-annotated.html#probl√®me-d√©chelle",
    "title": "G√©n√©ralit√©s",
    "section": "Probl√®me d‚Äô√©chelle",
    "text": "Probl√®me d‚Äô√©chelle\n\n\n\nAttention !\n\n\nLes distances \\(L_q\\) ne sont pas invariantes aux changements d‚Äô√©chelle."
  },
  {
    "objectID": "slides/03-generalities-distance-slides-annotated.html#probl√®me-d√©chelle---solution",
    "href": "slides/03-generalities-distance-slides-annotated.html#probl√®me-d√©chelle---solution",
    "title": "G√©n√©ralit√©s",
    "section": "Probl√®me d‚Äô√©chelle - solution",
    "text": "Probl√®me d‚Äô√©chelle - solution\n\nStandardiser les variables !\n\nDistance euclidienne standardis√©e :\n\\[d(x, y) = \\sum_{i=1}^{p} \\left( \\frac{x_i - y_i}{\\sigma_i} \\right)^2\\]\no√π \\(\\sigma_i\\) = √©cart-type de la variable \\(i\\)."
  },
  {
    "objectID": "slides/03-generalities-distance-slides-annotated.html#notion-de-similarit√©",
    "href": "slides/03-generalities-distance-slides-annotated.html#notion-de-similarit√©",
    "title": "G√©n√©ralit√©s",
    "section": "Notion de similarit√©",
    "text": "Notion de similarit√©\n\n\n\n\nMesure de similarit√© sur \\(\\mathcal{X}\\)\n\n\nUne fonction \\(s: \\mathcal{X} \\times \\mathcal{X} \\to \\mathbb{R}\\) telle que :\n\n\\(s(x, y) \\geq 0\\)\n\\(s(x, y) = s(y, x)\\) (sym√©trie)\n\\(s(x, x) = 1 \\geq s(x, y)\\) (maximum pour soi-m√™me)"
  },
  {
    "objectID": "slides/03-generalities-distance-slides-annotated.html#conversion-distance-similarit√©",
    "href": "slides/03-generalities-distance-slides-annotated.html#conversion-distance-similarit√©",
    "title": "G√©n√©ralit√©s",
    "section": "Conversion distance ‚ÜîÔ∏é similarit√©",
    "text": "Conversion distance ‚ÜîÔ∏é similarit√©\n\nDistance ‚Üí Similarit√© :\n\\[s(x, y) = \\frac{1}{1 + d(x, y)}\\]\n\n‚ö†Ô∏è Une mesure de similarit√© ne respecte g√©n√©ralement pas l‚Äôin√©galit√© triangulaire.\n\n\nSimilarit√© ‚Üí Dissemblance :\n\\[d^*(x, y) = 1 - s(x, y)\\]"
  },
  {
    "objectID": "slides/03-generalities-distance-slides-annotated.html#variables-qualitatives",
    "href": "slides/03-generalities-distance-slides-annotated.html#variables-qualitatives",
    "title": "G√©n√©ralit√©s",
    "section": "Variables qualitatives",
    "text": "Variables qualitatives\n\nProbl√®me : pour \\(\\mathcal{X} = \\{\\text{Rouge}, \\text{Vert}, \\text{Bleu}\\}\\).\n\n\n‚ùå Mauvaise pratique :\nRouge = 1, Vert = 2, Bleu = 3 ‚Üí Introduit un ordre artificiel !\n\n\n\n‚úÖ Bonne pratique : Encodage adapt√©"
  },
  {
    "objectID": "slides/03-generalities-distance-slides-annotated.html#encodage-1-parmi-k-one-hot-encoding",
    "href": "slides/03-generalities-distance-slides-annotated.html#encodage-1-parmi-k-one-hot-encoding",
    "title": "G√©n√©ralit√©s",
    "section": "Encodage ‚Äú1 parmi K‚Äù (One-hot encoding)",
    "text": "Encodage ‚Äú1 parmi K‚Äù (One-hot encoding)\n\n\nRouge ‚Üí \\((1, 0, 0)\\)\nVert ‚Üí \\((0, 1, 0)\\)\nBleu ‚Üí \\((0, 0, 1)\\)\n\n\nAvantages : Pas d‚Äôordre artificiel\nInconv√©nients : Grande dimension si beaucoup de modalit√©s"
  },
  {
    "objectID": "slides/03-generalities-distance-slides-annotated.html#distance-discr√®te",
    "href": "slides/03-generalities-distance-slides-annotated.html#distance-discr√®te",
    "title": "G√©n√©ralit√©s",
    "section": "Distance discr√®te",
    "text": "Distance discr√®te\n\n\n\nDistance de Hamming\n\n\nPour des vecteurs :\n\\[d(x, y) = \\sum_{i=1}^{p} \\mathbb{1}(x_i \\neq y_i)\\]"
  },
  {
    "objectID": "slides/03-generalities-distance-slides-annotated.html#exemple-distance-discr√®te-wooclap-kffpdj",
    "href": "slides/03-generalities-distance-slides-annotated.html#exemple-distance-discr√®te-wooclap-kffpdj",
    "title": "G√©n√©ralit√©s",
    "section": "Exemple : Distance discr√®te (WOOCLAP: KFFPDJ)",
    "text": "Exemple : Distance discr√®te (WOOCLAP: KFFPDJ)\n\n\n\nNom\nCouleur\nYeux\nCheveux\n\n\n\n\nAlice\nRouge\nVert\nBlond\n\n\nBob\nVert\nBleu\nRoux\n\n\nChris\nRouge\nVert\nBlond"
  },
  {
    "objectID": "slides/03-generalities-distance-slides-annotated.html#similarit√©-par-accords",
    "href": "slides/03-generalities-distance-slides-annotated.html#similarit√©-par-accords",
    "title": "G√©n√©ralit√©s",
    "section": "Similarit√© par accords",
    "text": "Similarit√© par accords\n\nCompter les accords normalis√©s :\n\\[s(x, y) = \\frac{1}{p}\\sum_{i=1}^{p} \\mathbb{1}(x_i = y_i)\\]\n\nExemple pr√©c√©dent :\n\n\\(s(\\text{Alice}, \\text{Bob}) = 0\\)\n\\(s(\\text{Alice}, \\text{Chris}) = 1\\)\n\\(s(\\text{Bob}, \\text{Chris}) = 0\\)"
  },
  {
    "objectID": "slides/03-generalities-distance-slides-annotated.html#probl√®me-des-variables-binaires",
    "href": "slides/03-generalities-distance-slides-annotated.html#probl√®me-des-variables-binaires",
    "title": "G√©n√©ralit√©s",
    "section": "Probl√®me des variables binaires",
    "text": "Probl√®me des variables binaires\n\nAvec l‚Äôencodage ‚Äú1 parmi K‚Äù :\n\nBeaucoup de variables binaires\nBeaucoup de 0, peu de 1\nDistance discr√®te peu informative\n\n\nSolution : Se concentrer sur les attributs √† 1 ‚Üí Distance de Jaccard"
  },
  {
    "objectID": "slides/03-generalities-distance-slides-annotated.html#indice-de-jaccard-d√©finition",
    "href": "slides/03-generalities-distance-slides-annotated.html#indice-de-jaccard-d√©finition",
    "title": "G√©n√©ralit√©s",
    "section": "Indice de Jaccard : D√©finition",
    "text": "Indice de Jaccard : D√©finition\n\nPour deux observations de \\(K\\) variables binaires :\n\n\\(M_{11}\\) : nombre de variables √† 1 pour x et y\n\\(M_{10}\\) : nombre de variables √† 1 pour x, 0 pour y\n\\(M_{01}\\) : nombre de variables √† 0 pour x, 1 pour y\n\\(M_{00}\\) : nombre de variables √† 0 pour x et y\n\n\n\\[J(x, y) = \\frac{M_{11}}{M_{10} + M_{01} + M_{11}} = \\frac{M_{11}}{K - M_{00}}\\]"
  },
  {
    "objectID": "slides/03-generalities-distance-slides-annotated.html#distance-de-jaccard",
    "href": "slides/03-generalities-distance-slides-annotated.html#distance-de-jaccard",
    "title": "G√©n√©ralit√©s",
    "section": "Distance de Jaccard",
    "text": "Distance de Jaccard\n\n\n\n\nDistance de Jaccard :\n\n\n\\[d(x, y) = 1 - J(x, y) = \\frac{M_{10} + M_{01}}{M_{01} + M_{10} + M_{11}}\\]\n\n\n\nIntuition : Proportion de d√©saccords parmi les variables ‚Äúactives‚Äù."
  },
  {
    "objectID": "slides/03-generalities-distance-slides-annotated.html#exemple-distance-de-jaccard",
    "href": "slides/03-generalities-distance-slides-annotated.html#exemple-distance-de-jaccard",
    "title": "G√©n√©ralit√©s",
    "section": "Exemple : Distance de Jaccard",
    "text": "Exemple : Distance de Jaccard\n\n\n\nNom\nQ1\nQ2\nQ3\nQ4\nQ5\n\n\n\n\nAlice\n1\n0\n1\n0\n1\n\n\nBob\n1\n0\n0\n1\n0"
  },
  {
    "objectID": "slides/03-generalities-distance-slides-annotated.html#r√©capitulatif-des-distances",
    "href": "slides/03-generalities-distance-slides-annotated.html#r√©capitulatif-des-distances",
    "title": "G√©n√©ralit√©s",
    "section": "R√©capitulatif des distances",
    "text": "R√©capitulatif des distances\n\n\n\n\nType de donn√©es\nDistance utilisable\n\n\n\n\nNum√©riques\nEuclidienne (standardis√©e)\n\n\nQualitatives\nDiscr√®te (Hamming)\n\n\nBinaires nombreuses\nJaccard\n\n\nMixtes\nCombinaison adapt√©e"
  },
  {
    "objectID": "slides/03-generalities-distance-slides-annotated.html#choix-pratique-de-la-distance",
    "href": "slides/03-generalities-distance-slides-annotated.html#choix-pratique-de-la-distance",
    "title": "G√©n√©ralit√©s",
    "section": "Choix pratique de la distance",
    "text": "Choix pratique de la distance\n\nQuestions √† se poser :\n\nQuel type de variables ? (num√©riques/qualitatives)\nM√™me √©chelle ou standardisation n√©cessaire ?\nBeaucoup de variables binaires ?\nLes double-absences sont-elles informatives ?\n\n\n‚Üí Le choix de distance influence directement les r√©sultats !"
  },
  {
    "objectID": "slides/03-generalities-distance-slides-annotated.html#conclusion",
    "href": "slides/03-generalities-distance-slides-annotated.html#conclusion",
    "title": "G√©n√©ralit√©s",
    "section": "Conclusion",
    "text": "Conclusion\n\n\nDistance ‚Üí mesure de dissemblance (4 propri√©t√©s math√©matiques)\nSimilarit√© ‚Üí mesure de ressemblance (concept oppos√©)\nVariables num√©riques ‚Üí Distance euclidienne (standardis√©e)\nVariables qualitatives ‚Üí Encodage + distance adapt√©e\nVariables binaires ‚Üí Distance de Jaccard\n\n\nProchaine √©tape ‚Üí Compromis biais/variance"
  },
  {
    "objectID": "slides/02-revisions-proba-slides-annotated.html#plan-de-la-pr√©sentation",
    "href": "slides/02-revisions-proba-slides-annotated.html#plan-de-la-pr√©sentation",
    "title": "R√©visions",
    "section": "Plan de la pr√©sentation",
    "text": "Plan de la pr√©sentation\n\n\nMod√©liser le hasard\nVariables al√©atoires\nVecteurs al√©atoires\nEstimation statistique"
  },
  {
    "objectID": "slides/02-revisions-proba-slides-annotated.html#espace-d√©v√©nements-et-√©v√©nements",
    "href": "slides/02-revisions-proba-slides-annotated.html#espace-d√©v√©nements-et-√©v√©nements",
    "title": "R√©visions",
    "section": "Espace d‚Äô√©v√©nements et √©v√©nements",
    "text": "Espace d‚Äô√©v√©nements et √©v√©nements\n\n\n\n\nD√©finitions\n\n\n\nEspace d‚Äô√©v√©nements \\(S\\) : ensemble de tous les r√©sultats possibles d‚Äôune exp√©rience.\n√âv√©nement : sous-ensemble de \\(S\\)."
  },
  {
    "objectID": "slides/02-revisions-proba-slides-annotated.html#espace-d√©v√©nements-et-√©v√©nements-exemples",
    "href": "slides/02-revisions-proba-slides-annotated.html#espace-d√©v√©nements-et-√©v√©nements-exemples",
    "title": "R√©visions",
    "section": "Espace d‚Äô√©v√©nements et √©v√©nements : exemples",
    "text": "Espace d‚Äô√©v√©nements et √©v√©nements : exemples\n\nExemple 1 : Lancer de pi√®ce\n\n\\(S = \\{0, 1\\}\\) (pile = 0, face = 1).\n√âv√©nement \\(E = \\{1\\}\\) : ‚Äúobtenir face‚Äù.\n\n\nExemple 2 : Dur√©e de vie d‚Äôun t√©l√©phone\n\n\\(S = \\mathbb{R}_+ = [0, +\\infty)\\) (en ann√©es).\n√âv√©nement \\(E = [10, \\infty)\\) : ‚Äúdur√©e de vie &gt; 10 ans‚Äù."
  },
  {
    "objectID": "slides/02-revisions-proba-slides-annotated.html#mesure-de-probabilit√©",
    "href": "slides/02-revisions-proba-slides-annotated.html#mesure-de-probabilit√©",
    "title": "R√©visions",
    "section": "Mesure de probabilit√©",
    "text": "Mesure de probabilit√©\n\n\n\nD√©finition\n\n\nUne mesure de probabilit√© \\(\\mathbb{P}\\) satisfait :\n\n\\(\\mathbb{P}(E) \\in [0, 1]\\) pour tout √©v√©nement \\(E\\);\n\\(\\mathbb{P}(S) = 1\\) (certitude totale);\nPour des √©v√©nements disjoints : \\(\\mathbb{P}(E_1 \\cup E_2 \\cup \\cdots) = \\mathbb{P}(E_1) + \\mathbb{P}(E_2) + \\cdots\\)."
  },
  {
    "objectID": "slides/02-revisions-proba-slides-annotated.html#mesure-de-probabilit√©-exemple-wooclap-kffpdj",
    "href": "slides/02-revisions-proba-slides-annotated.html#mesure-de-probabilit√©-exemple-wooclap-kffpdj",
    "title": "R√©visions",
    "section": "Mesure de probabilit√© : exemple (Wooclap: KFFPDJ)",
    "text": "Mesure de probabilit√© : exemple (Wooclap: KFFPDJ)\n\nExemple : Lancer d‚Äôun d√© √©quilibr√©\n\nEspace d‚Äô√©v√©nements : \\(S = \\{1, 2, 3, 4, 5, 6\\}\\).\n\\(\\mathbb{P}(\\{i\\}) = 1/6\\) pour \\(i = 1, \\ldots, 6\\).\n\\(\\mathbb{P}(\\{2, 4, 6\\}) = 1/6 + 1/6 + 1/6 = 1/2\\) (nombre pair)."
  },
  {
    "objectID": "slides/02-revisions-proba-slides-annotated.html#subjectivit√©-de-la-probabilit√©",
    "href": "slides/02-revisions-proba-slides-annotated.html#subjectivit√©-de-la-probabilit√©",
    "title": "R√©visions",
    "section": "Subjectivit√© de la probabilit√©",
    "text": "Subjectivit√© de la probabilit√©\n\nContexte : Nombre de jours sans neige √† Qu√©bec dans l‚Äôann√©e\n\nEspace d‚Äô√©v√©nements : \\(S = \\{0, 1, 2, \\ldots, 365\\}\\).\n√âv√©nement \\(E_1 = [0, 100]\\) : ‚Äúmoins de 100 jours sans neige‚Äù.\n√âv√©nement \\(E_2 = [100, 200]\\) : ‚Äúentre 100 et 200 jours sans neige‚Äù.\n\nPerspective subjective :\n\nNouvel arrivant : probabilit√©s uniformes sur \\(S\\).\nQu√©b√©cois : probabilit√©s plus √©lev√©es pour \\(E_1\\) que \\(E_2\\)."
  },
  {
    "objectID": "slides/02-revisions-proba-slides-annotated.html#ind√©pendance-et-probabilit√©-conditionnelle",
    "href": "slides/02-revisions-proba-slides-annotated.html#ind√©pendance-et-probabilit√©-conditionnelle",
    "title": "R√©visions",
    "section": "Ind√©pendance et probabilit√© conditionnelle",
    "text": "Ind√©pendance et probabilit√© conditionnelle\n\n\n\nD√©finitions\n\n\nInd√©pendance : Les √©v√©nements \\(E\\) et \\(F\\) sont ind√©pendants si \\[\\mathbb{P}(E \\cap F) = \\mathbb{P}(E) \\times \\mathbb{P}(F).\\] Probabilit√© conditionnelle : La probabilit√© de \\(E\\) sachant \\(F\\) est \\[\\mathbb{P}(E \\mid F) = \\frac{\\mathbb{P}(E \\cap F)}{\\mathbb{P}(F)}.\\]"
  },
  {
    "objectID": "slides/02-revisions-proba-slides-annotated.html#exemple-cartes-√†-jouer",
    "href": "slides/02-revisions-proba-slides-annotated.html#exemple-cartes-√†-jouer",
    "title": "R√©visions",
    "section": "Exemple : Cartes √† jouer",
    "text": "Exemple : Cartes √† jouer\n\nContexte : Tirage d‚Äôune carte dans un jeu standard (52 cartes)\n\n\\(E\\) : ‚Äútirer un roi‚Äù ‚Üí \\(\\mathbb{P}(E) = 4/52 = 1/13\\).\n\\(F\\) : ‚Äútirer un c≈ìur‚Äù ‚Üí \\(\\mathbb{P}(F) = 13/52 = 1/4\\).\n\nTest d‚Äôind√©pendance :\n\n\\(E \\cap F\\) : ‚Äútirer le roi de c≈ìur‚Äù ‚Üí \\(\\mathbb{P}(E \\cap F) = 1/52\\).\n\\(\\mathbb{P}(E) \\times \\mathbb{P}(F) = (1/13) \\times (1/4) = 1/52\\).\n\nConclusion : \\(E\\) et \\(F\\) sont ind√©pendants !"
  },
  {
    "objectID": "slides/02-revisions-proba-slides-annotated.html#variables-discr√®tes",
    "href": "slides/02-revisions-proba-slides-annotated.html#variables-discr√®tes",
    "title": "R√©visions",
    "section": "Variables discr√®tes",
    "text": "Variables discr√®tes\n\n\n\n\nD√©finition\n\n\nUne variable al√©atoire discr√®te prend au plus un nombre d√©nombrable de valeurs.\nSa distribution donn√©e par \\(\\mathbb{P}(X = x)\\) pour chaque valeur \\(x\\)."
  },
  {
    "objectID": "slides/02-revisions-proba-slides-annotated.html#variables-discr√®tes-exemple",
    "href": "slides/02-revisions-proba-slides-annotated.html#variables-discr√®tes-exemple",
    "title": "R√©visions",
    "section": "Variables discr√®tes : exemple",
    "text": "Variables discr√®tes : exemple\n\nExemple : Lancer de deux d√©s\n\nVariable al√©atoire \\(X\\) = somme des deux d√©s.\nEspace d‚Äô√©v√©nements : \\(S = \\{2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12\\}\\).\n\\(\\mathbb{P}(X = 7) = 6/36 = 1/6\\) (combinaisons : 1+6, 2+5, 3+4, 4+3, 5+2, 6+1).\n\\(\\mathbb{P}(X = 2) = 1/36\\) (une seule combinaison : 1+1)."
  },
  {
    "objectID": "slides/02-revisions-proba-slides-annotated.html#variables-continues",
    "href": "slides/02-revisions-proba-slides-annotated.html#variables-continues",
    "title": "R√©visions",
    "section": "Variables continues",
    "text": "Variables continues\n\n\n\nD√©finition\n\n\nLes probabilit√©s d‚Äôune variable al√©atoire continue sont donn√©es par les int√©grales \\[\\mathbb{P}(X \\in A) = \\int_A f(x) dx\\] o√π \\(f(x) \\geq 0\\) et \\(\\int_{-\\infty}^{\\infty} f(x) dx = 1\\).\nImportant : \\(\\mathbb{P}(X = x) = 0\\) pour tout \\(x\\) fix√© !"
  },
  {
    "objectID": "slides/02-revisions-proba-slides-annotated.html#variables-continues-exemple",
    "href": "slides/02-revisions-proba-slides-annotated.html#variables-continues-exemple",
    "title": "R√©visions",
    "section": "Variables continues : exemple",
    "text": "Variables continues : exemple\nVariable uniforme sur \\([0, 1]\\) : \\[f(x) = \\begin{cases} 1 & \\text{si } x \\in [0, 1] \\\\ 0 & \\text{sinon} \\end{cases}.\\]\nCalculs de probabilit√©s :\n\n\\(\\mathbb{P}(X \\leq 0.5) = \\int_0^{0.5} 1 \\, dx = 0.5\\).\n\\(\\mathbb{P}(0.2 &lt; X &lt; 0.7) = \\int_{0.2}^{0.7} 1 \\, dx = 0.5\\).\n\\(\\mathbb{P}(X = 0.3) = 0\\) (point isol√©)."
  },
  {
    "objectID": "slides/02-revisions-proba-slides-annotated.html#esp√©rance-math√©matique",
    "href": "slides/02-revisions-proba-slides-annotated.html#esp√©rance-math√©matique",
    "title": "R√©visions",
    "section": "Esp√©rance math√©matique",
    "text": "Esp√©rance math√©matique\n\n\n\nD√©finition\n\n\nL‚Äôesp√©rance \\(\\mathbb{E}(X)\\) est la valeur moyenne de \\(X\\).\n\nCas discret : \\(\\mathbb{E}(X) = \\sum_x x \\mathbb{P}(X = x)\\).\nCas continu : \\(\\mathbb{E}(X) = \\int_{-\\infty}^{\\infty} x  f(x) dx\\)."
  },
  {
    "objectID": "slides/02-revisions-proba-slides-annotated.html#esp√©rance-math√©matique-exemple-wooclap-kffpdj",
    "href": "slides/02-revisions-proba-slides-annotated.html#esp√©rance-math√©matique-exemple-wooclap-kffpdj",
    "title": "R√©visions",
    "section": "Esp√©rance math√©matique : exemple (Wooclap: KFFPDJ)",
    "text": "Esp√©rance math√©matique : exemple (Wooclap: KFFPDJ)\nExemple 1 : D√© √©quilibr√©\n\n\\[\\mathbb{E}(X) = 1 \\cdot \\frac{1}{6} + 2 \\cdot \\frac{1}{6} + \\cdots + 6 \\cdot \\frac{1}{6} = \\frac{21}{6} = 3.5.\\]\n\n\nExemple 2 : Loi uniforme sur \\([0, 1]\\)\n\n\n\\[\\mathbb{E}(X) = \\int_0^1 x \\cdot 1 \\, dx = \\left[\\frac{x^2}{2}\\right]_0^1 = \\frac{1}{2}.\\]"
  },
  {
    "objectID": "slides/02-revisions-proba-slides-annotated.html#th√©or√®me-de-transfert",
    "href": "slides/02-revisions-proba-slides-annotated.html#th√©or√®me-de-transfert",
    "title": "R√©visions",
    "section": "Th√©or√®me de transfert",
    "text": "Th√©or√®me de transfert\n\n\n\nTh√©or√®me\n\n\nPour une fonction \\(g\\), l‚Äôesp√©rance de \\(g(X)\\) est :\n\nCas discret : \\(\\mathbb{E}[g(X)] = \\sum_x g(x) \\mathbb{P}(X = x)\\).\nCas continu : \\(\\mathbb{E}[g(X)] = \\int_{-\\infty}^{\\infty} g(x) f(x) dx\\)."
  },
  {
    "objectID": "slides/02-revisions-proba-slides-annotated.html#th√©or√®me-de-transfert-exemple-wooclap-kffpdj",
    "href": "slides/02-revisions-proba-slides-annotated.html#th√©or√®me-de-transfert-exemple-wooclap-kffpdj",
    "title": "R√©visions",
    "section": "Th√©or√®me de transfert : exemple (Wooclap: KFFPDJ)",
    "text": "Th√©or√®me de transfert : exemple (Wooclap: KFFPDJ)\n\nPour \\(X\\) suivant une loi uniforme sur \\([0, 1]\\) et \\(g(x) = x^2\\) :\n\n\\[\\mathbb{E}[X^2] = \\int_0^1 x^2 \\cdot 1 \\, dx = \\left[\\frac{x^3}{3}\\right]_0^1 = \\frac{1}{3}.\\]"
  },
  {
    "objectID": "slides/02-revisions-proba-slides-annotated.html#variance-et-√©cart-type",
    "href": "slides/02-revisions-proba-slides-annotated.html#variance-et-√©cart-type",
    "title": "R√©visions",
    "section": "Variance et √©cart-type",
    "text": "Variance et √©cart-type\n\n\n\n\nD√©finitions\n\n\nVariance : \\(\\text{Var}(X) = \\mathbb{E}[(X - \\mathbb{E}(X))^2] = \\mathbb{E}[X^2] - \\mathbb{E}[X]^2\\)\n√âcart-type : \\(\\sigma(X) = \\sqrt{\\text{Var}(X)}\\)"
  },
  {
    "objectID": "slides/02-revisions-proba-slides-annotated.html#variance-et-√©cart-type-exemple",
    "href": "slides/02-revisions-proba-slides-annotated.html#variance-et-√©cart-type-exemple",
    "title": "R√©visions",
    "section": "Variance et √©cart-type : exemple",
    "text": "Variance et √©cart-type : exemple\n\nLoi uniforme sur \\([0, 1]\\)\n\n\\(\\mathbb{E}(X) = 1/2\\)\n\\(\\mathbb{E}(X^2) = 1/3\\)\n\\(\\text{Var}(X) = 1/3 - (1/2)^2 = 1/3 - 1/4 = 1/12\\)\n\\(\\sigma(X) = \\sqrt{1/12} \\approx 0.289\\)"
  },
  {
    "objectID": "slides/02-revisions-proba-slides-annotated.html#fonction-de-r√©partition",
    "href": "slides/02-revisions-proba-slides-annotated.html#fonction-de-r√©partition",
    "title": "R√©visions",
    "section": "Fonction de r√©partition",
    "text": "Fonction de r√©partition\n\n\n\n\nD√©finition\n\n\nLa fonction de r√©partition est donn√©e par : \\[F(t) = \\mathbb{P}(X \\leq t).\\]"
  },
  {
    "objectID": "slides/02-revisions-proba-slides-annotated.html#fonction-de-r√©partition-exemple",
    "href": "slides/02-revisions-proba-slides-annotated.html#fonction-de-r√©partition-exemple",
    "title": "R√©visions",
    "section": "Fonction de r√©partition : exemple",
    "text": "Fonction de r√©partition : exemple\nLoi uniforme sur \\([0, 1]\\)\n\\[F(t) = \\begin{cases}\n0 & \\text{si } t &lt; 0 \\\\\nt & \\text{si } 0 \\leq t \\leq 1 \\\\\n1 & \\text{si } t &gt; 1\n\\end{cases}.\\]\nCalcul : \\(\\mathbb{P}(0.2 &lt; X \\leq 0.7) = F(0.7) - F(0.2) = 0.7 - 0.2 = 0.5\\)."
  },
  {
    "objectID": "slides/02-revisions-proba-slides-annotated.html#densit√©s-marginales-et-ind√©pendance",
    "href": "slides/02-revisions-proba-slides-annotated.html#densit√©s-marginales-et-ind√©pendance",
    "title": "R√©visions",
    "section": "Densit√©s marginales et ind√©pendance",
    "text": "Densit√©s marginales et ind√©pendance\nVecteur al√©atoire : \\(X = (X_1, X_2)^\\top\\) avec densit√© conjointe \\(f_X(x, y)\\).\nDensit√©s marginales :\n\n\\(f_{X_1}(x) = \\int_{-\\infty}^{\\infty} f_X(x, y) dy\\)\n\\(f_{X_2}(y) = \\int_{-\\infty}^{\\infty} f_X(x, y) dx\\)\n\nInd√©pendance : \\(f_X(x, y) = f_{X_1}(x) \\cdot f_{X_2}(y)\\)."
  },
  {
    "objectID": "slides/02-revisions-proba-slides-annotated.html#exemple-variables-ind√©pendantes",
    "href": "slides/02-revisions-proba-slides-annotated.html#exemple-variables-ind√©pendantes",
    "title": "R√©visions",
    "section": "Exemple : Variables ind√©pendantes",
    "text": "Exemple : Variables ind√©pendantes\nContexte : Soient \\(X_1\\) et \\(X_2\\) uniformes ind√©pendantes sur \\([0, 1]\\).\n\\[f_X(x, y) = \\begin{cases} 1 & \\text{si } (x,y) \\in [0,1]^2 \\\\ 0 & \\text{sinon} \\end{cases}\\]\n\nCalculs de probabilit√©s :\n\n\\(\\mathbb{P}(X_1 \\leq 0.5, X_2 \\leq 0.8) = \\int_0^{0.5} \\int_0^{0.8} 1 \\, dy \\, dx = 0.5 \\times 0.8 = 0.4\\).\n\\(\\mathbb{P}(X_1 + X_2 \\leq 1) = \\int \\int_{x+y \\leq 1} 1 \\, dy \\, dx = 0.5\\) (aire du triangle)."
  },
  {
    "objectID": "slides/02-revisions-proba-slides-annotated.html#loi-normale-multivari√©e",
    "href": "slides/02-revisions-proba-slides-annotated.html#loi-normale-multivari√©e",
    "title": "R√©visions",
    "section": "Loi normale multivari√©e",
    "text": "Loi normale multivari√©e\n\n\n\nLoi normale \\(p\\)-dimensionnelle\n\n\nSoit \\(X\\) un vecteur al√©atoire suivant une loi normale \\(p\\)-dimensionnelle. Sa densit√© est donn√©e par : \\[f_X(x) = \\frac{1}{(2\\pi)^{p/2}} \\frac{1}{(\\det \\Sigma)^{1/2}} \\exp\\left\\{-\\frac{1}{2}(x-\\mu)^T \\Sigma^{-1} (x-\\mu)\\right\\}.\\]\nNotation : \\(X \\sim \\mathcal{N}_p(\\mu, \\Sigma)\\)."
  },
  {
    "objectID": "slides/02-revisions-proba-slides-annotated.html#exemple-loi-normale-bivari√©e",
    "href": "slides/02-revisions-proba-slides-annotated.html#exemple-loi-normale-bivari√©e",
    "title": "R√©visions",
    "section": "Exemple : Loi normale bivari√©e",
    "text": "Exemple : Loi normale bivari√©e\nCas simple : \\(X \\sim \\mathcal{N}_2(\\mu, I_2)\\) avec \\(\\mu = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\\)\n\\[f_X(x, y) = \\frac{1}{2\\pi} \\exp\\left\\{-\\frac{x^2 + y^2}{2}\\right\\}\\]"
  },
  {
    "objectID": "slides/02-revisions-proba-slides-annotated.html#covariance-et-corr√©lation",
    "href": "slides/02-revisions-proba-slides-annotated.html#covariance-et-corr√©lation",
    "title": "R√©visions",
    "section": "Covariance et corr√©lation",
    "text": "Covariance et corr√©lation\n\n\n\nD√©finitions\n\n\nSoit \\(X = (X_1, X_2)^\\top\\) un vecteur al√©atoire.\nCovariance : \\(\\text{Cov}(X_1, X_2) = \\mathbb{E}[(X_1 - \\mathbb{E}(X_1))(X_2 - \\mathbb{E}(X_2))].\\)\nCorr√©lation : \\(\\text{Corr}(X_1, X_2) = \\frac{\\text{Cov}(X_1, X_2)}{\\sigma(X_1) \\sigma(X_2)}.\\)"
  },
  {
    "objectID": "slides/02-revisions-proba-slides-annotated.html#interpr√©tation-des-signes",
    "href": "slides/02-revisions-proba-slides-annotated.html#interpr√©tation-des-signes",
    "title": "R√©visions",
    "section": "Interpr√©tation des signes",
    "text": "Interpr√©tation des signes\n\nCorr√©lation positive (\\(&gt; 0\\)) : \\(X_1 \\nearrow ~\\Rightarrow ~ X_2 \\nearrow\\).\nCorr√©lation n√©gative (\\(&lt; 0\\)) : \\(X_1 \\searrow ~\\Rightarrow ~ X_2 \\searrow\\).\nCorr√©lation nulle (\\(= 0\\)) : pas de relation lin√©aire (variables orthogonales)"
  },
  {
    "objectID": "slides/02-revisions-proba-slides-annotated.html#interpr√©tation-des-signes-exemple-wooclap-kffpdj",
    "href": "slides/02-revisions-proba-slides-annotated.html#interpr√©tation-des-signes-exemple-wooclap-kffpdj",
    "title": "R√©visions",
    "section": "Interpr√©tation des signes : exemple (Wooclap: KFFPDJ)",
    "text": "Interpr√©tation des signes : exemple (Wooclap: KFFPDJ)\nSoit \\(X = (X_1, X_2, X_3)^\\top\\) avec \\[\\text{Var}(X) = \\begin{pmatrix} 4 & 0 & 3 \\\\ 0 & 2 & 0 \\\\ 3 & 0 & 9 \\end{pmatrix}.\\] Calculer \\(\\text{Corr}(X_1, X_3)\\).\n\n\\[\\text{Corr}(X_1, X_3) = \\frac{\\text{Cov}(X_1, X_3)}{\\sigma(X_1)\\sigma(X_3)}\n= \\frac{3}{\\sqrt{4} \\times \\sqrt{9}} = \\frac{1}{2}.\\]"
  },
  {
    "objectID": "slides/02-revisions-proba-slides-annotated.html#propri√©t√©s-de-la-covariance",
    "href": "slides/02-revisions-proba-slides-annotated.html#propri√©t√©s-de-la-covariance",
    "title": "R√©visions",
    "section": "Propri√©t√©s de la covariance",
    "text": "Propri√©t√©s de la covariance\n\n\n\nPropri√©t√©s\n\n\n\n\\(\\text{Cov}(X_1, X_2) = \\mathbb{E}(X_1 X_2) - \\mathbb{E}(X_1)\\mathbb{E}(X_2)\\).\n\\(\\text{Cov}(X_1, X_2) = \\text{Cov}(X_2, X_1)\\) (sym√©trie).\n\\(\\text{Cov}(X_1 + \\lambda Y_1, X_2) = \\text{Cov}(X_1, X_2) + \\lambda \\text{Cov}(Y_1, X_2)\\) (lin√©arit√©)."
  },
  {
    "objectID": "slides/02-revisions-proba-slides-annotated.html#le-probl√®me-de-lestimation",
    "href": "slides/02-revisions-proba-slides-annotated.html#le-probl√®me-de-lestimation",
    "title": "R√©visions",
    "section": "Le probl√®me de l‚Äôestimation",
    "text": "Le probl√®me de l‚Äôestimation\n\n\nR√©alit√© : On ne conna√Æt pas exactement la distribution de \\(X\\).\nDonn√©es : √âchantillon \\(x_1, x_2, \\ldots, x_n\\) (observations).\nObjectif : Estimer les param√®tres inconnus (\\(\\mu\\), \\(\\Sigma\\), etc.)."
  },
  {
    "objectID": "slides/02-revisions-proba-slides-annotated.html#estimation-de-la-moyenne",
    "href": "slides/02-revisions-proba-slides-annotated.html#estimation-de-la-moyenne",
    "title": "R√©visions",
    "section": "Estimation de la moyenne",
    "text": "Estimation de la moyenne\n\n\n\nEstimateur de la moyenne\n\n\n\\[\\widehat{\\mu} = \\overline{X} = \\frac{1}{n} \\sum_{i=1}^n x_i.\\]\n\n\n\n\nExemple pratique : Tailles en cm : 170, 175, 168, 180, 172\n\\[\\widehat{\\mu} = \\frac{170 + 175 + 168 + 180 + 172}{5} = \\frac{865}{5} = 173 \\text{cm.}\\]"
  },
  {
    "objectID": "slides/02-revisions-proba-slides-annotated.html#estimation-de-la-variance",
    "href": "slides/02-revisions-proba-slides-annotated.html#estimation-de-la-variance",
    "title": "R√©visions",
    "section": "Estimation de la variance",
    "text": "Estimation de la variance\n\n\n\nEstimateur de la variance\n\n\n\\[\\widehat{\\Sigma} = \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\widehat{\\mu})(x_i - \\widehat{\\mu})^T\\]\nPourquoi diviser par \\(n-1\\) ? Correction du biais d√ª √† l‚Äôutilisation de \\(\\widehat{\\mu}\\)."
  },
  {
    "objectID": "slides/02-revisions-proba-slides-annotated.html#exemple-complet-de-calcul",
    "href": "slides/02-revisions-proba-slides-annotated.html#exemple-complet-de-calcul",
    "title": "R√©visions",
    "section": "Exemple complet de calcul",
    "text": "Exemple complet de calcul\nDonn√©es : 170, 175, 168, 180, 172 (tailles en cm)\n\n\\(\\widehat{\\mu} = 173\\)\n√âcarts au carr√© : \\((170-173)^2 = 9\\), \\((175-173)^2 = 4\\), \\((168-173)^2 = 25\\), \\((180-173)^2 = 49\\), \\((172-173)^2 = 1\\).\n\nVariance estim√©e :\n\\[\\widehat{\\sigma^2} = \\frac{9 + 4 + 25 + 49 + 1}{5-1} = \\frac{88}{4} = 22.\\]"
  },
  {
    "objectID": "slides/02-revisions-proba-slides-annotated.html#estimation-de-la-corr√©lation",
    "href": "slides/02-revisions-proba-slides-annotated.html#estimation-de-la-corr√©lation",
    "title": "R√©visions",
    "section": "Estimation de la corr√©lation",
    "text": "Estimation de la corr√©lation\nMatrice de corr√©lation estim√©e : \\(\\widehat{R} = D^{-1} \\widehat{\\Sigma} D^{-1}\\) o√π \\(D\\) est la matrice diagonale des √©carts-types.\n\nExemple 2D : Si \\(\\widehat{\\Sigma} = \\begin{pmatrix} 4 & 1.5 \\\\ 1.5 & 9 \\end{pmatrix}\\). Alors,\n\\[D = \\begin{pmatrix} 2 & 0 \\\\ 0 & 3 \\end{pmatrix}, \\quad\\text{et}\\quad\\widehat{R} = \\begin{pmatrix} 1 & 0.25 \\\\ 0.25 & 1 \\end{pmatrix}.\\]\nInterpr√©tation : corr√©lation faible positive (0.25) entre les variables."
  },
  {
    "objectID": "slides/02-revisions-proba-slides.html#plan-de-la-pr√©sentation",
    "href": "slides/02-revisions-proba-slides.html#plan-de-la-pr√©sentation",
    "title": "R√©visions",
    "section": "Plan de la pr√©sentation",
    "text": "Plan de la pr√©sentation\n\n\nMod√©liser le hasard\nVariables al√©atoires\nVecteurs al√©atoires\nEstimation statistique"
  },
  {
    "objectID": "slides/02-revisions-proba-slides.html#espace-d√©v√©nements-et-√©v√©nements",
    "href": "slides/02-revisions-proba-slides.html#espace-d√©v√©nements-et-√©v√©nements",
    "title": "R√©visions",
    "section": "Espace d‚Äô√©v√©nements et √©v√©nements",
    "text": "Espace d‚Äô√©v√©nements et √©v√©nements\n\n\n\n\nD√©finitions\n\n\n\nEspace d‚Äô√©v√©nements \\(S\\) : ensemble de tous les r√©sultats possibles d‚Äôune exp√©rience.\n√âv√©nement : sous-ensemble de \\(S\\)."
  },
  {
    "objectID": "slides/02-revisions-proba-slides.html#espace-d√©v√©nements-et-√©v√©nements-exemples",
    "href": "slides/02-revisions-proba-slides.html#espace-d√©v√©nements-et-√©v√©nements-exemples",
    "title": "R√©visions",
    "section": "Espace d‚Äô√©v√©nements et √©v√©nements : exemples",
    "text": "Espace d‚Äô√©v√©nements et √©v√©nements : exemples\n\nExemple 1 : Lancer de pi√®ce\n\n\\(S = \\{0, 1\\}\\) (pile = 0, face = 1).\n√âv√©nement \\(E = \\{1\\}\\) : ‚Äúobtenir face‚Äù.\n\n\nExemple 2 : Dur√©e de vie d‚Äôun t√©l√©phone\n\n\\(S = \\mathbb{R}_+ = [0, +\\infty)\\) (en ann√©es).\n√âv√©nement \\(E = [10, \\infty)\\) : ‚Äúdur√©e de vie &gt; 10 ans‚Äù."
  },
  {
    "objectID": "slides/02-revisions-proba-slides.html#mesure-de-probabilit√©",
    "href": "slides/02-revisions-proba-slides.html#mesure-de-probabilit√©",
    "title": "R√©visions",
    "section": "Mesure de probabilit√©",
    "text": "Mesure de probabilit√©\n\n\n\nD√©finition\n\n\nUne mesure de probabilit√© \\(\\mathbb{P}\\) satisfait :\n\n\\(\\mathbb{P}(E) \\in [0, 1]\\) pour tout √©v√©nement \\(E\\);\n\\(\\mathbb{P}(S) = 1\\) (certitude totale);\nPour des √©v√©nements disjoints : \\(\\mathbb{P}(E_1 \\cup E_2 \\cup \\cdots) = \\mathbb{P}(E_1) + \\mathbb{P}(E_2) + \\cdots\\)."
  },
  {
    "objectID": "slides/02-revisions-proba-slides.html#mesure-de-probabilit√©-exemple-wooclap-kffpdj",
    "href": "slides/02-revisions-proba-slides.html#mesure-de-probabilit√©-exemple-wooclap-kffpdj",
    "title": "R√©visions",
    "section": "Mesure de probabilit√© : exemple (Wooclap: KFFPDJ)",
    "text": "Mesure de probabilit√© : exemple (Wooclap: KFFPDJ)\n\nExemple : Lancer d‚Äôun d√© √©quilibr√©\n\nEspace d‚Äô√©v√©nements : \\(S = \\{1, 2, 3, 4, 5, 6\\}\\).\n\\(\\mathbb{P}(\\{i\\}) = 1/6\\) pour \\(i = 1, \\ldots, 6\\).\n\\(\\mathbb{P}(\\{2, 4, 6\\}) = 1/6 + 1/6 + 1/6 = 1/2\\) (nombre pair)."
  },
  {
    "objectID": "slides/02-revisions-proba-slides.html#subjectivit√©-de-la-probabilit√©",
    "href": "slides/02-revisions-proba-slides.html#subjectivit√©-de-la-probabilit√©",
    "title": "R√©visions",
    "section": "Subjectivit√© de la probabilit√©",
    "text": "Subjectivit√© de la probabilit√©\n\nContexte : Nombre de jours sans neige √† Qu√©bec dans l‚Äôann√©e\n\nEspace d‚Äô√©v√©nements : \\(S = \\{0, 1, 2, \\ldots, 365\\}\\).\n√âv√©nement \\(E_1 = [0, 100]\\) : ‚Äúmoins de 100 jours sans neige‚Äù.\n√âv√©nement \\(E_2 = [100, 200]\\) : ‚Äúentre 100 et 200 jours sans neige‚Äù.\n\nPerspective subjective :\n\nNouvel arrivant : probabilit√©s uniformes sur \\(S\\).\nQu√©b√©cois : probabilit√©s plus √©lev√©es pour \\(E_1\\) que \\(E_2\\)."
  },
  {
    "objectID": "slides/02-revisions-proba-slides.html#ind√©pendance-et-probabilit√©-conditionnelle",
    "href": "slides/02-revisions-proba-slides.html#ind√©pendance-et-probabilit√©-conditionnelle",
    "title": "R√©visions",
    "section": "Ind√©pendance et probabilit√© conditionnelle",
    "text": "Ind√©pendance et probabilit√© conditionnelle\n\n\n\nD√©finitions\n\n\nInd√©pendance : Les √©v√©nements \\(E\\) et \\(F\\) sont ind√©pendants si \\[\\mathbb{P}(E \\cap F) = \\mathbb{P}(E) \\times \\mathbb{P}(F).\\] Probabilit√© conditionnelle : La probabilit√© de \\(E\\) sachant \\(F\\) est \\[\\mathbb{P}(E \\mid F) = \\frac{\\mathbb{P}(E \\cap F)}{\\mathbb{P}(F)}.\\]"
  },
  {
    "objectID": "slides/02-revisions-proba-slides.html#exemple-cartes-√†-jouer",
    "href": "slides/02-revisions-proba-slides.html#exemple-cartes-√†-jouer",
    "title": "R√©visions",
    "section": "Exemple : Cartes √† jouer",
    "text": "Exemple : Cartes √† jouer\n\nContexte : Tirage d‚Äôune carte dans un jeu standard (52 cartes)\n\n\\(E\\) : ‚Äútirer un roi‚Äù ‚Üí \\(\\mathbb{P}(E) = 4/52 = 1/13\\).\n\\(F\\) : ‚Äútirer un c≈ìur‚Äù ‚Üí \\(\\mathbb{P}(F) = 13/52 = 1/4\\).\n\nTest d‚Äôind√©pendance :\n\n\\(E \\cap F\\) : ‚Äútirer le roi de c≈ìur‚Äù ‚Üí \\(\\mathbb{P}(E \\cap F) = 1/52\\).\n\\(\\mathbb{P}(E) \\times \\mathbb{P}(F) = (1/13) \\times (1/4) = 1/52\\).\n\nConclusion : \\(E\\) et \\(F\\) sont ind√©pendants !"
  },
  {
    "objectID": "slides/02-revisions-proba-slides.html#variables-discr√®tes",
    "href": "slides/02-revisions-proba-slides.html#variables-discr√®tes",
    "title": "R√©visions",
    "section": "Variables discr√®tes",
    "text": "Variables discr√®tes\n\n\n\n\nD√©finition\n\n\nUne variable al√©atoire discr√®te prend au plus un nombre d√©nombrable de valeurs.\nSa distribution donn√©e par \\(\\mathbb{P}(X = x)\\) pour chaque valeur \\(x\\)."
  },
  {
    "objectID": "slides/02-revisions-proba-slides.html#variables-discr√®tes-exemple",
    "href": "slides/02-revisions-proba-slides.html#variables-discr√®tes-exemple",
    "title": "R√©visions",
    "section": "Variables discr√®tes : exemple",
    "text": "Variables discr√®tes : exemple\n\nExemple : Lancer de deux d√©s\n\nVariable al√©atoire \\(X\\) = somme des deux d√©s.\nEspace d‚Äô√©v√©nements : \\(S = \\{2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12\\}\\).\n\\(\\mathbb{P}(X = 7) = 6/36 = 1/6\\) (combinaisons : 1+6, 2+5, 3+4, 4+3, 5+2, 6+1).\n\\(\\mathbb{P}(X = 2) = 1/36\\) (une seule combinaison : 1+1)."
  },
  {
    "objectID": "slides/02-revisions-proba-slides.html#variables-continues",
    "href": "slides/02-revisions-proba-slides.html#variables-continues",
    "title": "R√©visions",
    "section": "Variables continues",
    "text": "Variables continues\n\n\n\nD√©finition\n\n\nLes probabilit√©s d‚Äôune variable al√©atoire continue sont donn√©es par les int√©grales \\[\\mathbb{P}(X \\in A) = \\int_A f(x) dx\\] o√π \\(f(x) \\geq 0\\) et \\(\\int_{-\\infty}^{\\infty} f(x) dx = 1\\).\nImportant : \\(\\mathbb{P}(X = x) = 0\\) pour tout \\(x\\) fix√© !"
  },
  {
    "objectID": "slides/02-revisions-proba-slides.html#variables-continues-exemple",
    "href": "slides/02-revisions-proba-slides.html#variables-continues-exemple",
    "title": "R√©visions",
    "section": "Variables continues : exemple",
    "text": "Variables continues : exemple\nVariable uniforme sur \\([0, 1]\\) : \\[f(x) = \\begin{cases} 1 & \\text{si } x \\in [0, 1] \\\\ 0 & \\text{sinon} \\end{cases}.\\]\nCalculs de probabilit√©s :\n\n\\(\\mathbb{P}(X \\leq 0.5) = \\int_0^{0.5} 1 \\, dx = 0.5\\).\n\\(\\mathbb{P}(0.2 &lt; X &lt; 0.7) = \\int_{0.2}^{0.7} 1 \\, dx = 0.5\\).\n\\(\\mathbb{P}(X = 0.3) = 0\\) (point isol√©)."
  },
  {
    "objectID": "slides/02-revisions-proba-slides.html#esp√©rance-math√©matique",
    "href": "slides/02-revisions-proba-slides.html#esp√©rance-math√©matique",
    "title": "R√©visions",
    "section": "Esp√©rance math√©matique",
    "text": "Esp√©rance math√©matique\n\n\n\nD√©finition\n\n\nL‚Äôesp√©rance \\(\\mathbb{E}(X)\\) est la valeur moyenne de \\(X\\).\n\nCas discret : \\(\\mathbb{E}(X) = \\sum_x x \\mathbb{P}(X = x)\\).\nCas continu : \\(\\mathbb{E}(X) = \\int_{-\\infty}^{\\infty} x  f(x) dx\\)."
  },
  {
    "objectID": "slides/02-revisions-proba-slides.html#esp√©rance-math√©matique-exemple-wooclap-kffpdj",
    "href": "slides/02-revisions-proba-slides.html#esp√©rance-math√©matique-exemple-wooclap-kffpdj",
    "title": "R√©visions",
    "section": "Esp√©rance math√©matique : exemple (Wooclap: KFFPDJ)",
    "text": "Esp√©rance math√©matique : exemple (Wooclap: KFFPDJ)\nExemple 1 : D√© √©quilibr√©\n\n\\[\\mathbb{E}(X) = 1 \\cdot \\frac{1}{6} + 2 \\cdot \\frac{1}{6} + \\cdots + 6 \\cdot \\frac{1}{6} = \\frac{21}{6} = 3.5.\\]\n\n\nExemple 2 : Loi uniforme sur \\([0, 1]\\)\n\n\n\\[\\mathbb{E}(X) = \\int_0^1 x \\cdot 1 \\, dx = \\left[\\frac{x^2}{2}\\right]_0^1 = \\frac{1}{2}.\\]"
  },
  {
    "objectID": "slides/02-revisions-proba-slides.html#th√©or√®me-de-transfert",
    "href": "slides/02-revisions-proba-slides.html#th√©or√®me-de-transfert",
    "title": "R√©visions",
    "section": "Th√©or√®me de transfert",
    "text": "Th√©or√®me de transfert\n\n\n\nTh√©or√®me\n\n\nPour une fonction \\(g\\), l‚Äôesp√©rance de \\(g(X)\\) est :\n\nCas discret : \\(\\mathbb{E}[g(X)] = \\sum_x g(x) \\mathbb{P}(X = x)\\).\nCas continu : \\(\\mathbb{E}[g(X)] = \\int_{-\\infty}^{\\infty} g(x) f(x) dx\\)."
  },
  {
    "objectID": "slides/02-revisions-proba-slides.html#th√©or√®me-de-transfert-exemple-wooclap-kffpdj",
    "href": "slides/02-revisions-proba-slides.html#th√©or√®me-de-transfert-exemple-wooclap-kffpdj",
    "title": "R√©visions",
    "section": "Th√©or√®me de transfert : exemple (Wooclap: KFFPDJ)",
    "text": "Th√©or√®me de transfert : exemple (Wooclap: KFFPDJ)\n\nPour \\(X\\) suivant une loi uniforme sur \\([0, 1]\\) et \\(g(x) = x^2\\) :\n\n\\[\\mathbb{E}[X^2] = \\int_0^1 x^2 \\cdot 1 \\, dx = \\left[\\frac{x^3}{3}\\right]_0^1 = \\frac{1}{3}.\\]"
  },
  {
    "objectID": "slides/02-revisions-proba-slides.html#variance-et-√©cart-type",
    "href": "slides/02-revisions-proba-slides.html#variance-et-√©cart-type",
    "title": "R√©visions",
    "section": "Variance et √©cart-type",
    "text": "Variance et √©cart-type\n\n\n\n\nD√©finitions\n\n\nVariance : \\(\\text{Var}(X) = \\mathbb{E}[(X - \\mathbb{E}(X))^2] = \\mathbb{E}[X^2] - \\mathbb{E}[X]^2\\)\n√âcart-type : \\(\\sigma(X) = \\sqrt{\\text{Var}(X)}\\)"
  },
  {
    "objectID": "slides/02-revisions-proba-slides.html#variance-et-√©cart-type-exemple",
    "href": "slides/02-revisions-proba-slides.html#variance-et-√©cart-type-exemple",
    "title": "R√©visions",
    "section": "Variance et √©cart-type : exemple",
    "text": "Variance et √©cart-type : exemple\n\nLoi uniforme sur \\([0, 1]\\)\n\n\\(\\mathbb{E}(X) = 1/2\\)\n\\(\\mathbb{E}(X^2) = 1/3\\)\n\\(\\text{Var}(X) = 1/3 - (1/2)^2 = 1/3 - 1/4 = 1/12\\)\n\\(\\sigma(X) = \\sqrt{1/12} \\approx 0.289\\)"
  },
  {
    "objectID": "slides/02-revisions-proba-slides.html#fonction-de-r√©partition",
    "href": "slides/02-revisions-proba-slides.html#fonction-de-r√©partition",
    "title": "R√©visions",
    "section": "Fonction de r√©partition",
    "text": "Fonction de r√©partition\n\n\n\n\nD√©finition\n\n\nLa fonction de r√©partition est donn√©e par : \\[F(t) = \\mathbb{P}(X \\leq t).\\]"
  },
  {
    "objectID": "slides/02-revisions-proba-slides.html#fonction-de-r√©partition-exemple",
    "href": "slides/02-revisions-proba-slides.html#fonction-de-r√©partition-exemple",
    "title": "R√©visions",
    "section": "Fonction de r√©partition : exemple",
    "text": "Fonction de r√©partition : exemple\nLoi uniforme sur \\([0, 1]\\)\n\\[F(t) = \\begin{cases}\n0 & \\text{si } t &lt; 0 \\\\\nt & \\text{si } 0 \\leq t \\leq 1 \\\\\n1 & \\text{si } t &gt; 1\n\\end{cases}.\\]\nCalcul : \\(\\mathbb{P}(0.2 &lt; X \\leq 0.7) = F(0.7) - F(0.2) = 0.7 - 0.2 = 0.5\\)."
  },
  {
    "objectID": "slides/02-revisions-proba-slides.html#densit√©s-marginales-et-ind√©pendance",
    "href": "slides/02-revisions-proba-slides.html#densit√©s-marginales-et-ind√©pendance",
    "title": "R√©visions",
    "section": "Densit√©s marginales et ind√©pendance",
    "text": "Densit√©s marginales et ind√©pendance\nVecteur al√©atoire : \\(X = (X_1, X_2)^\\top\\) avec densit√© conjointe \\(f_X(x, y)\\).\nDensit√©s marginales :\n\n\\(f_{X_1}(x) = \\int_{-\\infty}^{\\infty} f_X(x, y) dy\\)\n\\(f_{X_2}(y) = \\int_{-\\infty}^{\\infty} f_X(x, y) dx\\)\n\nInd√©pendance : \\(f_X(x, y) = f_{X_1}(x) \\cdot f_{X_2}(y)\\)."
  },
  {
    "objectID": "slides/02-revisions-proba-slides.html#exemple-variables-ind√©pendantes",
    "href": "slides/02-revisions-proba-slides.html#exemple-variables-ind√©pendantes",
    "title": "R√©visions",
    "section": "Exemple : Variables ind√©pendantes",
    "text": "Exemple : Variables ind√©pendantes\nContexte : Soient \\(X_1\\) et \\(X_2\\) uniformes ind√©pendantes sur \\([0, 1]\\).\n\\[f_X(x, y) = \\begin{cases} 1 & \\text{si } (x,y) \\in [0,1]^2 \\\\ 0 & \\text{sinon} \\end{cases}\\]\n\nCalculs de probabilit√©s :\n\n\\(\\mathbb{P}(X_1 \\leq 0.5, X_2 \\leq 0.8) = \\int_0^{0.5} \\int_0^{0.8} 1 \\, dy \\, dx = 0.5 \\times 0.8 = 0.4\\).\n\\(\\mathbb{P}(X_1 + X_2 \\leq 1) = \\int \\int_{x+y \\leq 1} 1 \\, dy \\, dx = 0.5\\) (aire du triangle)."
  },
  {
    "objectID": "slides/02-revisions-proba-slides.html#loi-normale-multivari√©e",
    "href": "slides/02-revisions-proba-slides.html#loi-normale-multivari√©e",
    "title": "R√©visions",
    "section": "Loi normale multivari√©e",
    "text": "Loi normale multivari√©e\n\n\n\nLoi normale \\(p\\)-dimensionnelle\n\n\nSoit \\(X\\) un vecteur al√©atoire suivant une loi normale \\(p\\)-dimensionnelle. Sa densit√© est donn√©e par : \\[f_X(x) = \\frac{1}{(2\\pi)^{p/2}} \\frac{1}{(\\det \\Sigma)^{1/2}} \\exp\\left\\{-\\frac{1}{2}(x-\\mu)^T \\Sigma^{-1} (x-\\mu)\\right\\}.\\]\nNotation : \\(X \\sim \\mathcal{N}_p(\\mu, \\Sigma)\\)."
  },
  {
    "objectID": "slides/02-revisions-proba-slides.html#exemple-loi-normale-bivari√©e",
    "href": "slides/02-revisions-proba-slides.html#exemple-loi-normale-bivari√©e",
    "title": "R√©visions",
    "section": "Exemple : Loi normale bivari√©e",
    "text": "Exemple : Loi normale bivari√©e\nCas simple : \\(X \\sim \\mathcal{N}_2(\\mu, I_2)\\) avec \\(\\mu = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\\)\n\\[f_X(x, y) = \\frac{1}{2\\pi} \\exp\\left\\{-\\frac{x^2 + y^2}{2}\\right\\}\\]"
  },
  {
    "objectID": "slides/02-revisions-proba-slides.html#covariance-et-corr√©lation",
    "href": "slides/02-revisions-proba-slides.html#covariance-et-corr√©lation",
    "title": "R√©visions",
    "section": "Covariance et corr√©lation",
    "text": "Covariance et corr√©lation\n\n\n\nD√©finitions\n\n\nSoit \\(X = (X_1, X_2)^\\top\\) un vecteur al√©atoire.\nCovariance : \\(\\text{Cov}(X_1, X_2) = \\mathbb{E}[(X_1 - \\mathbb{E}(X_1))(X_2 - \\mathbb{E}(X_2))].\\)\nCorr√©lation : \\(\\text{Corr}(X_1, X_2) = \\frac{\\text{Cov}(X_1, X_2)}{\\sigma(X_1) \\sigma(X_2)}.\\)"
  },
  {
    "objectID": "slides/02-revisions-proba-slides.html#interpr√©tation-des-signes",
    "href": "slides/02-revisions-proba-slides.html#interpr√©tation-des-signes",
    "title": "R√©visions",
    "section": "Interpr√©tation des signes",
    "text": "Interpr√©tation des signes\n\nCorr√©lation positive (\\(&gt; 0\\)) : \\(X_1 \\nearrow ~\\Rightarrow ~ X_2 \\nearrow\\).\nCorr√©lation n√©gative (\\(&lt; 0\\)) : \\(X_1 \\searrow ~\\Rightarrow ~ X_2 \\searrow\\).\nCorr√©lation nulle (\\(= 0\\)) : pas de relation lin√©aire (variables orthogonales)"
  },
  {
    "objectID": "slides/02-revisions-proba-slides.html#interpr√©tation-des-signes-exemple-wooclap-kffpdj",
    "href": "slides/02-revisions-proba-slides.html#interpr√©tation-des-signes-exemple-wooclap-kffpdj",
    "title": "R√©visions",
    "section": "Interpr√©tation des signes : exemple (Wooclap: KFFPDJ)",
    "text": "Interpr√©tation des signes : exemple (Wooclap: KFFPDJ)\nSoit \\(X = (X_1, X_2, X_3)^\\top\\) avec \\[\\text{Var}(X) = \\begin{pmatrix} 4 & 0 & 3 \\\\ 0 & 2 & 0 \\\\ 3 & 0 & 9 \\end{pmatrix}.\\] Calculer \\(\\text{Corr}(X_1, X_3)\\).\n\n\\[\\text{Corr}(X_1, X_3) = \\frac{\\text{Cov}(X_1, X_3)}{\\sigma(X_1)\\sigma(X_3)}\n= \\frac{3}{\\sqrt{4} \\times \\sqrt{9}} = \\frac{1}{2}.\\]"
  },
  {
    "objectID": "slides/02-revisions-proba-slides.html#propri√©t√©s-de-la-covariance",
    "href": "slides/02-revisions-proba-slides.html#propri√©t√©s-de-la-covariance",
    "title": "R√©visions",
    "section": "Propri√©t√©s de la covariance",
    "text": "Propri√©t√©s de la covariance\n\n\n\nPropri√©t√©s\n\n\n\n\\(\\text{Cov}(X_1, X_2) = \\mathbb{E}(X_1 X_2) - \\mathbb{E}(X_1)\\mathbb{E}(X_2)\\).\n\\(\\text{Cov}(X_1, X_2) = \\text{Cov}(X_2, X_1)\\) (sym√©trie).\n\\(\\text{Cov}(X_1 + \\lambda Y_1, X_2) = \\text{Cov}(X_1, X_2) + \\lambda \\text{Cov}(Y_1, X_2)\\) (lin√©arit√©)."
  },
  {
    "objectID": "slides/02-revisions-proba-slides.html#le-probl√®me-de-lestimation",
    "href": "slides/02-revisions-proba-slides.html#le-probl√®me-de-lestimation",
    "title": "R√©visions",
    "section": "Le probl√®me de l‚Äôestimation",
    "text": "Le probl√®me de l‚Äôestimation\n\n\nR√©alit√© : On ne conna√Æt pas exactement la distribution de \\(X\\).\nDonn√©es : √âchantillon \\(x_1, x_2, \\ldots, x_n\\) (observations).\nObjectif : Estimer les param√®tres inconnus (\\(\\mu\\), \\(\\Sigma\\), etc.)."
  },
  {
    "objectID": "slides/02-revisions-proba-slides.html#estimation-de-la-moyenne",
    "href": "slides/02-revisions-proba-slides.html#estimation-de-la-moyenne",
    "title": "R√©visions",
    "section": "Estimation de la moyenne",
    "text": "Estimation de la moyenne\n\n\n\nEstimateur de la moyenne\n\n\n\\[\\widehat{\\mu} = \\overline{X} = \\frac{1}{n} \\sum_{i=1}^n x_i.\\]\n\n\n\n\nExemple pratique : Tailles en cm : 170, 175, 168, 180, 172\n\\[\\widehat{\\mu} = \\frac{170 + 175 + 168 + 180 + 172}{5} = \\frac{865}{5} = 173 \\text{cm.}\\]"
  },
  {
    "objectID": "slides/02-revisions-proba-slides.html#estimation-de-la-variance",
    "href": "slides/02-revisions-proba-slides.html#estimation-de-la-variance",
    "title": "R√©visions",
    "section": "Estimation de la variance",
    "text": "Estimation de la variance\n\n\n\nEstimateur de la variance\n\n\n\\[\\widehat{\\Sigma} = \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\widehat{\\mu})(x_i - \\widehat{\\mu})^T\\]\nPourquoi diviser par \\(n-1\\) ? Correction du biais d√ª √† l‚Äôutilisation de \\(\\widehat{\\mu}\\)."
  },
  {
    "objectID": "slides/02-revisions-proba-slides.html#exemple-complet-de-calcul",
    "href": "slides/02-revisions-proba-slides.html#exemple-complet-de-calcul",
    "title": "R√©visions",
    "section": "Exemple complet de calcul",
    "text": "Exemple complet de calcul\nDonn√©es : 170, 175, 168, 180, 172 (tailles en cm)\n\n\\(\\widehat{\\mu} = 173\\)\n√âcarts au carr√© : \\((170-173)^2 = 9\\), \\((175-173)^2 = 4\\), \\((168-173)^2 = 25\\), \\((180-173)^2 = 49\\), \\((172-173)^2 = 1\\).\n\nVariance estim√©e :\n\\[\\widehat{\\sigma^2} = \\frac{9 + 4 + 25 + 49 + 1}{5-1} = \\frac{88}{4} = 22.\\]"
  },
  {
    "objectID": "slides/02-revisions-proba-slides.html#estimation-de-la-corr√©lation",
    "href": "slides/02-revisions-proba-slides.html#estimation-de-la-corr√©lation",
    "title": "R√©visions",
    "section": "Estimation de la corr√©lation",
    "text": "Estimation de la corr√©lation\nMatrice de corr√©lation estim√©e : \\(\\widehat{R} = D^{-1} \\widehat{\\Sigma} D^{-1}\\) o√π \\(D\\) est la matrice diagonale des √©carts-types.\n\nExemple 2D : Si \\(\\widehat{\\Sigma} = \\begin{pmatrix} 4 & 1.5 \\\\ 1.5 & 9 \\end{pmatrix}\\). Alors,\n\\[D = \\begin{pmatrix} 2 & 0 \\\\ 0 & 3 \\end{pmatrix}, \\quad\\text{et}\\quad\\widehat{R} = \\begin{pmatrix} 1 & 0.25 \\\\ 0.25 & 1 \\end{pmatrix}.\\]\nInterpr√©tation : corr√©lation faible positive (0.25) entre les variables."
  }
]