[
  {
    "objectID": "contents/03-generalities.html",
    "href": "contents/03-generalities.html",
    "title": "Généralités",
    "section": "",
    "text": "Slides:\n\nProjet d’analyse de données\nEspaces\nDistances - annoté\nBiais / Variance - annoté\nÉvaluation de modèles - annoté\n\nTD: link\nTP: link",
    "crumbs": [
      "Modules",
      "03 - Généralités"
    ]
  },
  {
    "objectID": "contents/03-generalities.html#sommaire",
    "href": "contents/03-generalities.html#sommaire",
    "title": "Généralités",
    "section": "Sommaire",
    "text": "Sommaire\n\nProjet d’analyse de données\nEspaces\nDistances\nBiais / Variance\nÉvaluation de modèles\n\n\n\n\nCorrelation (xkcd:552).",
    "crumbs": [
      "Modules",
      "03 - Généralités"
    ]
  },
  {
    "objectID": "contents/unsupervised/01-kmeans.html",
    "href": "contents/unsupervised/01-kmeans.html",
    "title": "\\(k\\)-means",
    "section": "",
    "text": "On dispose de \\(n\\) observations \\(X_{1}, \\dots, X_n\\) décrites par \\(p\\) variables numériques. Les variables sont généralement standardisées pour éviter qu’une variable domine les autres à cause de son échelle. Notre objectif est de regrouper ces \\(n\\) observations en \\(K\\) groupes (ou classes), de sorte que :\n\nLes observations au sein d’un même groupe soient les plus similaires possible (dans un certain sens).\nLes observations appartenant à des groupes différents soient les moins similaires possible.\n\nAutrement dit, nous cherchons une fonction de classification : \\[C: \\{ 1, \\dots, n \\} \\to \\{ 1, \\dots, K \\}\\] qui, à chaque observation \\(i \\in \\{ 1, \\dots, n \\}\\) associe une étiquette de groupe \\(C(i) \\in \\{ 1, \\dots, K \\}\\).\n\n\n\n\n\n\nAvertissementDéfinition : fonction de coût\n\n\n\nLa qualité d’une partition \\(C\\) est mesurée à l’aide d’une fonction de coût \\(W\\), qui évalue la somme des distances intra-groupes : \\[W(C) = \\sum_{k = 1}^{K} \\sum_{i : C(i) = k} \\sum_{j : C(j) = k} d(X_i, X_j),\\] où \\(d(X_i, X_j)\\) est une mesure de la dissimilarité entre les observations \\(X_i\\) et \\(X_j\\). Par exemple, on peut utiliser la distance euclidienne.\n\n\nPlus \\(W(C)\\) est faible, meilleure est la qualité de la partition au sens de la cohésion intra-groupe. Le problème de la classication non-supervisée est donc un problème d’optimisation combinatoire. Il s’agit de trouver la fonction \\(C\\) qui minimise \\(W(C)\\).\nCependant, ce problème est très difficile (voir impossible) à résourdre exactement. En effet, il existe \\(K^n\\) combinaisons possibles (chaque observation pouvant appartenir à un des \\(K\\) groupes). Il est donc compliqué d’explorer toutes les solutions possibles même pour de petites valeurs de \\(n\\) et de \\(K\\).\nPour résoudre ce problème, on utilise des algorithmes gloutons (greedy algorithm) qui procèdent de manière itérative. L’idée est de, premièrement, explorer un sous-ensemble restreint de l’espace des partitions. Ensuite, l’algorithme améliore progressivement la solution de manière itérative. Ceux-ci ne garantissent pas d’atteindre le minimum global, mais plutôt un minimum local qui est souvent une bonne solution en pratique.\n\n\n\n\n\n\nAstuceHypothèses\n\n\n\n\nLes \\(p\\) variables sont numériques, catégorielles ou ordinales, souvent centées-réduites.\nLe nombre de groueps \\(K\\) est fixé avant de lancer l’algorithme (choisi par l’utilisateur).",
    "crumbs": [
      "Modules",
      "06 - Non-supervisée",
      "$k$-means"
    ]
  },
  {
    "objectID": "contents/unsupervised/01-kmeans.html#objectif-de-la-classication-non-supervisée",
    "href": "contents/unsupervised/01-kmeans.html#objectif-de-la-classication-non-supervisée",
    "title": "\\(k\\)-means",
    "section": "",
    "text": "On dispose de \\(n\\) observations \\(X_{1}, \\dots, X_n\\) décrites par \\(p\\) variables numériques. Les variables sont généralement standardisées pour éviter qu’une variable domine les autres à cause de son échelle. Notre objectif est de regrouper ces \\(n\\) observations en \\(K\\) groupes (ou classes), de sorte que :\n\nLes observations au sein d’un même groupe soient les plus similaires possible (dans un certain sens).\nLes observations appartenant à des groupes différents soient les moins similaires possible.\n\nAutrement dit, nous cherchons une fonction de classification : \\[C: \\{ 1, \\dots, n \\} \\to \\{ 1, \\dots, K \\}\\] qui, à chaque observation \\(i \\in \\{ 1, \\dots, n \\}\\) associe une étiquette de groupe \\(C(i) \\in \\{ 1, \\dots, K \\}\\).\n\n\n\n\n\n\nAvertissementDéfinition : fonction de coût\n\n\n\nLa qualité d’une partition \\(C\\) est mesurée à l’aide d’une fonction de coût \\(W\\), qui évalue la somme des distances intra-groupes : \\[W(C) = \\sum_{k = 1}^{K} \\sum_{i : C(i) = k} \\sum_{j : C(j) = k} d(X_i, X_j),\\] où \\(d(X_i, X_j)\\) est une mesure de la dissimilarité entre les observations \\(X_i\\) et \\(X_j\\). Par exemple, on peut utiliser la distance euclidienne.\n\n\nPlus \\(W(C)\\) est faible, meilleure est la qualité de la partition au sens de la cohésion intra-groupe. Le problème de la classication non-supervisée est donc un problème d’optimisation combinatoire. Il s’agit de trouver la fonction \\(C\\) qui minimise \\(W(C)\\).\nCependant, ce problème est très difficile (voir impossible) à résourdre exactement. En effet, il existe \\(K^n\\) combinaisons possibles (chaque observation pouvant appartenir à un des \\(K\\) groupes). Il est donc compliqué d’explorer toutes les solutions possibles même pour de petites valeurs de \\(n\\) et de \\(K\\).\nPour résoudre ce problème, on utilise des algorithmes gloutons (greedy algorithm) qui procèdent de manière itérative. L’idée est de, premièrement, explorer un sous-ensemble restreint de l’espace des partitions. Ensuite, l’algorithme améliore progressivement la solution de manière itérative. Ceux-ci ne garantissent pas d’atteindre le minimum global, mais plutôt un minimum local qui est souvent une bonne solution en pratique.\n\n\n\n\n\n\nAstuceHypothèses\n\n\n\n\nLes \\(p\\) variables sont numériques, catégorielles ou ordinales, souvent centées-réduites.\nLe nombre de groueps \\(K\\) est fixé avant de lancer l’algorithme (choisi par l’utilisateur).",
    "crumbs": [
      "Modules",
      "06 - Non-supervisée",
      "$k$-means"
    ]
  },
  {
    "objectID": "contents/unsupervised/01-kmeans.html#algorithme-k-moyennes",
    "href": "contents/unsupervised/01-kmeans.html#algorithme-k-moyennes",
    "title": "\\(k\\)-means",
    "section": "Algorithme \\(k\\)-moyennes",
    "text": "Algorithme \\(k\\)-moyennes\nL’algorithme des \\(k\\)-moyennes (\\(k\\)-means) est une méthode classique de classication non-supervisée. L’objectif est de regrouper \\(n\\) observations en \\(K\\) groupes homogènes, i.e. tels que les observations au sein d’un même groupe soient aussi proches que possible, tandis que celles appartenant à des groupes différents soient aussi éloignées que possible. Cette méthode repose sur une mesure de dissimilarité, généralement la distance euclienne pour des données quantitatives.\n\n\n\n\n\n\nAstuceAlgorithme\n\n\n\nVoici les principales étapes de l’algorithme des \\(k\\)-moyennes.\n\nChoix du nombre de groupes \\(K\\) : Le nombre de classes doit être choisi à l’avance.\nInitialisation : On partitionne aléatoirement les \\(n\\) observations en \\(K\\) groupes (ou on choisit aléatoirement \\(K\\) observations comme centres initiaux).\nCalcul des centroïdes : Pour chaque groupe \\(k\\), on calcule leur centre de gravité\n\n\\[\\mu_k = \\frac{1}{N_k} \\sum_{i: C(i) = k} X_i, \\quad \\text{pour}\\quad k = 1, \\dots, K\\] où \\(N_k\\) est le nombre d’observations dans le groupe \\(k\\).\n\nRéaffectation : Chaque observation est affectée au centre le plus proche, i.e. au groupe dont le centroïde minimise la distance à l’observation.\nItération : On répète les étapes 3 et 4 jusqu’’à la statibilisation des groupes, i.e. jusqu’à ce qu’aucune observation ne change de groupe.\n\n\n\nLa convergence de l’algorithme est garantie en un nombre fini d’itérations, car chaque étape réduit l’inertie intra-groupe, i.e. la somme des distances des observations à leur centroïdes respectif. En revanche, rien ne garantit que l’algorithme atteigne un minimum global de \\(W\\), il peut converger vers un minimum locale.\nExemple avec une petite visualisation.\nCette algorithme présente plusieurs limites. Tout d’abord, il est sensible à l’initialisation des groupes. Les résultats peuvent varier d’un essai à l’autre en fonction du choix initial des centroïdes. Ce problème peut être résolu en utilisant l’algorithme kmean++ (cf. link). Deuxièmement, l’algorithme nécessite de connaître à l’avance le nombre de groupes \\(K\\), ce qui n’est pas toujours évident. Il existe plusieurs critères pour guider ce choix, tels que le coefficient de silhouette, la méthode du coude (elbow method) ou encore des critères d’information comme le BIC ou l’AIC dans un cadre probabiliste. Troisièmement, à chaque itération, l’algorithme requiert le recalcul de toutes les distances entre les observations et les centroïdes. Cela peut devenir coûteux en temps de calcul lorsque le nombre d’observations \\(n\\) ou le nombre de variables \\(p\\) est élevé. Enfin, l’algorithme est assez sensible aux valeurs extrêmes. La moyenne est en effet influencée par les observations atypiques, ce qui peut fausser le calcul des centroïdes et engendrer des regroupements incohérents.",
    "crumbs": [
      "Modules",
      "06 - Non-supervisée",
      "$k$-means"
    ]
  },
  {
    "objectID": "contents/unsupervised/01-kmeans.html#algorithme-k-médoides",
    "href": "contents/unsupervised/01-kmeans.html#algorithme-k-médoides",
    "title": "\\(k\\)-means",
    "section": "Algorithme \\(k\\)-médoides",
    "text": "Algorithme \\(k\\)-médoides\nPour remédier à certaines des limites des \\(k\\)-moyennes, on peut utiliser une variante plus robuste : l’algorithme des \\(k\\)-médoïdes. Contrairement aux \\(k\\)-moyennes, les \\(k\\)-médoïdes utilisent des observations réelles comme représentants. Plus précisément, dans chaque groupe, le médoïde est l’observation qui minimise la somme des distances aux autres observations du même groupe.\nCette méthode présente plusieurs avantages par rapport aux \\(k\\)-moyennes. D’abord, elle est plus robuste aux valeurs extrêmes, car les médoïdes sont moins influencés par les observations atypiques que les moyennes. Ensuite, elle est compatible avec des variables ordinales ou catégorielles, à condition d’utiliser une mesure de dissimilarité appropriée. On peut aussi spécifier une matrice de dissimilarité sur mesure, adaptée à la nature des données.\nEn revanche, l’algorithme des \\(k\\)-médoïdes partage certains inconvénients avec celui des \\(k\\)-moyennes. Il faut aussi spécifier à l’avance le nombre de groupes \\(K\\). De plus, son coût computationnel est plus élevé, en particulier si l’on utilise des distances non euclidiennes ou si l’on travaille avec un grand nombre d’observations.\nExemple avec une comparison avec le \\(k\\)-means.",
    "crumbs": [
      "Modules",
      "06 - Non-supervisée",
      "$k$-means"
    ]
  },
  {
    "objectID": "contents/dimension/03-mca.html",
    "href": "contents/dimension/03-mca.html",
    "title": "Analyse des correspondances multiples",
    "section": "",
    "text": "L’analyse des correspondances multiples (ACM) peut être présentée comme un prolongement de l’AFC. Elle permet la representation graphique de tableaux de fréquences contnant plus de deux variables. Un exemple classique d’un tableau de fréquences avec plus de deux variables qualitatives est un tableau présentant les réponses d’individus à un questionnaire contenant \\(Q\\) questions à choix multiples. L’ACM est donc très utile pour visualiser les résultats d’une étude par questionnaire.\nL’ACM peut aussi être vue comme une version de l’ACP quand les variables sont mixtes, i.e. comprenant à la fois des variables quantitatives et des variables qualitatives. Le traitement conjoint de ces deux types de données repose sur leur transformation préalable appelée codage disjonctif complet.",
    "crumbs": [
      "Modules",
      "04 - Dimension",
      "Analyse des correspondances multiples"
    ]
  },
  {
    "objectID": "contents/dimension/03-mca.html#notation",
    "href": "contents/dimension/03-mca.html#notation",
    "title": "Analyse des correspondances multiples",
    "section": "Notation",
    "text": "Notation\nNotons \\(n\\) le nombre d’individus (ou d’observations) et \\(Q\\) le nombre de variables (ou de questions dans le cas d’un questionnaire). Chaque variable possède \\(J_q\\) modalités et le nombre total de modalités est égal à \\(J\\).\n\n\n\n\n\n\nAvertissementDéfinition\n\n\n\nLe tableau binaire, i.e. ne contenant que des \\(0\\) et des \\(1\\), à \\(n\\) lignes et \\(J\\) colonnes est appelé tableau de codage disjonctif complet. On le note \\(Z\\).\n\n\nAinsi, une variable n’est pas traitée telle quelle mais à travers ses modalités. Elle est découpée en modalités et tout individus est alors codé \\(1\\) pour la modalité qu’il possède et \\(0\\) dans les autres (i.e. qu’il ne possède pas, les modalités étant exclusives). Ce codage est immédiat pour des variables qualitatives. Cependant, pour une variable qualitative, on procède en découpant au préalable la variable en classes. Ainsi, chaque individu n’appartient qu’à une seule classe. Ce processus de transformation de l’information est appelé codage disjonctif complet. Il s’agit bien d’un codage, car l’information initiale est transformée, disjonctif, car tout individu possède au plus une modalité, et complet, car tout individu a au moins une modalité.\n\n\n\n\n\n\nNoteExemple\n\n\n\nPrenons par exemple un ensemble de produits avec différents types (Hoodie, Joggers et Sneakers) et différents prix. On a \\(7\\) observations (\\(7\\) produits). La variable Type est une variable qualitative et la variable Prix est une variable quantitative.\n\nJeu de données de produits.\n\n\nProduit\nType\nPrix ($)\n\n\n\n\nNike Tech Fleece\nHoodie\n256.72\n\n\nPuma Joggers\nJoggers\n221.26\n\n\nOff-White Hoodie\nHoodie\n198.45\n\n\nSupreme Hoodie\nHoodie\n235.50\n\n\nJordan 1 High\nSneakers\n298.22\n\n\nNike Dunk Low\nSneakers\n273.00\n\n\nNike Tech Fleece\nHoodie\n162.38\n\n\n\nPour coder l’information en tableau de codage disjonctif complet, on définit trois classes de prix (prix inférieur à \\(200\\$\\), prix compris entre \\(200\\$\\) et \\(250\\$\\) et prix supérieur à \\(250\\$\\)). Ainsi, on peut encoder la variable Prix grâce aux classes précédentes. Ainsi, le tableau disjonctif complet est donné par la tableau suivant.\n\nJeu de données de produits en codage disjonctif complet.\n\n\n\n\n\n\n\n\n\n\n\nProduit\nHoodie\nJoggers\nSneakers\n\\(&lt;200\\$\\)\nentre \\(200\\$\\) et \\(250\\$\\)\n\\(&gt;250\\$\\)\n\n\n\n\nNike Tech Fleece\n1\n0\n0\n0\n0\n1\n\n\nPuma Joggers\n0\n1\n0\n0\n1\n1\n\n\nOff-White Hoodie\n1\n0\n0\n1\n0\n0\n\n\nSupreme Hoodie\n1\n0\n0\n0\n1\n0\n\n\nJordan 1 High\n0\n0\n1\n0\n0\n1\n\n\nNike Dunk Low\n0\n0\n1\n0\n0\n1\n\n\nNike Tech Fleece\n1\n0\n0\n1\n0\n0\n\n\n\n\n\n\n\n\n\n\n\nAstuceRemarque\n\n\n\nLorsque l’on veut transformer des variables quantitatives en tableau de codage disjonctif complet, on perd de l’information. En effet, comme on doit découper les variables qualitatives en classes, l’appartenance à une classe est moins informatif qu’une valeur précise d’une variable. Dans l’exemple précédent, on perd de l’information sur le prix.\n\n\n\n\n\n\n\n\nImportantPropriétés\n\n\n\n\nLa somme des éléments d’une même ligne est constante et vaut \\(Q\\).\nLa somme de tous les éléments du tableau est égale à \\(nQ\\).\nLa somme des éléments d’une même colonne est égale à l’effectif \\(n_j\\) possédant la modalité \\(j\\) de la variable \\(q\\).\n\n\n\n\n\n\n\n\n\nNotePreuve\n\n\n\n\n\nComme \\(Z\\) est un tableau disjonctif complet, on a \\[\\sum_{j = 1}^{J_q} z_{ij} = 1.\\] Donc, on trouve que \\[z_{i \\bullet} = \\sum_{j = 1}^{J} z_{ij} = \\sum_{q = 1}^{Q} \\sum_{j = 1}^{J_q} z_{ij} = Q, \\]\n\\[z_{\\bullet j} = \\sum_{i = 1}^{n} z_{ij} = n_j,\\]\n\\[z_{\\bullet \\bullet} = \\sum_{i = 1}^{n} \\sum_{j = 1}^{J} z_{ij} = nQ.\\]",
    "crumbs": [
      "Modules",
      "04 - Dimension",
      "Analyse des correspondances multiples"
    ]
  },
  {
    "objectID": "contents/dimension/03-mca.html#tableau-de-burt",
    "href": "contents/dimension/03-mca.html#tableau-de-burt",
    "title": "Analyse des correspondances multiples",
    "section": "Tableau de Burt",
    "text": "Tableau de Burt\n\n\n\n\n\n\nAvertissementDéfinition\n\n\n\nLe tableau de Burt, noté \\(B\\), est le produit de la transposé de \\(Z\\) par \\(Z\\) : \\[B = Z^{\\top} Z.\\]\n\n\n\n\n\n\n\n\nImportantPropriétés de \\(B\\)\n\n\n\n\nLe tableau de Burt est carré et sa taille est égale au nombre total de modalités \\(J\\) possédées par les \\(Q\\) variables.\nLes blocs diagonaux de \\(B\\) sont eux-mêmes des matrices diagonales. Ils sont donnés par \\(B_qq = Z^{\\top}_q Z_q\\) et leurs éléments diagonaux correspondent à l’effectif de chaque modalité pour la variable \\(q\\).\nLes blocs non-diagonaux de \\(B\\) sont donnés par \\(B_{q q^\\prime} = Z^{\\top}_q Z_{q^\\prime}, q \\neq q^\\prime\\). Ils correspondent aux tableaux de contingence croisant les variables \\(q\\) et \\(q^\\prime\\).\nLe tableau de Burt est symétrique car \\(B_{q^\\prime q} = Z^{\\top}_{q^\\prime} Z_{q}\\) est la transposé de \\(B_{q q^\\prime} = Z^{\\top}_q Z_{q^\\prime}\\).\n\n\n\nD’un point de vue mathématique, l’ACM est une AFC effectuée sur la matrice logique \\(Z\\) ou sur le tableau de Burt \\(B\\). On peut démontrer que l’on obtient les mêmes facteurs, et ce, peu importe la matrice utilisé pour l’analyse.",
    "crumbs": [
      "Modules",
      "04 - Dimension",
      "Analyse des correspondances multiples"
    ]
  },
  {
    "objectID": "contents/dimension/03-mca.html#éléments-propres-du-tableau-z",
    "href": "contents/dimension/03-mca.html#éléments-propres-du-tableau-z",
    "title": "Analyse des correspondances multiples",
    "section": "Éléments propres du tableau \\(Z\\)",
    "text": "Éléments propres du tableau \\(Z\\)\nOn peut calculer les éléments propres du tableau \\(Z\\) en utilisant la même méthode que pour l’AFC. Par analogie avec l’AFC, on cherche donc les vecteurs propres de la matrice \\[S = \\frac{1}{Q} Z^{\\top} Z D_J^{-1},\\] où \\(D_J\\) est la matrice diagonale de terme \\(n_j, j = 1, \\dots, J\\). On peut calculer de la même façon les coordonnées des profils-lignes sur les axes factoriels : \\[\\Phi_k = n Z D_J^{-1} u_k,\\] où \\(u_k\\) est le \\(k\\)e vecteur propre associé à la valeur propre \\(\\lambda_k\\) de la matrice \\(S\\).\nOn peut aussi s’intéresser à l’analyse duale du tableau \\(Z\\). Toujours par analogie avec l’AFC, on cherche les vecteurs propres de la matrice \\[T = \\frac{1}{Q} Z D_J^{-1} Z^{\\top}.\\] De même, on peut calculer les coordonnées des profils-colonnes sur les axes factoriels : \\[\\Psi_k = n D_J^{-1} Z^{\\top} v_k.\\]",
    "crumbs": [
      "Modules",
      "04 - Dimension",
      "Analyse des correspondances multiples"
    ]
  },
  {
    "objectID": "contents/dimension/03-mca.html#éléments-propres-du-tableau-de-burt-b",
    "href": "contents/dimension/03-mca.html#éléments-propres-du-tableau-de-burt-b",
    "title": "Analyse des correspondances multiples",
    "section": "Éléments propres du tableau de Burt \\(B\\)",
    "text": "Éléments propres du tableau de Burt \\(B\\)\nLe tableau de Burt étant symétrique, l’analyse direct et l’analyse duale coïcident. On peut aussi l’analyse en analogie avec l’AFC. La somme des éléments d’une même ligne (ou d’une même colonne) de \\(B\\) vaut \\(Q n_j\\) et la somme des éléments de \\(B\\) est \\(n Q^2\\). On cherche les vecteurs propres de la matrice \\[S^\\prime = \\frac{1}{Q^2} B^{\\top} D_J^{-1} B D_J^{-1}.\\]\nOn remarque alors que cette matrice \\(S^\\prime\\) a les mêmes vecteurs propres que la matrice \\(S\\). En effet, \\[S^\\prime = \\frac{1}{Q^2} B^{\\top} D_J^{-1} B D_J^{-1} = \\frac{1}{Q^2} Z^{\\top} Z D_J^{-1} Z^{\\top} Z D_J^{-1}.\\] Et soit \\(u\\) et \\(\\lambda\\) vérifiant \\(Z^{\\top} Z D_J^{-1} = \\lambda u\\), alors \\[ Z^{\\top} Z D_J^{-1} Z^{\\top} Z D_J^{-1} u = Z^{\\top} Z D_J^{-1} \\lambda u = \\lambda^2 u.\\]\nFinalement, l’analyse de \\(Z\\) ou de \\(B\\) fournit les mêmes vecteurs propres et pour tout \\(k = 1, \\dots, Q\\), la \\(k\\)e valeur propre de \\(B\\) est la carré de la \\(k\\)e valeur propre de \\(Z\\).",
    "crumbs": [
      "Modules",
      "04 - Dimension",
      "Analyse des correspondances multiples"
    ]
  },
  {
    "objectID": "contents/dimension/03-mca.html#lencodage-des-variables",
    "href": "contents/dimension/03-mca.html#lencodage-des-variables",
    "title": "Analyse des correspondances multiples",
    "section": "L’encodage des variables",
    "text": "L’encodage des variables\nL’encodage des variables, et en particulier le choix des bornes des classes, est primordiale en ACM. Pour les variables continues, les bornes devraient être pertinentes au regard du problème étudié. Par exemple, on ne va pas définir une classe \\(&gt; 1000\\$\\) dans l’exemple précédent. Pour obtenir des bornes pertinentes, on peut regarder les distributions des variables, e.g. avec un histogramme. Dans certains cas particuliers, il est possible de découper la variable en modalités d’effectifs égaux. Cependant, cette approche peut conduire à des modalités peu pertinentes.\nDans le cas de variables qualitatives, le choix des classes ne se pose pas; il est donné par la variable. Cependant, les modalités “naturelles” peuvent conduire à des effectifs (très) déséquilibrés. Dans ce cas, on doit généralement procéder à des regroupements. Ici encore, une bonne connaissance du domaine étudiée est nécessaire. En tout cas, on préferera faire des regroupements de modalités, plutôt que répartir de manière aléatoire les modalités à effectif faible dans les autres modalités (ce qui est parfois proposé dans les logiciels).",
    "crumbs": [
      "Modules",
      "04 - Dimension",
      "Analyse des correspondances multiples"
    ]
  },
  {
    "objectID": "contents/dimension/01-pca.html",
    "href": "contents/dimension/01-pca.html",
    "title": "Analyse en composantes principales",
    "section": "",
    "text": "Travailler avec un grand nombre de variables peut poser plusieurs problèmes pratiques et théorique :\n\nVisualisation compliquée : il est impossible de représenter visuellement des données au-delà de 3 dimensions.\nSéparation des classes difficile : dans des problèmes de classification, la séparation entre les groupes peut être cachée dans une combinaison de variables plutôt que dans les variables prises individuellement.\nCoût computationnel élévé : des modèles complexes peuvent devenir difficiles à ajuster lorsque le nombre de variables est grand.\nCorrélations fortes : des variables redondantes rendent les modèles instables ou difficiles à interpréter.\n\nLa question naturelle à se poser est donc : peut-on réduire la dimension du jeu de données sans perdre trop d’information ?\nRéduire la dimension, ce n’est pas simplement la suppression de variables. En effet, cela risquerait de faire disparaître de l’information pouvant être utile au modèle. Une meilleure approche consiste à construire de nouvelles variables, obtenues comme combinaisons linéaires des variables initiales, qui résument l’information essentielle du jeu de données. Une méthode possible pour cela est l’Analyse en Composantes Principales (ACP).",
    "crumbs": [
      "Modules",
      "04 - Dimension",
      "Analyse en composantes principales"
    ]
  },
  {
    "objectID": "contents/dimension/01-pca.html#pourquoi-changer-de-dimension",
    "href": "contents/dimension/01-pca.html#pourquoi-changer-de-dimension",
    "title": "Analyse en composantes principales",
    "section": "",
    "text": "Travailler avec un grand nombre de variables peut poser plusieurs problèmes pratiques et théorique :\n\nVisualisation compliquée : il est impossible de représenter visuellement des données au-delà de 3 dimensions.\nSéparation des classes difficile : dans des problèmes de classification, la séparation entre les groupes peut être cachée dans une combinaison de variables plutôt que dans les variables prises individuellement.\nCoût computationnel élévé : des modèles complexes peuvent devenir difficiles à ajuster lorsque le nombre de variables est grand.\nCorrélations fortes : des variables redondantes rendent les modèles instables ou difficiles à interpréter.\n\nLa question naturelle à se poser est donc : peut-on réduire la dimension du jeu de données sans perdre trop d’information ?\nRéduire la dimension, ce n’est pas simplement la suppression de variables. En effet, cela risquerait de faire disparaître de l’information pouvant être utile au modèle. Une meilleure approche consiste à construire de nouvelles variables, obtenues comme combinaisons linéaires des variables initiales, qui résument l’information essentielle du jeu de données. Une méthode possible pour cela est l’Analyse en Composantes Principales (ACP).",
    "crumbs": [
      "Modules",
      "04 - Dimension",
      "Analyse en composantes principales"
    ]
  },
  {
    "objectID": "contents/dimension/01-pca.html#analyse-en-composantes-principales",
    "href": "contents/dimension/01-pca.html#analyse-en-composantes-principales",
    "title": "Analyse en composantes principales",
    "section": "Analyse en composantes principales",
    "text": "Analyse en composantes principales\nL’ACP est une méthode non-supervisée (sans variables à expliquer) permettant de réduire la dimension d’un jeu de données tout en conservant le plus d’information possible. Cette méthode est utilisée lorsque l’on dispose de \\(n\\) observations de \\(p\\) variables numériques continues avec \\(p\\) trop “grand” pour permettre une modélisation ou une visualisation efficace. La méthode a été introduite par dans Hotelling (1933).\n\n\n\n\n\n\nAstuceApplications courantes\n\n\n\n\nVisualisation d’un jeu de données multidimensionnelles.\nRéduction du nombre de variables de \\(p\\) à \\(k \\ll p\\) pour faciliter la construction de modèle.\nCompression d’images ou de signaux.\nExploration de données biologiques, textuelles ou environnementales.\n\n\n\n\n\n\n\n\n\nNoteExemples\n\n\n\n\nComparer des équipes de hockey sur la base de six statistiques de fin de saison.\nRésumer la criminalité entre les provinces canadiennes sur la base des taux de sept types de crimes différents.\nCompresser des images formées de \\(1084 \\times 1084\\) pixels en quelques variables.\nIdentifier le nombre de variantes d’un type de tumeur à partir du degré d’expression de millions de gènes.\n\n\n\n\nFormulation mathématique\nSoit un vecteur aléatoire composé de \\(p\\) variables \\(X = \\left( X_{1}, \\dots, X_p \\right)^{\\top}\\), centré et ayant comme matrice de variance-covariance \\(\\Sigma\\). Notons \\(\\alpha_{1} = \\left( \\alpha_{11}, \\dots, \\alpha_{1p} \\right)^{\\top}\\), un vecteur de coefficients. On cherche une combinaison linéaire\n\\[Y_{1} = \\alpha_{1}^{\\top} X = \\sum_{k = 1}^{p} \\alpha_{1k}X_k,\\]\ntelle que la variance de \\(Y_{1}\\) soit maximale. L’idée est simple: on désire combiner \\(p\\) variables en une seule, mais en “capturant” la plus grande partie possible de la variabilité.\nIl faut d’abord ajouter une contrainte sur \\(\\alpha_{1}\\), puisque sinon on n’aurait qu’à prendre \\(\\alpha_{1k} = \\pm \\infty\\) et on aurait \\(\\mathrm{Var}(Y_{1}) = +\\infty\\) ce qui est définitivement maximal ! On contraint donc \\(\\alpha_{1}\\) de sorte qu’il ait une norme égale à \\(1\\).\nCela revient à calculer : \\[\\max_{\\alpha_1^\\top \\alpha_1 = 1} \\mathrm{Var}(Y_1) = \\max_{\\alpha_1^\\top \\alpha_1 = 1} \\alpha_1^\\top \\Sigma \\alpha_{1}.\\]\nCe problème se résout par les multiplicateurs de Lagrange. Il conduit à l’équation \\[\\Sigma \\alpha_1 = \\lambda_{1} \\alpha_{1},\\] où \\(\\lambda_{1}\\) est la plus grande valeur propre de \\(\\Sigma\\) et \\(\\alpha_{1}\\) le vecteur propre associé.\nOn définit ainsi la première composante principale. On construit les suivantes en imposant des conditions d’orthogonalité (indépendance linéaire) avec les précédentes, ce qui revient à chercher les vecteurs propres suivants : \\[\\Sigma \\alpha_k = \\lambda_k \\alpha_k, \\quad \\text{avec}\\quad \\lambda_{1} \\geq \\lambda_2 \\geq \\dots \\lambda_p.\\] Les composantes principales sont donc données par \\[Y_k = \\alpha_k^\\top X, \\quad\\text{avec } \\alpha_k \\text{ vecteur propre associé à } \\lambda_k.\\]\n\n\n\n\n\n\nAstuceRemarque\n\n\n\nSi \\(\\lambda_{1} &gt; \\dots &gt; \\lambda_{p}\\), alors les composantes principales sont uniques, à un signe près.\n\n\n\n\n\n\n\n\nNotePreuve\n\n\n\n\n\nOn cherche à calculer \\[\\max_{\\alpha_1^\\top \\alpha_1 = 1} \\mathrm{Var}(Y_1) = \\max_{\\alpha_1^\\top \\alpha_1 = 1} \\alpha_1^\\top \\Sigma \\alpha_{1}.\\]\nLe problème est donc de maximiser \\[F(\\alpha_{1}) = \\alpha_{1}^\\top \\Sigma \\alpha_1, \\quad\\text{s.c.}\\quad \\alpha_1^\\top \\alpha_1 = 1.\\]\nOn peut récrire ce problème à l’aide des multiplicateurs de Lagrange, soit maximiser \\[F(\\alpha_1, \\lambda) = \\alpha_1^\\top \\Sigma \\alpha_1 - \\lambda (\\alpha_1^\\top \\alpha_1 -1 ),\\] où \\(\\lambda\\) est un multiplicateur de Lagrange.\nPour solutionner ce problème, on dérive \\(F\\) par rapport à \\(\\alpha_{1}\\) et à \\(\\lambda\\).\n\\[\\begin{cases}\n\\frac{\\partial F(\\alpha_{1}, \\lambda)}{\\partial \\alpha_{1}} = 2 \\Sigma \\alpha_{1} - 2 \\lambda \\alpha_{1},\\\\\n\\frac{\\partial F(\\alpha_{1}, \\lambda)}{\\partial \\lambda} = 1 - \\alpha_{1}^\\top \\alpha_{1}.\n\\end{cases}\n\\]\nEnsuite, on égalise à \\(0\\), ce qui donne: \\[\\begin{cases}\n\\Sigma \\alpha_{1} = \\lambda \\alpha_{1} \\\\\n\\alpha_{1}^\\top \\alpha_{1} = 1\n\\end{cases}\n.\\] La seconde équation est bien entendue notre contrainte. La première équation est celle qui nous intéresse. En utilisant cette équation et la définition des éléments propres, on déduit que\n\n\\(\\alpha_{1}\\) est un vecteur propre (normé) de \\(\\Sigma\\);\n\\(\\lambda\\) est la valeur propre correspondante.\n\nOn a donc que \\[\\mathrm{Var}(Y_{1}) = \\alpha_{1}^\\top \\Sigma \\alpha_{1} = \\lambda \\alpha_{1}^\\top \\alpha_1 = \\lambda.\\] Puisque l’on veut maximiser cette quantité, on conclut que:\n\n\\(\\lambda = \\lambda_{1}\\), la plus grande valeur propre de \\(\\Sigma\\);\n\\(\\alpha_{1}\\), le vecteur propre normé correspondant.\n\nOn continue ensuite avec le calcul de la deuxième composante. Ici, on poursuit simultanément deux objectifs:\n\nConserver le maximum de variation présente dans \\(X\\);\nSimplifier la structure de dépendance pour faciliter l’interprétation et assurer la stabilité numérique d’éventuelles méthodes qui utiliseront les composantes principales obtenues.\n\nÉtant donné \\(Y_{1}\\), la deuxième composante principale \\(Y_{2} = \\alpha_{2}^\\top X\\) est définie telle que\n\n\\(\\mathrm{Var}(Y_{2}) = \\alpha_{2}^\\top \\Sigma \\alpha_{2}\\) est maximale;\n\\(\\alpha_{2}^\\top \\alpha_{2} = 1\\);\n\\(\\mathrm{Cov}(Y_{1}, Y_{2}) = 0\\).\n\nOr, on a que \\[\\mathrm{Cov}(Y_{1}, Y_{2}) = \\mathrm{Cov}(\\alpha_{1}^\\top X, \\alpha_{2}^\\top X) = \\alpha_{1}^\\top \\Sigma \\alpha_{2} = \\alpha_{2}^\\top \\Sigma \\alpha_{1} = \\lambda_{1} \\alpha_{2}^\\top \\alpha_{1}.\\]\nOn cherche donc le vecteur \\(\\alpha_{2}\\) qui maximise: \\[F(\\alpha_{2}, \\lambda, \\kappa) = \\alpha_{2}^\\top \\Sigma \\alpha_{2} - \\lambda (\\alpha_{2}^\\top \\alpha_{2} - 1) - \\kappa (\\alpha_{2}^\\top \\alpha_{1} - 0).\\]\nDe même que pour la première composante, on dérive \\(F\\) par rapport à \\(\\alpha_{2}\\), \\(\\lambda\\) et \\(\\kappa\\).\n\\[\\begin{cases}\n\\frac{\\partial F(\\alpha_{2}, \\lambda, \\kappa)}{\\partial \\alpha_{2}} = 2 \\Sigma \\alpha_{2} - 2 \\lambda \\alpha_{2} - \\kappa \\alpha_1 \\\\\n\\frac{\\partial F(\\alpha_{2}, \\lambda, \\kappa)}{\\partial \\lambda} = 1 - \\alpha_{2}^\\top \\alpha_{2} \\\\\n\\frac{\\partial F(\\alpha_{2}, \\lambda, \\kappa)}{\\partial \\kappa} = - \\alpha_2^\\top \\alpha_1\n\\end{cases}\n\\]\nEn égalisant les équations à \\(0\\), on retrouve les deux équations des contraintes, ainsi que\n\\[2 \\Sigma \\alpha_{2} - 2 \\lambda \\alpha_{2} - \\kappa \\alpha_{1} = 0.\\]\nEn multipliant cette équation à gauche et à droite par \\(\\alpha_{1}^\\top\\), on trouve\n\\[2 \\alpha_{1}^\\top \\Sigma \\alpha_{2} - 2 \\alpha_{1}^\\top \\lambda \\alpha_{2} - \\kappa \\alpha_{1}^\\top \\alpha_{1} = 0.\\]\nOr \\(\\alpha_{1}^\\top \\Sigma = \\lambda_{1} \\alpha_{1}^\\top\\), et \\(\\alpha_{1}^\\top \\alpha_{1} = 1\\), donc \\[2 \\alpha_{1}^\\top \\lambda \\alpha_{2} - 2 \\alpha_{1}^\\top \\lambda \\alpha_{2} - \\kappa \\alpha_{1}^\\top \\alpha_{1} = 0 \\implies - \\kappa = 0.\\]\nEn substituant ce résulat, on obtient \\[\\Sigma \\alpha_{2} = \\lambda \\alpha_{2}.\\]\net donc \\(\\lambda\\) est une autre valeur propre de \\(\\Sigma\\). Puisque \\[\\mathrm{Var}(Y_{2}) = \\alpha_{2}^\\top \\Sigma \\alpha_{2} = \\alpha_{2}^\\top \\lambda \\alpha_{2} = \\lambda,\\] on a que cette variance est maximale si \\(\\lambda = \\lambda_{2}\\), la deuxième plus grande valeur propre de \\(\\Sigma\\), et conséquemment \\(\\alpha_{2}\\) est le vecteur propre normé correspondant.\nOn peut généraliser ce résultat en utilisant des maximisations successives. On en conclut que \\[Y_k = \\alpha_k^\\top X,\\] où \\(\\alpha_k\\) est le vecteur propre normé associé à \\(\\lambda_k\\), la \\(k\\)e plus grande valeur propre de \\(\\Sigma\\).\n\n\n\nIl est possible d’avoir une représentation plus compacte de l’ACP à l’aide de matrices. Soit \\(A = \\left( \\alpha_{1}, \\dots, \\alpha_p \\right) \\in \\mathbb{R}^{p \\times p}\\), la matrice dont les colonnes sont les vecteurs propres. On a \\(Y = AX\\) et la covariance des composantes principales s’écrit \\[\\mathrm{Var}(Y) = A^\\top \\Sigma A.\\]\n\n\n\n\n\n\nImportantPropriétés de \\(A\\)\n\n\n\n\n\\(A^{\\top} A = A A^{\\top} = I_p\\);\n\\(A^{\\top} = A^{-1}\\);\n\\(\\Sigma A = A \\Lambda\\), où \\(\\Lambda = \\text{diag}(\\lambda_{1}, \\dots, \\lambda_p)\\);\n\\(\\mathrm{Var}(Y) = A^{\\top} \\Sigma A = \\Lambda \\implies \\mathrm{Cov}(Y_k, Y_l) = 0\\) si \\(k \\neq l\\) et \\(\\mathrm{Var}(Y_k) = \\lambda_k \\geq \\mathrm{Var}(Y_l) = \\lambda_l\\) si et seulement si \\(k \\geq l\\).\n\n\n\n\n\n\n\n\n\nNotePreuves\n\n\n\n\n\n\nPar construction, les colonnes de \\(A\\) sont orthogonaux deux à deux et de norme \\(1\\) (cf. contraintes sur les vecteurs propres). La matrice \\(A\\) est donc une matrice orthogonale. Et donc \\(A^{\\top} A = A A^{\\top} = I_p\\).\nDe même, comme \\(A\\) est orthogonal, on a \\(A^{\\top} = A^{-1}\\).\nLe résultat est immédiat en faisant le produit de matrices.\nOn a \\(\\mathrm{Var}(Y) = A^{\\top} \\Sigma A = A^{\\top} A \\Lambda = \\Lambda\\). Comme \\(\\Lambda\\) est une matrice diagonale, \\(\\mathrm{Cov}(Y_k, Y_l) = 0\\) si \\(k \\neq l\\) (car pas sur la diagonale) et comme \\(\\lambda_{1} \\geq \\dots \\geq \\lambda_p\\), on a \\(\\mathrm{Var}(Y_1) \\geq \\dots \\geq \\mathrm{Var}(Y_p)\\).\n\n\n\n\nUne mesure globale de la variation presente dans les données est donnée par la trace de la matrice \\(\\Sigma\\): \\[\\text{tr}(\\Sigma) = \\text{tr}(\\Lambda) = \\sum_{i = 1}^{p} \\lambda_i = \\sum_{k = 1}^{p} \\mathrm{Var}(Y_k).\\]\nLa proportion de variation expliquée par la composante principale \\(Y_k\\) est donc donnée par le ratio entre la valeur propre \\(k\\) et la somme des valeurs propres : \\[\\frac{\\lambda_k}{\\lambda_{1} + \\cdots + \\lambda_p} = \\frac{\\mathrm{Var}(Y_k)}{\\text{tr}(\\Sigma)}.\\]\nDe façon similaire, les \\(m\\) premières composantes expliquent \\[100\\% \\times \\frac{\\sum_{k = 1}^{m} \\lambda_k}{\\sum_{k = 1}^{p} \\lambda_k} = 100\\% \\times \\frac{\\sum_{k = 1}^{m} \\mathrm{Var}(Y_k)}{\\sum_{k = 1}^{p} \\mathrm{Var}(Y_k)}\\] de la variabilité dans les variables.",
    "crumbs": [
      "Modules",
      "04 - Dimension",
      "Analyse en composantes principales"
    ]
  },
  {
    "objectID": "contents/dimension/01-pca.html#pratique-de-lacp",
    "href": "contents/dimension/01-pca.html#pratique-de-lacp",
    "title": "Analyse en composantes principales",
    "section": "Pratique de l’ACP",
    "text": "Pratique de l’ACP\n\nEstimation de la matrice de variance-covariance\nEn pratique, la matrice de variance-covariance \\(\\Sigma\\) est inconnue. Pour faire une ACP, il est nécessaire d’estimer \\(\\Sigma\\) à partir d’un échantillon aléatoire \\(X_{1}, \\dots, X_n\\) de réalisation indépendante de \\(X\\). Un estimateur (sans biais) de \\(\\Sigma\\) est donné par \\[\\widehat{\\Sigma} = \\frac{1}{n - 1} \\sum_{i = 1}^{n} \\left( X_i - \\overline{X} \\right)\\left( X_i - \\widehat{X} \\right)^{\\top},\\] où \\(\\overline{X}\\) est la moyenne empirique de l’échantillon.\nLa matrice \\(\\widehat{\\Sigma}\\) ainsi obtenue est symétrique à coefficients réels donc diagonalisable. Elle admet une décomposition spectrale de la forme \\[\\widehat{\\Sigma} = \\widehat{A} \\widehat{\\Lambda} \\widehat{A}^{\\top},\\] où \\(\\widehat{A}\\) est une matrice orthogonale dont les colonnes sont les estimateurs des vecteurs propres de \\(\\Sigma\\) et \\(\\widehat{\\Lambda}\\) est une matrice diagonale contenant les estimateurs des valeurs propres de \\(\\Sigma\\), supposées ordonnées de façon décroissante.\nLes composantes principales sont obtenues en projetant les observations \\(X_i\\) dans la base des vecteurs propres : \\[Y_i = \\widehat{A}^{\\top} X_i.\\]\n\n\nQuelques remarques\n\n\n\n\n\n\nAstuceSensibilité à l’échelle de \\(X_1, \\dots, X_p\\)\n\n\n\nPuisque l’on cherche une combinaison linéaire de \\(X_{1}, \\dots, X_p\\) qui maximise la variance, une variable \\(X_k\\) ayant une grande variance aura un poids démesuré dans les composantes principales, ce qui peut fausser l’interprétation. On peut, par exemple, penser à la mesure de distance. En effet, exprimer une distance entre mètres plutôt qu’en kilomètres multiplierait la variance de cette variable par \\(1\\) million (\\((10^3)^2\\)). Cette variable aurait donc un poids majeur dans toutes les composantes.\nAinsi, si les variables sont exprimées dans des unités différentes ou présentent des ordres de grandeurs très variés, il est recommandé de standardiser les variables avant d’effectuer une ACP. Cela revient à effectuer une ACP sur la matrice des corrélations.\n\n\n\n\n\n\n\n\nAstucePremière étape dans une analyse prédictive\n\n\n\nIl arrive que l’ACP soit effectuée parce que l’on veuille prédire la valeur de variable \\(Z\\) à partir des valeurs de variables \\(X_{1}, \\dots, X_p\\) mais que \\(p\\) soit simplement trop grand. Dans ce cas, on applique l’ACP pour obtenir les \\(k \\ll p\\) premières composantes principales \\(Y_1, \\dots, Y_k\\) et on les utilise pour prédire \\(Z\\).\nPuisque les composantes principales retiennent la majeure partie de l’information contenue dans les variables originales, c’est généralement une façon raisonnable de faire.\n\n\n\n\n\n\n\n\nAstuceRotation des axes et qualité de représentation\n\n\n\nPuisque la matrice \\(A\\) est orthogonale, le produit matriciel \\(Y = A^{\\top}X\\) représente une rotation de l’espace des variables. Les nouveaux axes correspondent aux directions orthogonales de variation maximale successives, en supposant que \\(\\lambda_{1} &gt; \\dots &gt; \\lambda_p\\). Ainsi, \\(Y_i = A^{\\top} X_i\\) donne les coordonnées de l’observation \\(X_i\\) dans le nouveau système d’axes. On appelle \\(Y_{ik}\\) le score de l’observation \\(X_i\\) sur l’axe principal \\(k\\) et se calcule comme \\[Y_{ik} = \\alpha_k^\\top X_i = \\sum_{l = 1}^{p} \\alpha_{kl}X_{il}.\\]\nLa qualité de la représentation de l’observation \\(i\\) sur l’axe \\(k\\) est donnée par \\[Q_{ik} = \\frac{Y_{ik}^2}{d^2(0, Y_i)} = \\frac{Y_{ik}^2}{\\sqrt{Y_{i1}^2 + \\dots + Y_{i p}^2}}.\\]\n\n\n\n\nChoix du nombre de composantes\nUn enjeu central en ACP est de choisir combien de composantes principales retenir. En conserver trop ne réduit pas la dimension et en conserver trop peu peut faire perdre de l’information pertinente. Voici les principales règles empiriques utilisées :\n\nRègle des \\(80\\%\\) : Retenir le nombre minimal de composantes nécessaires pour expliquer au moins \\(80\\%\\) de la variance totale. Ce seuil est arbitraire, mais il donne souvent une bonne intuition.\nRègle de Kaiser : Si l’ACP est faite à partir de la matrice des corrélations, alors la moyenne des valeurs propres vaut \\(1\\). On recommande de ne garder que les composantes ayant une valeur propre supérieure à la moyenne des valeurs propres, soit \\(1\\).\nRègle de Joliffe : Variante plus stricte de la règle de Kaiser, qui suggère de conserver les composantes avec une valeur propre supérieure à \\(0.7\\) pour une ACP faite avec la matrice des corrélations.\nRègle de Cattell (ou du coude) : On trace les valeurs propres \\(\\lambda_k\\) en fonction de leur rang \\(k\\) et on cherche un point de rupture dans la décroissance. Au-delà de ce point, les composantes supplémentaires expliquent peu de variance supplémentaire.\n\nCes règles sont des outils d’aide à la décision, mais le choix du nombre de composantes dépend aussi du contexte, des objectifs de l’analyse, et de la facilité d’interprétation.",
    "crumbs": [
      "Modules",
      "04 - Dimension",
      "Analyse en composantes principales"
    ]
  },
  {
    "objectID": "contents/generalities/02-spaces.html",
    "href": "contents/generalities/02-spaces.html",
    "title": "Espaces",
    "section": "",
    "text": "Avant de pouvoir modéliser ou analyser des données, il est fondamental de bien comprendre la nature des variables que l’on manipule. En effet, le type de variables détermine :\nDans cette section, nous présentons les types de variables les plus courants, ainsi que les espaces associés.",
    "crumbs": [
      "Modules",
      "03 - Généralités",
      "Espaces"
    ]
  },
  {
    "objectID": "contents/generalities/02-spaces.html#unité-statistique",
    "href": "contents/generalities/02-spaces.html#unité-statistique",
    "title": "Espaces",
    "section": "Unité statistique",
    "text": "Unité statistique\nUne unité statistique est l’élément de base sur lequel une observation est effectué. Moralement, c’est le “porteur” de l’information qui est utilisé pour déterminer le niveau d’agrégation de l’analyse. L’unité statistique est un choix du modélisateur.\n\n\n\n\n\n\nNoteExemples\n\n\n\n\nDans le cas d’une enquête sur les revenus, on peut choisir l’individu comme unité.\nDans le cas d’une étude sur les classes d’un lycée, on peut choisir la classe comme unité.\nDans le cas d’une base de données d’imagerie médicale, on choisir l’image comme unité.\n\n\n\n\n\n\n\n\n\nMise en gardeParfois, une même base de données peut être analysée à plusieurs niveaux.\n\n\n\nUne image est constituée de pixels, chacun pouvant être décrit par des variables numériques (e.g. valeurs RVB, opacité, …). On peut choisir d’analyser chaque pixel, et donc prendre le pixel pour unité, ou bien analyser chaque image comme un tout, et donc prendre l’image comme unité.",
    "crumbs": [
      "Modules",
      "03 - Généralités",
      "Espaces"
    ]
  },
  {
    "objectID": "contents/generalities/02-spaces.html#types-de-variables",
    "href": "contents/generalities/02-spaces.html#types-de-variables",
    "title": "Espaces",
    "section": "Types de variables",
    "text": "Types de variables\nOn distingue généralement quatre types de variables, que l’on identifie au niveau de la plus petite unité statistique du jeu de données.\n\n\n\n\n\n\nAvertissementVariable numérique (ou quantitative)\n\n\n\nUne variable numérique (ou quantitative) est une variable dont les valeurs sont des nombres représentant une quantité mesurable.\nExemples: revenu en dollars, masse, âge, …\n\n\n\n\n\n\n\n\nAvertissementVariable ordinale\n\n\n\nUne variable ordinale est une variable qualitative (ou catégorielle) dont les modalités peuvent être ordonnées naturellement, sans que l’écart entre les modalités soit quantifiable.\nExemples: niveau de revenu (faible, moyen ou élevé), niveau de satisfaction (“tout-à-fait en désaccord”, “en désaccord”, “pas d’avis”, “d’accord”, “tout-à-fait d’accord”), …\n\n\n\n\n\n\n\n\nAvertissementVariable nominale symétrique\n\n\n\nUne variable nominale symétrique est une variable qualitative (ou catégorielle) dont toutes les modalités sont aussi informatives l’une que l’autre.\nExemples: nationalité, filière de formation, …\n\n\n\n\n\n\n\n\nAvertissementVariable nominale asymétrique\n\n\n\nUne variable nominale asymétrique est une variable qualitative (ou catégorielle) dont l’une des modalités a un statut particulier, souvent plus fréquente ou considérée comme la valeur “par défaut”. Ainsi, avoir deux observations avec la valeur “par défaut” de cette variable nominale asymétrique ne nous apprend pas grand chose sur celles-ci; alors que on peut retirer beaucoup plus d’information de deux observations qui n’ont pas la valeur “par défaut”.\nExemples: présence ou absence d’un symptôme, transaction frauduleuse ou non, …\n\n\nBien que ces types de variables soient les plus communs, on peut trouver beaucoup d’autres types de variables. Par exemple, on peut s’intéresser à de la comparaison de courbes, de textes, d’images, de réseaux, etc. Dans ces situations, le choix de la représentation dépend du niveau auquel on souhaite se placer, et donc de l’unité statistique.",
    "crumbs": [
      "Modules",
      "03 - Généralités",
      "Espaces"
    ]
  },
  {
    "objectID": "contents/generalities/02-spaces.html#espaces-associés",
    "href": "contents/generalities/02-spaces.html#espaces-associés",
    "title": "Espaces",
    "section": "Espaces associés",
    "text": "Espaces associés\nUne fois que nos données ont été collectés, la première étape d’une analyse statistique consiste à choisir un espace mathématique dans lequel travailler. Cette espace, que l’on appelle parfois espace d’observation et que l’on note \\(\\mathcal{X}\\), dépend du type de données observées. Il constitue le cadre formel dans lequel nos variables prennent leurs valeurs, et il guide les choix méthodologiques qui suivront.\n\n\n\n\n\n\nNoteCas d’une variable numérique\n\n\n\nLorsque l’on observe un variable numérique (e.g. la température d’un pays), l’espace naturel dans lequel travailler est l’ensemble des réels, \\(\\mathcal{X} = \\mathbb{R}\\). Dans certains cas, on peut restreindre cet espace à un intervalle spécifique. Par exemple, si on s’intéresse à la taille d’une personne, on peut prendre \\(\\mathcal{X} = [0, +\\infty)\\) car la variable considérée ne peut pas être négative.\n\n\n\n\n\n\n\n\nNoteCas d’une variable nominale (ou qualitative, ou catégorielle)\n\n\n\nPour une variable nominal, l’espace est un ensemble fini de modalité, l’ensemble des modalités prises par la variable. Par exemple, si on étudie les résultats d’un lancer de dés, la variable peut prendre les valeurs \\(1\\) à \\(6\\), et l’espace associé sera donc \\(\\mathcal{X} = \\{ 1, 2, 3, 4, 5, 6 \\}\\).\n\n\nLorsque les données sont plus conplexes, il faut choisir des espaces plus adaptés. Pour de l’analyse de courbes ou de signaux, on peut travailler dans un espace de fonctions. Par exemple, on peut considérer l’espace des fonctions continues sur un intervalle fermé \\([a, b]\\), noté \\(\\mathcal{X} = \\mathcal{C}([a, b])\\). Pour de l’analyse de texte (vu comme une séquence de caractères), l’espace de travail peut être un alphabet. Par exemple, on peut considérer \\(\\mathcal{X} = \\{ \\text{A}, \\text{B}, \\dots, \\text{Z} \\}\\).\nSouvent, on observe plusieurs variables en même temps, e.g. la taille, le poids et le sexe d’un individus. Dans ce cas, l’espace d’observation sera le produit cartésien (aussi appelé ensemble produit) des espaces associés à chaque variable : \\[\\mathcal{X} = \\mathcal{X}_1 \\times \\mathcal{X}_2 \\times \\dots \\mathcal{X}_p,\\] où \\(p\\) est le nombre de variables. Dans le cas où on observe \\(p\\) variables numérique, on notera plus simplement \\(\\mathcal{X} = \\mathbb{R}^p\\)",
    "crumbs": [
      "Modules",
      "03 - Généralités",
      "Espaces"
    ]
  },
  {
    "objectID": "contents/generalities/01-stat.html",
    "href": "contents/generalities/01-stat.html",
    "title": "Projet d’analyse de données",
    "section": "",
    "text": "On présente ici les différentes étapes d’un projet d’analyse de données.",
    "crumbs": [
      "Modules",
      "03 - Généralités",
      "Projet d'analyse de données"
    ]
  },
  {
    "objectID": "contents/generalities/01-stat.html#projet-danalyse-données",
    "href": "contents/generalities/01-stat.html#projet-danalyse-données",
    "title": "Projet d’analyse de données",
    "section": "Projet d’analyse données",
    "text": "Projet d’analyse données\nUn projet d’analyse de données suit généralement une structure bien définie en plusieurs étapes. Nous en dégagons ici cinq principales, chacune avec ses objectifs, ses enjeux et son importance relative dans le succès du projet :\n\nDéfinition des objectifs\nCollecte et préparation des données\nÉlaboration et validation des modèles\nImplémentation et mise en production\nSuivi de la performance et amélioration continue\n\nLors de la planification d’un projet, il faut prendre en compte que chaque étape à une importance différente, mais aussi que chacune ne prend pas le même temps d’exécution. Pyle (1999) donne une estimation du temps de chaque étape, ainsi que de leur importance dans la réussite du projet (donné en pourcentage du total, cf. Table 1).\n\n\n\nTable 1: Découpage d’un projet d’analyse des données.\n\n\n\n\n\nÉtape\nTemps\nImportance\n\n\n\n\nComprendre le problème\n\\(10\\%\\)\n\\(15\\%\\)\n\n\nExplorer la solution\n\\(9\\%\\)\n\\(14\\%\\)\n\n\nImplementer la solution\n\\(1\\%\\)\n\\(51\\%\\)\n\n\nPréparer les données\n\\(60\\%\\)\n\\(15\\%\\)\n\n\nAnalyser les données\n\\(15\\%\\)\n\\(3\\%\\)\n\n\nModéliser les données\n\\(5\\%\\)\n\\(2\\%\\)\n\n\n\n\n\n\nOn remarque deux faits importants. L’importance d’une étape n’est pas proportionnelle au temps passé dessus. Par exemple, l’implémentation de la solution est une étape essentielle (sinon il n’y a pas de résultat), mais peut n’exiger que peu de temps (parfois quelques lignes de code). À l’inverse, la préparation des données, souvent sous-estimée, est généralement chronophage, notamment pour gérer les données manquantes, les données aberrantes, ou encore les éventuels accents pour des données en français.",
    "crumbs": [
      "Modules",
      "03 - Généralités",
      "Projet d'analyse de données"
    ]
  },
  {
    "objectID": "contents/generalities/01-stat.html#définition-des-objectifs",
    "href": "contents/generalities/01-stat.html#définition-des-objectifs",
    "title": "Projet d’analyse de données",
    "section": "Définition des objectifs",
    "text": "Définition des objectifs\nToute analyse commence par une question claire : que cherche-t-on à accomplir ? Visualiser des données ? Tester une hypothèse ? Prédire un comportement ? Segmenter une population ? Une définition précise des objectifs est essentielle pour orienter les étapes suivantes. Cela permet de guider la collecte et la structuration des données. Cela permet de définir un modèle adéquat (e.g. classification, régression, …). Cela permet de faciliter l’interprétation et la communication des résultats. Cette phase évite aussi les explorations aveugles et les interprétations biaisées.\nComment fait-on en pratique pour formuler un bon objectif ? On pose des questions ! Tout d’abord, il faut clarifier les termes. Qui va utiliser le modèle et comment ? Quelle est la population cible ? Quelle décision dépendra résultats ?\n\n\n\n\n\n\nNoteExemple\n\n\n\nLa Banque National du Canada voudrait lancer un nouveau produit d’épargne et vous donne accès à sa base de données clients.\nMauvais objectif: Analyser les données de la base clients.\nMeilleur objectif: Peut-on prédire quels clients sont susceptibles d’acheter ce nouveau produit d’épargne ?\n\n\n\n\n\n\n\n\nNoteExemple\n\n\n\nL’équipe de hockey des Canadiens de Montréal souhaite mieux connaître ses adversaires pour développer de nouvelles tactiques de jeu.\nMauvais objectif: Analyser les données des adversaires.\nMeilleur objectif: Peut-on caractériser le style de jeu des adversaires pour identifier leurs faiblesses ?\n\n\n\n\n\n\n\n\nNoteExemple\n\n\n\nPharmascience souhaite évaluer l’efficacité d’un nouveau médicament.\nMauvais objectif: Analyser les données du médicament.\nMeilleur objectif: Peut-on concevoir un protocole statistique permettant de tester l’efficacité du médicament ?",
    "crumbs": [
      "Modules",
      "03 - Généralités",
      "Projet d'analyse de données"
    ]
  },
  {
    "objectID": "contents/generalities/01-stat.html#données",
    "href": "contents/generalities/01-stat.html#données",
    "title": "Projet d’analyse de données",
    "section": "Données",
    "text": "Données\nLes données sont le coeur du sujet. Pour être utile, les données doivent être disponibles et de bonnes qualités. Une fois les objectifs définis, on effectue une traitement préliminaire et une exploration basique des données pour ensuite aller vers des modèles plus développés.\n\nOù trouver des données ?\nRéponse simple : Internet ! Voici une liste de sites (non-exhaustives) qui regroupent des jeux de données :\n\nGoogle datasets;\nKaggle;\nUC Irvine Machine Learning Repository;\nTime Series Machine Learning website;\nPhysionet Database.\n\nOn peut aussi regarder les sites officiels de sources de données que l’on peut trouver pour une grande partie des pays du monde :\n\nCanada: StatCan;\nFrance: data.gouv.fr;\nUSA: data.gov;\nAngleterre: data.gouv.uk;\netc.\n\nPour des données sur des sujets plus spécifiques, les agences gouvernementales sont souvent de bonnes ressources. Par exemple, le Centre Canadien de cartographie et d’observation de la terre fournit les données géospatiales du Canada (ici).\nLorsque que l’on travaille pour une entreprise, on a généralement accès aux sources de données internes, e.g. base de données sur la production, les clients et les employés, les listes de transactions et de clients potentiels, des informations sur les visites web, etc.\n\n\nQualité\nIl y a un dicton populaire en informatique, s’appliquant aussi en analyse de données: “Garbage in, garbage out”. Même le meilleur modèle ne peut compenser des données biaisées, incomplètes ou erronées.\nPour nous assurer de la qualité des données, on pourra se poser les questions suivantes :\n\nLes données sont-elles représentatives de la population cible ?\nSont-elles exactes, complètes, pertinentes ?\nY a-t-il des valeurs manquantes, des doublons, des incohérences ?\n\n\n\nConstitution de la base de données\nUne fois nos données collectées, il faut les charger en mémoire pour ensuite pouvoir faire des analyses. En Python, les librairies pandas et polars permettent de lire la plupart des formats de fichiers auxquels nous aurons affaire. En ce qui concerne R, plusieurs packages sont utilisés selon le format (cf. Table 2).\n\n\n\nTable 2: Différentes libraries pour différents formats de fichiers.\n\n\n\n\n\nFormat\nExtension\nLibrarie\n\n\n\n\nTexte\n.txt; .csv\nreadr\n\n\nExcel\n.xlsx\nreadxl\n\n\nSAS\n.sas7bdat\nhaven\n\n\nSPSS\n.sav; .zsav\nhaven\n\n\nJSON\n.json\njsonlite\n\n\n\n\n\n\nDepuis une dizaine d’année, le concept de “tidy data” a emergé (cf. Wickham (2014)). Chaque jeu de données “tidy” respecte trois principes:\n\nChaque variable est une colonne du tableau.\nChaque observation est une ligne du tableau.\nChaque cellule du tableau contient une valeur unique.\n\nCela permet d’avoir une approche unifiée pour l’analyse de données. De manière général, on essaiera toujours de mettre son jeu de données sous format “tidy”. Le package tidyr en R et les librairies pandas et polars en Python permettent de mettre en forme les données en format “tidy”.\n\n\nExploration et traitement préliminaire\nUne fois les données chargées et mise sous le format “tidy”, une phase d’exploration préliminaire est nécessaire avant l’étape de modélisation. Cette étape, bien que souvent négligée, est très importante, mais elle n’est pas le coeur de ce cours. Cette étape permet de détecter les problèmes potentiels, de mieux comprendre la structure des données et d’orienter les choix méthodologiques. Voici quelques trucs à faire concernant cette première exploration:\n\nNettoyage de données: supprimer les doublons, uniformiser les modalités, vérifier le format des valeurs spéciales, etc.\nExploration des données: identification des modalités rares ou trop nombreuses, analyse des éventuelles asymétries, détection des classes déséquilibrées, identification des valeurs extrêmes ou aberrantes, recherche des corrélations fortes entre les variables, évaluation des valeurs manquantes.",
    "crumbs": [
      "Modules",
      "03 - Généralités",
      "Projet d'analyse de données"
    ]
  },
  {
    "objectID": "contents/generalities/01-stat.html#élaboration-et-validation-des-modèles",
    "href": "contents/generalities/01-stat.html#élaboration-et-validation-des-modèles",
    "title": "Projet d’analyse de données",
    "section": "Élaboration et validation des modèles",
    "text": "Élaboration et validation des modèles\nCe cours concerne l’élaboration et la validation de modèles. Pour l’instant, on peut retenir quatre composantes principales :\n\nUn espace (mathématique) de représentation: il s’agit du cadre mathématique dans lequel on travaille.\nUne distance (ou similarité): elle permet de comparer les observations entre elles.\nUn modèle (ou algorithme): c’est la méthode utilisée pour apprendre à partir des données.\nUne fonction de coût: elle mesure la qualité du modèle.\n\nCes éléments seront étudiés en détails dans les sections suivantes du cours.",
    "crumbs": [
      "Modules",
      "03 - Généralités",
      "Projet d'analyse de données"
    ]
  },
  {
    "objectID": "contents/generalities/01-stat.html#mise-en-oeuvre",
    "href": "contents/generalities/01-stat.html#mise-en-oeuvre",
    "title": "Projet d’analyse de données",
    "section": "Mise en oeuvre",
    "text": "Mise en oeuvre\nUne fois le modèle choisi et validé, il peut être déployé en production. La mise en production signifie le rendre opérationnel dans un environnement réel, souvent en automatisant l’ensemble du processus de traitement des données. Généralement, cela consiste à automatiser la collecte, le nettoyage et la transformation des données, à intégrer le modèle créé dans une application ou un système décisionnel, et à générer des rapports ou des prédictions en temps réel ou à intervalles réguliers. Cette partie est le domaine du data engineering. Un data engineer conçoit et maintient la pipeline de traitement depuis la source des données jusqu’à la sortie du modèle.",
    "crumbs": [
      "Modules",
      "03 - Généralités",
      "Projet d'analyse de données"
    ]
  },
  {
    "objectID": "contents/generalities/01-stat.html#suivi-de-la-performance-et-amélioration",
    "href": "contents/generalities/01-stat.html#suivi-de-la-performance-et-amélioration",
    "title": "Projet d’analyse de données",
    "section": "Suivi de la performance et amélioration",
    "text": "Suivi de la performance et amélioration\nFinalement, une fois que le modèle est mis en production, il faut assurer un suivi de sa performance dans le temps. En effet, les données évoluent, de même que les comportements qu’elles décrivent. Ainsi, les distributions des données peuvent changer (un phénomène appelé data drift), les hypothèses initiales peuvent ne plus être valides ou encore de nouvelles données ou de nouvelles variables peuvent améliorer la performance. Pour surveiller la performance du modèle, on peut faire un monitoring régulier des performances. On peut aussi réentraîner le modèle avec des données récentes ou l’améliorer en intégrant de nouvelles hypothèses.\nUn bon modèle n’est donc pas seulement performant à un instant donné, il est aussi robuste et adaptable dans le temps.",
    "crumbs": [
      "Modules",
      "03 - Généralités",
      "Projet d'analyse de données"
    ]
  },
  {
    "objectID": "contents/02-revisions.html",
    "href": "contents/02-revisions.html",
    "title": "Révisions",
    "section": "",
    "text": "Slides:\n\nAlgèbre linéaire - annoté\nProbabilités et statistiques - annoté\nAlgorithmique / Programmation\n\nTD: link\nTP: link",
    "crumbs": [
      "Modules",
      "02 - Révisions"
    ]
  },
  {
    "objectID": "contents/02-revisions.html#sommaire",
    "href": "contents/02-revisions.html#sommaire",
    "title": "Révisions",
    "section": "Sommaire",
    "text": "Sommaire\n\nAlgèbre linéaire\nProbabilités et statistiques\nAlgorithmique / Programmation\n\n\n\n\nPrediction (xkcd:2370).",
    "crumbs": [
      "Modules",
      "02 - Révisions"
    ]
  },
  {
    "objectID": "contents/01-introduction.html",
    "href": "contents/01-introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "Slides: link",
    "crumbs": [
      "Modules",
      "01 - Introduction"
    ]
  },
  {
    "objectID": "contents/01-introduction.html#quest-ce-que-lanalyse-de-données",
    "href": "contents/01-introduction.html#quest-ce-que-lanalyse-de-données",
    "title": "Introduction",
    "section": "Qu’est-ce que l’analyse de données ?",
    "text": "Qu’est-ce que l’analyse de données ?\nL’analyse de données est un ensemble de méthodes permettant de retirer de l’information d’un jeu de données. On parle aussi d’apprentissage statistique (statistical learning). L’idée est d’utiliser des modèles statistiques pour comprendre comment les données sont structurées et comment elles intéragissent l’une avec l’autre.\n\n\n\n\n\n\nNoteExemple\n\n\n\nImaginons que vous êtes employé par l’Organisation des Nations Unies (ONU). Votre mission est d’analyser l’espérance de vie à travers le monde. Pour cela, vous disposez d’une mesure de l’espérance de vie dans chaque pays membre de l’ONU, bien sûr, mais aussi le PIB par habitant, les montants des dépenses liés à la santé, le taux de fertilité, le taux d’urbanisation, le niveau d’éducation du pays, etc. Le but de l’analyse de données est de trouver des liens entre ses différentes variables et la variable d’intérêt, l’espérance de vie, de visualiser ces données, et éventuellement de prédire l’espérance de vie à partir des autres variables.",
    "crumbs": [
      "Modules",
      "01 - Introduction"
    ]
  },
  {
    "objectID": "contents/01-introduction.html#objectifs-du-cours",
    "href": "contents/01-introduction.html#objectifs-du-cours",
    "title": "Introduction",
    "section": "Objectifs du cours",
    "text": "Objectifs du cours\nDans ce cours, on cherche à introduire des méthodes qui permettent une étude d’un jeu de données de “haute dimension” (dans le sens où l’on ne peut pas faire un simple graphique de l’ensemble des variables pour chaque observation) sans avoir recours à un modèle probabiliste. Les différentes techniques que l’on va voir peuvent servir à:\n\nvisualiser les données;\nréduire la dimension des données;\nidentifier certains liens entre les variables;\ndiviser le jeu de données en groupes/classes.\n\nCe cours n’a pas vocation à être exhaustif, dans le sens de présenter toutes les méthodes possibles. Ce cours n’a pas non plus vocation à être à l’état de l’art, dans le sens où on ne s’intéressera pas aux derniers développements en apprentissage machine. Ce cours n’est pas non plus un cours de programmation.\nPour finir cette indroduction, voici un passage de Statistical Rethinking de Richard McElreath (McElreath 2020) trouvant résonnance dans ce cours.\n\nStatistics courses […] tend to resemble horoscopes. There are two senses to this resemblance. First, in order to remain plausibly correct, they must remain tremendously vague. This is because the targets of the advice, for both horoscopes and statistical advices, are diverse. But only the most general advice applies to all cases. A horoscope uses only the basic facts of birth to forecast life events, and a […] statistical guide uses only the basic facts of measurement and design to dictate a model. It is easy to do better, once more detail is available. In the case of statistical analysis, it is typically only the scientist who can provide that detail, not the statistician. Second, there are strong incentives for both astrologers and statisticians to exaggerate the power and importance of their advice. No one likes an astrologer who forecasts doom, and few want a statistician who admits the answers as desired are not in the data as collected. Scientists desire results, and they will buy and attend to statisticians and statistical procedures that promise them. What we end up with is too often horoscopic: vague and optimistic, but still claiming critical importance.\n\n\n\n\nMachine learning (xkcd:1838).",
    "crumbs": [
      "Modules",
      "01 - Introduction"
    ]
  },
  {
    "objectID": "contents/misc/good-practices-python.html",
    "href": "contents/misc/good-practices-python.html",
    "title": "Bonnes pratiques en Python",
    "section": "",
    "text": "Cette page est inspirée sur ce document.\nDe même que R, Python est “juste” un langage de programmation, il est donc possible d’écrire du code Python dans un éditeur de texte et de lancer ce code via un terminal. Bien que ce soit faisable, il est plus simple d’utilser un éditeur de code. Bien qu’il soit techniquement possible d’utiliser RStudio comme éditeur pour Python, je vous recommanderais dans utiliser spécialiser comme PyCharm. Le format notebook est aussi très populaire en Python. Dans ce cas, je vous conseillerais la librarie marimo qui permet de transformer ses fichiers .py en notebook.\nDe plus, il vaut mieux s’assurer de travailler avec la dernière version stable de Python comptatible avec les packages les utilisés (e.g. numpy, matplotlib, sklearn). En effet, très souvent, il y a un délai pour que les dernières versions des packages soient comptatibles avec la dernière version de Python. Il est donc important de partager la version des packages utilisés avec l’analyse.\nConcernant la documentation, le standard en Python est de suivre les recommendations du PEP 257. La libraries Sphinx permet de générer une documentation à partir des docstring.\nUne syntaxe uniforme rend un code beaucoup plus facile à lire et à comprendre. Un guide de style énonce des normes pour avoir une syntaxe uniforme. Le guide de style PEP 8 est recommandé en Python.\nRetour à la ligne et indentation\nL’indentation fait partie de Python, i.e. les blocks if...else et for sont définis grâce aux indentations. Il est possible d’utiliser un tab ou des espaces. Le choix vous appartient mais il est important d’être consistant dans votre code.\nConvention de noms\nL’utilisation des noms de variables avec un seul caractère est à éviter. Sinon, les variables, fonctions, méthodes, packages et modules sont nommés avec des lettres minuscules, des nombres et l’underscore _. Les classes et exceptions doivent être nommés avec des majuscules pour séparer les mots (UneClasse). Les constants sont en majuscules.\nOrganisation du code\nDe même que pour R, lorsque le code commence à devenir long, il devient avantageux de le séparer en plusieurs fichiers. Par exemple, on peut avoir un fichier par partie de l’analyse (un pour le nettoyage des données, un pour l’analyse, un pour la visualisation, …). On peut aussi créer plusieurs sous-dosssiers.\nQuelques trucs à faire\n\nNe pas faire de comparaison à True, False or None.\n\n\nif attr:\n    print(\"True!\")\n\nif not attr:\n    print(\"False!\")\n\nif attr is None:\n    print(\"None!\")\n\n\nUtiliser la compréhension de liste lorsque cela est possible.\n\n\na = [3, 4, 5]\nb = [i for i in a if i &gt; 4]\n\n\nCharger un fichier avec with. Cela permet d’être sur que la connexion avec le fichier est fermé une fois qu’il a été lu.\n\n\nwith open(\"file.txt\") as f:\n    read(f)\n\n\nUtiliser un maximum de 80 caractères par ligne.\nUtiliser des parenthèses pour aller à la ligne dans les longues chaînes de caractères."
  },
  {
    "objectID": "contents/misc/good-practices-R.html",
    "href": "contents/misc/good-practices-R.html",
    "title": "Bonnes pratiques en R",
    "section": "",
    "text": "Cette page est basée sur un document qu’Aurélien Nicosia (ULaval) a créé en 2023 appelé “Bonnes pratiques de programmation en R”. Celui-ci a été mis à jour.\nR étant “juste” un langage de programmation, il est techniquement possible d’utiliser un simple éditeur de texte pour écrire du R et un terminal pour lancer le code. Il est cependant bien plus commode—en particulier, pour les débutants—d’utiliser un éditeur de code comme RStudio pour lancer son code. En effet, celui-ci permet de lancer son code de façon intéractive, de pouvoir voir son environnement de travail, de tester son code, …\nDe plus, il vaut mieux s’assurer de travailler avec la dernière version de R et des packages dont nous avons besoin. Ainsi, nous risquons moins de rencontrer des bogues et nous pouvons profiter des dernières fonctionnalités. Un point d’attention, cependant, je vous déconseillerais de changer la version de R ou des packages une fois que vous avez commencé à travailler sur quelquechose. En effet, ce changement pourrais introduire des bogues au milieu de l’analyse (il faut donc toujours reporter les versions des packages utilisés pour la reproducibilité). Il est possible d’avoir un résumé de l’environnement de travail avec les fonctions systemInfo() et packageVersion(pkg).\nConcernant la documentation, le package roxygen permet de convertir les commentaires de vos fonctions et packages en une documentation propre.\nUne syntaxe uniforme rend un code beaucoup plus facile à lire et à comprendre. Un guide de style énonce des normes pour avoir une syntaxe uniforme. Le guide de style du tidyverse est recommandé en R. Avant de présenter quelques conventions de style, notons qu’il est possible de modifier rapidement la mise en forme d’un bout de code R dans RStudio dans le menu “Code -&gt; Reformat Selection”.\nRetour à la ligne et indentation\nUne façon simple de rendre som code plus lisible est d’y insérer des retours à la ligne et des indentations appropriés. Par exemple, supposons que nous avons la chaîne de caractère suivante :\n\ntext &lt;- \"Ceci est un example\"\n\nNous souhaitons corriger deux fautes dans cette phrase : le mot “example” écrit en anglais plutôt qu’en français et le point manquant à la fin de la phrase. Ceci peut se faire avec l’instruction :\n\npaste0(gsub(pattern = \"example\", replacement = \"exemple\", x = text), \".\")\n\nCette instruction comporte un appel de fonction imbriqué dans un autre. Elle est bien plus facile à lire comme suit :\n\npaste0(\n  gsub(\n    pattern = \"example\",\n    replacement = \"exemple\",\n    x = text),\n  \".\"\n)\n\nOpérateur d’assignation\nEn R, on utilise &lt;- pour assigner une valeur à une variable et on utilise = pour passer des valeurs à des arguments dans un appel de fonctions.\nConventions de noms\nLe guide de style du tydiverse préconise l’utilisation de lettres minuscules, de nombres et de l’underscore _ pour nommer variables et fonctions. Les underscores sont utilisés pour séparer les mots dans un nom. Bien que l’on puisse trouver d’autres conventions, celles-ci sont à éviter. Dans tous les cas, il est important de choisir une convention et de la respecter. De plus, il est préférable d’éviter les accents dans les noms de variables.\nOrganisation du code\nLorsque le code commence à devenir long, il devient avantageux de le séparer en plusieurs fichiers. Par exemple, on peut avoir un fichier par partie de l’analyse (un pour le nettoyage des données, un pour l’analyse, un pour la visualisation, …). De plus, une analyse de données n’est généralement pas constitué uniquement de code R, e.g. fichiers de code C++, fichiers de données, fichiers du configuration, etc. Il est donc recommandé des créer des sous-dosssiers regroupant les fichiers du même type. Les projets RStudio sont parfaits pour rassembler au même endroit tous les fichiers relatifs au projet. De plus, ils permettent de faciliter le travail sur plusieurs projets simultanément en gérant le passage d’un répertoire de travail à un autre.\nQuelques trucs à faire\n\nRédiger son code dans un script et l’enregistrer fréquemment. Cela permet d’éviter de perdre la trace de certaines instructions importantes parce qu’elles ont été écrites directement dans la console.\nIl est préférable de débuter toute session de travail en R avec un environnement de travail vide. Pour ce faire, il faut désactiver la restauration automatique d’une image de session dans les paramètres. Cela permet d’être conscient de la présence des différents objets dans l’environnement de travail.\nNe pas utiliser la fonction load lorsque l’environnement de travail n’est pas vide. Cela permet de ne pas modifier un objet de l’environnement de travail en l’écrasant.\nNe pas utiliser la fonction attach. Cela permet de ne pas modifier le chemin de recherche des fichiers.\nSauvegarder les options et paramètres graphiques avant de les modifier.\nNe pas utiliser T et F à la place de TRUE et FALSE."
  },
  {
    "objectID": "contents/supervised/01-discriminant.html",
    "href": "contents/supervised/01-discriminant.html",
    "title": "Analyse discriminante",
    "section": "",
    "text": "L’analyse discriminante vise à classer des individus dans différents groupes à partir de variables explicatives continues. C’est une méthode supervisée : les groupes sont connus dans les données d’apprentissage et l’objectif est d’apprendre une règle de classification optimale.",
    "crumbs": [
      "Modules",
      "05 - Supervisée",
      "Analyse discriminante"
    ]
  },
  {
    "objectID": "contents/supervised/01-discriminant.html#notation",
    "href": "contents/supervised/01-discriminant.html#notation",
    "title": "Analyse discriminante",
    "section": "Notation",
    "text": "Notation\nSoit \\(X = (X_{i j}) \\in \\mathbb{R}^{n \\times p}\\) un matrice de données, où \\(n\\) est le nombre d’individus dans l’échantillon, \\(p\\) est le nombre de variables et \\(X_{i j}\\) est la valeur de la \\(j\\)e variable pour le \\(i\\)e individus.\nOn suppose qu’il y a \\(K\\) groupes dans la population et que chaque individu appartient à l’un des \\(K\\) groupes. Pour \\(k \\in \\{ 1, \\dots, K \\}\\), on note \\(I_k\\) l’ensemble des individus du groupe \\(k\\), \\(n_k\\) le nombre d’observations dans \\(I_k\\). On a donc \\(\\sum_{k = 1}^{K} n_k = n\\).",
    "crumbs": [
      "Modules",
      "05 - Supervisée",
      "Analyse discriminante"
    ]
  },
  {
    "objectID": "contents/supervised/01-discriminant.html#objectif",
    "href": "contents/supervised/01-discriminant.html#objectif",
    "title": "Analyse discriminante",
    "section": "Objectif",
    "text": "Objectif\nNos observations sont dans \\(\\mathbb{R}^p\\). Pour faire de la classification à partir de \\(X_{1}, \\dots, X_p\\), on doit partionner \\(\\mathbb{R}^p\\) en \\(K\\) sous-ensembles de sorte que chacun des \\(K\\) sous-ensembles soit associé à l’un des \\(K\\) groupes. Dans le cas de l’analyse discriminante, l’idée est de passer de \\(\\mathbb{R}^p\\) à \\(\\mathbb{R}\\) en calculant, pour chaque observation, un score \\(f(X_{1}, \\dots, X_p) \\in \\mathbb{R}\\) et ensuite utiliser ce score pour déterminer le groupe d’appartenance (et donc de partionner \\(\\mathbb{R}\\)). Le score proposé par Fisher est une combinaison linéaire des variables, i.e. \\[f(X_{1}, \\dots, X_p) = a^{\\top} X + b = a_{1} X_{1} + \\cdots + a_p X_p + b.\\] Cette fonction de score nous permet de regrouper les individus d’un même groupe dans des zones proches du score et de séparer autant que possible les différents groupes selon ce score. On en déduira \\(K\\) intervalles de décision \\(S_{1}, \\dots, S_K\\) associés aux groupes.\n\n\n\n\n\n\nAstuceRemarque\n\n\n\nSans perte de généralité, on peut choisir \\[-b = a_{1} \\overline{X}_1 + \\dots + a_p \\overline{X}_p = a^{\\top} \\overline{X}\\] ce qui permet de centrer les variables en enlevant le vecteur de moyenne \\[\\overline{X} = \\left( \\overline{X}_1, \\dots, \\overline{X}_p \\right).\\]\nIl ne reste plus qu’à choisir le vecteur \\(a = (a_{1}, \\dots, a_p)\\).",
    "crumbs": [
      "Modules",
      "05 - Supervisée",
      "Analyse discriminante"
    ]
  },
  {
    "objectID": "contents/supervised/01-discriminant.html#critère-de-fisher",
    "href": "contents/supervised/01-discriminant.html#critère-de-fisher",
    "title": "Analyse discriminante",
    "section": "Critère de Fisher",
    "text": "Critère de Fisher\nL’idée centrale de Fisher est d’optimiser le rapport entre la variabilité inter-groupes et la variabilité intra-groupe en fonction du vectuer \\(a\\). Dit autrement, on voudrait choisir le vecteur \\(a\\) de sorte que les scores soient, à la fois, très différents entre les groupes et très similaires à l’intérieur d’un groupe. On s’intéresse donc à la variabilité des scores à l’intérieur des groupes et entre les groupes.\nNotons\n\n\\(S\\), la matrice de variance-covariance totale donnée par\n\n\\[S = \\sum_{i = 1}^{n} (X_i - \\overline{X})(X_i - \\overline{X})^{\\top} \\quad \\text{où}\\quad \\overline{X} = \\frac{1}{n} \\sum_{i = 1}^{n} X_i;\\]\n\n\\(W\\), la matrice de variance-covariance intra-groupe donnée par\n\n\\[W = \\sum_{k = 1}^{K} \\sum_{i \\in I_k} (X_i - \\overline{X}_k)(X_i - \\overline{X}_k)^{\\top} \\quad \\text{où} \\quad \\overline{X}_k = \\frac{1}{n_k} \\sum_{i \\in I_k} X_i;\\]\n\n\\(B\\), la matrice de variance-covariance inter-groupe donnée par\n\n\\[B = \\sum_{k = 1}^{K} n_k (\\overline{X}_k - \\overline{X})(\\overline{X}_k - \\overline{X})^{\\top}.\\]\nÉtant donné que \\(a \\in \\mathbb{R}^p\\), on a: \\[\\mathrm{Var}(f(X_{1}, \\dots, X_p)) = \\mathrm{Var}(a^{\\top} X) = a^{\\top} \\mathrm{Var}(X) a.\\] Comme \\(S\\) est un estimateur de la variance totale de \\(X\\), nous pouvons estimer cette variance par \\[\\widehat{\\mathrm{Var}}(f(X_{1}, \\dots, X_p)) = \\frac{1}{n} a^{\\top} S a.\\]\nLa base de l’analyse discriminante repose sur le fait que \\[S = W + B.\\]\n\n\n\n\n\n\nAstucePreuve que \\(S = W + B\\)\n\n\n\n\n\nOn peut prouver ce résultat en considérant la définition des matrices \\(S, W\\) et \\(B\\). La moyenne de la variable \\(j\\) pour tous les individus de l’échantillon est \\[\\overline{X}_j = \\frac{1}{n}\\sum_{i = 1}^{n} X_{i j}.\\]\nLa moyenne de la variable \\(j\\) pour les individus du groupe \\(k\\) est \\[\\overline{X}_{k j} = \\frac{1}{n_{k}} \\sum_{i \\in I_k} X_{i j}.\\]\nLa somme des carrés totale est \\[s_{j j^\\prime} \\sum_{i = 1}^{n} (X_{i j} - \\overline{X}_j)(X_{i j^\\prime} - \\overline{X}_{j^\\prime}).\\]\nOn tirerait de la matrice \\(S\\) une estimation de \\(\\mathrm{Cov}(X_j, X_{j^\\prime})\\) si toutes les observations provenaient d’un même groupe. On définit \\(s_{j j^\\prime}\\) comme étant \\[s_{j j^\\prime} = w_{j j^\\prime} + b_{j j^\\prime},\\] où \\[w_{j j^\\prime} = \\sum_{k = 1}^{q} \\sum_{i \\in I_k} (X_{i j} - \\overline{X}_{k j})(X_{i j^\\prime} - \\overline{X}_{k j^\\prime}),\\]\n\\[b_{j j^\\prime} = \\sum_{k = 1}^{q} n_k (\\overline{X}_{k j} - \\overline{X}_j)(\\overline{X}_{k j^\\prime} - \\overline{X}_{j^\\prime}).\\]\nPreuve:\n\nPoser \\(X_{ij} - \\overline{X}_j = X_{ij} - \\overline{X}_{kj} + \\overline{X}_{kj} - \\overline{X}_j\\), dans la définition de \\(s_{j j^\\prime}\\), iden pour \\(X_{i j^\\prime}\\).\nDévelopper les produits.\nRemplacer \\(\\sum_{i = 1}^{n}\\) par \\(\\sum_{k = 1}^{q} \\sum_{i \\in I_k}\\).\nFaire les simplications appropriées.\n\n\n\n\nAinsi, en remplaçant \\(S\\) par \\(W + B\\), on obtient \\[\\widehat{\\mathrm{Var}}(a^{\\top} X) = \\frac{1}{n} a^{\\top} S a = \\frac{1}{n} \\left( a^{\\top} W a + a^{\\top} B a \\right).\\] Le critère de Fisher s’écrit donc, pour \\(a \\in \\mathbb{R}^p\\), \\[J(a) = \\frac{a^{\\top} B a}{a^{\\top} W a}.\\] Celui-ci peut se réécrire de façon équivalente comme \\[J(a) = \\frac{a^{\\top} B a}{a^{\\top} S a}.\\] On cherche à maximiser \\(J(a)\\). Ce problème peut être reformulé des façons suivantes, toutes équivalentes:\n\nMaximiser \\(J(a)\\) sous la contrainte que \\(a^{\\top} a = 1\\).\nMaximiser \\(a^{\\top} B a\\) sous la contrainte que \\(a^{\\top} S a = 1\\).\nMaximiser \\(c^{\\top} S^{-1/2} B S^{-1/2} c\\) sous la contrainte que \\(c^{\\top} c = 1\\), où \\(c = S^{1/2} a\\).\n\nEn réécrivant la troisième formulation, on obtient \\[c^{\\top} \\left( S^{-1/2} B S^{-1/2} \\right) c \\quad\\text{s.c.}\\quad c^{\\top} c = 1,\\] on peut prendre \\(a = S^{-1/2} c\\), où \\(c\\) est un vecteur propre normé associé à \\(\\lambda_{1}\\), la première valeur propre de \\(S^{-1/2} B S^{-1/2}\\). De façon équivalente, de la deuxième formulation, on peut prendre \\(a\\), un vecteur propre normé associé à \\(\\lambda_{1}\\) la première valeur propre de \\(S^{-1} B\\). Notons que comme, pour \\(\\lambda\\) une valeur propre quelconque de \\(S^{-1/2} B S^{-1/2}\\), \\[S^{-1/2} B S^{-1/2} c = \\lambda c \\quad\\text{et}\\quad a = S^{-1/2} c,\\] alors \\[S^{-1/2} B a = \\lambda S^{1/2} a \\Rightarrow S^{-1} B a = \\lambda a.\\] Les valeurs propres de \\(S^{-1} B\\) et de \\(S^{-1/2} B S^{-1/2}\\) sont donc les mêmes et on peut facilement passer des vecteurs propres de \\(S^{-1} B\\) à ceux de \\(S^{-1/2} B S^{-1/2}\\).",
    "crumbs": [
      "Modules",
      "05 - Supervisée",
      "Analyse discriminante"
    ]
  },
  {
    "objectID": "contents/supervised/01-discriminant.html#fonction-discriminante",
    "href": "contents/supervised/01-discriminant.html#fonction-discriminante",
    "title": "Analyse discriminante",
    "section": "Fonction discriminante",
    "text": "Fonction discriminante\nLa fonction discriminante de Fisher est donc \\[f(x) = a^{\\top} (x - \\overline{X}),\\] où \\(a\\) est le vecteur propre normé associé à la plus grande valeur propre de \\(S^{-1} B\\). Les scores, i.e. la représentation des observations dans \\(\\mathbb{R}\\), sont données par \\(U_i = a^{\\top} (X_{i} - \\overline{X})\\). Ces scores sont des combinaisons linéaires des variables et ils maximisent le rapport variance intra-groupe / variance inter-groupes.\n\n\n\n\n\n\nNoteRemarque\n\n\n\nOn peut aussi prendre \\(U_i = a^{\\top} X_i\\), car ajouter la même constante à toutes les observations \\(i = 1, \\dots, n\\) ne fait que décaler la représentation dans \\(\\mathbb{R}\\).\n\n\n\nPouvoir discriminant\nPuisque la matrice \\(S^{-1/2} B S^{-1/2}\\) est symétrique et définie positive, ses valeurs propres sont toutes réelles et positives. De plus, on a que \\(S^{-1} B a = \\lambda_{1} a\\). Ainsi, \\[B a = \\lambda_{1} S a \\Rightarrow a^{\\top} B a = \\lambda_{1} a^{\\top} S a \\Rightarrow \\lambda_{1} = \\frac{a^{\\top} B a}{a^{\\top} S a}.\\]\nOn a donc \\(0 \\leq \\lambda_{1} \\leq 1\\). La valeur propre \\(\\lambda_{1}\\) peut donc être vue comme le pouvoir discriminant de \\(f\\):\n\n\\(\\lambda_{1} = 1 \\Rightarrow a^{\\top} B a = a^{\\top} S a\\), donc \\(100\\%\\) de la variabilité est entre les groupes et il n’y a aucune variabilité à l’intérieur des groupes. Cela représente le cas idéal.\n\\(\\lambda_{1} = 0 \\Rightarrow a^{\\top} B a = 0\\), donc il n’y a aucune variabilité entre les gorupes et toute la variabilité est concentrée à l’intérieur des groupes. Cela représente le cas où l’analyse discriminante est inutile.\n\n\n\nRègle de classification\nAprès avoir estimer la fonction discriminante \\(f(x)\\), on peut calculer le score moyen de chaque groupe \\(k\\) défini comme étant \\[m_k = a^{\\top} \\left( \\overline{X}_{k 1}, \\dots, \\overline{X}_{k p} \\right)^{\\top},\\] où \\(\\overline{X}_{k j}, j = 1, \\dots, p\\) est la moyenne de la \\(j\\)e variable pour les individus appartenant au \\(k\\)e groupe.\nConsidérons maintenant une nouvelle observation \\(X_{0} \\in \\mathbb{R}^p\\) qui n’était pas dans le jeu de données lors de l’estimation de \\(f(x)\\). Pour classer ce nouvel individu dans un groupe de la population, on calcule son score \\(f(X_{0}) = a^{\\top} X_{0}\\). Ensuite, on l’assigne au groupe \\(k^\\star\\) dont le score moyen \\(m_k\\) est le plus proche de \\(f(X_0)\\), i.e. le groupe tel que \\[\\left| a^{\\top} X_{0} - m_{k_0} \\right| = \\min_{1 \\leq k \\leq K} \\left| a^{\\top} X_{0} - m_k \\right|.\\] Autrement dit, on a \\[k^\\star = \\arg\\min_{k} \\left| a^{\\top} X_0 - m_k \\right|.\\] En applicant cette règle à l’échantillon \\(X_{1}, \\dots, X_n\\), on peut estimer les risques de mauvaise classification avec la matrice de confusion.\n\n\nCas particulier de la classification binaire\nDans le cas où \\(K = 2\\), il est possible de calculer explicitement la valeur de vecteur propre \\(a\\). Soit \\[C =  \\sqrt{\\frac{n_{1} n_{2}}{n}} (\\overline{X}_1 - \\overline{X}_2),\\] \\(\\overline{X}_i, i = 1, 2\\) sont les vecteurs moyens des caractériques dans chaque groupe, alors \\[B = C C^{\\top}, \\quad a = S^{-1} C.\\]\nSupposons que \\[ m_{1} = a^{\\top} \\overline{X}_1 &gt; a^{\\top} \\overline{X}_2 = m_{2}.\\] Alors, on classe un individu \\(X_0\\) dans le premier groupe si \\[a^{\\top} X_0 &gt; \\overline{m} = \\frac{m_{1} + m_{2}}{2} = a^{\\top} \\left( \\frac{\\overline{X}_1 + \\overline{X}_2}{2} \\right),\\] et dans le second groupe sinon. Cette règle de classification est équivalente à \\[(\\overline{X}_1 - \\overline{X}_2)^\\top S^{-1} X_0 &gt; \\frac{1}{2}(\\overline{X}_1 - \\overline{X}_2)^\\top S^{-1} (\\overline{X}_1 + \\overline{X}_2).\\]",
    "crumbs": [
      "Modules",
      "05 - Supervisée",
      "Analyse discriminante"
    ]
  },
  {
    "objectID": "contents/supervised/02-tree.html",
    "href": "contents/supervised/02-tree.html",
    "title": "Arbres",
    "section": "",
    "text": "Les arbres de classification et de régression (Classification And Regression Trees, CART) font parties des méthodes d’apprentissage supervisée. Ils permettent de prédire une variable réponse à partir de plusieurs variables explicatives \\(X_1, \\dots, X_p\\). On dispose donc de \\(n\\) observations appartenant à \\(\\mathbb{R}^p\\). Un arbre de classification cherche à assigner une observation \\(i\\) à l’un des \\(K\\) groupes sur la base de \\(X_{i1}, \\dots, X_{ip}\\), les valeurs des variables pour cette observation \\(i\\). Tandis qu’un arbre de régression cherche à prédire la valeur d’une variable numérique continue \\(Y\\) à partir des observations de \\(X_{1}, \\dots, X_p\\).",
    "crumbs": [
      "Modules",
      "05 - Supervisée",
      "Arbres"
    ]
  },
  {
    "objectID": "contents/supervised/02-tree.html#objectif",
    "href": "contents/supervised/02-tree.html#objectif",
    "title": "Arbres",
    "section": "Objectif",
    "text": "Objectif\nL’objectif de l’algorithme CART est de partionner l’espace des observations, i.e. \\(\\mathbb{R}^p\\), en sous-ensembles homogènes à l’aide de découpages successifs. Chaque sous-ensemble est représenté par une feuille de l’arbre, à laquelle est associée une prédiction.\nDe façon plus précise, l’algorithme CART est un algorithme récursif de découpage binaire. Il procède de la façon suivante :\n\nLe découpage de l’espace \\(\\mathbb{R}^p\\) en sous-régions rectangulaires (que l’on appelle hyper-rectangles), en coupant selon une variable \\(X_j, j = 1, \\dots, p\\) à une certaine valeur seuil \\(s\\).\nÀ chaque étape, on choisit la variable \\(X_j, j = 1, \\dots, p\\) et la valeur seuil \\(s\\) qui offrent la meilleure amélioration d’un critère d’homogénéité (cf. la section “Critères d’homogénéité”).\nOn répète ce découpage tant qu’un critère d’arrêt n’est pas atteint (cf. la section “Choix de la complexité”).\nChaque feuille contient des observations similaires et fournit une prédiction unique : la classe majoritaire pour la classification et la moyenne des \\(Y\\) pour la régression. Ainsi, toutes les observations dans une même feuille reçoivent la même prédiction.\n\n\n\n\n\n\n\nAstuceRemarque\n\n\n\nL’algorithme CART est un algortihme glouton : à chaque étape, cette algorithme choisit localement la meilleure coupure. Mais cela ne garantit pas l’optimum global !\nIl serait théoriquement préférable de chercher l’arbre à \\(K\\) feuilles qui minimise une fonction de coût globale : \\[\\min_{R_1, \\dots, R_K} \\sum_{k = 1}^{K} c_k,\\] où \\(c_k\\) mesure l’erreur dans le région \\(R_k\\). Cependant, il y aurait trop de combinaisons possibles pour résoudre ce problème. C’est un problème intraitable (intractable).",
    "crumbs": [
      "Modules",
      "05 - Supervisée",
      "Arbres"
    ]
  },
  {
    "objectID": "contents/supervised/02-tree.html#algorithme",
    "href": "contents/supervised/02-tree.html#algorithme",
    "title": "Arbres",
    "section": "Algorithme",
    "text": "Algorithme\nL’algorithme se déroule comme suit, pour chaque noeud :\n\nPour chaque variable \\(X_j, j = 1, \\dots, p\\) et chaque seuil (valeur de coupure) possible \\(s\\), on calcule la réduction du critère d’homogénéité entre les deux sous-ensembles.\nOn sélectionne le couple \\((X_j, s)\\) qui maximise la réduction du critère d’homogénéité, i.e. qui offre le gain maximal.\nOn divise le noeud en deux sous-noeuds (sous-ensembles) selon la règle \\(X_j \\leq s\\) vs. \\(X_j &gt; s\\).\nOn répète ce processus jusqu’à ce qu’un nombre maximal de feuilles soit atteint ou bien que le nombre d’observations dans chaque feuille soit suffisant.\n\nLe noeud de départ consiste en l’ensemble des observations.",
    "crumbs": [
      "Modules",
      "05 - Supervisée",
      "Arbres"
    ]
  },
  {
    "objectID": "contents/supervised/02-tree.html#critères-dhomogénéité-pour-la-classification",
    "href": "contents/supervised/02-tree.html#critères-dhomogénéité-pour-la-classification",
    "title": "Arbres",
    "section": "Critères d’homogénéité pour la classification",
    "text": "Critères d’homogénéité pour la classification\nSoit \\(\\widehat{p}_{jk}\\), la proportion d’observations de la région \\(R_j\\) de l’ensemble \\(\\mathbb{R}^p\\) qui appartiennent à la classe \\(k\\). Trois critères sont habituellement utilisés pour déterminer les divisions optimales à chaque étape de la construction de l’arbre:\n\nLe Taux d’erreur de classification correspond à la proportion d’observations mal classées dans une feuille. Il est défini comme \\[E_j = 1 - \\max_k \\widehat{p}_{jk}.\\] Ce critère est facile à interpréter, mais il est peu sensible et donc il est rarement utilisé pour la construction de l’arbre. Il est plutôt emplyé pour évaluer la performance du modèle une fois l’arbre construit.\nL’Indice de Gini est un des critères les plus utilisés. Il est défini comme \\[G_j = \\sum_{k = 1}^{K} \\widehat{p}_{jk} (1 - \\widehat{p}_{i j}).\\] Ce critère est minimale et égale à \\(0\\) lorsque tous les individus d’une région appartiennent à une unique classe, ce qui reflète une homogénéité parfaite. On cherche donc à avoir un indice de Gini faible.\nL’Entropie croisée est un autre critère couramment utilisé. Il est basé sur la théorie de l’information de Shannon et est défini comme \\[D_j = - \\sum_{k = 1}^{K}  \\widehat{p}_{jk} \\log(\\widehat{p}_{j k}).\\] Ce critère atteint aussi sa valeur minimale lorsque la classe est parfaitement prédite. On cherche donc aussi à avoir une entropie croisée faible.\n\nLors de la construction de l’arbre, on évalue pour chaque division possible la réduction d’hétérogénéité qu’elle entraîne, appelée gain d’information. Par exemple, pour l’indice de Gini, le gain d’information s’écrit comme la différence entre l’indice avant la division et une moyenne pondérée des indices dans les sous-groupes obtenus : \\[\\Delta G = G_{\\text{avant la division}} - \\left( \\frac{n_1}{n} G_1 + \\frac{n_2}{n} G_2 \\right),\\] où \\(n_1\\) et \\(n_2\\) représente le nombre d’observations dans les deux sous-ensembles et \\(n = n_{1} + n_{2}\\). On choisit alors la division qui maximise ce gain.",
    "crumbs": [
      "Modules",
      "05 - Supervisée",
      "Arbres"
    ]
  },
  {
    "objectID": "contents/supervised/02-tree.html#choix-de-la-complexité-élagage-de-larbre",
    "href": "contents/supervised/02-tree.html#choix-de-la-complexité-élagage-de-larbre",
    "title": "Arbres",
    "section": "Choix de la complexité : élagage de l’arbre",
    "text": "Choix de la complexité : élagage de l’arbre\nPlus l’arbre devient profond, i.e plus le nombre de feuilles augmente, plus il s’adapte fidèlement aux données d’apprentissage. Toutefois, cette grande adaptation conduit au phénomène de sur-ajustement (cf. Section sur l’évaluation de modèles). Ainsi, dans ce cas, le modèle ne sera plus capable de généraliser à des données inconnues. Cependant, si l’arbre n’a pas assez de feuilles, il y a un manque d’ajustement aux données car la classification est trop simpliste.\nPour éviter ce phénomène, on recourt à une stratégie d’élagage (purning). Dans un premier temps, on fait croître l’arbre jusqu’à un développement complet, i.e. jusqu’à ce que toutes les feuilles soient pures (ne contiennent qu’une seule observation) ou qu’aucune division ne soit pertinente. Dans un second temps, on coupe l’arbre en se débarassant des branches qui n’apportent que peu d’amélioration en termes de performance.\nL’élagage peut être réalisé à l’aide d’un critère appelé coût-complexité. On cherche alors à minimiser une fonction combinant l’erreur de prédiction et la complexité de l’arbre. Cette fonction prend la forme suivante : \\[\\sum_{j = 1}^{|T|} c_j + \\alpha |T|,\\] où \\(|T|\\) est le nombre de feuille de l’arbre, \\(c_j\\) le coût de classification ou de régression dans la feuille \\(j\\) et \\(\\alpha\\) un paramètre de régularisation contrôlant la pénalité associée à la taille de l’arbre. Plus le paramètre \\(\\alpha\\) est élevé, plus on favorise les arbres simples. Le choix optimal de \\(\\alpha\\) peut se faire par validation croisée, en comparant les performances sur des jeux de validation pour différentes valeurs de ce paramètre.",
    "crumbs": [
      "Modules",
      "05 - Supervisée",
      "Arbres"
    ]
  },
  {
    "objectID": "contents/supervised/02-tree.html#quelques-commentaires",
    "href": "contents/supervised/02-tree.html#quelques-commentaires",
    "title": "Arbres",
    "section": "Quelques commentaires",
    "text": "Quelques commentaires\nLes arbres présentent quelques avantages intéressants. Ils sont d’abord très interprétables. En effet, chaque prédiction peut être expliquée à l’aide d’un chemin dans l’arbre, ce qui les rend particulièrement adaptés à une utilisation dans un contexte appliqué. Ils sont également robustes aux valeurs extrêmes et aux transformations monotones des variables explicatives. De plus, ils gèrent naturellement les interactions entre les variables sans qu’il soit nécessaire de les expliciter. Ils peuvent être utilisés à la fois avec des variables quantitatives et quantitatives, et ne reposent sur aucune hypothèse de distribution des données. Ils effectuent automatiquement une sélection des variables au cours de leur construction. Les variables importantes auront tendance à être choisies en premières pour la découpe des noeuds. Enfin, ils constituent la base de nombreuses méthodes modernes d’apprentissage statistique, comme les forêts aléatoires ou bien le gradient boosting (cf. Section “Méthodes ensemblistes”).\nMalgré leurs nombreux avantages, les arbres présentent aussi certaines limites. Ils sont instables. En effet, de légères modifications dans les données peuvent entraîner des arbres très différents. Ainsi, lorsqu’ils sont utilisés seuls, ils peuvent être moins performants que d’autres méthodes, notemment lorsque les données sont bruitées. La forme des divisions (en rectangles alignés sur les axes) peut être inadaptée si les frontières entre les classes sont non-linéaires. Par ailleurs, sans élagage, les arbres ont tendance à surajuster les données. Ils sont également sensibles au déséquilibre des classes, i.e. lorsque certaines classes sont fortement sous-représentées dans les données. Ainsi, on leur préférera généralement une forme agrégée (e.g. forêts aléatoires, boosting) pour améliorer leur stabilité et leur performance.",
    "crumbs": [
      "Modules",
      "05 - Supervisée",
      "Arbres"
    ]
  },
  {
    "objectID": "contents/08-conclusion.html",
    "href": "contents/08-conclusion.html",
    "title": "Conclusion",
    "section": "",
    "text": "Pour conclure, il n’existe aucun algorithme universellement meilleur permettant de résoudre tous les problèmes de classification ou de prédiction. On appelle ce phénomène le no free lunch theorem. Chaque jeu de données, chaque contexte d’analyse, chaque objectif de modélisation est unique. Il n’y a donc pas de solution pré-établie : c’est donc au statisticien ou au data scientist de faire preuve de discernement, d’esprit critique, et de créativité pour adapter les méthodes aux données et à la question posée.\nCela dit, on peut dégager certains principes qui peuvent nous guider dans une démarche rigoureuse :\n\nCommencer par une exploration descriptive des données, en étudiant les distributions, les relations entre variables, les valeurs manquantes, etc.\nTirer parti de l’expertise métier, en consultant les personnes qui connaissent le contexte des données (experts du domaine, utilisateurs finaux, etc.).\nS’inspirer des méthodes ayant déjà donné de bons résultats dans des situations similaires, en gardant toutefois un regard critique.\n\nUne des principales difficultés en pratique est souvent liée à la dimensionnalité du problème : le nombre de modèles ou d’approches possibles augmente rapidement avec le nombre de variables disponibles. Il devient alors crucial de simplifier l’espace de recherche, e.g. réduire la dimension avec une ACP, qui permettent de résumer l’information tout en limitant le bruit. Une approche fréquente consiste à appliquer des méthodes d’analyse à un sous-ensemble de variables soigneusement choisies, ou à utiliser les composantes principales comme nouvelles variables explicatives. Mais là encore, aucune règle générale ne permet d’identifier automatiquement le “bon” sous-ensemble…\nEnfin, on peut noter que plusieurs questions centrales en analyse de données ont été laissées de côté dans ce cours. Pourtant, celles-ci sont omniprésentes en pratique. Parmi ces questions :\n\nComment définir les variables à utiliser comme prédicteurs (feature engineering) ?\nComment repérer et gérer les données aberrantes (*outliers) ?\nComment scinder les données entre apprentissage, validation et test ?\nQue faire en présence de données manquantes ?\nLes données analysées sont-elles représentatives de la population cible ?\n\nCes questions soulignent que l’analyse de données est autant un art qu’une science. Il ne suffit pas d’appliquer mécaniquement des algorithmes : il faut formuler des hypothèses claires, évaluer leurs limites, et rester attentif aux enjeux éthiques, sociaux et pratiques.",
    "crumbs": [
      "Modules",
      "08 - Conclusion"
    ]
  },
  {
    "objectID": "contents/remainders/01-linear-algebra.html",
    "href": "contents/remainders/01-linear-algebra.html",
    "title": "Algèbre linéaire",
    "section": "",
    "text": "Dans cette partie, on présente quelques résultats d’algèbre linéaire utiles dans le cadre de ce cours. Pour plus d’information, vous pouvez vous référer au cours MAT-1200, à Deisenroth, Faisal, et Ong (2020) (en anglais) et à Grifone (2024) (en français).",
    "crumbs": [
      "Modules",
      "02 - Révisions",
      "Algèbre linéaire"
    ]
  },
  {
    "objectID": "contents/remainders/01-linear-algebra.html#quelques-propriétés-matricielles",
    "href": "contents/remainders/01-linear-algebra.html#quelques-propriétés-matricielles",
    "title": "Algèbre linéaire",
    "section": "Quelques propriétés matricielles",
    "text": "Quelques propriétés matricielles\nNotons \\(M_{n, m}(\\mathbb{R})\\), l’ensemble des matrices à \\(n\\) lignes et \\(m\\) colonnes dont les entrées appartiennent à \\(\\mathbb{R}\\). Notons \\(M_{n}(\\mathbb{R})\\), l’ensemble des matrices carrées de taille \\(n\\), i.e. à \\(n\\) lignes et \\(n\\) colonnes dont les entrées appartiennent à \\(\\mathbb{R}\\). Soient \\(M\\), \\(N\\) et \\(P\\) des matrices appartenant à \\(M_{n, m}(\\mathbb{R})\\). Soient \\(A\\) et \\(B\\) des matrices appartenant à \\(M_{n}(\\mathbb{R})\\). Notons \\(I_n\\) la matrice identité de taille \\(n\\), i.e. qui contient des \\(1\\) sur le diagonale et des \\(0\\) sur les éléments hors de la diagonale. Soient \\(u\\) et \\(v\\) appartenant à \\(\\mathbb{R}^n\\), i.e. des vecteurs colonnes de taille \\(n\\).\n\n\n\n\n\n\nImportantPropriétés de l’inverse de matrices\n\n\n\nSupposons que les matrices \\(A\\) et \\(B\\) soient inversibles. Alors le produit matriciel \\(AB\\) est inversible et est donné par:\n\\[(AB)^{-1} = B^{-1} A^{-1}.\\]\n\n\n\n\n\n\n\n\nNotePreuve\n\n\n\n\n\nPosons \\(C = AB\\) et \\(D = B^{-1} A^{-1}\\). Alors\n\\[\\begin{align*}\n  CD &= A B B^{-1} A^{-1} \\\\\n     &= A A^{-1} \\\\\n     &= I_n\n\\end{align*}\\]\nDe la même façon, on trouve que \\(DC = I_n\\). Ainsi, \\(AB\\) est inversible et son inverse est donné par \\(B^{-1} A^{-1}\\).\n\n\n\n\n\n\n\n\n\nImportantPropriétés du déterminant de matrices\n\n\n\nConsidérant les matrices définies en début de section, on a :\n\n\\(\\text{det}(A^\\top) = \\text{det}(A)\\),\n\\(\\text{det}(AB) = \\text{det}(A)\\text{det}(B)\\),\n\\(\\text{det}(A^{-1}) = 1 / \\text{det}(A)\\).\n\n\n\n\n\n\n\n\n\nNotePreuve\n\n\n\n\n\nLes preuves des propriétés \\(1\\) et \\(2\\) sont techniques et sont omises, mais peuvent être trouvées, par exemple, ici. Pour ce qui est de la troisième propriété, par définition, on a \\(A A^{-1} = I_n\\). Le déterminant de \\(I_n\\) est égale à \\(1\\) (produit des éléments sur la diagonale). Donc \\(\\text{det}(A A^{-1}) = 1\\). Or, d’après la deuxième propriété, \\(\\text{det}(A A^{-1}) = \\text{det}(A)\\text{det}(A^{-1})\\). On a donc bien \\(\\text{det}(A^{-1}) = 1 / \\text{det}(A)\\).\n\n\n\n\n\n\n\n\n\nImportantPropriétés de la trace de matrices\n\n\n\nConsidérant les matrices définies en début de section, on a :\n\n\\(\\text{tr}(A) = \\text{tr}(A^{\\top})\\),\n\\(\\text{tr}(A + B) = \\text{tr}(A) + \\text{tr}(B)\\),\n\\(\\text{tr}(MN^{\\top}) = \\text{tr}(N^{\\top}M)\\).\n\n\n\n\n\n\n\n\n\nNotePreuve\n\n\n\n\n\nPour une matrice carré \\(A\\), notons \\(a_{ij}\\), l’élément de la matrice \\(A\\) à la ligne \\(i\\) et à la colonne \\(j\\). La trace de \\(A\\) est donnée par la somme des éléments diagonaux, i.e. \\(\\text{tr}(A) = \\sum_{i = 1}^{n} a_{ii}\\).\n\nLa transposition ne changeant pas les éléments diagonaux, le résultat est direct.\nNotons \\(C = A + B\\). Comme \\(A\\) et \\(B\\) sont des matrices carrées, \\(C\\) est une matrice carrée. On a \\(c_{ij} = a_{ij} + b_{ij}\\) pour tout \\(i, j = 1, \\dots, n\\). Donc \\[\\text{tr}(A + B) = \\text{tr}(C) = \\sum_{i = 1}^{n} c_{ii} = \\sum_{i = 1}^{n} a_{ii} + b_{ii} = \\sum_{i = 1}^{n} a_{ii} + \\sum_{i = 1}^{n} b_{ii} = \\text{tr}(A) + \\text{tr}(B).\\]\nLes matrices \\(M N^{\\top}\\) et \\(N^{\\top} M\\) sont carrées, de dimension respectives \\(n \\times n\\) et \\(m \\times m\\), on peut donc bien calculer leur trace. Notons \\(C = M N^{\\top}\\) et \\(D = N^{\\top} M\\). \\[\\text{tr}(M N^{\\top}) = \\text{tr}(C) = \\sum_{i = 1}^{n} c_{ii} = \\sum_{i = 1}^{n} \\sum_{j = 1}^{m} m_{ij} n_{ji} = \\sum_{j = 1}^{m} \\sum_{i = 1}^{n} n_{ji} m_{ij} = \\sum_{j = 1}^{m} d_{jj} = \\text{tr}(D)  = \\text{tr}(N^{\\top} M).\\]\n\n\n\n\n\n\n\n\n\n\nAvertissementDéfinition\n\n\n\n\nSoit \\(A\\) une matrice symétrique appartenant à \\(M_n(\\mathbb{R})\\). \\(A\\) est définie positive si \\(u^\\top A u &gt; 0\\) pour tout \\(u \\in \\mathbb{R}^n\\) tel que \\(u \\neq 0\\).\nSoit \\(A\\) appartenant à \\(M_n(\\mathbb{R})\\). \\(A\\) est orthogonal si \\(A^\\top A = A A^\\top = I_n\\).",
    "crumbs": [
      "Modules",
      "02 - Révisions",
      "Algèbre linéaire"
    ]
  },
  {
    "objectID": "contents/remainders/01-linear-algebra.html#valeurs-et-vecteurs-propres",
    "href": "contents/remainders/01-linear-algebra.html#valeurs-et-vecteurs-propres",
    "title": "Algèbre linéaire",
    "section": "Valeurs et vecteurs propres",
    "text": "Valeurs et vecteurs propres\n\n\n\n\n\n\nAvertissementDéfinition\n\n\n\nSoit \\(A\\) appartenant à \\(M_n(\\mathbb{R})\\). On dit que \\(\\lambda \\in \\mathbb{R}\\) est une valeur propre de \\(A\\) s’il existe un vecteur \\(u \\in \\mathbb{R}^n\\) non nul tel que \\[Au = \\lambda u. \\tag{1}\\] Le vecteur \\(u\\) est appelé vecteur propre de \\(A\\) correspondant à la valeur propre \\(\\lambda\\).\nL’ensemble des nombres réels \\(\\lambda\\) satisfaisant Équation 1 est appelé spectre de la matrice \\(A\\) et noté \\(\\text{sp}(A)\\).\n\n\n\n\n\n\n\n\nImportantPropriété des vecteurs propres\n\n\n\n\nSi \\(u\\) est un vecteur propre de \\(A\\) correspondant à une valeur propre \\(\\lambda\\), alors le vecteur \\(cu\\), \\(c \\in \\mathbb{R}^\\star\\) est également un vecteur propre de \\(A\\) correspondant à \\(\\lambda\\).\nSi \\(A\\) est symétrique et \\(u_{1}\\) et \\(u_{2}\\) sont des vecteurs propres correspondant à des valeurs propres différentes de \\(A\\), alors \\(u_{1}\\) et \\(u_{2}\\) sont orthogonaux, i.e. \\(u_{1}^\\top u_{2} = 0\\).\n\n\n\n\n\n\n\n\n\nNotePreuve\n\n\n\n\n\n\nSoit \\(c \\in \\mathbb{R}^\\star\\) et \\(u\\) un vecteur propre de \\(A\\) associé à la valeur propre \\(\\lambda\\). On a : \\[A(cu) = cAu = c \\lambda u = \\lambda (cu).\\] Donc, le vecteur \\(cu\\) est aussi vecteur propre de \\(A\\) associé à la valeur propre \\(\\lambda\\).\nSoient \\(\\lambda_{1}\\) et \\(\\lambda_{2}\\), les valeurs propres associées à \\(u_{1}\\) et \\(u_{2}\\), tel que \\(\\lambda_{1} \\neq \\lambda_{2}\\). On a \\(A u_{1} = \\lambda_{1} u_{1}\\) et \\(A u_{2} = \\lambda_{2} u_{2}\\). Ensuite \\[\\lambda_{1} u_{1}^{\\top} u_{2} = u_{1}^\\top A u_{2} = \\lambda_{2} u_{1}^\\top u_{2}.\\] Cela implique que \\((\\lambda_{1} - \\lambda_{2})u_{1}^\\top u_{2} = 0\\). Or, \\(\\lambda_{1} \\neq \\lambda_{2}\\). Donc, nécessairement, \\(u_{1}^\\top u_{2} = 0\\).\n\n\n\n\nCette deuxième propriété nous sera utile lorque l’on s’intéressera à la réduction de dimension et, en particulier, à l’analyse en composantes principales.\n\n\n\n\n\n\nImportantCaractérisation de matrices avec ses éléments propres\n\n\n\n\nSi \\(A\\) est symétrique, alors toutes ses valeurs propres sont réelles.\nSi \\(A\\) est définie positive, alors toutes ses valeurs propres sont strictement positives.\n\n\n\n\n\n\n\n\n\nNotePreuve\n\n\n\n\n\n\nConsidérons le cas plus général où \\(A\\) est une matrice hermitenne. La matrice \\(A\\) est égale la transposé de son conjugué, noté \\(A^*\\). Notons \\(\\lambda\\) une valeur propre associée à un vecteur propre \\(u\\), éventuellement complexe. On a : \\[\\begin{align}\n\\overline{u}^{\\top} A u &= \\overline{u}^\\top \\lambda u = \\lambda \\overline{u}^{\\top} u, \\\\\n\\overline{u}^\\top A u &= \\overline{u}^\\top A^* u = \\overline{Au}^\\top u = \\overline{\\lambda} \\overline{u}^\\top u.\n\\end{align}\\] Cela implique que \\((\\lambda - \\overline{\\lambda}) \\overline{u}^{\\top} u = 0\\). Comme \\(u \\neq 0\\), on a \\(\\lambda = \\overline{\\lambda}\\). Donc \\(\\lambda \\in \\mathbb{R}\\).\nConsidérons \\(u\\), vecteur propre de \\(A\\) associé à la valeur propre \\(\\lambda\\). On a que \\(u^{\\top} A u = \\lambda u^{\\top} u\\). Or, comme \\(u \\neq 0\\), \\(u^{\\top}u \\neq 0\\). Donc \\[\\lambda = \\frac{u^{\\top} A u}{u^{\\top} u}.\\] Comme \\(A\\) est définie postive, \\(u^{\\top} A u &gt; 0\\) pour tout vecteur \\(u\\) non nul. On en déduit que \\(\\lambda &gt; 0\\).",
    "crumbs": [
      "Modules",
      "02 - Révisions",
      "Algèbre linéaire"
    ]
  },
  {
    "objectID": "contents/remainders/01-linear-algebra.html#diagonalisation-de-matrices",
    "href": "contents/remainders/01-linear-algebra.html#diagonalisation-de-matrices",
    "title": "Algèbre linéaire",
    "section": "Diagonalisation de matrices",
    "text": "Diagonalisation de matrices\n\n\n\n\n\n\nAvertissementDéfinition\n\n\n\nSoit \\(A\\) appartenant à \\(M_n(\\mathbb{R})\\). On dit que \\(A\\) est diagonalisable s’il existe une matrice \\(P\\) appartenant à \\(M_n(\\mathbb{R})\\) non-singulière et une matrice diagonale \\(D\\) appartenant à \\(M_n(\\mathbb{R})\\) telles que \\[P^{-1} A P = D \\Longleftrightarrow A = P D P^{-1}.\\]\n\n\n\n\n\n\n\n\nMise en gardeThéorème de décomposition spectrale\n\n\n\nSoit \\(A\\) une matrice symmétrique appartenant à \\(M_n(\\mathbb{R})\\) et \\(\\lambda_{1}, \\dots, \\lambda_n\\), ses \\(n\\) valeurs propres. Alors, il existe une matrice orthogonal \\(P\\) appartenant à \\(M_n(\\mathbb{R})\\) telle que \\[A = P \\Lambda P^\\top, \\quad\\text{où}\\quad \\Lambda = \\text{diag}(\\lambda_1, \\dots, \\lambda_n).\\]\n\n\nSi \\(A\\) admet \\(n\\) valeurs propres positives distinctes, alors on peut prendre \\(P\\) comme étant la matrice dont la \\(k\\)-ième colonne est le vecteur propre normé correspondant à la \\(k\\)-ième valeur propre \\(\\lambda_k\\) de \\(A\\).\nSoit deux matrices symétriques, \\(A\\) et \\(B\\), comment déterminer le vecteur \\(u\\) tel que \\(u^{\\top} A u\\) soit maximal, sachant que \\(u^{\\top} B u = 1\\) ? Il suffit de prendre \\(u\\) comme le vecteur propre de \\(B^{-1}A\\) associé à \\(\\lambda\\) la valeur propre maximale de \\(B^{-1}A\\). On obtient ainsi \\[u^{\\top} A u = u^{\\top}\\lambda M u = \\lambda U^{\\top} M u = \\lambda.\\]\n\n\n\n\n\n\nImportantCaractérisation du déterminant et de la trace de matrices avec ses éléments propres\n\n\n\nSi \\(A\\) a comme valeurs propres (réelles, mais pas forcément distinctes) \\(\\lambda_{1}, \\dots, \\lambda_{n}\\), alors\n\n\\(\\text{det}(A) = \\prod_{i = 1}^{n} \\lambda_i\\)\n\\(\\text{tr}(A) = \\sum_{i = 1}^{n} \\lambda_i.\\)\n\n\n\n\n\n\n\n\n\nNotePreuve\n\n\n\n\n\nEn utilisant le théorème de décomposition spectrale, il existe une matrice \\(P\\) inversible tel que \\(A = P \\Lambda P^{-1}\\), où \\(\\Lambda\\) est une matrice diagonale contenant les valeurs propres. On a donc, pour le déterminant,\n\\[\\text{det}(A) = \\text{det}(P \\Lambda P^{-1}) = \\text{det}(P)\\text{det}(\\Lambda)\\text{det}(P^{-1}) = \\text{det}(P)\\text{det}(\\Lambda)\\text{det}(P)^{-1} = \\text{det}(\\lambda) = \\prod_{i = 1}^{n} \\lambda_i, \\]\net, pour la trace,\n\\[\\text{tr}(A) = \\text{tr}(P \\Lambda P^{-1}) = \\text{tr}(P^{-1} P \\Lambda) = \\text{tr}(\\Lambda) = \\sum_{i = 1}^{n} \\lambda_i.\\]",
    "crumbs": [
      "Modules",
      "02 - Révisions",
      "Algèbre linéaire"
    ]
  },
  {
    "objectID": "contents/remainders/02-probabilities.html",
    "href": "contents/remainders/02-probabilities.html",
    "title": "Probabilités et Statistiques",
    "section": "",
    "text": "Dans cette partie, on présente quelques résultats en probabilités et statistiques dans le cadre de ce cours. Pour plus d’information, vous pouvez vous référer au cours STT-1000, à Wasserman (2010) (en anglais) et à Delmas (2013) (en français).",
    "crumbs": [
      "Modules",
      "02 - Révisions",
      "Probabilités et Statistiques"
    ]
  },
  {
    "objectID": "contents/remainders/02-probabilities.html#modéliser-le-hasard",
    "href": "contents/remainders/02-probabilities.html#modéliser-le-hasard",
    "title": "Probabilités et Statistiques",
    "section": "Modéliser le hasard",
    "text": "Modéliser le hasard\nBeaucoup de phénomènes réels ne sont pas prévisibles et généralement, leurs résultats contiennent une certaine variabilité. Cette variabilité est prise en compte grâce à une mesure de l’incertitude que l’on appelle mesure de probabilités.\n\n\n\n\n\n\nAvertissementDéfinition\n\n\n\nL’espace d’évènements \\(S\\) est l’ensemble de tous les résultats possibles d’un phénomène. Un évènement est un sous-ensemble de l’espace d’évènements \\(S\\).\n\n\n\n\n\n\n\n\nNoteExemples\n\n\n\n\nSi l’expérience consiste à lancer un pièce, \\(S = \\{0, 1\\}\\). Le résultat de cette expérience ne peut pas être connu à l’avance. Par exemple, \\(E = \\{1\\}\\) est un évènement de \\(S\\).\nSi on s’intéresse à la durée de vie d’un téléphone, \\(S = \\mathbb{R}_{+}\\). On peut aussi choisir \\(S = [0, M]\\), car cette durée de vie n’est probablement pas infini ! L’évènement \\(E = [10, \\infty)\\) représente l’évènement “une durée de vie de plus de 10 unités de temps”.\nPour le nombre de jours sans neige à Québec dans l’année, on peut choisir \\(S = \\mathbb{N}\\). L’évènement \\(E = (0, 5]\\) représente l’évènement “moins de 5 jours sans neige à Québec dans l’année”.\n\n\n\n\n\n\n\n\n\nAvertissementDéfinition\n\n\n\nUne mesure de probabilités \\(\\mathbb{P}\\) sur \\(S\\) est une application (fonction) définie sur l’espace d’évènements et satisfaisant les propriétés suivantes :\n\nPour chaque évènement \\(E\\), \\(\\mathbb{P}(E) \\in [0, 1]\\).\n\\(\\mathbb{P}(S) = 1\\).\nSoient \\(E_{1}, E_{2}, \\dots\\), une séquence d’évènements (finie ou infinie) mutuellement exclusive, i.e. \\(\\forall i \\neq j, E_{i} \\cap E_{j} = \\varnothing\\). On a \\[\\mathbb{P}(\\bigcup_{n = 1}^{\\infty} E_n) = \\sum_{n = 1}^{\\infty} \\mathbb{P}(E_n).\\]\n\nOn appelle \\(\\mathbb{P}(E)\\), la probabilité de l’évènement \\(E\\).\n\n\nLa définition de mesures de probabilités peut être subjective et lié à l’expérience du statisticien. En reprenant l’exemple 3 sur le nombre de jours sans neige à Québec dans l’année. Une personne venant d’arriver au Canada peut vouloir donner la même probabilité à chacun des jours, alors qu’un Québécois aura plus d’information et pourras faire varier les probabilités en fonction de cette connaissance.\n\n\n\n\n\n\nAvertissementDéfinition\n\n\n\nDeux évènements \\(E\\) et \\(F\\) sont dits indépendants si \\(\\mathbb{P}(E \\cap F) = \\mathbb{P}(E) \\times \\mathbb{P}(F)\\).\n\n\n\n\n\n\n\n\nAvertissementDéfinition\n\n\n\nSoient \\(E\\) et \\(F\\), deux évènements, la probabilité conditionelle que \\(E\\) se réalise sachant que \\(F\\) s’est réalisé est définie par : \\[\\mathbb{P}(E \\mid F) = \\frac{\\mathbb{P}(E \\cap F)}{\\mathbb{P}(F)}.\\]\n\n\nDe façon intuitive, deux évènements sont indépendants si la connaissance de l’un ne donne aucune information sur la réalisation de l’autre. On a aussi \\(\\mathbb{P}(E \\mid F) = \\mathbb{P}(E)\\).",
    "crumbs": [
      "Modules",
      "02 - Révisions",
      "Probabilités et Statistiques"
    ]
  },
  {
    "objectID": "contents/remainders/02-probabilities.html#variables-aléatoires",
    "href": "contents/remainders/02-probabilities.html#variables-aléatoires",
    "title": "Probabilités et Statistiques",
    "section": "Variables aléatoires",
    "text": "Variables aléatoires\nEn probabilité, la convention est d’exprimer le résultat d’expériences comme la valeur d’une fonction appelé variable aléatoire. Cette caractérisation est toujours possible.\n\n\n\n\n\n\nAvertissementDéfinition\n\n\n\nSoit une variable aléatoire \\(X\\). La distribution de cette variable aléatoire est définie par l’application \\(A \\mapsto \\mathbb{P}(X \\in A)\\).\n\n\n\n\n\n\n\n\nAvertissementDéfinition\n\n\n\nSoit une variable aléatoire \\(X\\). Cette variable aléatoire est discrète si elle prend, au plus, un nombre dénombrable de valeurs. Dans ce cas, la distribution de \\(X\\) est donnée par les probabilités \\(\\mathbb{P}(X = x)\\) pour tout résultat \\(x\\).\n\n\n\n\n\n\n\n\nAvertissementDéfinition\n\n\n\nSoit une variable aléatoire \\(X\\). Cette variable aléatoire est continue si les probabilités \\(\\mathbb{P}(X \\in A)\\) sont données par des intégrales de la forme \\(\\int_{A} f(x) dx\\) où \\(f: \\mathbb{R}^d \\to \\mathbb{R}_+\\) est une fonction intégrable tel que \\(\\int_{\\mathbb{R}^d} f(x) dx = 1\\). Notons que, pour un résultat \\(x\\) fixé, \\(\\mathbb{P}(X = x) = 0\\).\n\n\n\n\n\n\n\n\nAvertissementDéfinition\n\n\n\nSoit une variable aléatoire \\(X\\). L’espérance mathématique \\(\\mathbb{E}(X)\\) de \\(X\\) est la valeur moyenne du résultat de \\(X\\) par rapport à sa distribution de probabilité. L’espérance est généralement noté \\(\\mu\\).\n\n\nSoit \\(F\\) un ensemble dénombrable. Une variable aléatoire discrète \\(X\\) a pour espérance \\(\\mathbb{E}(X) = \\sum_{x \\in F} x \\mathbb{P}(X = x)\\). Soit une variable aléatoire continue \\(X\\) ayant pour densité \\(f\\), son espérance est donnée par \\(\\mathbb{E}(X) = \\int_{\\mathbb{R}^d} x f(x) dx\\).\n\n\n\n\n\n\nMise en gardeThéorème de transfert\n\n\n\nSoit une variable aléatoire \\(X\\). Soit \\(g: \\mathbb{R}^d \\mapsto \\mathbb{R}\\) une fonction telle que \\(\\mathbb{E}\\left[ g(X) \\right]\\) existe. On a :\n\nSi \\(X\\) est une variable aléatoire discrète, \\(\\mathbb{E}\\left[ g(X) \\right] = \\sum_{x \\in F} g(x) \\mathbb{P}(X = x)\\);\nSi \\(X\\) est une variable aléatoire continue de densité \\(f\\), \\(\\mathbb{E}\\left[ g(X) \\right] = \\int_{\\mathbb{R}^d} g(x)f(x) dx\\).\n\n\n\n\n\n\n\n\n\nImportantPropriétés: Linéarité de l’espérance\n\n\n\nSoient \\(X\\) et \\(Y\\), deux variables aléatoires, dont les espérances sont définies et soit \\(\\lambda \\in R\\). On a :\n\n\\(\\mathbb{E}(X + Y) = \\mathbb{E}(X) + \\mathbb{E}(Y)\\);\n\\(\\mathbb{E}(\\lambda X) = \\lambda \\mathbb{E}(X)\\).\n\n\n\n\n\n\n\n\n\nNotePreuve\n\n\n\n\n\nLa preuve se déduit du théorème de transfert et de la linéarité de l’addition et de l’intégration.\n\n\n\n\n\n\n\n\n\nAvertissementDéfinition\n\n\n\nSoit \\(X\\) une variable aléatoire telle que l’espérance de son carré existe. La variance de \\(X\\) est définie par \\[\\mathrm{Var}(X) = \\mathbb{E}\\left[ \\left( X - \\mathbb{E}(X) \\right)^2 \\right] = \\mathbb{E}\\left[ X^2 \\right] - \\mathbb{E}\\left[ X \\right]^2.\\]\n\n\nLa variance mesure la dispersion d’une variable aléatoire autour de sa moyenne. On peut aussi s’intéresser à l’écart-type, défini comme la racine carrée de la variance : \\(\\sigma(X) = \\sqrt{\\mathrm{Var}(X)}\\).\n\n\n\n\n\n\nAvertissementDéfinition\n\n\n\nSoient \\(X\\) et \\(Y\\), deux variables aléatoires et \\(A\\) et \\(B\\), deux évenements. Si les évenements \\(\\left\\{ X \\in A \\right\\}\\) et \\(\\left\\{ Y \\in B \\right\\}\\) sont indépendants, alors on dit que les variables aléatoires \\(X\\) et \\(Y\\) sont indépendantes.\n\n\nDe cette définition, on en déduit que :\n\npour des fonctions \\(f\\) et \\(g\\), les variables aléatoires \\(f(X)\\) et \\(g(Y)\\) sont indépendantes;\nsi les variables aléatoires \\(X\\) et \\(Y\\) sont à valeurs réelles et que leur espérance existe, alors l’espérance du produit \\(XY\\) existe et \\(\\mathbb{E}(XY) = \\mathbb{E}(X) \\times \\mathbb{E}(Y)\\).\n\n\n\n\n\n\n\nAvertissementDéfinition\n\n\n\nSoit \\(X\\) une variable aléatoire. La fonction de répartition \\(F: \\mathbb{R} \\mapsto [0, 1]\\) de \\(X\\) est définie par \\[F(t) = \\mathbb{P}(X \\leq t), \\quad t \\in \\mathbb{R}.\\]",
    "crumbs": [
      "Modules",
      "02 - Révisions",
      "Probabilités et Statistiques"
    ]
  },
  {
    "objectID": "contents/remainders/02-probabilities.html#vecteurs-aléatoires",
    "href": "contents/remainders/02-probabilities.html#vecteurs-aléatoires",
    "title": "Probabilités et Statistiques",
    "section": "Vecteurs aléatoires",
    "text": "Vecteurs aléatoires\nSupposons que \\(X = (X_{1}, X_{2})\\) est une variable aléatoire de dimension \\(2\\) de densité \\(f_{X}\\). On appelle généralement les variables aléatoires de dimension supérieure à \\(1\\), des vecteurs aléatoires. Les densités de \\(X_{1}\\) et \\(X_{2}\\) sont appelées les densités marginales. Lorsque \\(X_{1}\\) et \\(X_{2}\\) sont indépendantes, on a : \\[f_X(x, y) = f_{X_{1}}(x) \\cdot f_{X_{2}}(y), \\quad (x, y) \\in \\mathbb{R}^2.\\]\n\n\n\n\n\n\nNoteExemple de la loi normale multidimensionnelle\n\n\n\nOn dit qu’un vecteur aléatoire \\(X\\) de dimension \\(p\\) suit une loi normale multidimensionnelle de moyenne \\(\\mu\\) et de variance \\(\\Sigma\\), si sa densité est donnée par \\[f_X(x) = \\frac{1}{(2 \\pi)^{p /2}} \\cdot \\frac{1}{(\\text{det} \\Sigma)^{1/2}} \\cdot \\exp\\left\\{ -\\frac{1}{2}\\left( x - \\mu \\right)^\\top \\Sigma^{-1} \\left( x - \\mu \\right) \\right\\}, \\quad x \\in \\mathbb{R}^p.\\]\nOn note \\(X \\sim \\mathcal{N}_{p}(\\mu, \\Sigma)\\).\n\n\nEn statistiques, une quantité importante à mesurer est la dépendance linéaire entre \\(X_{1}\\) et \\(X_{2}\\). Pour cela, on peut utiliser la covariance ou la correlation.\n\n\n\n\n\n\nAvertissementDéfinition\n\n\n\nSoit \\(X = (X_{1}, X_{2})\\) un vecteur aléatoire tel que l’espérance du carré de \\(X_{1}\\) et de \\(X_{2}\\) existe. La covariance entre \\(X_{1}\\) et \\(X_{2}\\) est donnée par \\[\\mathrm{Cov}(X_{1}, X_{2}) = \\mathbb{E}\\left[ (X_{1} - \\mathbb{E}(X_{1})) (X_{2} - \\mathbb{E}(X_{2}))\\right].\\]\nLa corrélation entre \\(X_{1}\\) et \\(X_{2}\\) est une version de la covariance normalisée par l’écart-type des variables aléatoires. Elle est donnée par \\[\\mathrm{Corr}(X_{1}, X_{2}) = \\frac{\\mathrm{Cov}(X_{1}, X_{2})}{\\sigma(X_{1}) \\sigma(X_{2})}.\\]\n\n\nOn peut interpréter le signe de la covariance et de la corrélation. Si elles sont strictement positives, \\(X_{1}\\) et \\(X_{2}\\) ont tendance à aller dans la même direction. Si \\(X_{1}\\) augmente, alors \\(X_{2}\\) aussi, et inversement. Si elles sont strictement négatives, \\(X_{1}\\) et \\(X_{2}\\) ont tendance à aller dans des directions opposées. Si \\(X_{1}\\) augmente, alors \\(X_{2}\\) diminue, et inversement. Si la covariance est égales à \\(0\\), il n’y a pas de règles et \\(X_{1}\\) et \\(X_{2}\\) sont dites orthogonales.\n\n\n\n\n\n\nImportantPropriétés\n\n\n\nSoit \\(X = (X_{1}, X_{2})\\) un vecteur aléatoire. On a\n\n\\(\\mathrm{Cov}(X_{1}, X_{2}) = \\mathbb{E}(X_{1}X_{2}) - \\mathbb{E}(X_{1})\\mathbb{E}(X_{2})\\);\n\\(\\mathrm{Cov}(X_{1}, X_{2}) = \\mathrm{Cov}(X_{2}, X_{1})\\);\n\\(\\mathrm{Cov}(X_{1} + \\lambda Y_{1}, X_{2}) = \\mathrm{Cov}(X_{1}, X_{2}) + \\lambda \\mathrm{Cov}(Y_{1}, X_{2})\\).\n\n\n\n\n\n\n\n\n\nNotePreuve\n\n\n\n\nOn trouve le résultat en développement le produit dans la définition de la covariance.\nEn utilisant le point 1. et la commutativité de la multiplication.\nEn utilisant le point 1. et la linéarité de l’espérance.",
    "crumbs": [
      "Modules",
      "02 - Révisions",
      "Probabilités et Statistiques"
    ]
  },
  {
    "objectID": "contents/remainders/02-probabilities.html#estimation",
    "href": "contents/remainders/02-probabilities.html#estimation",
    "title": "Probabilités et Statistiques",
    "section": "Estimation",
    "text": "Estimation\nEn practique, nous n’avons pas une connaissance parfaite de nos vecteurs aléatoires, mais seulement des réalisations de ceux-ci (que l’on appelle échantillon). Notons \\(x_{1}, \\dots, x_{n}\\), \\(n\\) réalisations indépendantes d’un vecteur aléatoire \\(X\\) de moyenne \\(\\mu\\) et de variance \\(\\Sigma\\).\nL’estimateur de la moyenne \\(\\mu\\) est donné par \\[\\widehat{\\mu} = \\overline{X} \\coloneqq \\frac{1}{n} \\sum_{i = 1}^{n} x_i.\\]\nL’estimateur de la variance \\(\\Sigma\\) est donné par \\[\\widehat{\\Sigma} \\coloneqq \\frac{1}{n - 1}\\sum_{i = 1}^{n} (x_i - \\widehat{\\mu})(x_i - \\widehat{\\mu})^\\top.\\]\nPourquoi divise t-on cette somme par \\(n -1\\) et non par \\(n\\) pour estimer la variance ? Si l’on divise par \\(n\\), \\(\\widehat{\\Sigma}\\) est un estimateur biaisé de la variance. En effet, il faut prendre en compte que l’on utilise un estimateur biaisé de la moyenne dans l’estimateur de la variance et donc corriger pour cette estimation.\nNotons \\(D = \\{\\text{diag}(\\widehat{\\Sigma})\\}^{1/2}\\), la matrice des écarts-types calculés sur l’échantillon. On peut estimer la matrice des corrélations sur l’échantillon par \\[\\widehat{R} = D^{-1} \\widehat{\\Sigma} D^{-1}.\\]",
    "crumbs": [
      "Modules",
      "02 - Révisions",
      "Probabilités et Statistiques"
    ]
  },
  {
    "objectID": "slides/03-generalities-model-evaluation-slides.html#introduction",
    "href": "slides/03-generalities-model-evaluation-slides.html#introduction",
    "title": "Généralités",
    "section": "Introduction",
    "text": "Introduction\n\nComment mesurer la performance prédictive réelle de nos modèles ?\nMétriques classiques :\n\nErreur quadratique moyenne (EQM / MSE) → variables quantitatives.\nTaux d’erreur (ER) → variables qualitatives.\n\n→ Cependant, évaluer uniquement sur les données d’entraînement pose des problèmes fondamentaux."
  },
  {
    "objectID": "slides/03-generalities-model-evaluation-slides.html#plan",
    "href": "slides/03-generalities-model-evaluation-slides.html#plan",
    "title": "Généralités",
    "section": "Plan",
    "text": "Plan\n\n\nLe sur-ajustement\nLe sous-ajustement\nApproche par jeu de validation\nValidation croisée"
  },
  {
    "objectID": "slides/03-generalities-model-evaluation-slides.html#le-problème-du-sur-ajustement",
    "href": "slides/03-generalities-model-evaluation-slides.html#le-problème-du-sur-ajustement",
    "title": "Généralités",
    "section": "Le problème du sur-ajustement",
    "text": "Le problème du sur-ajustement\n\nNotre modèle s’adapte trop aux données d’entraînement, y compris au bruit aléatoire.\n\nDonc:\n\nErreur faible sur les données d’entraînement.\nErreur élevée sur de nouvelles observations.\nMauvaise capacité de généralisation.\n\n\n\nPourquoi: L’estimateur \\(\\hat{f}\\) a été ajusté pour minimiser l’erreur sur ces mêmes données."
  },
  {
    "objectID": "slides/03-generalities-model-evaluation-slides.html#le-problème-du-sous-ajustement",
    "href": "slides/03-generalities-model-evaluation-slides.html#le-problème-du-sous-ajustement",
    "title": "Généralités",
    "section": "Le problème du sous-ajustement",
    "text": "Le problème du sous-ajustement\n\nNotre modèle ne s’adapte pas aux données d’entraînement.\n\nDonc:\n\nErreur élevée sur les données d’entraînement.\nErreur élevée sur de nouvelles observations.\nNe prend pas en compte les données.\n\n\n\nPourquoi: L’estimateur \\(\\hat{f}\\) a été implémenté sans prendre en compte les données."
  },
  {
    "objectID": "slides/03-generalities-model-evaluation-slides.html#sur-ajustement-vs-sous-ajustement",
    "href": "slides/03-generalities-model-evaluation-slides.html#sur-ajustement-vs-sous-ajustement",
    "title": "Généralités",
    "section": "Sur-ajustement vs Sous-ajustement",
    "text": "Sur-ajustement vs Sous-ajustement\n\nSur-ajustement (overfitting):\n\nModèle trop flexible.\nCapture le bruit plutôt que le signal.\n\nSous-ajustement (underfitting):\n\nModèle trop rigide.\nNe capture pas la structure des données.\n\nObjectif → Trouver le bon compromis entre les deux."
  },
  {
    "objectID": "slides/03-generalities-model-evaluation-slides.html#approche-idéale",
    "href": "slides/03-generalities-model-evaluation-slides.html#approche-idéale",
    "title": "Généralités",
    "section": "Approche idéale",
    "text": "Approche idéale\n\nSéparer complètement les données d’entraînement et d’évaluation.\n\nJeu d’entraînement : Utilisé pour ajuster le modèle.\nJeu de test : Utilisé pour évaluer la performance prédictive.\n\n\nProblème pratique : Nous n’avons généralement pas accès à un jeu de test indépendant.\n\n\nSolution → Utiliser des méthodes de validation."
  },
  {
    "objectID": "slides/03-generalities-model-evaluation-slides.html#approche-par-jeu-de-validation",
    "href": "slides/03-generalities-model-evaluation-slides.html#approche-par-jeu-de-validation",
    "title": "Généralités",
    "section": "Approche par jeu de validation",
    "text": "Approche par jeu de validation\n\n\nApproche par jeu de validation."
  },
  {
    "objectID": "slides/03-generalities-model-evaluation-slides.html#limites-du-jeu-de-validation",
    "href": "slides/03-generalities-model-evaluation-slides.html#limites-du-jeu-de-validation",
    "title": "Généralités",
    "section": "Limites du jeu de validation",
    "text": "Limites du jeu de validation\n\nEstimation instable : L’erreur dépend fortement des observations dans le jeu de validation.\n\nPerte d’information : Moins de données pour ajuster le modèle, ce qui peut surestimer l’erreur réelle.\n\n\nVariabilité : Différents jeux de validation peuvent donner des résultats très différents.\n\n\nSolution → Validation croisée pour une approche plus robuste."
  },
  {
    "objectID": "slides/03-generalities-model-evaluation-slides.html#approche-par-validation-croisée",
    "href": "slides/03-generalities-model-evaluation-slides.html#approche-par-validation-croisée",
    "title": "Généralités",
    "section": "Approche par validation croisée",
    "text": "Approche par validation croisée\n\nApproche par validation croisée."
  },
  {
    "objectID": "slides/03-generalities-model-evaluation-slides.html#choix-du-paramètre-k",
    "href": "slides/03-generalities-model-evaluation-slides.html#choix-du-paramètre-k",
    "title": "Généralités",
    "section": "Choix du paramètre K",
    "text": "Choix du paramètre K\n\nRègle du pouce: \\(K = 5\\) ou \\(K = 10\\).\nCompromis à considérer :\n\nCoût computationnel : Plus \\(K\\) est grand, plus c’est coûteux en temps de calcul.\nPrécision de l’estimation : Plus \\(K\\) est grand, moins de biais.\nStabilité : Valeurs modérées de \\(K\\) réduisent la variance\n\n\nCas limite: \\(K = n\\) (Leave-One-Out Cross-Validation - LOOCV)."
  },
  {
    "objectID": "slides/03-generalities-model-evaluation-slides.html#conclusion",
    "href": "slides/03-generalities-model-evaluation-slides.html#conclusion",
    "title": "Généralités",
    "section": "Conclusion",
    "text": "Conclusion\n\nLa validation appropriée des modèles est essentielle pour éviter une illusion de performance.\nLe sur-ajustement est un piège courant qui nécessite une évaluation rigoureuse.\nLa validation croisée offre une solution robuste et pratique pour estimer la performance réelle.\nLe choix des paramètres de validation doit équilibrer précision, stabilité et coût computationnel.\n\n\nProchaine étape → Réduction de dimension."
  },
  {
    "objectID": "slides/02-revisions-proba-slides.html#plan-de-la-présentation",
    "href": "slides/02-revisions-proba-slides.html#plan-de-la-présentation",
    "title": "Révisions",
    "section": "Plan de la présentation",
    "text": "Plan de la présentation\n\n\nModéliser le hasard\nVariables aléatoires\nVecteurs aléatoires\nEstimation statistique"
  },
  {
    "objectID": "slides/02-revisions-proba-slides.html#espace-dévénements-et-événements",
    "href": "slides/02-revisions-proba-slides.html#espace-dévénements-et-événements",
    "title": "Révisions",
    "section": "Espace d’événements et événements",
    "text": "Espace d’événements et événements\n\n\n\n\nDéfinitions\n\n\n\nEspace d’événements \\(S\\) : ensemble de tous les résultats possibles d’une expérience.\nÉvénement : sous-ensemble de \\(S\\)."
  },
  {
    "objectID": "slides/02-revisions-proba-slides.html#espace-dévénements-et-événements-exemples",
    "href": "slides/02-revisions-proba-slides.html#espace-dévénements-et-événements-exemples",
    "title": "Révisions",
    "section": "Espace d’événements et événements : exemples",
    "text": "Espace d’événements et événements : exemples\n\nExemple 1 : Lancer de pièce\n\n\\(S = \\{0, 1\\}\\) (pile = 0, face = 1).\nÉvénement \\(E = \\{1\\}\\) : “obtenir face”.\n\n\nExemple 2 : Durée de vie d’un téléphone\n\n\\(S = \\mathbb{R}_+ = [0, +\\infty)\\) (en années).\nÉvénement \\(E = [10, \\infty)\\) : “durée de vie &gt; 10 ans”."
  },
  {
    "objectID": "slides/02-revisions-proba-slides.html#mesure-de-probabilité",
    "href": "slides/02-revisions-proba-slides.html#mesure-de-probabilité",
    "title": "Révisions",
    "section": "Mesure de probabilité",
    "text": "Mesure de probabilité\n\n\n\nDéfinition\n\n\nUne mesure de probabilité \\(\\mathbb{P}\\) satisfait :\n\n\\(\\mathbb{P}(E) \\in [0, 1]\\) pour tout événement \\(E\\);\n\\(\\mathbb{P}(S) = 1\\) (certitude totale);\nPour des événements disjoints : \\(\\mathbb{P}(E_1 \\cup E_2 \\cup \\cdots) = \\mathbb{P}(E_1) + \\mathbb{P}(E_2) + \\cdots\\)."
  },
  {
    "objectID": "slides/02-revisions-proba-slides.html#mesure-de-probabilité-exemple-wooclap-kffpdj",
    "href": "slides/02-revisions-proba-slides.html#mesure-de-probabilité-exemple-wooclap-kffpdj",
    "title": "Révisions",
    "section": "Mesure de probabilité : exemple (Wooclap: KFFPDJ)",
    "text": "Mesure de probabilité : exemple (Wooclap: KFFPDJ)\n\nExemple : Lancer d’un dé équilibré\n\nEspace d’événements : \\(S = \\{1, 2, 3, 4, 5, 6\\}\\).\n\\(\\mathbb{P}(\\{i\\}) = 1/6\\) pour \\(i = 1, \\ldots, 6\\).\n\\(\\mathbb{P}(\\{2, 4, 6\\}) = 1/6 + 1/6 + 1/6 = 1/2\\) (nombre pair)."
  },
  {
    "objectID": "slides/02-revisions-proba-slides.html#subjectivité-de-la-probabilité",
    "href": "slides/02-revisions-proba-slides.html#subjectivité-de-la-probabilité",
    "title": "Révisions",
    "section": "Subjectivité de la probabilité",
    "text": "Subjectivité de la probabilité\n\nContexte : Nombre de jours sans neige à Québec dans l’année\n\nEspace d’événements : \\(S = \\{0, 1, 2, \\ldots, 365\\}\\).\nÉvénement \\(E_1 = [0, 100]\\) : “moins de 100 jours sans neige”.\nÉvénement \\(E_2 = [100, 200]\\) : “entre 100 et 200 jours sans neige”.\n\nPerspective subjective :\n\nNouvel arrivant : probabilités uniformes sur \\(S\\).\nQuébécois : probabilités plus élevées pour \\(E_1\\) que \\(E_2\\)."
  },
  {
    "objectID": "slides/02-revisions-proba-slides.html#indépendance-et-probabilité-conditionnelle",
    "href": "slides/02-revisions-proba-slides.html#indépendance-et-probabilité-conditionnelle",
    "title": "Révisions",
    "section": "Indépendance et probabilité conditionnelle",
    "text": "Indépendance et probabilité conditionnelle\n\n\n\nDéfinitions\n\n\nIndépendance : Les événements \\(E\\) et \\(F\\) sont indépendants si \\[\\mathbb{P}(E \\cap F) = \\mathbb{P}(E) \\times \\mathbb{P}(F).\\] Probabilité conditionnelle : La probabilité de \\(E\\) sachant \\(F\\) est \\[\\mathbb{P}(E \\mid F) = \\frac{\\mathbb{P}(E \\cap F)}{\\mathbb{P}(F)}.\\]"
  },
  {
    "objectID": "slides/02-revisions-proba-slides.html#exemple-cartes-à-jouer",
    "href": "slides/02-revisions-proba-slides.html#exemple-cartes-à-jouer",
    "title": "Révisions",
    "section": "Exemple : Cartes à jouer",
    "text": "Exemple : Cartes à jouer\n\nContexte : Tirage d’une carte dans un jeu standard (52 cartes)\n\n\\(E\\) : “tirer un roi” → \\(\\mathbb{P}(E) = 4/52 = 1/13\\).\n\\(F\\) : “tirer un cœur” → \\(\\mathbb{P}(F) = 13/52 = 1/4\\).\n\nTest d’indépendance :\n\n\\(E \\cap F\\) : “tirer le roi de cœur” → \\(\\mathbb{P}(E \\cap F) = 1/52\\).\n\\(\\mathbb{P}(E) \\times \\mathbb{P}(F) = (1/13) \\times (1/4) = 1/52\\).\n\nConclusion : \\(E\\) et \\(F\\) sont indépendants !"
  },
  {
    "objectID": "slides/02-revisions-proba-slides.html#variables-discrètes",
    "href": "slides/02-revisions-proba-slides.html#variables-discrètes",
    "title": "Révisions",
    "section": "Variables discrètes",
    "text": "Variables discrètes\n\n\n\n\nDéfinition\n\n\nUne variable aléatoire discrète prend au plus un nombre dénombrable de valeurs.\nSa distribution donnée par \\(\\mathbb{P}(X = x)\\) pour chaque valeur \\(x\\)."
  },
  {
    "objectID": "slides/02-revisions-proba-slides.html#variables-discrètes-exemple",
    "href": "slides/02-revisions-proba-slides.html#variables-discrètes-exemple",
    "title": "Révisions",
    "section": "Variables discrètes : exemple",
    "text": "Variables discrètes : exemple\n\nExemple : Lancer de deux dés\n\nVariable aléatoire \\(X\\) = somme des deux dés.\nEspace d’événements : \\(S = \\{2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12\\}\\).\n\\(\\mathbb{P}(X = 7) = 6/36 = 1/6\\) (combinaisons : 1+6, 2+5, 3+4, 4+3, 5+2, 6+1).\n\\(\\mathbb{P}(X = 2) = 1/36\\) (une seule combinaison : 1+1)."
  },
  {
    "objectID": "slides/02-revisions-proba-slides.html#variables-continues",
    "href": "slides/02-revisions-proba-slides.html#variables-continues",
    "title": "Révisions",
    "section": "Variables continues",
    "text": "Variables continues\n\n\n\nDéfinition\n\n\nLes probabilités d’une variable aléatoire continue sont données par les intégrales \\[\\mathbb{P}(X \\in A) = \\int_A f(x) dx\\] où \\(f(x) \\geq 0\\) et \\(\\int_{-\\infty}^{\\infty} f(x) dx = 1\\).\nImportant : \\(\\mathbb{P}(X = x) = 0\\) pour tout \\(x\\) fixé !"
  },
  {
    "objectID": "slides/02-revisions-proba-slides.html#variables-continues-exemple",
    "href": "slides/02-revisions-proba-slides.html#variables-continues-exemple",
    "title": "Révisions",
    "section": "Variables continues : exemple",
    "text": "Variables continues : exemple\nVariable uniforme sur \\([0, 1]\\) : \\[f(x) = \\begin{cases} 1 & \\text{si } x \\in [0, 1] \\\\ 0 & \\text{sinon} \\end{cases}.\\]\nCalculs de probabilités :\n\n\\(\\mathbb{P}(X \\leq 0.5) = \\int_0^{0.5} 1 \\, dx = 0.5\\).\n\\(\\mathbb{P}(0.2 &lt; X &lt; 0.7) = \\int_{0.2}^{0.7} 1 \\, dx = 0.5\\).\n\\(\\mathbb{P}(X = 0.3) = 0\\) (point isolé)."
  },
  {
    "objectID": "slides/02-revisions-proba-slides.html#espérance-mathématique",
    "href": "slides/02-revisions-proba-slides.html#espérance-mathématique",
    "title": "Révisions",
    "section": "Espérance mathématique",
    "text": "Espérance mathématique\n\n\n\nDéfinition\n\n\nL’espérance \\(\\mathbb{E}(X)\\) est la valeur moyenne de \\(X\\).\n\nCas discret : \\(\\mathbb{E}(X) = \\sum_x x \\mathbb{P}(X = x)\\).\nCas continu : \\(\\mathbb{E}(X) = \\int_{-\\infty}^{\\infty} x  f(x) dx\\)."
  },
  {
    "objectID": "slides/02-revisions-proba-slides.html#espérance-mathématique-exemple-wooclap-kffpdj",
    "href": "slides/02-revisions-proba-slides.html#espérance-mathématique-exemple-wooclap-kffpdj",
    "title": "Révisions",
    "section": "Espérance mathématique : exemple (Wooclap: KFFPDJ)",
    "text": "Espérance mathématique : exemple (Wooclap: KFFPDJ)\nExemple 1 : Dé équilibré\n\n\\[\\mathbb{E}(X) = 1 \\cdot \\frac{1}{6} + 2 \\cdot \\frac{1}{6} + \\cdots + 6 \\cdot \\frac{1}{6} = \\frac{21}{6} = 3.5.\\]\n\n\nExemple 2 : Loi uniforme sur \\([0, 1]\\)\n\n\n\\[\\mathbb{E}(X) = \\int_0^1 x \\cdot 1 \\, dx = \\left[\\frac{x^2}{2}\\right]_0^1 = \\frac{1}{2}.\\]"
  },
  {
    "objectID": "slides/02-revisions-proba-slides.html#théorème-de-transfert",
    "href": "slides/02-revisions-proba-slides.html#théorème-de-transfert",
    "title": "Révisions",
    "section": "Théorème de transfert",
    "text": "Théorème de transfert\n\n\n\nThéorème\n\n\nPour une fonction \\(g\\), l’espérance de \\(g(X)\\) est :\n\nCas discret : \\(\\mathbb{E}[g(X)] = \\sum_x g(x) \\mathbb{P}(X = x)\\).\nCas continu : \\(\\mathbb{E}[g(X)] = \\int_{-\\infty}^{\\infty} g(x) f(x) dx\\)."
  },
  {
    "objectID": "slides/02-revisions-proba-slides.html#théorème-de-transfert-exemple-wooclap-kffpdj",
    "href": "slides/02-revisions-proba-slides.html#théorème-de-transfert-exemple-wooclap-kffpdj",
    "title": "Révisions",
    "section": "Théorème de transfert : exemple (Wooclap: KFFPDJ)",
    "text": "Théorème de transfert : exemple (Wooclap: KFFPDJ)\n\nPour \\(X\\) suivant une loi uniforme sur \\([0, 1]\\) et \\(g(x) = x^2\\) :\n\n\\[\\mathbb{E}[X^2] = \\int_0^1 x^2 \\cdot 1 \\, dx = \\left[\\frac{x^3}{3}\\right]_0^1 = \\frac{1}{3}.\\]"
  },
  {
    "objectID": "slides/02-revisions-proba-slides.html#variance-et-écart-type",
    "href": "slides/02-revisions-proba-slides.html#variance-et-écart-type",
    "title": "Révisions",
    "section": "Variance et écart-type",
    "text": "Variance et écart-type\n\n\n\n\nDéfinitions\n\n\nVariance : \\(\\text{Var}(X) = \\mathbb{E}[(X - \\mathbb{E}(X))^2] = \\mathbb{E}[X^2] - \\mathbb{E}[X]^2\\)\nÉcart-type : \\(\\sigma(X) = \\sqrt{\\text{Var}(X)}\\)"
  },
  {
    "objectID": "slides/02-revisions-proba-slides.html#variance-et-écart-type-exemple",
    "href": "slides/02-revisions-proba-slides.html#variance-et-écart-type-exemple",
    "title": "Révisions",
    "section": "Variance et écart-type : exemple",
    "text": "Variance et écart-type : exemple\n\nLoi uniforme sur \\([0, 1]\\)\n\n\\(\\mathbb{E}(X) = 1/2\\)\n\\(\\mathbb{E}(X^2) = 1/3\\)\n\\(\\text{Var}(X) = 1/3 - (1/2)^2 = 1/3 - 1/4 = 1/12\\)\n\\(\\sigma(X) = \\sqrt{1/12} \\approx 0.289\\)"
  },
  {
    "objectID": "slides/02-revisions-proba-slides.html#fonction-de-répartition",
    "href": "slides/02-revisions-proba-slides.html#fonction-de-répartition",
    "title": "Révisions",
    "section": "Fonction de répartition",
    "text": "Fonction de répartition\n\n\n\n\nDéfinition\n\n\nLa fonction de répartition est donnée par : \\[F(t) = \\mathbb{P}(X \\leq t).\\]"
  },
  {
    "objectID": "slides/02-revisions-proba-slides.html#fonction-de-répartition-exemple",
    "href": "slides/02-revisions-proba-slides.html#fonction-de-répartition-exemple",
    "title": "Révisions",
    "section": "Fonction de répartition : exemple",
    "text": "Fonction de répartition : exemple\nLoi uniforme sur \\([0, 1]\\)\n\\[F(t) = \\begin{cases}\n0 & \\text{si } t &lt; 0 \\\\\nt & \\text{si } 0 \\leq t \\leq 1 \\\\\n1 & \\text{si } t &gt; 1\n\\end{cases}.\\]\nCalcul : \\(\\mathbb{P}(0.2 &lt; X \\leq 0.7) = F(0.7) - F(0.2) = 0.7 - 0.2 = 0.5\\)."
  },
  {
    "objectID": "slides/02-revisions-proba-slides.html#densités-marginales-et-indépendance",
    "href": "slides/02-revisions-proba-slides.html#densités-marginales-et-indépendance",
    "title": "Révisions",
    "section": "Densités marginales et indépendance",
    "text": "Densités marginales et indépendance\nVecteur aléatoire : \\(X = (X_1, X_2)^\\top\\) avec densité conjointe \\(f_X(x, y)\\).\nDensités marginales :\n\n\\(f_{X_1}(x) = \\int_{-\\infty}^{\\infty} f_X(x, y) dy\\)\n\\(f_{X_2}(y) = \\int_{-\\infty}^{\\infty} f_X(x, y) dx\\)\n\nIndépendance : \\(f_X(x, y) = f_{X_1}(x) \\cdot f_{X_2}(y)\\)."
  },
  {
    "objectID": "slides/02-revisions-proba-slides.html#exemple-variables-indépendantes",
    "href": "slides/02-revisions-proba-slides.html#exemple-variables-indépendantes",
    "title": "Révisions",
    "section": "Exemple : Variables indépendantes",
    "text": "Exemple : Variables indépendantes\nContexte : Soient \\(X_1\\) et \\(X_2\\) uniformes indépendantes sur \\([0, 1]\\).\n\\[f_X(x, y) = \\begin{cases} 1 & \\text{si } (x,y) \\in [0,1]^2 \\\\ 0 & \\text{sinon} \\end{cases}\\]\n\nCalculs de probabilités :\n\n\\(\\mathbb{P}(X_1 \\leq 0.5, X_2 \\leq 0.8) = \\int_0^{0.5} \\int_0^{0.8} 1 \\, dy \\, dx = 0.5 \\times 0.8 = 0.4\\).\n\\(\\mathbb{P}(X_1 + X_2 \\leq 1) = \\int \\int_{x+y \\leq 1} 1 \\, dy \\, dx = 0.5\\) (aire du triangle)."
  },
  {
    "objectID": "slides/02-revisions-proba-slides.html#loi-normale-multivariée",
    "href": "slides/02-revisions-proba-slides.html#loi-normale-multivariée",
    "title": "Révisions",
    "section": "Loi normale multivariée",
    "text": "Loi normale multivariée\n\n\n\nLoi normale \\(p\\)-dimensionnelle\n\n\nSoit \\(X\\) un vecteur aléatoire suivant une loi normale \\(p\\)-dimensionnelle. Sa densité est donnée par : \\[f_X(x) = \\frac{1}{(2\\pi)^{p/2}} \\frac{1}{(\\det \\Sigma)^{1/2}} \\exp\\left\\{-\\frac{1}{2}(x-\\mu)^T \\Sigma^{-1} (x-\\mu)\\right\\}.\\]\nNotation : \\(X \\sim \\mathcal{N}_p(\\mu, \\Sigma)\\)."
  },
  {
    "objectID": "slides/02-revisions-proba-slides.html#exemple-loi-normale-bivariée",
    "href": "slides/02-revisions-proba-slides.html#exemple-loi-normale-bivariée",
    "title": "Révisions",
    "section": "Exemple : Loi normale bivariée",
    "text": "Exemple : Loi normale bivariée\nCas simple : \\(X \\sim \\mathcal{N}_2(\\mu, I_2)\\) avec \\(\\mu = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\\)\n\\[f_X(x, y) = \\frac{1}{2\\pi} \\exp\\left\\{-\\frac{x^2 + y^2}{2}\\right\\}\\]"
  },
  {
    "objectID": "slides/02-revisions-proba-slides.html#covariance-et-corrélation",
    "href": "slides/02-revisions-proba-slides.html#covariance-et-corrélation",
    "title": "Révisions",
    "section": "Covariance et corrélation",
    "text": "Covariance et corrélation\n\n\n\nDéfinitions\n\n\nSoit \\(X = (X_1, X_2)^\\top\\) un vecteur aléatoire.\nCovariance : \\(\\text{Cov}(X_1, X_2) = \\mathbb{E}[(X_1 - \\mathbb{E}(X_1))(X_2 - \\mathbb{E}(X_2))].\\)\nCorrélation : \\(\\text{Corr}(X_1, X_2) = \\frac{\\text{Cov}(X_1, X_2)}{\\sigma(X_1) \\sigma(X_2)}.\\)"
  },
  {
    "objectID": "slides/02-revisions-proba-slides.html#interprétation-des-signes",
    "href": "slides/02-revisions-proba-slides.html#interprétation-des-signes",
    "title": "Révisions",
    "section": "Interprétation des signes",
    "text": "Interprétation des signes\n\nCorrélation positive (\\(&gt; 0\\)) : \\(X_1 \\nearrow ~\\Rightarrow ~ X_2 \\nearrow\\).\nCorrélation négative (\\(&lt; 0\\)) : \\(X_1 \\searrow ~\\Rightarrow ~ X_2 \\searrow\\).\nCorrélation nulle (\\(= 0\\)) : pas de relation linéaire (variables orthogonales)"
  },
  {
    "objectID": "slides/02-revisions-proba-slides.html#interprétation-des-signes-exemple-wooclap-kffpdj",
    "href": "slides/02-revisions-proba-slides.html#interprétation-des-signes-exemple-wooclap-kffpdj",
    "title": "Révisions",
    "section": "Interprétation des signes : exemple (Wooclap: KFFPDJ)",
    "text": "Interprétation des signes : exemple (Wooclap: KFFPDJ)\nSoit \\(X = (X_1, X_2, X_3)^\\top\\) avec \\[\\text{Var}(X) = \\begin{pmatrix} 4 & 0 & 3 \\\\ 0 & 2 & 0 \\\\ 3 & 0 & 9 \\end{pmatrix}.\\] Calculer \\(\\text{Corr}(X_1, X_3)\\).\n\n\\[\\text{Corr}(X_1, X_3) = \\frac{\\text{Cov}(X_1, X_3)}{\\sigma(X_1)\\sigma(X_3)}\n= \\frac{3}{\\sqrt{4} \\times \\sqrt{9}} = \\frac{1}{2}.\\]"
  },
  {
    "objectID": "slides/02-revisions-proba-slides.html#propriétés-de-la-covariance",
    "href": "slides/02-revisions-proba-slides.html#propriétés-de-la-covariance",
    "title": "Révisions",
    "section": "Propriétés de la covariance",
    "text": "Propriétés de la covariance\n\n\n\nPropriétés\n\n\n\n\\(\\text{Cov}(X_1, X_2) = \\mathbb{E}(X_1 X_2) - \\mathbb{E}(X_1)\\mathbb{E}(X_2)\\).\n\\(\\text{Cov}(X_1, X_2) = \\text{Cov}(X_2, X_1)\\) (symétrie).\n\\(\\text{Cov}(X_1 + \\lambda Y_1, X_2) = \\text{Cov}(X_1, X_2) + \\lambda \\text{Cov}(Y_1, X_2)\\) (linéarité)."
  },
  {
    "objectID": "slides/02-revisions-proba-slides.html#le-problème-de-lestimation",
    "href": "slides/02-revisions-proba-slides.html#le-problème-de-lestimation",
    "title": "Révisions",
    "section": "Le problème de l’estimation",
    "text": "Le problème de l’estimation\n\n\nRéalité : On ne connaît pas exactement la distribution de \\(X\\).\nDonnées : Échantillon \\(x_1, x_2, \\ldots, x_n\\) (observations).\nObjectif : Estimer les paramètres inconnus (\\(\\mu\\), \\(\\Sigma\\), etc.)."
  },
  {
    "objectID": "slides/02-revisions-proba-slides.html#estimation-de-la-moyenne",
    "href": "slides/02-revisions-proba-slides.html#estimation-de-la-moyenne",
    "title": "Révisions",
    "section": "Estimation de la moyenne",
    "text": "Estimation de la moyenne\n\n\n\nEstimateur de la moyenne\n\n\n\\[\\widehat{\\mu} = \\overline{X} = \\frac{1}{n} \\sum_{i=1}^n x_i.\\]\n\n\n\n\nExemple pratique : Tailles en cm : 170, 175, 168, 180, 172\n\\[\\widehat{\\mu} = \\frac{170 + 175 + 168 + 180 + 172}{5} = \\frac{865}{5} = 173 \\text{cm.}\\]"
  },
  {
    "objectID": "slides/02-revisions-proba-slides.html#estimation-de-la-variance",
    "href": "slides/02-revisions-proba-slides.html#estimation-de-la-variance",
    "title": "Révisions",
    "section": "Estimation de la variance",
    "text": "Estimation de la variance\n\n\n\nEstimateur de la variance\n\n\n\\[\\widehat{\\Sigma} = \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\widehat{\\mu})(x_i - \\widehat{\\mu})^T\\]\nPourquoi diviser par \\(n-1\\) ? Correction du biais dû à l’utilisation de \\(\\widehat{\\mu}\\)."
  },
  {
    "objectID": "slides/02-revisions-proba-slides.html#exemple-complet-de-calcul",
    "href": "slides/02-revisions-proba-slides.html#exemple-complet-de-calcul",
    "title": "Révisions",
    "section": "Exemple complet de calcul",
    "text": "Exemple complet de calcul\nDonnées : 170, 175, 168, 180, 172 (tailles en cm)\n\n\\(\\widehat{\\mu} = 173\\)\nÉcarts au carré : \\((170-173)^2 = 9\\), \\((175-173)^2 = 4\\), \\((168-173)^2 = 25\\), \\((180-173)^2 = 49\\), \\((172-173)^2 = 1\\).\n\nVariance estimée :\n\\[\\widehat{\\sigma^2} = \\frac{9 + 4 + 25 + 49 + 1}{5-1} = \\frac{88}{4} = 22.\\]"
  },
  {
    "objectID": "slides/02-revisions-proba-slides.html#estimation-de-la-corrélation",
    "href": "slides/02-revisions-proba-slides.html#estimation-de-la-corrélation",
    "title": "Révisions",
    "section": "Estimation de la corrélation",
    "text": "Estimation de la corrélation\nMatrice de corrélation estimée : \\(\\widehat{R} = D^{-1} \\widehat{\\Sigma} D^{-1}\\) où \\(D\\) est la matrice diagonale des écarts-types.\n\nExemple 2D : Si \\(\\widehat{\\Sigma} = \\begin{pmatrix} 4 & 1.5 \\\\ 1.5 & 9 \\end{pmatrix}\\). Alors,\n\\[D = \\begin{pmatrix} 2 & 0 \\\\ 0 & 3 \\end{pmatrix}, \\quad\\text{et}\\quad\\widehat{R} = \\begin{pmatrix} 1 & 0.25 \\\\ 0.25 & 1 \\end{pmatrix}.\\]\nInterprétation : corrélation faible positive (0.25) entre les variables."
  },
  {
    "objectID": "slides/04-dimension-pca-slides.html#pourquoi-réduire-la-dimension",
    "href": "slides/04-dimension-pca-slides.html#pourquoi-réduire-la-dimension",
    "title": "Dimension",
    "section": "Pourquoi réduire la dimension ?",
    "text": "Pourquoi réduire la dimension ?\n\nTravailler avec un grand nombre de variables pose plusieurs défis :\n\nVisualisation compliquée : impossible de représenter au-delà de 3 dimensions.\nSéparation des classes difficile : structure cachée dans des combinaisons.\nCoût computationnel élevé : modèles complexes difficiles à ajuster.\nCorrélations fortes : variables redondantes, modèles instables.\n\n\n→ Peut-on réduire la dimension sans perdre trop d’information ?"
  },
  {
    "objectID": "slides/04-dimension-pca-slides.html#plan",
    "href": "slides/04-dimension-pca-slides.html#plan",
    "title": "Dimension",
    "section": "Plan",
    "text": "Plan\n\n\nLa théorie de l’analyse en composantes principales\nL’ACP en pratique\nExemple : Le top 10 des artistes Spotify"
  },
  {
    "objectID": "slides/04-dimension-pca-slides.html#lapproche-pca",
    "href": "slides/04-dimension-pca-slides.html#lapproche-pca",
    "title": "Dimension",
    "section": "L’approche PCA",
    "text": "L’approche PCA\n\nPrincipe → Ne pas simplement supprimer des variables, mais construire de nouvelles variables.\n\nMéthode → Créer des combinaisons linéaires des variables initiales qui résument l’information essentielle."
  },
  {
    "objectID": "slides/04-dimension-pca-slides.html#lapproche-pca-1",
    "href": "slides/04-dimension-pca-slides.html#lapproche-pca-1",
    "title": "Dimension",
    "section": "L’approche PCA",
    "text": "L’approche PCA\n\nAnalyse en Composantes Principales (ACP) → Méthode non-supervisée pour réduire la dimension tout en conservant le maximum d’information.\n\n→ Introduite par Hotelling (1933)."
  },
  {
    "objectID": "slides/04-dimension-pca-slides.html#applications-courantes",
    "href": "slides/04-dimension-pca-slides.html#applications-courantes",
    "title": "Dimension",
    "section": "Applications courantes",
    "text": "Applications courantes\n\n\nVisualisation : Représenter des données multidimensionnelles.\nRéduction dimensionnelle : Passer de \\(p\\) à \\(k \\ll p\\) variables.\nCompression : Images ou signaux.\nExploration : Données biologiques, textuelles, environnementales, …"
  },
  {
    "objectID": "slides/04-dimension-pca-slides.html#exemples-concrets",
    "href": "slides/04-dimension-pca-slides.html#exemples-concrets",
    "title": "Dimension",
    "section": "Exemples concrets",
    "text": "Exemples concrets\n\n\nSport : Comparer des équipes de hockey sur 6 statistiques.\nCriminologie : Résumer la criminalité entre provinces sur 7 types de crimes.\nImagerie : Compresser des images de \\(1084 \\times 1084\\) pixels.\nBiologie : Identifier des variants de tumeurs à partir de millions de gènes."
  },
  {
    "objectID": "slides/04-dimension-pca-slides.html#formulation-mathématique",
    "href": "slides/04-dimension-pca-slides.html#formulation-mathématique",
    "title": "Dimension",
    "section": "Formulation mathématique",
    "text": "Formulation mathématique\n\nSoit \\(X = (X_1, \\ldots, X_p)^{\\top}\\) un vecteur aléatoire centré avec matrice de covariance \\(\\Sigma\\).\nOn cherche une combinaison linéaire des variables \\(X_k\\) : \\[Y_1 = \\alpha_1^{\\top} X = \\sum_{k=1}^p \\alpha_{1k} X_k.\\]\nObjectif → Maximiser \\(\\text{Var}(Y_1)\\) sous contrainte \\(\\|\\alpha_1\\| = 1\\)."
  },
  {
    "objectID": "slides/04-dimension-pca-slides.html#problème-doptimisation",
    "href": "slides/04-dimension-pca-slides.html#problème-doptimisation",
    "title": "Dimension",
    "section": "Problème d’optimisation",
    "text": "Problème d’optimisation\n\nLe problème d’optimisation s’écrit : \\[\\max_{\\alpha_1^{\\top} \\alpha_1 = 1} \\text{Var}(Y_1) = \\max_{\\alpha_1^{\\top} \\alpha_1 = 1} \\alpha_1^{\\top} \\Sigma \\alpha_1.\\]"
  },
  {
    "objectID": "slides/04-dimension-pca-slides.html#problème-doptimisation-1",
    "href": "slides/04-dimension-pca-slides.html#problème-doptimisation-1",
    "title": "Dimension",
    "section": "Problème d’optimisation",
    "text": "Problème d’optimisation"
  },
  {
    "objectID": "slides/04-dimension-pca-slides.html#problème-doptimisation-2",
    "href": "slides/04-dimension-pca-slides.html#problème-doptimisation-2",
    "title": "Dimension",
    "section": "Problème d’optimisation",
    "text": "Problème d’optimisation\n\nSolution (multiplicateurs de Lagrange) : \\[\\Sigma \\alpha_1 = \\lambda_1 \\alpha_1\\]\noù \\(\\lambda_1\\) est la plus grande valeur propre de \\(\\Sigma\\) et \\(\\alpha_1\\) le vecteur propre associé."
  },
  {
    "objectID": "slides/04-dimension-pca-slides.html#construction-des-composantes",
    "href": "slides/04-dimension-pca-slides.html#construction-des-composantes",
    "title": "Dimension",
    "section": "Construction des composantes",
    "text": "Construction des composantes\n\nLes composantes suivantes sont construites par orthogonalité :\n\\[\\Sigma \\alpha_k = \\lambda_k \\alpha_k, \\quad \\lambda_1 \\geq \\lambda_2 \\geq \\ldots \\geq \\lambda_p\\]\nComposantes principales : \\[Y_k = \\alpha_k^{\\top} X\\]\noù \\(\\alpha_k\\) est le vecteur propre associé à \\(\\lambda_k\\)."
  },
  {
    "objectID": "slides/04-dimension-pca-slides.html#représentation-matricielle",
    "href": "slides/04-dimension-pca-slides.html#représentation-matricielle",
    "title": "Dimension",
    "section": "Représentation matricielle",
    "text": "Représentation matricielle\n\nSoit \\(A = (\\alpha_1, \\ldots, \\alpha_p) \\in \\mathbb{R}^{p \\times p}\\) la matrice des vecteurs propres.\n\\[Y = A^{\\top} X\\]\nPropriétés de \\(A\\) :\n\n\\(A^{\\top} A = AA^{\\top} = I_p\\) (orthogonale)\n\\(A^{\\top} = A^{-1}\\)\n\\(\\Sigma A = A \\Lambda\\) où \\(\\Lambda = \\text{diag}(\\lambda_1, \\ldots, \\lambda_p)\\)"
  },
  {
    "objectID": "slides/04-dimension-pca-slides.html#variance-des-composantes",
    "href": "slides/04-dimension-pca-slides.html#variance-des-composantes",
    "title": "Dimension",
    "section": "Variance des composantes",
    "text": "Variance des composantes\n\n\\[\\text{Var}(Y) = A^{\\top} \\Sigma A = \\Lambda\\]\nConséquences :\n\n\\(\\text{Cov}(Y_k, Y_l) = 0\\) si \\(k \\neq l\\) (orthogonalité)\n\\(\\text{Var}(Y_k) = \\lambda_k\\) (variance décroissante)\n\\(\\text{Var}(Y_1) \\geq \\text{Var}(Y_2) \\geq \\ldots \\geq \\text{Var}(Y_p)\\)"
  },
  {
    "objectID": "slides/04-dimension-pca-slides.html#variation-totale",
    "href": "slides/04-dimension-pca-slides.html#variation-totale",
    "title": "Dimension",
    "section": "Variation totale",
    "text": "Variation totale\n\nLa trace de Pillai est une mesure globale de la variation présente dans les données :\n\\[\\text{tr}(\\Sigma) = \\sum_{i=1}^p \\lambda_i = \\sum_{k=1}^p \\text{Var}(Y_k).\\]"
  },
  {
    "objectID": "slides/04-dimension-pca-slides.html#proportion-expliquée-par-y_k",
    "href": "slides/04-dimension-pca-slides.html#proportion-expliquée-par-y_k",
    "title": "Dimension",
    "section": "Proportion expliquée par \\(Y_k\\)",
    "text": "Proportion expliquée par \\(Y_k\\)\n\nLa proportion de variance expliquée par la composante principale \\(Y_k\\) est\n\\[\\frac{\\lambda_k}{\\lambda_1 + \\cdots + \\lambda_p} = \\frac{\\text{Var}(Y_k)}{\\text{tr}(\\Sigma)}.\\]"
  },
  {
    "objectID": "slides/04-dimension-pca-slides.html#proportion-expliquée-par-les-m-premières",
    "href": "slides/04-dimension-pca-slides.html#proportion-expliquée-par-les-m-premières",
    "title": "Dimension",
    "section": "Proportion expliquée par les \\(m\\) premières",
    "text": "Proportion expliquée par les \\(m\\) premières\n\nLa variabilité des données expliquée par les \\(m\\) premières composantes est :\n\\[100\\% \\times \\frac{\\sum_{k=1}^m \\lambda_k}{\\sum_{k=1}^p \\lambda_k}.\\]"
  },
  {
    "objectID": "slides/04-dimension-pca-slides.html#estimation-pratique",
    "href": "slides/04-dimension-pca-slides.html#estimation-pratique",
    "title": "Dimension",
    "section": "Estimation pratique",
    "text": "Estimation pratique\n\n\nEn pratique, \\(\\Sigma\\) est inconnue. On l’estime par : \\[\\hat{\\Sigma} = \\frac{1}{n-1} \\sum_{i=1}^n (X_i - \\overline{X})(X_i - \\overline{X})^{\\top}.\\]\nDécomposition spectrale : \\(\\hat{\\Sigma} = \\hat{A} \\hat{\\Lambda} \\hat{A}^{\\top}.\\)\nComposantes estimées : \\(Y_i = \\hat{A}^{\\top} X_i.\\)"
  },
  {
    "objectID": "slides/04-dimension-pca-slides.html#sensibilité-à-léchelle",
    "href": "slides/04-dimension-pca-slides.html#sensibilité-à-léchelle",
    "title": "Dimension",
    "section": "Sensibilité à l’échelle",
    "text": "Sensibilité à l’échelle\n\nProblème → Variables avec grandes variances dominent l’ACP.\nExemple → Distance en m vs km multiplie la variance par \\(10^6\\).\nSolution → Standardiser les variables si elles ont des unités différentes.\nÉquivalence → ACP sur matrice de corrélations au lieu de covariances."
  },
  {
    "objectID": "slides/04-dimension-pca-slides.html#interprétation-géométrique",
    "href": "slides/04-dimension-pca-slides.html#interprétation-géométrique",
    "title": "Dimension",
    "section": "Interprétation géométrique",
    "text": "Interprétation géométrique\n\n\nRotation des axes → \\(Y = A^{\\top}X\\) représente une rotation orthogonale.\nNouveaux axes → Directions de variation maximale successive.\nScores → \\(Y_{ik} = \\alpha_k^{\\top} X_i\\) (coordonnées dans le nouveau système).\nQualité de représentation \\[Q_{ik} = \\frac{Y_{ik}^2}{\\sum_{j=1}^p Y_{ij}^2}\\]"
  },
  {
    "objectID": "slides/04-dimension-pca-slides.html#choix-du-nombre-de-composantes",
    "href": "slides/04-dimension-pca-slides.html#choix-du-nombre-de-composantes",
    "title": "Dimension",
    "section": "Choix du nombre de composantes",
    "text": "Choix du nombre de composantes\n\n\nRègle des 80% : Retenir assez de composantes pour expliquer 80% de la variance.\nRègle de Kaiser : Conserver les composantes avec \\(\\lambda_k &gt; 1\\) (si matrice de corrélations).\nRègle de Joliffe : Variante plus stricte, \\(\\lambda_k &gt; 0.7\\).\nRègle de Cattell (coude) : Chercher un point de rupture dans le graphique des valeurs propres."
  },
  {
    "objectID": "slides/04-dimension-pca-slides.html#considérations-pratiques",
    "href": "slides/04-dimension-pca-slides.html#considérations-pratiques",
    "title": "Dimension",
    "section": "Considérations pratiques",
    "text": "Considérations pratiques\n\nUsage dans les modèles : Utiliser les \\(k\\) premières composantes comme prédicteurs.\n\nAvantages :\n\nÉlimine la multicolinéarité\nConserve l’information essentielle\n\n\n\nCompromis :\n\nTrop de composantes : pas de simplification\nTrop peu : perte d’information pertinente"
  },
  {
    "objectID": "slides/04-dimension-pca-slides.html#exemple-le-top-10-spotify",
    "href": "slides/04-dimension-pca-slides.html#exemple-le-top-10-spotify",
    "title": "Dimension",
    "section": "Exemple : le top 10 Spotify",
    "text": "Exemple : le top 10 Spotify\nOn considère le jeu de données suivant :\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrack_name\ntrack_artist\ntrack_popularity\ndanceability\nenergy\nloudness\nspeechiness\nacousticness\nliveness\nvalence\ntempo\n\n\n\n\nI Don’t Care (with Justin Bieber) - Loud Luxury Remix\nEd Sheeran\n66\n0.748\n0.9160\n-2.634\n0.0583\n0.102000\n0.0653\n0.5180\n122.036\n\n\nBeautiful People (feat. Khalid) - Jack Wins Remix\nEd Sheeran\n67\n0.675\n0.9190\n-5.385\n0.1270\n0.079900\n0.1430\n0.5850\n124.982\n\n\nCross Me (feat. Chance the Rapper & PnB Rock) - M-22 Remix\nEd Sheeran\n58\n0.679\n0.9230\n-6.500\n0.1810\n0.146000\n0.1240\n0.7520\n121.984\n\n\nSouth of the Border (feat. Camila Cabello & Cardi B) - Andy Jarvis Remix\nEd Sheeran\n66\n0.805\n0.8350\n-4.603\n0.0896\n0.130000\n0.3650\n0.7220\n125.028\n\n\nbad guy (with Justin Bieber)\nBillie Eilish\n83\n0.670\n0.4530\n-11.265\n0.2950\n0.252000\n0.1170\n0.6800\n135.055\n\n\nSouth of the Border (feat. Camila Cabello & Cardi B) - Sam Feldt Remix\nEd Sheeran\n65\n0.789\n0.8580\n-4.388\n0.0404\n0.220000\n0.3350\n0.4910\n110.012\n\n\nFriends (with BloodPop®)\nJustin Bieber\n76\n0.743\n0.7330\n-5.337\n0.0375\n0.004020\n0.3040\n0.6220\n104.977\n\n\nSorry\nJustin Bieber\n81\n0.654\n0.7600\n-3.669\n0.0450\n0.079700\n0.2990\n0.4100\n99.945\n\n\nPoker Face\nLady Gaga\n69\n0.851\n0.8060\n-4.618\n0.0786\n0.120000\n0.1210\n0.7760\n119.001\n\n\nSouth of the Border (feat. Camila Cabello & Cardi B) - Cheat Codes Remix\nEd Sheeran\n69\n0.845\n0.7660\n-5.727\n0.0658\n0.212000\n0.1110\n0.7820\n114.054\n\n\nHappier - Tiësto’s AFTR:HRS Remix\nEd Sheeran\n53\n0.684\n0.8230\n-4.881\n0.0296\n0.005760\n0.2010\n0.3840\n117.993\n\n\nbad guy\nBillie Eilish\n95\n0.701\n0.4250\n-10.965\n0.3750\n0.328000\n0.1000\n0.5620\n135.128\n\n\nA Sky Full of Stars\nColdplay\n78\n0.551\n0.6360\n-7.125\n0.0281\n0.007130\n0.2170\n0.1530\n124.996\n\n\neverything i wanted\nBillie Eilish\n97\n0.704\n0.2250\n-14.454\n0.0994\n0.902000\n0.1060\n0.2430\n120.006\n\n\nYummy\nJustin Bieber\n95\n0.662\n0.5190\n-6.554\n0.1060\n0.404000\n0.1210\n0.4950\n145.841\n\n\nBlinding Lights\nThe Weeknd\n98\n0.513\n0.7960\n-4.075\n0.0629\n0.001470\n0.0938\n0.3450\n171.017\n\n\nHeartless\nThe Weeknd\n93\n0.531\n0.7500\n-5.831\n0.1110\n0.006320\n0.1170\n0.1980\n169.954\n\n\nLover (Remix) [feat. Shawn Mendes]\nTaylor Swift\n85\n0.448\n0.6030\n-7.176\n0.0640\n0.433000\n0.0862\n0.4220\n205.272\n\n\nThe Morning\nThe Weeknd\n70\n0.652\n0.5490\n-7.555\n0.0390\n0.317000\n0.1040\n0.1860\n120.073\n\n\nShape of You\nEd Sheeran\n86\n0.825\n0.6520\n-3.183\n0.0802\n0.581000\n0.0931\n0.9310\n95.977\n\n\nBeautiful People (feat. Khalid)\nEd Sheeran\n85\n0.640\n0.6480\n-8.113\n0.1870\n0.124000\n0.0802\n0.5480\n92.977\n\n\nSouth of the Border (feat. Camila Cabello & Cardi B)\nEd Sheeran\n91\n0.857\n0.6210\n-6.376\n0.0824\n0.148000\n0.0865\n0.6680\n97.986\n\n\nI Don’t Care (with Justin Bieber)\nEd Sheeran\n84\n0.798\n0.6750\n-5.041\n0.0442\n0.091200\n0.0894\n0.8420\n101.956\n\n\nOrphans\nColdplay\n78\n0.503\n0.8080\n-5.051\n0.0450\n0.025400\n0.5570\n0.2830\n107.975\n\n\nApplause\nLady Gaga\n70\n0.669\n0.7800\n-4.287\n0.0530\n0.026500\n0.1430\n0.7380\n139.945\n\n\nJudas\nLady Gaga\n56\n0.661\n0.9370\n-3.830\n0.0744\n0.000967\n0.3550\n0.5350\n131.001\n\n\nBlank Space\nTaylor Swift\n78\n0.760\n0.7030\n-5.412\n0.0540\n0.103000\n0.0913\n0.5700\n95.997\n\n\nThe Cure\nLady Gaga\n70\n0.697\n0.5060\n-4.863\n0.0325\n0.079200\n0.0893\n0.5160\n100.002\n\n\nUmbrella\nRihanna\n80\n0.583\n0.8290\n-4.603\n0.1340\n0.008640\n0.0426\n0.5750\n174.028\n\n\nJust The Way You Are\nBruno Mars\n20\n0.637\n0.8430\n-5.413\n0.0432\n0.015100\n0.0876\n0.4340\n109.012\n\n\nDon’t Stop The Music\nRihanna\n17\n0.835\n0.6690\n-5.582\n0.0643\n0.033600\n0.0535\n0.5420\n122.668\n\n\nPhotograph\nEd Sheeran\n84\n0.614\n0.3790\n-10.480\n0.0476\n0.607000\n0.0986\n0.2010\n107.989\n\n\nYou Need To Calm Down\nTaylor Swift\n86\n0.771\n0.6710\n-5.617\n0.0553\n0.009290\n0.0637\n0.7140\n85.026\n\n\n22\nTaylor Swift\n0\n0.661\n0.7240\n-6.548\n0.0373\n0.002880\n0.0631\n0.6760\n104.011\n\n\nBaby\nJustin Bieber\n76\n0.728\n0.8600\n-5.237\n0.1370\n0.040100\n0.1110\n0.5350\n65.043\n\n\nViva La Vida\nColdplay\n78\n0.485\n0.6190\n-7.115\n0.0289\n0.094100\n0.1090\n0.4160\n138.017\n\n\nBad Romance\nLady Gaga\n78\n0.696\n0.9210\n-3.755\n0.0363\n0.003140\n0.0842\n0.7140\n119.001\n\n\nI Knew You Were Trouble.\nTaylor Swift\n76\n0.622\n0.4690\n-6.798\n0.0363\n0.004540\n0.0335\n0.6790\n77.019\n\n\nOur Song\nTaylor Swift\n61\n0.667\n0.6590\n-4.932\n0.0298\n0.113000\n0.3250\n0.5750\n89.128\n\n\nMean\nTaylor Swift\n62\n0.570\n0.7470\n-3.978\n0.0426\n0.445000\n0.2190\n0.8080\n164.004\n\n\nThe Lazy Song\nBruno Mars\n73\n0.738\n0.8040\n-3.851\n0.0621\n0.332000\n0.2790\n0.9490\n174.943\n\n\nLove The Way You Lie (Part II) - Pt. 2\nRihanna\n65\n0.472\n0.8330\n-4.486\n0.1620\n0.297000\n0.0560\n0.4130\n166.271\n\n\nJust Dance\nLady Gaga\n73\n0.822\n0.7390\n-4.541\n0.0311\n0.026400\n0.1810\n0.7450\n118.992\n\n\nPaparazzi\nLady Gaga\n69\n0.762\n0.6920\n-3.973\n0.0438\n0.113000\n0.0940\n0.3970\n114.906\n\n\nAlejandro\nLady Gaga\n8\n0.626\n0.7990\n-6.612\n0.0460\n0.000358\n0.3600\n0.3690\n98.992\n\n\nHotline Bling\nDrake\n0\n0.905\n0.6170\n-8.039\n0.0596\n0.002870\n0.0484\n0.5720\n134.972\n\n\nAdventure of a Lifetime\nColdplay\n74\n0.638\n0.9240\n-3.887\n0.0359\n0.002050\n0.1490\n0.5290\n111.995\n\n\nHymn for the Weekend\nColdplay\n77\n0.490\n0.7430\n-5.155\n0.0409\n0.255000\n0.3000\n0.3630\n89.974\n\n\nI Feel It Coming\nThe Weeknd\n80\n0.773\n0.8190\n-5.946\n0.1180\n0.426000\n0.0679\n0.5850\n92.990\n\n\nParadise\nColdplay\n77\n0.449\n0.5850\n-6.762\n0.0268\n0.052200\n0.0833\n0.2030\n139.627\n\n\nLook What You Made Me Do\nTaylor Swift\n74\n0.766\n0.7090\n-6.471\n0.1230\n0.204000\n0.1260\n0.5060\n128.070\n\n\nThat’s What I Like\nBruno Mars\n82\n0.853\n0.5600\n-4.961\n0.0406\n0.013000\n0.0944\n0.8600\n134.066\n\n\nGorgeous\nTaylor Swift\n67\n0.800\n0.5350\n-6.684\n0.1350\n0.071300\n0.2130\n0.4510\n92.027\n\n\nStarboy\nThe Weeknd\n84\n0.678\n0.5880\n-7.015\n0.2760\n0.141000\n0.1370\n0.4860\n186.005\n\n\nWe Are Never Ever Getting Back Together\nTaylor Swift\n74\n0.628\n0.6760\n-5.911\n0.0916\n0.009570\n0.1210\n0.7500\n85.984\n\n\nAs Long As You Love Me\nJustin Bieber\n72\n0.571\n0.8730\n-3.382\n0.0997\n0.080700\n0.3610\n0.6130\n139.691\n\n\nMistletoe\nJustin Bieber\n84\n0.658\n0.5600\n-9.049\n0.0467\n0.498000\n0.0794\n0.8540\n161.994\n\n\nJust the Way You Are\nBruno Mars\n80\n0.637\n0.8430\n-5.413\n0.0432\n0.015100\n0.0876\n0.4340\n109.012\n\n\nThinking out Loud\nEd Sheeran\n67\n0.781\n0.4450\n-6.061\n0.0295\n0.474000\n0.1840\n0.5910\n78.998\n\n\nDiamonds\nRihanna\n66\n0.564\n0.7100\n-4.920\n0.0461\n0.001250\n0.1090\n0.3930\n91.972\n\n\nCan’t Feel My Face\nThe Weeknd\n80\n0.705\n0.7690\n-5.526\n0.0426\n0.112000\n0.1050\n0.5900\n107.939\n\n\nOne Dance\nDrake\n20\n0.791\n0.6190\n-5.886\n0.0532\n0.007840\n0.3510\n0.3710\n103.989\n\n\nThe Hills\nThe Weeknd\n6\n0.587\n0.5660\n-7.017\n0.0528\n0.084300\n0.1450\n0.1550\n113.025\n\n\nVersace On The Floor (Bruno Mars vs. David Guetta)\nBruno Mars\n63\n0.747\n0.7010\n-4.902\n0.0429\n0.108000\n0.0712\n0.5890\n100.012\n\n\nToo Good\nDrake\n12\n0.804\n0.6480\n-7.805\n0.1170\n0.057300\n0.1020\n0.3920\n117.983\n\n\nShake It Off\nTaylor Swift\n18\n0.648\n0.7850\n-5.414\n0.1650\n0.056100\n0.1480\n0.9430\n160.020\n\n\nLocked Out of Heaven\nBruno Mars\n78\n0.726\n0.6980\n-4.165\n0.0431\n0.049000\n0.3090\n0.8670\n143.994\n\n\nBest I Ever Had\nDrake\n72\n0.431\n0.8940\n-2.673\n0.3300\n0.095100\n0.1880\n0.6040\n162.193\n\n\nHymn for the Weekend - Seeb Remix\nColdplay\n71\n0.565\n0.8490\n-3.516\n0.0517\n0.008680\n0.1200\n0.4270\n102.034\n\n\nFourFiveSeconds\nRihanna\n21\n0.579\n0.2680\n-5.650\n0.0442\n0.868000\n0.1200\n0.3210\n205.903\n\n\nGalway Girl\nEd Sheeran\n80\n0.624\n0.8760\n-3.374\n0.1000\n0.073500\n0.3270\n0.7810\n99.943\n\n\nOnly Girl (In The World)\nRihanna\n8\n0.768\n0.7380\n-3.017\n0.0406\n0.052400\n0.0561\n0.6850\n125.928\n\n\nWe Found Love\nRihanna\n19\n0.734\n0.7660\n-4.485\n0.0383\n0.025000\n0.1080\n0.6000\n127.986\n\n\nIn My Feelings\nDrake\n83\n0.835\n0.6260\n-5.833\n0.1250\n0.058900\n0.3960\n0.3500\n91.030\n\n\nGod’s Plan\nDrake\n86\n0.754\n0.4490\n-9.211\n0.1090\n0.033200\n0.5520\n0.3570\n77.169\n\n\nPassionfruit\nDrake\n15\n0.809\n0.4630\n-11.377\n0.0396\n0.256000\n0.1090\n0.3640\n111.980\n\n\nYellow\nColdplay\n81\n0.429\n0.6610\n-7.227\n0.0281\n0.002390\n0.2340\n0.2850\n173.365\n\n\nFix You\nColdplay\n81\n0.209\n0.4180\n-8.740\n0.0338\n0.163000\n0.1130\n0.1230\n138.265\n\n\nControlla\nDrake\n73\n0.611\n0.4680\n-11.084\n0.1790\n0.078100\n0.1010\n0.3490\n92.092\n\n\nOcean Eyes - Blackbear Remix\nBillie Eilish\n73\n0.661\n0.4190\n-9.807\n0.0404\n0.114000\n0.0848\n0.1450\n144.980\n\n\nThe Scientist\nColdplay\n83\n0.560\n0.4420\n-7.224\n0.0243\n0.727000\n0.1100\n0.2120\n146.448\n\n\nClocks\nColdplay\n75\n0.577\n0.7490\n-7.215\n0.0279\n0.599000\n0.1830\n0.2610\n130.969\n\n\nIn My Place\nColdplay\n70\n0.425\n0.5870\n-5.453\n0.0278\n0.056500\n0.2980\n0.1940\n144.642\n\n\nA Sky Full of Stars - Robin Schulz Edit\nColdplay\n51\n0.638\n0.8470\n-5.648\n0.0338\n0.004050\n0.4000\n0.3330\n123.042\n\n\nWhat Do You Mean?\nJustin Bieber\n78\n0.845\n0.5670\n-8.118\n0.0956\n0.590000\n0.0811\n0.7930\n125.020\n\n\nTreasure\nBruno Mars\n76\n0.874\n0.6920\n-5.280\n0.0431\n0.041200\n0.3240\n0.9370\n116.017\n\n\nWork\nRihanna\n77\n0.725\n0.5340\n-6.238\n0.0946\n0.075200\n0.0919\n0.5580\n91.974\n\n\nWhat’s My Name?\nRihanna\n68\n0.690\n0.7860\n-2.959\n0.0692\n0.229000\n0.0797\n0.5830\n100.049\n\n\n24K Magic\nBruno Mars\n78\n0.818\n0.8030\n-4.282\n0.0797\n0.034000\n0.1530\n0.6320\n106.970\n\n\nME! (feat. Brendon Urie of Panic! At The Disco)\nTaylor Swift\n29\n0.614\n0.8390\n-4.143\n0.0702\n0.029100\n0.1120\n0.6570\n182.123\n\n\nmy boy\nBillie Eilish\n80\n0.692\n0.3940\n-8.745\n0.2070\n0.472000\n0.1170\n0.3240\n89.936\n\n\nparty favor\nBillie Eilish\n76\n0.826\n0.1880\n-12.181\n0.0883\n0.882000\n0.1030\n0.2050\n130.022\n\n\nShallow - Radio Edit\nLady Gaga\n78\n0.575\n0.3300\n-6.557\n0.0310\n0.416000\n0.0872\n0.2770\n95.801\n\n\nbury a friend\nBillie Eilish\n87\n0.905\n0.3890\n-14.505\n0.3320\n0.740000\n0.1060\n0.1960\n120.046\n\n\nyou should see me in a crown\nBillie Eilish\n84\n0.678\n0.5330\n-10.485\n0.1860\n0.462000\n0.1390\n0.3230\n150.455\n\n\nall the good girls go to hell\nBillie Eilish\n86\n0.726\n0.4440\n-8.922\n0.3720\n0.283000\n0.1770\n0.5690\n185.044\n\n\nBehind Barz - Bonus\nDrake\n77\n0.854\n0.5540\n-4.684\n0.1720\n0.769000\n0.0749\n0.2720\n142.069\n\n\nOver\nDrake\n63\n0.350\n0.8450\n-5.614\n0.2000\n0.010700\n0.1230\n0.4500\n99.643\n\n\nMoney In The Grave (Drake ft. Rick Ross)\nDrake\n88\n0.831\n0.5020\n-4.045\n0.0460\n0.101000\n0.1220\n0.1010\n100.541\n\n\nNice For What\nDrake\n18\n0.567\n0.9130\n-6.471\n0.0736\n0.093400\n0.1140\n0.7920\n93.350\n\n\nBack To Back\nDrake\n69\n0.893\n0.4800\n-3.728\n0.3560\n0.008540\n0.1160\n0.3820\n86.976\n\n\nJumpman\nDrake\n75\n0.852\n0.5530\n-7.286\n0.1870\n0.056300\n0.3320\n0.6560\n142.072\n\n\nForever\nDrake\n67\n0.415\n0.9260\n-1.304\n0.3740\n0.253000\n0.2560\n0.5420\n158.423\n\n\nNonstop\nDrake\n13\n0.909\n0.4050\n-8.133\n0.1400\n0.030600\n0.1050\n0.4550\n154.984\n\n\nBitch Better Have My Money\nRihanna\n75\n0.781\n0.7280\n-4.981\n0.0621\n0.050900\n0.2570\n0.3950\n102.990\n\n\nSwine\nLady Gaga\n1\n0.712\n0.8610\n-3.625\n0.1360\n0.006200\n0.6340\n0.2090\n127.890\n\n\nNeeded Me\nRihanna\n14\n0.671\n0.3140\n-8.091\n0.2440\n0.110000\n0.0825\n0.2960\n110.898\n\n\nRude Boy\nRihanna\n12\n0.563\n0.7500\n-4.496\n0.1270\n0.113000\n0.0788\n0.8120\n173.906\n\n\nPortland\nDrake\n74\n0.925\n0.4430\n-8.216\n0.2640\n0.017200\n0.2760\n0.3500\n136.078\n\n\nGyalchester\nDrake\n70\n0.877\n0.3910\n-8.196\n0.0630\n0.031700\n0.0945\n0.1220\n131.018\n\n\nCan’t Have Everything\nDrake\n60\n0.856\n0.5720\n-8.429\n0.3730\n0.013600\n0.1210\n0.4540\n82.445\n\n\nSacrifices\nDrake\n1\n0.903\n0.4390\n-11.813\n0.3970\n0.040700\n0.1380\n0.5320\n139.112\n\n\nUptown (feat. Bun B & Lil Wayne)\nDrake\n62\n0.416\n0.8240\n-3.075\n0.3430\n0.003910\n0.1040\n0.4170\n85.433\n\n\nStarted From the Bottom\nDrake\n1\n0.790\n0.5400\n-6.959\n0.1830\n0.039100\n0.2660\n0.4590\n86.308\n\n\nThe Motto\nDrake\n10\n0.774\n0.4310\n-8.407\n0.3460\n0.000146\n0.1120\n0.3530\n201.843\n\n\nHold On, We’re Going Home\nDrake\n1\n0.765\n0.4520\n-6.644\n0.0680\n0.003300\n0.0916\n0.3310\n100.002\n\n\nKMT\nDrake\n64\n0.875\n0.6000\n-6.818\n0.0435\n0.018900\n0.1320\n0.0906\n119.987\n\n\nThe Hills - RL Grime Remix\nThe Weeknd\n51\n0.571\n0.8970\n-2.463\n0.0432\n0.113000\n0.1580\n0.4460\n139.996\n\n\nMyBoi - TroyBoi Remix\nBillie Eilish\n71\n0.879\n0.5800\n-4.797\n0.0883\n0.006260\n0.0653\n0.3120\n92.004\n\n\nGrammys\nDrake\n1\n0.711\n0.4400\n-9.509\n0.4710\n0.001750\n0.0798\n0.1210\n145.116\n\n\nDigital Dash\nDrake\n1\n0.889\n0.5430\n-7.666\n0.1280\n0.003110\n0.2340\n0.3140\n140.029\n\n\nAlways in My Head\nColdplay\n65\n0.499\n0.5020\n-9.307\n0.0254\n0.012800\n0.0680\n0.0397\n97.544\n\n\nMagic\nColdplay\n72\n0.738\n0.4830\n-8.915\n0.0403\n0.026200\n0.1060\n0.3550\n93.458\n\n\nInk\nColdplay\n62\n0.688\n0.7060\n-7.310\n0.0324\n0.066100\n0.1180\n0.6890\n112.019\n\n\nTrue Love\nColdplay\n58\n0.482\n0.5740\n-7.963\n0.0280\n0.003820\n0.1230\n0.1840\n119.994\n\n\nMidnight\nColdplay\n64\n0.548\n0.4750\n-12.420\n0.0355\n0.615000\n0.0944\n0.0349\n126.976\n\n\nAnother’s Arms\nColdplay\n57\n0.518\n0.5720\n-9.037\n0.0291\n0.007150\n0.1390\n0.3470\n160.005\n\n\nOceans\nColdplay\n55\n0.194\n0.1320\n-15.783\n0.0349\n0.919000\n0.1590\n0.0400\n74.848\n\n\nO\nColdplay\n63\n0.230\n0.2160\n-14.727\n0.0376\n0.911000\n0.0934\n0.0618\n92.022\n\n\nAll Your Friends\nColdplay\n36\n0.507\n0.3170\n-12.177\n0.0283\n0.183000\n0.1120\n0.2010\n74.980\n\n\nGhost Story\nColdplay\n37\n0.595\n0.6380\n-7.941\n0.0307\n0.104000\n0.0699\n0.5100\n122.005\n\n\nO - Reprise\nColdplay\n32\n0.201\n0.3270\n-11.975\n0.0285\n0.869000\n0.1170\n0.0945\n107.019\n\n\nTalk\nColdplay\n62\n0.413\n0.5570\n-10.841\n0.0335\n0.005480\n0.1550\n0.1580\n120.318\n\n\nTrouble\nColdplay\n71\n0.561\n0.5470\n-7.496\n0.0321\n0.189000\n0.1700\n0.1900\n139.755\n\n\nLost!\nColdplay\n52\n0.359\n0.7790\n-7.520\n0.0412\n0.005700\n0.3790\n0.5820\n86.951\n\n\nBeautiful People (feat. Khalid) - NOTD Remix\nEd Sheeran\n66\n0.702\n0.7940\n-5.913\n0.0562\n0.007150\n0.1100\n0.3700\n103.972\n\n\nStarboy - Kygo Remix\nThe Weeknd\n5\n0.751\n0.6230\n-7.215\n0.0939\n0.209000\n0.1680\n0.3830\n93.010\n\n\nGalway Girl - Martin Jensen Remix\nEd Sheeran\n58\n0.726\n0.7480\n-4.576\n0.0438\n0.024500\n0.1410\n0.7310\n100.055\n\n\nPhotograph - Felix Jaehn Remix\nEd Sheeran\n0\n0.733\n0.8210\n-4.982\n0.0407\n0.081500\n0.1220\n0.3970\n120.011\n\n\nThinking out Loud - Alex Adair Remix\nEd Sheeran\n49\n0.828\n0.7450\n-6.717\n0.1520\n0.210000\n0.0870\n0.5170\n117.008\n\n\nOften - Kygo Remix\nThe Weeknd\n65\n0.523\n0.4910\n-9.876\n0.0491\n0.123000\n0.3310\n0.4080\n80.156\n\n\nMarry You\nBruno Mars\n72\n0.620\n0.8320\n-4.848\n0.0360\n0.332000\n0.1040\n0.4810\n144.926\n\n\nPrincess of China - Acoustic\nColdplay\n52\n0.245\n0.4710\n-7.952\n0.0313\n0.652000\n0.1260\n0.2870\n165.996\n\n\nThe A Team - Acoustic\nEd Sheeran\n50\n0.649\n0.2640\n-9.899\n0.2800\n0.727000\n0.2920\n0.4240\n121.815\n\n\nMagic - Live at the Enmore Theatre, Sydney\nColdplay\n54\n0.500\n0.5020\n-11.775\n0.0424\n0.180000\n0.7290\n0.3620\n93.361\n\n\nA Sky Full of Stars - Live at the Royal Albert Hall, London\nColdplay\n54\n0.467\n0.7650\n-7.846\n0.0311\n0.004160\n0.7080\n0.1860\n125.013\n\n\nNothing On You (feat. Paulo Londra & Dave)\nEd Sheeran\n59\n0.747\n0.6490\n-6.218\n0.2190\n0.333000\n0.1720\n0.8400\n88.011\n\n\nSomebody To Love Remix\nJustin Bieber\n54\n0.704\n0.8360\n-4.619\n0.0283\n0.003880\n0.1600\n0.5530\n129.989\n\n\nEenie Meenie\nJustin Bieber\n4\n0.718\n0.6050\n-4.240\n0.0307\n0.044300\n0.0962\n0.8180\n121.236\n\n\nOne Time\nJustin Bieber\n68\n0.691\n0.8530\n-2.528\n0.0372\n0.063100\n0.0820\n0.7620\n145.999\n\n\nOne Less Lonely Girl\nJustin Bieber\n67\n0.580\n0.7500\n-4.421\n0.1940\n0.159000\n0.2740\n0.7280\n86.938\n\n\nLove Me\nJustin Bieber\n65\n0.729\n0.8680\n-3.319\n0.0609\n0.009390\n0.2830\n0.6320\n124.957\n\n\nStupid In Love\nRihanna\n47\n0.738\n0.5620\n-5.435\n0.0347\n0.633000\n0.1020\n0.4760\n127.452\n\n\nGrenade\nBruno Mars\n75\n0.706\n0.5580\n-7.237\n0.0593\n0.146000\n0.1180\n0.2270\n110.443\n\n\nHard\nRihanna\n56\n0.310\n0.7460\n-3.599\n0.1110\n0.012100\n0.6490\n0.1630\n182.032\n\n\nRussian Roulette\nRihanna\n65\n0.480\n0.4860\n-5.754\n0.0447\n0.046000\n0.1070\n0.2650\n80.051\n\n\nS&M\nRihanna\n72\n0.766\n0.6820\n-5.020\n0.0419\n0.011300\n0.1040\n0.8330\n127.966\n\n\nComplicated\nRihanna\n48\n0.710\n0.8970\n-3.802\n0.0372\n0.175000\n0.0918\n0.3300\n137.972\n\n\nPop Style\nDrake\n51\n0.713\n0.4620\n-10.027\n0.1230\n0.192000\n0.1050\n0.0595\n133.054\n\n\nFake Love\nDrake\n6\n0.924\n0.5430\n-7.474\n0.2980\n0.182000\n0.1030\n0.7360\n133.907\n\n\nWhen I Was Your Man\nBruno Mars\n82\n0.612\n0.2800\n-8.648\n0.0434\n0.932000\n0.0880\n0.3870\n72.795\n\n\nYou Belong With Me\nTaylor Swift\n52\n0.688\n0.7750\n-4.436\n0.0381\n0.151000\n0.1040\n0.4690\n129.969\n\n\nI Knew You Were Trouble\nTaylor Swift\n16\n0.499\n0.4670\n-6.744\n0.0461\n0.005930\n0.0361\n0.6370\n76.760\n\n\nWildest Dreams\nTaylor Swift\n65\n0.553\n0.6640\n-7.417\n0.0741\n0.070900\n0.1060\n0.4670\n140.060\n\n\nPon de Replay - Radio Edit\nRihanna\n34\n0.750\n0.6320\n-8.681\n0.2150\n0.000147\n0.1080\n0.5320\n98.997\n\n\nPerfect\nEd Sheeran\n86\n0.599\n0.4480\n-6.312\n0.0232\n0.163000\n0.1060\n0.1680\n95.050\n\n\nPray For Me (with Kendrick Lamar)\nThe Weeknd\n76\n0.735\n0.6770\n-4.979\n0.0930\n0.076200\n0.1110\n0.1880\n100.584\n\n\nCross Me (feat. Chance the Rapper & PnB Rock)\nEd Sheeran\n73\n0.746\n0.7870\n-6.373\n0.1200\n0.214000\n0.0669\n0.6070\n95.005\n\n\nThe Zone\nThe Weeknd\n61\n0.619\n0.5210\n-8.092\n0.0744\n0.260000\n0.1010\n0.2540\n139.886\n\n\nTill Dawn (Here Comes The Sun)\nThe Weeknd\n53\n0.697\n0.5420\n-8.946\n0.0437\n0.821000\n0.1340\n0.5470\n99.988\n\n\nSame Old Song\nThe Weeknd\n55\n0.771\n0.5600\n-5.636\n0.0692\n0.415000\n0.5360\n0.3890\n109.978\n\n\nEchoes Of Silence\nThe Weeknd\n55\n0.214\n0.1560\n-17.515\n0.0443\n0.951000\n0.1010\n0.1320\n75.654\n\n\nValerie\nThe Weeknd\n62\n0.641\n0.5390\n-6.351\n0.0463\n0.346000\n0.0837\n0.4360\n148.004\n\n\nGone\nThe Weeknd\n54\n0.494\n0.6970\n-7.232\n0.3090\n0.708000\n0.0602\n0.3440\n107.669\n\n\nThe Party & The After Party\nThe Weeknd\n60\n0.438\n0.6340\n-7.456\n0.3370\n0.476000\n0.1130\n0.3970\n203.585\n\n\nLosers\nThe Weeknd\n62\n0.666\n0.4850\n-9.850\n0.0527\n0.035500\n0.5040\n0.2880\n91.009\n\n\nTell Your Friends\nThe Weeknd\n68\n0.565\n0.4540\n-11.091\n0.0461\n0.265000\n0.1810\n0.3120\n74.809\n\n\nOften\nThe Weeknd\n76\n0.569\n0.6290\n-9.837\n0.0485\n0.218000\n0.1350\n0.0726\n134.035\n\n\nDark Times\nThe Weeknd\n69\n0.501\n0.4030\n-9.607\n0.0752\n0.114000\n0.1240\n0.2700\n132.608\n\n\nPrisoner\nThe Weeknd\n64\n0.550\n0.4040\n-12.755\n0.0398\n0.441000\n0.1100\n0.3290\n135.016\n\n\nSecrets\nThe Weeknd\n71\n0.665\n0.7710\n-5.779\n0.0533\n0.015500\n0.4600\n0.7640\n109.945\n\n\nTrue Colors\nThe Weeknd\n69\n0.624\n0.4240\n-8.237\n0.2400\n0.284000\n0.1730\n0.5840\n76.669\n\n\nSidewalks\nThe Weeknd\n70\n0.542\n0.7210\n-5.136\n0.0625\n0.147000\n0.3480\n0.6230\n149.831\n\n\nSix Feet Under\nThe Weeknd\n69\n0.772\n0.5040\n-8.219\n0.0761\n0.129000\n0.1250\n0.2440\n140.035\n\n\nDie For You\nThe Weeknd\n76\n0.586\n0.5250\n-7.163\n0.0615\n0.111000\n0.1340\n0.5080\n133.629\n\n\nCall Out My Name\nThe Weeknd\n24\n0.449\n0.5990\n-4.892\n0.0369\n0.211000\n0.3270\n0.1650\n134.144\n\n\nTry Me\nThe Weeknd\n19\n0.466\n0.6720\n-5.777\n0.0680\n0.622000\n0.1470\n0.1080\n91.673\n\n\nWasted Times\nThe Weeknd\n18\n0.562\n0.6740\n-6.049\n0.0851\n0.622000\n0.1110\n0.2930\n122.699\n\n\nI Was Never There\nThe Weeknd\n17\n0.324\n0.7350\n-4.054\n0.0337\n0.135000\n0.1690\n0.1690\n113.923\n\n\nHurt You\nThe Weeknd\n17\n0.484\n0.7160\n-7.193\n0.1670\n0.300000\n0.1120\n0.3650\n184.131\n\n\nPrivilege\nThe Weeknd\n16\n0.391\n0.5950\n-7.094\n0.0411\n0.913000\n0.1450\n0.3790\n93.780\n\n\nTwenty Eight\nThe Weeknd\n61\n0.639\n0.6330\n-7.338\n0.0352\n0.574000\n0.1110\n0.2430\n129.996\n\n\nCrew Love\nDrake\n51\n0.418\n0.5460\n-9.057\n0.2380\n0.227000\n0.2290\n0.2940\n160.152\n\n\nGet It Together\nDrake\n69\n0.781\n0.7210\n-9.133\n0.0514\n0.094500\n0.1040\n0.8490\n123.011\n\n\nSummers Over Interlude\nDrake\n57\n0.698\n0.2550\n-8.647\n0.0303\n0.405000\n0.0985\n0.2430\n132.036\n\n\nTears In The Rain\nThe Weeknd\n43\n0.655\n0.6360\n-7.840\n0.0453\n0.109000\n0.0842\n0.1850\n160.010\n\n\nAdaptation\nThe Weeknd\n44\n0.642\n0.7900\n-6.800\n0.0681\n0.151000\n0.3050\n0.6130\n114.960\n\n\nJaded\nDrake\n68\n0.766\n0.2710\n-5.697\n0.0412\n0.037000\n0.1750\n0.0371\n116.020\n\n\nDreams Money Can Buy\nDrake\n61\n0.423\n0.5870\n-6.635\n0.5290\n0.765000\n0.1140\n0.3300\n180.331\n\n\nFeels (feat. Young Thug & J Hus)\nEd Sheeran\n37\n0.665\n0.6800\n-4.899\n0.2900\n0.112000\n0.0960\n0.6900\n91.670\n\n\nHow Bout Now\nDrake\n66\n0.789\n0.4040\n-8.364\n0.2300\n0.205000\n0.2190\n0.4720\n122.062\n\n\nJodeci Freestyle (feat. J. Cole)\nDrake\n57\n0.498\n0.8440\n-2.834\n0.2680\n0.011100\n0.1020\n0.2820\n88.435\n\n\nShallow\nLady Gaga\n88\n0.572\n0.3850\n-6.362\n0.0308\n0.371000\n0.2310\n0.3230\n95.799\n\n\nI See Fire\nEd Sheeran\n23\n0.641\n0.1760\n-11.692\n0.0349\n0.638000\n0.2520\n0.2970\n76.031\n\n\n8\nBillie Eilish\n80\n0.735\n0.2350\n-13.239\n0.3150\n0.698000\n0.1070\n0.4620\n62.446\n\n\ni love you\nBillie Eilish\n85\n0.421\n0.1310\n-18.435\n0.0382\n0.952000\n0.1090\n0.1200\n137.446\n\n\ngoodbye\nBillie Eilish\n77\n0.153\n0.1380\n-21.877\n0.0503\n0.837000\n0.2540\n0.0503\n74.318\n\n\nxanny\nBillie Eilish\n83\n0.521\n0.1250\n-17.832\n0.2390\n0.751000\n0.2650\n0.0528\n111.554\n\n\nocean eyes\nBillie Eilish\n85\n0.511\n0.3630\n-7.650\n0.0410\n0.816000\n0.0840\n0.1690\n144.892\n\n\nBest Part of Me (feat. YEBBA)\nEd Sheeran\n74\n0.565\n0.2420\n-8.367\n0.0318\n0.762000\n0.2850\n0.2370\n81.657\n\n\nhostage\nBillie Eilish\n79\n0.427\n0.1230\n-15.228\n0.0497\n0.868000\n0.0732\n0.0394\n129.565\n\n\n&burn (with Vince Staples)\nBillie Eilish\n68\n0.776\n0.3790\n-9.351\n0.3310\n0.213000\n0.0969\n0.3460\n82.531\n\n\nSummer Games\nDrake\n4\n0.776\n0.4820\n-11.268\n0.0356\n0.481000\n0.1100\n0.1280\n136.015\n\n\nJungle\nDrake\n70\n0.690\n0.2350\n-8.651\n0.0541\n0.618000\n0.1070\n0.3780\n99.906\n\n\nTake Care\nDrake\n71\n0.629\n0.5150\n-10.358\n0.2650\n0.026700\n0.0888\n0.2990\n121.845\n\n\nMarvins Room\nDrake\n74\n0.492\n0.2600\n-17.341\n0.0921\n0.646000\n0.0705\n0.3120\n111.519\n\n\nlovely (with Khalid)\nBillie Eilish\n89\n0.351\n0.2960\n-10.109\n0.0333\n0.934000\n0.0950\n0.1200\n115.284\n\n\nMan Down\nRihanna\n59\n0.479\n0.9060\n-4.030\n0.1880\n0.040600\n0.0510\n0.5360\n155.887\n\n\nidontwannabeyouanymore\nBillie Eilish\n85\n0.483\n0.4120\n-8.461\n0.0402\n0.737000\n0.1160\n0.2470\n170.163\n\n\nwhen the party’s over\nBillie Eilish\n26\n0.498\n0.1040\n-14.080\n0.0621\n0.979000\n0.0895\n0.2050\n124.001\n\n\n0 To 100 / The Catch Up\nDrake\n5\n0.559\n0.7160\n-3.964\n0.3900\n0.737000\n0.2520\n0.5040\n176.618\n\n\nThe Language\nDrake\n53\n0.823\n0.6620\n-8.426\n0.3180\n0.011800\n0.0805\n0.5330\n120.117\n\n\n6 Man\nDrake\n1\n0.812\n0.5350\n-9.306\n0.4490\n0.217000\n0.1140\n0.3210\n78.974\n\n\nUsed To\nDrake\n0\n0.926\n0.4270\n-7.584\n0.1070\n0.070300\n0.0818\n0.3050\n96.023\n\n\nNo Tellin’\nDrake\n1\n0.823\n0.4670\n-10.394\n0.3010\n0.543000\n0.1350\n0.6180\n95.024\n\n\nPurpose\nJustin Bieber\n71\n0.481\n0.2850\n-10.848\n0.0424\n0.900000\n0.1230\n0.3160\n130.029\n\n\nBellyache - Marian Hill Remix\nBillie Eilish\n71\n0.832\n0.3680\n-8.627\n0.1330\n0.200000\n0.3530\n0.2000\n100.045\n\n\nmy strange addiction\nBillie Eilish\n82\n0.939\n0.3050\n-10.952\n0.3540\n0.350000\n0.1050\n0.5630\n100.029\n\n\nilomilo\nBillie Eilish\n81\n0.855\n0.4230\n-15.044\n0.0585\n0.724000\n0.0896\n0.5720\n120.020\n\n\nlisten before i go\nBillie Eilish\n81\n0.319\n0.0561\n-23.023\n0.0450\n0.935000\n0.3880\n0.0820\n79.764\n\n\nBored\nBillie Eilish\n76\n0.614\n0.3180\n-12.695\n0.0478\n0.896000\n0.0795\n0.1120\n119.959\n\n\nSex With Me\nRihanna\n69\n0.580\n0.5320\n-5.668\n0.0424\n0.294000\n0.0948\n0.3900\n112.049\n\n\nPour It Up\nRihanna\n62\n0.785\n0.4780\n-8.735\n0.0419\n0.025400\n0.0901\n0.3900\n134.002\n\n\nFinesse - Remix; feat. Cardi B\nBruno Mars\n77\n0.704\n0.8590\n-4.877\n0.0996\n0.018500\n0.0215\n0.9260\n105.115\n\n\nEmotionless\nDrake\n6\n0.413\n0.6760\n-5.124\n0.2130\n0.023900\n0.0811\n0.0827\n173.478\n\n\nI Forgot That You Existed\nTaylor Swift\n79\n0.664\n0.3160\n-10.345\n0.5190\n0.298000\n0.0812\n0.5410\n92.875\n\n\nEnd Game\nTaylor Swift\n68\n0.649\n0.5890\n-6.237\n0.0558\n0.008450\n0.1080\n0.1510\n159.073\n\n\nPerfect Duet (Ed Sheeran & Beyoncé)\nEd Sheeran\n77\n0.587\n0.2990\n-7.365\n0.0263\n0.779000\n0.1230\n0.3560\n94.992\n\n\nSigns\nDrake\n70\n0.794\n0.7350\n-4.204\n0.0540\n0.108000\n0.1120\n0.2230\n107.374\n\n\nLove Yourself\nJustin Bieber\n83\n0.609\n0.3780\n-9.828\n0.4380\n0.835000\n0.2800\n0.5150\n100.418\n\n\nKiss It Better - KAYTRANADA Edition\nRihanna\n45\n0.854\n0.4980\n-7.922\n0.3320\n0.022900\n0.3000\n0.3430\n108.049\n\n\nHigher\nRihanna\n1\n0.292\n0.5670\n-4.191\n0.0416\n0.112000\n0.1190\n0.3920\n153.199\n\n\nJames Joint\nRihanna\n0\n0.399\n0.2030\n-11.649\n0.0298\n0.660000\n0.2700\n0.4050\n134.202\n\n\nCan’t Feel My Face - Martin Garrix Remix\nThe Weeknd\n2\n0.607\n0.7070\n-5.509\n0.1060\n0.008420\n0.3480\n0.1170\n128.014\n\n\nShe Don’t Like The Lights - Acoustic Version\nJustin Bieber\n36\n0.612\n0.4280\n-8.520\n0.0292\n0.750000\n0.0921\n0.6850\n123.822\n\n\nWork - R3hab Remix\nRihanna\n2\n0.715\n0.8000\n-5.488\n0.0555\n0.010400\n0.3190\n0.2640\n100.004\n\n\nBitch Better Have My Money - R3hab Remix\nRihanna\n1\n0.788\n0.9210\n-3.210\n0.0550\n0.006000\n0.1050\n0.5510\n128.007\n\n\nBitch Better Have My Money - GTA Remix\nRihanna\n0\n0.768\n0.8770\n-3.114\n0.1510\n0.000768\n0.0739\n0.4800\n103.036\n\n\nYou Need To Calm Down - Clean Bandit Remix\nTaylor Swift\n54\n0.726\n0.8800\n-4.137\n0.0581\n0.072000\n0.0906\n0.7390\n95.037\n\n\n…Ready For It? - BloodPop® Remix\nTaylor Swift\n50\n0.582\n0.8350\n-4.899\n0.2220\n0.133000\n0.1020\n0.4960\n160.090\n\n\n24K Magic - R3hab Remix\nBruno Mars\n59\n0.763\n0.7080\n-5.692\n0.2090\n0.146000\n0.3300\n0.5660\n106.175\n\n\nPerfect - Mike Perry Remix\nEd Sheeran\n46\n0.732\n0.7560\n-4.309\n0.0324\n0.260000\n0.1300\n0.3430\n113.980\n\n\nKiss It Better - R3hab Remix\nRihanna\n1\n0.728\n0.7380\n-3.939\n0.0455\n0.050500\n0.1040\n0.4580\n102.000\n\n\nNeeded Me - R3hab Remix\nRihanna\n44\n0.659\n0.6830\n-5.485\n0.0488\n0.008350\n0.6250\n0.2780\n99.987\n\n\nParadise - Tiësto Remix\nColdplay\n48\n0.637\n0.7360\n-6.003\n0.0404\n0.012300\n0.0798\n0.6430\n130.015\n\n\nCalifornia King Bed - Bassjackers Radio\nRihanna\n0\n0.589\n0.8210\n-3.385\n0.0413\n0.000318\n0.3900\n0.1590\n127.986\n\n\nShape of You - Galantis Remix\nEd Sheeran\n51\n0.838\n0.7650\n-2.332\n0.0462\n0.051400\n0.0297\n0.9050\n100.021\n\n\nThat’s What I Like - Alan Walker Remix\nBruno Mars\n63\n0.692\n0.8960\n-4.017\n0.0502\n0.012100\n0.1990\n0.7980\n134.056\n\n\nBad Blood\nTaylor Swift\n62\n0.652\n0.8020\n-6.114\n0.1810\n0.087100\n0.1480\n0.2950\n170.157\n\n\nBeauty And A Beat\nJustin Bieber\n72\n0.601\n0.8430\n-4.870\n0.0593\n0.000704\n0.0681\n0.5340\n128.002\n\n\nDelicate\nTaylor Swift\n71\n0.750\n0.4040\n-10.178\n0.0682\n0.216000\n0.0911\n0.0499\n95.045\n\n\nFinesse\nBruno Mars\n68\n0.741\n0.7980\n-5.342\n0.0718\n0.007810\n0.0232\n0.9390\n104.953\n\n\nA Sky Full of Stars - Hardwell Remix\nColdplay\n18\n0.439\n0.6480\n-3.643\n0.0325\n0.019000\n0.3430\n0.0546\n127.954"
  },
  {
    "objectID": "slides/04-dimension-pca-slides.html#exemple-le-top-10-spotify-1",
    "href": "slides/04-dimension-pca-slides.html#exemple-le-top-10-spotify-1",
    "title": "Dimension",
    "section": "Exemple : le top 10 Spotify",
    "text": "Exemple : le top 10 Spotify\n\nlibrary(tidyverse)\n\nggplot(df, aes(x = danceability, y = energy, color = track_artist)) +\n  geom_point() +\n  theme_bw()"
  },
  {
    "objectID": "slides/04-dimension-pca-slides.html#exemple-le-top-10-spotify-2",
    "href": "slides/04-dimension-pca-slides.html#exemple-le-top-10-spotify-2",
    "title": "Dimension",
    "section": "Exemple : le top 10 Spotify",
    "text": "Exemple : le top 10 Spotify\n\nlibrary(tidyverse)\n\nggplot(df, aes(x = loudness, y = speechiness, color = track_artist)) +\n  geom_point() +\n  theme_bw()"
  },
  {
    "objectID": "slides/04-dimension-pca-slides.html#exemple-le-top-10-spotify-3",
    "href": "slides/04-dimension-pca-slides.html#exemple-le-top-10-spotify-3",
    "title": "Dimension",
    "section": "Exemple : le top 10 Spotify",
    "text": "Exemple : le top 10 Spotify\n\nCodePlotValeurs propres\n\n\n\nlibrary(FactoMineR)\nlibrary(factoextra)\npca_not_scaled &lt;- df |&gt; select(\n    danceability, energy, loudness, speechiness,\n    acousticness, liveness, valence, tempo\n  ) |&gt; PCA(scale.unit = FALSE, graph = FALSE)\nfviz_eig(pca_not_scaled)\nget_eigenvalue(pca_scaled)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\neigenvalue\nvariance.percent\ncumulative.variance.percent\n\n\n\n\nDim.1\n810.6332465\n98.6720169\n98.67202\n\n\nDim.2\n10.7552317\n1.3091499\n99.98117\n\n\nDim.3\n0.0567231\n0.0069045\n99.98807\n\n\nDim.4\n0.0441830\n0.0053781\n99.99345\n\n\nDim.5\n0.0210597\n0.0025634\n99.99601\n\n\nDim.6\n0.0134860\n0.0016415\n99.99765\n\n\nDim.7\n0.0100835\n0.0012274\n99.99888\n\n\nDim.8\n0.0091876\n0.0011183\n100.00000"
  },
  {
    "objectID": "slides/04-dimension-pca-slides.html#exemple-le-top-10-spotify-4",
    "href": "slides/04-dimension-pca-slides.html#exemple-le-top-10-spotify-4",
    "title": "Dimension",
    "section": "Exemple : le top 10 Spotify",
    "text": "Exemple : le top 10 Spotify\n\nCodePlotValeurs propres\n\n\n\nlibrary(FactoMineR)\nlibrary(factoextra)\npca_scaled &lt;- df |&gt; select(\n    danceability, energy, loudness, speechiness,\n    acousticness, liveness, valence, tempo\n  ) |&gt; PCA(scale.unit = TRUE, graph = FALSE)\nfviz_eig(pca_scaled)\nget_eigenvalue(pca_scaled)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\neigenvalue\nvariance.percent\ncumulative.variance.percent\n\n\n\n\nDim.1\n2.8090242\n35.112803\n35.11280\n\n\nDim.2\n1.3209372\n16.511715\n51.62452\n\n\nDim.3\n1.1505090\n14.381362\n66.00588\n\n\nDim.4\n0.9610515\n12.013144\n78.01902\n\n\nDim.5\n0.6455193\n8.068991\n86.08801\n\n\nDim.6\n0.5897927\n7.372408\n93.46042\n\n\nDim.7\n0.3466595\n4.333244\n97.79367\n\n\nDim.8\n0.1765066\n2.206333\n100.00000"
  },
  {
    "objectID": "slides/04-dimension-pca-slides.html#exemple-le-top-10-spotify-5",
    "href": "slides/04-dimension-pca-slides.html#exemple-le-top-10-spotify-5",
    "title": "Dimension",
    "section": "Exemple : le top 10 Spotify",
    "text": "Exemple : le top 10 Spotify\n\nCodePlot\n\n\n\nfviz_pca_var(\n  pca_scaled,\n  col.var = \"cos2\",\n  gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"), \n  repel = TRUE # Avoid text overlapping\n)"
  },
  {
    "objectID": "slides/04-dimension-pca-slides.html#exemple-le-top-10-spotify-6",
    "href": "slides/04-dimension-pca-slides.html#exemple-le-top-10-spotify-6",
    "title": "Dimension",
    "section": "Exemple : le top 10 Spotify",
    "text": "Exemple : le top 10 Spotify\n\nCodeContribution à la première composanteContribution à la deuxième composante\n\n\n\nfviz_contrib(pca_scaled, choice = \"var\", axes = 1)\nfviz_contrib(pca_scaled, choice = \"var\", axes = 2)"
  },
  {
    "objectID": "slides/04-dimension-pca-slides.html#exemple-le-top-10-spotify-7",
    "href": "slides/04-dimension-pca-slides.html#exemple-le-top-10-spotify-7",
    "title": "Dimension",
    "section": "Exemple : le top 10 Spotify",
    "text": "Exemple : le top 10 Spotify\n\nCodeGraphique des individusAvec ellipse\n\n\n\nfviz_pca_ind(\n  pca_scaled,\n  geom.ind = \"point\", # show points only (nbut not \"text\")\n  col.ind = df$track_artist, # color by groups\n  legend.title = \"Artists\",\n) + scale_shape_manual(values = rep(19, 10))"
  },
  {
    "objectID": "slides/04-dimension-pca-slides.html#exemple-le-top-10-spotify-8",
    "href": "slides/04-dimension-pca-slides.html#exemple-le-top-10-spotify-8",
    "title": "Dimension",
    "section": "Exemple : le top 10 Spotify",
    "text": "Exemple : le top 10 Spotify\n\nCodeBiplotClean\n\n\n\nfviz_pca_biplot(\n  pca_scaled,\n  repel = TRUE,\n  col.var = \"#2E9FDF\", # Variables color\n  col.ind = \"#696969\"  # Individuals color\n)"
  },
  {
    "objectID": "slides/04-dimension-pca-slides.html#conclusion",
    "href": "slides/04-dimension-pca-slides.html#conclusion",
    "title": "Dimension",
    "section": "Conclusion",
    "text": "Conclusion\n\nL’ACP est un outil puissant pour la réduction de dimension qui :\n\nPréserve l’information maximale.\nÉlimine la redondance d’information.\nFacilite la visualisation et la modélisation.\nNécessite des choix méthodologiques.\n\nProchaine étape → Que faire si les données ne sont pas qualitatives ?"
  },
  {
    "objectID": "slides/03-generalities-model-evaluation-slides-annotated.html#introduction",
    "href": "slides/03-generalities-model-evaluation-slides-annotated.html#introduction",
    "title": "Généralités",
    "section": "Introduction",
    "text": "Introduction\n\nComment mesurer la performance prédictive réelle de nos modèles ?\nMétriques classiques :\n\nErreur quadratique moyenne (EQM / MSE) → variables quantitatives.\nTaux d’erreur (ER) → variables qualitatives.\n\n→ Cependant, évaluer uniquement sur les données d’entraînement pose des problèmes fondamentaux."
  },
  {
    "objectID": "slides/03-generalities-model-evaluation-slides-annotated.html#plan",
    "href": "slides/03-generalities-model-evaluation-slides-annotated.html#plan",
    "title": "Généralités",
    "section": "Plan",
    "text": "Plan\n\n\nLe sur-ajustement\nLe sous-ajustement\nApproche par jeu de validation\nValidation croisée"
  },
  {
    "objectID": "slides/03-generalities-model-evaluation-slides-annotated.html#le-problème-du-sur-ajustement",
    "href": "slides/03-generalities-model-evaluation-slides-annotated.html#le-problème-du-sur-ajustement",
    "title": "Généralités",
    "section": "Le problème du sur-ajustement",
    "text": "Le problème du sur-ajustement\n\nNotre modèle s’adapte trop aux données d’entraînement, y compris au bruit aléatoire.\n\nDonc:\n\nErreur faible sur les données d’entraînement.\nErreur élevée sur de nouvelles observations.\nMauvaise capacité de généralisation.\n\n\n\nPourquoi: L’estimateur \\(\\hat{f}\\) a été ajusté pour minimiser l’erreur sur ces mêmes données."
  },
  {
    "objectID": "slides/03-generalities-model-evaluation-slides-annotated.html#le-problème-du-sous-ajustement",
    "href": "slides/03-generalities-model-evaluation-slides-annotated.html#le-problème-du-sous-ajustement",
    "title": "Généralités",
    "section": "Le problème du sous-ajustement",
    "text": "Le problème du sous-ajustement\n\nNotre modèle ne s’adapte pas aux données d’entraînement.\n\nDonc:\n\nErreur élevée sur les données d’entraînement.\nErreur élevée sur de nouvelles observations.\nNe prend pas en compte les données.\n\n\n\nPourquoi: L’estimateur \\(\\hat{f}\\) a été implémenté sans prendre en compte les données."
  },
  {
    "objectID": "slides/03-generalities-model-evaluation-slides-annotated.html#sur-ajustement-vs-sous-ajustement",
    "href": "slides/03-generalities-model-evaluation-slides-annotated.html#sur-ajustement-vs-sous-ajustement",
    "title": "Généralités",
    "section": "Sur-ajustement vs Sous-ajustement",
    "text": "Sur-ajustement vs Sous-ajustement\n\nSur-ajustement (overfitting):\n\nModèle trop flexible.\nCapture le bruit plutôt que le signal.\n\nSous-ajustement (underfitting):\n\nModèle trop rigide.\nNe capture pas la structure des données.\n\nObjectif → Trouver le bon compromis entre les deux."
  },
  {
    "objectID": "slides/03-generalities-model-evaluation-slides-annotated.html#approche-idéale",
    "href": "slides/03-generalities-model-evaluation-slides-annotated.html#approche-idéale",
    "title": "Généralités",
    "section": "Approche idéale",
    "text": "Approche idéale\n\nSéparer complètement les données d’entraînement et d’évaluation.\n\nJeu d’entraînement : Utilisé pour ajuster le modèle.\nJeu de test : Utilisé pour évaluer la performance prédictive.\n\n\nProblème pratique : Nous n’avons généralement pas accès à un jeu de test indépendant.\n\n\nSolution → Utiliser des méthodes de validation."
  },
  {
    "objectID": "slides/03-generalities-model-evaluation-slides-annotated.html#approche-par-jeu-de-validation",
    "href": "slides/03-generalities-model-evaluation-slides-annotated.html#approche-par-jeu-de-validation",
    "title": "Généralités",
    "section": "Approche par jeu de validation",
    "text": "Approche par jeu de validation\n\n\nApproche par jeu de validation."
  },
  {
    "objectID": "slides/03-generalities-model-evaluation-slides-annotated.html#limites-du-jeu-de-validation",
    "href": "slides/03-generalities-model-evaluation-slides-annotated.html#limites-du-jeu-de-validation",
    "title": "Généralités",
    "section": "Limites du jeu de validation",
    "text": "Limites du jeu de validation\n\nEstimation instable : L’erreur dépend fortement des observations dans le jeu de validation.\n\nPerte d’information : Moins de données pour ajuster le modèle, ce qui peut surestimer l’erreur réelle.\n\n\nVariabilité : Différents jeux de validation peuvent donner des résultats très différents.\n\n\nSolution → Validation croisée pour une approche plus robuste."
  },
  {
    "objectID": "slides/03-generalities-model-evaluation-slides-annotated.html#approche-par-validation-croisée",
    "href": "slides/03-generalities-model-evaluation-slides-annotated.html#approche-par-validation-croisée",
    "title": "Généralités",
    "section": "Approche par validation croisée",
    "text": "Approche par validation croisée\n\nApproche par validation croisée."
  },
  {
    "objectID": "slides/03-generalities-model-evaluation-slides-annotated.html#choix-du-paramètre-k",
    "href": "slides/03-generalities-model-evaluation-slides-annotated.html#choix-du-paramètre-k",
    "title": "Généralités",
    "section": "Choix du paramètre K",
    "text": "Choix du paramètre K\n\nRègle du pouce: \\(K = 5\\) ou \\(K = 10\\).\nCompromis à considérer :\n\nCoût computationnel : Plus \\(K\\) est grand, plus c’est coûteux en temps de calcul.\nPrécision de l’estimation : Plus \\(K\\) est grand, moins de biais.\nStabilité : Valeurs modérées de \\(K\\) réduisent la variance\n\n\nCas limite: \\(K = n\\) (Leave-One-Out Cross-Validation - LOOCV)."
  },
  {
    "objectID": "slides/03-generalities-model-evaluation-slides-annotated.html#conclusion",
    "href": "slides/03-generalities-model-evaluation-slides-annotated.html#conclusion",
    "title": "Généralités",
    "section": "Conclusion",
    "text": "Conclusion\n\nLa validation appropriée des modèles est essentielle pour éviter une illusion de performance.\nLe sur-ajustement est un piège courant qui nécessite une évaluation rigoureuse.\nLa validation croisée offre une solution robuste et pratique pour estimer la performance réelle.\nLe choix des paramètres de validation doit équilibrer précision, stabilité et coût computationnel.\n\n\nProchaine étape → Réduction de dimension."
  },
  {
    "objectID": "slides/03-generalities-bias-variance-slides-annotated.html#objectif-de-modélisation",
    "href": "slides/03-generalities-bias-variance-slides-annotated.html#objectif-de-modélisation",
    "title": "Généralités",
    "section": "Objectif de modélisation",
    "text": "Objectif de modélisation\n\nModéliser la relation entre :\n\nVariable réponse \\(Y\\) (quantitative, qualitative, etc.).\nVariables explicatives \\(X = (X_1, \\ldots, X_p)\\)."
  },
  {
    "objectID": "slides/03-generalities-bias-variance-slides-annotated.html#modèle-général",
    "href": "slides/03-generalities-bias-variance-slides-annotated.html#modèle-général",
    "title": "Généralités",
    "section": "Modèle général",
    "text": "Modèle général\n\n\\[Y = f(X) + \\varepsilon\\]\n\n\\(f\\) : fonction déterministe (information systématique).\n\\(\\varepsilon\\) : terme d’erreur aléatoire."
  },
  {
    "objectID": "slides/03-generalities-bias-variance-slides-annotated.html#hypothèses-du-modèle",
    "href": "slides/03-generalities-bias-variance-slides-annotated.html#hypothèses-du-modèle",
    "title": "Généralités",
    "section": "Hypothèses du modèle",
    "text": "Hypothèses du modèle\n\nSur le terme d’erreur \\(\\varepsilon\\) :\n\nIndépendant des variables explicatives \\(X\\).\n\\(\\mathbb{E}[\\varepsilon] = 0\\) (espérance nulle).\n\\(\\mathrm{Var}(\\varepsilon) = \\sigma^2\\) (variance constante).\n\n\n→ Cadre général pour toutes les méthodes du cours."
  },
  {
    "objectID": "slides/03-generalities-bias-variance-slides-annotated.html#plan",
    "href": "slides/03-generalities-bias-variance-slides-annotated.html#plan",
    "title": "Généralités",
    "section": "Plan",
    "text": "Plan\n\n\nMesures de qualité - MSE et taux d’erreur\nCompromis biais/variance - Décomposition fondamentale\nFlexibilité des modèles - Rigidité vs adaptabilité\nOptimisation pratique - Trouver l’équilibre"
  },
  {
    "objectID": "slides/03-generalities-bias-variance-slides-annotated.html#comment-mesurer-la-qualité",
    "href": "slides/03-generalities-bias-variance-slides-annotated.html#comment-mesurer-la-qualité",
    "title": "Généralités",
    "section": "Comment mesurer la qualité ?",
    "text": "Comment mesurer la qualité ?\n\nUne fois \\(\\hat{f}\\) estimé, comment évaluer \\(\\hat{Y} = \\hat{f}(X)\\) ?\n\n\nIdée : Mesurer à quel point \\(\\hat{Y}\\) est proche de la vraie valeur \\(Y\\)."
  },
  {
    "objectID": "slides/03-generalities-bias-variance-slides-annotated.html#variables-quantitatives-eqm-mse",
    "href": "slides/03-generalities-bias-variance-slides-annotated.html#variables-quantitatives-eqm-mse",
    "title": "Généralités",
    "section": "Variables quantitatives : EQM (MSE)",
    "text": "Variables quantitatives : EQM (MSE)\n\n\n\nErreur Quadratique Moyenne (MSE)\n\n\n\\[\\text{MSE}(Y, \\hat{Y}) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{f}(x_i))^2\\]\n\n\n\n\nInterprétation → Distance moyenne entre valeurs observées et prédites.\n\n\nMSE faible → Prédictions proches des observations."
  },
  {
    "objectID": "slides/03-generalities-bias-variance-slides-annotated.html#variables-qualitatives-taux-derreur-er",
    "href": "slides/03-generalities-bias-variance-slides-annotated.html#variables-qualitatives-taux-derreur-er",
    "title": "Généralités",
    "section": "Variables qualitatives : Taux d’erreur (ER)",
    "text": "Variables qualitatives : Taux d’erreur (ER)\n\n\n\nTaux d’erreur (ER)\n\n\n\\[\\text{ER}(Y, \\hat{Y}) = \\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{1}(y_i \\neq \\hat{f}(x_i))\\]\n\n\n\n\nInterprétation → Proportion de mauvaises prédictions.\n\n\nER faible → Prédictions proches des observations."
  },
  {
    "objectID": "slides/03-generalities-bias-variance-slides-annotated.html#exactitude-vs-interprétabilité",
    "href": "slides/03-generalities-bias-variance-slides-annotated.html#exactitude-vs-interprétabilité",
    "title": "Généralités",
    "section": "Exactitude vs Interprétabilité",
    "text": "Exactitude vs Interprétabilité\n\nModèles simples (ex: régression linéaire) :\n\nFaciles à interpréter\nRelations complexes mal captées\n\nModèles flexibles (ex: forêt aléatoire) :\n\nMeilleures prédictions\nDifficiles à interpréter\n\n\nLe choix dépend de l’objectif : compréhension ou performance ?"
  },
  {
    "objectID": "slides/03-generalities-bias-variance-slides-annotated.html#no-free-lunch-in-statistics",
    "href": "slides/03-generalities-bias-variance-slides-annotated.html#no-free-lunch-in-statistics",
    "title": "Généralités",
    "section": "“No Free Lunch in Statistics”",
    "text": "“No Free Lunch in Statistics”\n\n\n\n\nIl n’existe pas de méthode universellement optimale !\n\n\nUne méthode performante dans un contexte peut échouer ailleurs.\n→ Toujours adapter l’approche au problème."
  },
  {
    "objectID": "slides/03-generalities-bias-variance-slides-annotated.html#erreur-de-prédiction",
    "href": "slides/03-generalities-bias-variance-slides-annotated.html#erreur-de-prédiction",
    "title": "Généralités",
    "section": "Erreur de prédiction",
    "text": "Erreur de prédiction\n\nNotre vrai objectif : Minimiser l’erreur sur de nouvelles données\n\\[\\mathbb{E}[(Y - \\hat{Y})^2] = \\mathbb{E}[(Y - \\hat{f}(X))^2]\\]\n\n\nCette erreur se décompose en 3 parties…"
  },
  {
    "objectID": "slides/03-generalities-bias-variance-slides-annotated.html#décomposition-biaisvariance",
    "href": "slides/03-generalities-bias-variance-slides-annotated.html#décomposition-biaisvariance",
    "title": "Généralités",
    "section": "Décomposition biais/variance",
    "text": "Décomposition biais/variance\n\n\n\nDécomposition biais/variance\n\n\n\\[\\mathbb{E}[(Y - \\hat{f}(X))^2] = \\text{Biais}(\\hat{f}(X))^2 + \\text{Var}(\\hat{f}(X)) + \\sigma^2\\]\n\n\n\nTrois composantes :\n\nBiais\\(^2\\) → Erreur systématique d’approximation.\nVariance → Sensibilité aux fluctuations d’échantillon.\n\\(\\sigma^2\\) → Erreur irréductible (bruit intrinsèque)."
  },
  {
    "objectID": "slides/03-generalities-bias-variance-slides-annotated.html#décomposition-biaisvariance-2",
    "href": "slides/03-generalities-bias-variance-slides-annotated.html#décomposition-biaisvariance-2",
    "title": "Généralités",
    "section": "Décomposition biais/variance",
    "text": "Décomposition biais/variance\n\nModèle peu flexible (ex: régression linéaire)\n\n✅ Variance faible\n❌ Biais élevé\n\nModèle très flexible (ex: régression avec beaucoup de polynômes)\n\n❌ Variance élevée\n✅ Biais faible\n\n\n→ Besoin d’un équilibre optimal !"
  },
  {
    "objectID": "slides/03-generalities-bias-variance-slides-annotated.html#décomposition-biaisvariance-3",
    "href": "slides/03-generalities-bias-variance-slides-annotated.html#décomposition-biaisvariance-3",
    "title": "Généralités",
    "section": "Décomposition biais/variance",
    "text": "Décomposition biais/variance\n\n\nPar Bigbossfarin - Own work, CC0, Link."
  },
  {
    "objectID": "slides/03-generalities-bias-variance-slides-annotated.html#stratégies-doptimisation",
    "href": "slides/03-generalities-bias-variance-slides-annotated.html#stratégies-doptimisation",
    "title": "Généralités",
    "section": "Stratégies d’optimisation",
    "text": "Stratégies d’optimisation\n\nComment trouver l’équilibre ?\n\nValidation croisée → Estimation de l’erreur de prédiction.\nSélection de modèle → Comparer différentes des modèles avec différentes flexibilités.\nMéthodes d’ensembles → Combiner plusieurs modèles.\nRégularisation → Contrôler la complexité du modèle."
  },
  {
    "objectID": "slides/03-generalities-bias-variance-slides-annotated.html#implications-pour-la-pratique",
    "href": "slides/03-generalities-bias-variance-slides-annotated.html#implications-pour-la-pratique",
    "title": "Généralités",
    "section": "Implications pour la pratique",
    "text": "Implications pour la pratique\n\nQuestions à se poser :\n\nAi-je assez de données pour un modèle flexible ?\nMon objectif est-il la prédiction ou la compréhension ?\nQuelle est la complexité réelle du phénomène étudié ?\nComment évaluer la performance sur nouvelles données ?"
  },
  {
    "objectID": "slides/03-generalities-bias-variance-slides-annotated.html#conclusion",
    "href": "slides/03-generalities-bias-variance-slides-annotated.html#conclusion",
    "title": "Généralités",
    "section": "Conclusion",
    "text": "Conclusion\n\nErreur totale = Biais\\(^2\\) + Variance + Bruit irréductible.\nCompromis entre biais et variance.\nModèles rigides → Fort biais, faible variance.\nModèles flexibles → Faible biais, forte variance.\nOptimum → Équilibre minimisant l’erreur totale.\nPas de solution universelle → Il faut s’adapter au contexte.\n\n\nProchaine étape → Validation et sélection de modèles."
  },
  {
    "objectID": "slides/05-supervised-discriminant-slides.html#plan",
    "href": "slides/05-supervised-discriminant-slides.html#plan",
    "title": "Supervisée",
    "section": "Plan",
    "text": "Plan\n\n\nLa théorie de l’analyse discriminante\nL’analyse discriminante en pratique\nExemple"
  },
  {
    "objectID": "slides/05-supervised-discriminant-slides.html#notation",
    "href": "slides/05-supervised-discriminant-slides.html#notation",
    "title": "Supervisée",
    "section": "Notation",
    "text": "Notation\nDonnées:\n\n\\(X = (X_{ij}) \\in \\mathbb{R}^{n \\times p}\\) : matrice de données.\n\\(n\\) : nombre d’individus / d’observations.\n\\(p\\) : nombre de variables.\n\n\nGroupes:\n\n\\(K\\) groupes dans la population.\n\\(I_k\\) : ensemble des individus du groupe \\(k\\).\n\\(n_k\\) : nombre d’observations dans \\(I_k\\) et \\(\\sum_{k=1}^{K} n_k = n\\)."
  },
  {
    "objectID": "slides/05-supervised-discriminant-slides.html#objectif-principal",
    "href": "slides/05-supervised-discriminant-slides.html#objectif-principal",
    "title": "Supervisée",
    "section": "Objectif principal",
    "text": "Objectif principal\n\nPrincipe:\n\nPartitionner \\(\\mathbb{R}^p\\) en \\(K\\) sous-ensembles.\nPasser de \\(\\mathbb{R}^p\\) à \\(\\mathbb{R}\\) via un score.\n\n\n\nScore de Fisher\n\\[f(X_1, \\ldots, X_p) = a^{\\top} X + b = a_1 X_1 + \\cdots + a_p X_p + b\\]\n\n\nUtiliser ce score pour déterminer le groupe d’appartenance."
  },
  {
    "objectID": "slides/05-supervised-discriminant-slides.html#simplification",
    "href": "slides/05-supervised-discriminant-slides.html#simplification",
    "title": "Supervisée",
    "section": "Simplification",
    "text": "Simplification\n\nEn centrant les variables: \\(-b = a^{\\top} \\overline{X}\\).\n\nIl reste à déterminer le vecteur \\(a = (a_1, \\ldots, a_p)\\)."
  },
  {
    "objectID": "slides/05-supervised-discriminant-slides.html#intuition",
    "href": "slides/05-supervised-discriminant-slides.html#intuition",
    "title": "Supervisée",
    "section": "Intuition",
    "text": "Intuition\n\nOn voudrait choisir le vecteur \\(a\\) de sorte que les scores soient\n\ntrès différents entre les groupes\ntrès similaire à l’intérieur des groupes\n\n\nOn s’intéresse donc à la variabilité des scores à l’intérieur des groupes et entre les groupes.\n\n\nOptimiser le rapport entre la variabilité inter-groupes (maximiser) et la variabilité intra-groupe (minimiser)."
  },
  {
    "objectID": "slides/05-supervised-discriminant-slides.html#matrices-de-variance-covariance",
    "href": "slides/05-supervised-discriminant-slides.html#matrices-de-variance-covariance",
    "title": "Supervisée",
    "section": "Matrices de variance-covariance",
    "text": "Matrices de variance-covariance\n\n\n\\(S\\) : matrice de variance totale\n\\(W\\) : matrice de variance intra-groupe\n\\(B\\) : matrice de variance inter-groupe\n\n\nRelation fondamentale:\n\\[S = W + B\\]"
  },
  {
    "objectID": "slides/05-supervised-discriminant-slides.html#matrices-de-variance-covariance-1",
    "href": "slides/05-supervised-discriminant-slides.html#matrices-de-variance-covariance-1",
    "title": "Supervisée",
    "section": "Matrices de variance-covariance",
    "text": "Matrices de variance-covariance\n\nMatrice totale: \\(S = \\sum_{i=1}^{n} (X_i - \\overline{X})(X_i - \\overline{X})^{\\top}\\)\n\n\nMatrice intra-groupe: \\(W = \\sum_{k=1}^{K} \\sum_{i \\in I_k} (X_i - \\overline{X}_k)(X_i - \\overline{X}_k)^{\\top}\\)\n\n\n\nMatrice inter-groupe: \\(B = \\sum_{k=1}^{K} n_k (\\overline{X}_k - \\overline{X})(\\overline{X}_k - \\overline{X})^{\\top}\\)"
  },
  {
    "objectID": "slides/05-supervised-discriminant-slides.html#variabilité-des-scores",
    "href": "slides/05-supervised-discriminant-slides.html#variabilité-des-scores",
    "title": "Supervisée",
    "section": "Variabilité des scores",
    "text": "Variabilité des scores\n\nÉtant donné que \\(a \\in \\mathbb{R}^p\\), on a: \\[\\mathrm{Var}(f(X_{1}, \\dots, X_p)) = \\mathrm{Var}(a^{\\top} X) = a^{\\top} \\mathrm{Var}(X) a.\\]\nComme \\(S\\) est un estimateur de la variance totale de \\(X\\),\n\\[\\widehat{\\mathrm{Var}}(f(X_{1}, \\dots, X_p)) = \\frac{1}{n} a^{\\top} S a = \\frac{1}{n} \\left( a^{\\top} W a + a^{\\top} B a \\right).\\]"
  },
  {
    "objectID": "slides/05-supervised-discriminant-slides.html#critère-doptimisation",
    "href": "slides/05-supervised-discriminant-slides.html#critère-doptimisation",
    "title": "Supervisée",
    "section": "Critère d’optimisation",
    "text": "Critère d’optimisation\n\n\\[J(a) = \\frac{a^{\\top} B a}{a^{\\top} W a} = \\frac{a^{\\top} B a}{a^{\\top} S a}\\]\n\nFormulations équivalentes\n\nMaximiser \\(J(a)\\) sous contrainte \\(a^{\\top} a = 1\\)\nMaximiser \\(a^{\\top} B a\\) sous contrainte \\(a^{\\top} S a = 1\\)\nMaximiser \\(c^{\\top} S^{-1/2} B S^{-1/2} c\\) sous contrainte \\(c^{\\top} c = 1\\) où \\(c = S^{1/2} a\\)."
  },
  {
    "objectID": "slides/05-supervised-discriminant-slides.html#solution-optimale",
    "href": "slides/05-supervised-discriminant-slides.html#solution-optimale",
    "title": "Supervisée",
    "section": "Solution optimale",
    "text": "Solution optimale\n\nEn réécrivant la troisième formulation\n\\[c^{\\top} \\left( S^{-1/2} B S^{-1/2} \\right) c \\quad\\text{s.c.}\\quad c^{\\top} c = 1,\\]\n\nOn peut prendre \\(a = S^{-1/2} c\\), où \\(c\\) est un vecteur propre normé associé à \\(\\lambda_{1}\\), la première valeur propre de \\(S^{-1/2} B S^{-1/2}\\)."
  },
  {
    "objectID": "slides/05-supervised-discriminant-slides.html#fonction-discriminante",
    "href": "slides/05-supervised-discriminant-slides.html#fonction-discriminante",
    "title": "Supervisée",
    "section": "Fonction discriminante",
    "text": "Fonction discriminante\n\n\\[f(x) = a^{\\top} (x - \\overline{X})\\]\n\nScores des observations \\[U_i = a^{\\top} (X_i - \\overline{X})\\] Ces scores maximisent le rapport variance inter-groupe / variance intra-groupe"
  },
  {
    "objectID": "slides/05-supervised-discriminant-slides.html#exemple",
    "href": "slides/05-supervised-discriminant-slides.html#exemple",
    "title": "Supervisée",
    "section": "Exemple",
    "text": "Exemple\n\nCodePlot des donnéesCombinaison linéaireCombinaison linéaire optimale\n\n\n\nlibrary(MASS)\nlibrary(tidyverse)\n\nn &lt;- 50\nR &lt;- matrix(\n  c(1, 0.8, 0.8, 1), \n  nrow = 2, ncol = 2, byrow = TRUE\n)\n            \nmu_1 &lt;- c(X = 7.5, Y = 3)\ndata_1 &lt;- MASS::mvrnorm(n, mu = mu_1, Sigma = R)\nmu_2 &lt;- c(X = 8.5, Y = 4)\ndata_2 &lt;- MASS::mvrnorm(n, mu = mu_2, Sigma = R)\ndata &lt;- data.frame(rbind(data_1, data_2))\ndata$group &lt;- as.factor(rep(c(1, 2), each = n))\n\nmodel &lt;- lda(group ~ ., data = data)\ndata$new_X &lt;- predict(model, data)$x"
  },
  {
    "objectID": "slides/05-supervised-discriminant-slides.html#pouvoir-discriminant",
    "href": "slides/05-supervised-discriminant-slides.html#pouvoir-discriminant",
    "title": "Supervisée",
    "section": "Pouvoir discriminant",
    "text": "Pouvoir discriminant\n\nInterprétation de \\(\\lambda_1\\) → pouvoir discriminant de \\(f\\) \\[\\lambda_1 = \\frac{a^{\\top} B a}{a^{\\top} S a} \\quad \\text{avec } 0 \\leq \\lambda_1 \\leq 1\\]\n\n\\(\\lambda_1 = 1\\) : Cas idéal\n\n\n\\(\\lambda_1 = 0\\) : Analyse inutile"
  },
  {
    "objectID": "slides/05-supervised-discriminant-slides.html#règle-de-classification",
    "href": "slides/05-supervised-discriminant-slides.html#règle-de-classification",
    "title": "Supervisée",
    "section": "Règle de classification",
    "text": "Règle de classification\n\nScore moyen par groupe \\[m_k = a^{\\top} (\\overline{X}_{k1}, \\ldots, \\overline{X}_{kp})^{\\top}\\]\n\nClassification d’une nouvelle observation \\(X_0\\)\n\nCalculer le score: \\(f(X_0) = a^{\\top} X_0\\)\nAssigner au groupe \\(k^*\\) tel que:\n\n\\[k^* = \\arg\\min_{k} |a^{\\top} X_0 - m_k|\\]"
  },
  {
    "objectID": "slides/05-supervised-discriminant-slides.html#cas-binaire-k-2",
    "href": "slides/05-supervised-discriminant-slides.html#cas-binaire-k-2",
    "title": "Supervisée",
    "section": "Cas binaire (\\(K = 2\\))",
    "text": "Cas binaire (\\(K = 2\\))\n\nSolution explicite \\[C = \\sqrt{\\frac{n_1 n_2}{n}} (\\overline{X}_1 - \\overline{X}_2)\\] \\[B = CC^{\\top}, \\quad a = S^{-1}C\\]"
  },
  {
    "objectID": "slides/05-supervised-discriminant-slides.html#cas-binaire-k-2-1",
    "href": "slides/05-supervised-discriminant-slides.html#cas-binaire-k-2-1",
    "title": "Supervisée",
    "section": "Cas binaire (\\(K = 2\\))",
    "text": "Cas binaire (\\(K = 2\\))\n\nRègle de classification\nSi \\(m_{1} = a^{\\top} \\overline{X}_1 &gt; a^{\\top} \\overline{X}_2 = m_{2}\\), classer \\(X_0\\) dans le groupe 1 si: \\[a^{\\top} X_0 &gt; \\frac{m_1 + m_2}{2}\\]"
  },
  {
    "objectID": "slides/05-supervised-discriminant-slides.html#évaluation-de-la-performance",
    "href": "slides/05-supervised-discriminant-slides.html#évaluation-de-la-performance",
    "title": "Supervisée",
    "section": "Évaluation de la performance",
    "text": "Évaluation de la performance\n\nMatrice de confusion\n\nAppliquer la règle de classification à l’échantillon d’apprentissage\nComparer les classifications prédites aux vraies classes\nEstimer les risques de mauvaise classification"
  },
  {
    "objectID": "slides/05-supervised-discriminant-slides.html#exemple-1",
    "href": "slides/05-supervised-discriminant-slides.html#exemple-1",
    "title": "Supervisée",
    "section": "Exemple",
    "text": "Exemple\n\\(49\\) hommes âgés ont été déclarés soit en bonne santé mentale (Groupe I), soit séniles (Groupe II) à la suite d’examens psychiatriques poussés. Les mêmes sujets ont été soumis à \\(4\\) tests standards beaucoup moins coûteux :\n\n\n\nTest\nGroup I (\\(n_1 = 37\\))\nGroup II (\\(n_2 = 12\\))\n\n\n\n\nInformation\n12.57\n8.75\n\n\nSimilitudes\n9.57\n5.33\n\n\nArithmétique\n11.49\n8.50\n\n\nComplétion de dessins\n7.97\n4.75"
  },
  {
    "objectID": "slides/05-supervised-discriminant-slides.html#exemple-2",
    "href": "slides/05-supervised-discriminant-slides.html#exemple-2",
    "title": "Supervisée",
    "section": "Exemple",
    "text": "Exemple\nPour cette étude, on a trouvé que\n\\[\\frac{S}{n} = \\begin{pmatrix}\n  11.2552 & 9.4042 & 7.1489 & 3.3830 \\\\\n   & 13.5318 & 7.3830 & 2.5532 \\\\\n   & & 11.5744 & 2.6170 \\\\\n   & & & 5.8085\n\\end{pmatrix},\\] \\[C = \\sqrt{\\frac{n_1 n_2}{n}} (\\overline{X}_1 - \\overline{X}_2) = \\sqrt{\\frac{37 \\times 12}{49}} \\begin{pmatrix} 3.82 \\\\ 4.24 \\\\ 2.99 \\\\ 3.22\\end{pmatrix}\\]"
  },
  {
    "objectID": "slides/05-supervised-discriminant-slides.html#exemple-3",
    "href": "slides/05-supervised-discriminant-slides.html#exemple-3",
    "title": "Supervisée",
    "section": "Exemple",
    "text": "Exemple\n En principe, on a \\[a = S^{-1} C,\\] mais on peut tout aussi bien utiliser \\[a = n S^{-1} C\\] puisque \\(a\\) n’est défini qu’à une constante multiplicative près."
  },
  {
    "objectID": "slides/05-supervised-discriminant-slides.html#exemple-4",
    "href": "slides/05-supervised-discriminant-slides.html#exemple-4",
    "title": "Supervisée",
    "section": "Exemple",
    "text": "Exemple\n\nEn effectuant les calculs décrits précédemment, on trouve \\[m_1 = a^\\top \\overline{X}_1 = 5.97 \\quad\\text{et}\\quad m_2 = a^\\top \\overline{X}_2 = 3.54,\\] ce qui conduit à déclarer un individu sénile si \\[a^\\top X &gt; \\frac{m_1 + m_2}{2} = 4.755.\\]"
  },
  {
    "objectID": "slides/05-supervised-discriminant-slides.html#exemple-5",
    "href": "slides/05-supervised-discriminant-slides.html#exemple-5",
    "title": "Supervisée",
    "section": "Exemple",
    "text": "Exemple\n Matrice de confusion :\n\n\n\nDiagnostic clinique →\nOK\nSénile\nTotaux\n\n\n\n\nClassé OK\n29\n4\n33\n\n\nClassé sénile\n8\n8\n16\n\n\nTotaux\n37\n12\n49"
  },
  {
    "objectID": "slides/05-supervised-discriminant-slides.html#conclusion",
    "href": "slides/05-supervised-discriminant-slides.html#conclusion",
    "title": "Supervisée",
    "section": "Conclusion",
    "text": "Conclusion\n\nL’analyse discrimante est un algorithme de classification supervisée qui :\n\nest simple et interprétable.\na une solution analytique explicite.\npermet une réduction de dimension naturelle (\\(\\mathbb{R}^p \\to \\mathbb{R}\\)).\n\n\nProchaine étape → Des arbres !"
  },
  {
    "objectID": "slides/03-generalities-space-slides.html#pourquoi-sintéresser-à-lespace-mathématique",
    "href": "slides/03-generalities-space-slides.html#pourquoi-sintéresser-à-lespace-mathématique",
    "title": "Généralités",
    "section": "Pourquoi s’intéresser à l’espace mathématique ?",
    "text": "Pourquoi s’intéresser à l’espace mathématique ?\n\nLe type de variables détermine :\n\nL’espace mathématique dans lequel elle vit.\nLes mesures de distance qu’on peut utiliser.\nLes modèles pertinents à utiliser.\n\n\n→ Il est donc fondamental de connaître l’espace mathématique avant toute modélisation !"
  },
  {
    "objectID": "slides/03-generalities-space-slides.html#plan",
    "href": "slides/03-generalities-space-slides.html#plan",
    "title": "Généralités",
    "section": "Plan",
    "text": "Plan\n\n\nUnité statistique - Le choix du niveau d’analyse.\nTypes de variables - 4 catégories principales.\nEspaces associés - Le cadre mathématique \\(\\mathcal{X}\\)."
  },
  {
    "objectID": "slides/03-generalities-space-slides.html#unité-statistique",
    "href": "slides/03-generalities-space-slides.html#unité-statistique",
    "title": "Généralités",
    "section": "Unité statistique",
    "text": "Unité statistique\n\nDéfinition : Élément de base définissant une observation.\n\nC’est un choix du modélisateur qui détermine le niveau d’agrégation de l’analyse.\n\n\nExemples :\n\nEnquête sur les revenus\nÉtude d’un lycée\nImagerie médicale"
  },
  {
    "objectID": "slides/03-generalities-space-slides.html#attention-niveaux-danalyse-multiples",
    "href": "slides/03-generalities-space-slides.html#attention-niveaux-danalyse-multiples",
    "title": "Généralités",
    "section": "Attention : Niveaux d’analyse multiples",
    "text": "Attention : Niveaux d’analyse multiples\n\n\n\n\nExemple : Image médicale\n\n\nUne image = collection de pixels\nChoix 1 :\nPixel comme unité → Variables : RVB, opacité par pixel.\nChoix 2 :\nImage comme unité → Variables : caractéristiques globales."
  },
  {
    "objectID": "slides/03-generalities-space-slides.html#types-de-variables-vue-densemble",
    "href": "slides/03-generalities-space-slides.html#types-de-variables-vue-densemble",
    "title": "Généralités",
    "section": "Types de variables : Vue d’ensemble",
    "text": "Types de variables : Vue d’ensemble\n\n4 types principaux pour les scalaires :\n\nNumérique (quantitative)\nOrdinale\nNominale symétrique\nNominale asymétrique"
  },
  {
    "objectID": "slides/03-generalities-space-slides.html#variable-numérique",
    "href": "slides/03-generalities-space-slides.html#variable-numérique",
    "title": "Généralités",
    "section": "1. Variable numérique",
    "text": "1. Variable numérique\n\n\n\nDéfinition\n\n\nUne variable numérique représente une quantité mesurable.\nExemples :\n\nRevenu en dollars\nMasse corporelle\nTempérature"
  },
  {
    "objectID": "slides/03-generalities-space-slides.html#variable-ordinale",
    "href": "slides/03-generalities-space-slides.html#variable-ordinale",
    "title": "Généralités",
    "section": "2. Variable ordinale",
    "text": "2. Variable ordinale\n\n\n\nDéfinition\n\n\nUne variable qualitative dont les modalités peuvent être ordonnées naturellement, sans que l’écart ne soit quantifiable.\nExemples :\n\nNiveau de revenu : faible &lt; moyen &lt; élevé\nSatisfaction : pas du tout &lt; peu &lt; moyennement &lt; très &lt; tout à fait"
  },
  {
    "objectID": "slides/03-generalities-space-slides.html#variable-nominale-symétrique",
    "href": "slides/03-generalities-space-slides.html#variable-nominale-symétrique",
    "title": "Généralités",
    "section": "3. Variable nominale symétrique",
    "text": "3. Variable nominale symétrique\n\n\n\nDéfinition\n\n\nUne variable qualitative où toutes les modalités sont aussi informatives les unes que les autres.\nExemples :\n\nNationalité : française, canadienne, allemande…\nFilière : mathématiques, physique, biologie…\nCouleur : rouge, vert, bleu…"
  },
  {
    "objectID": "slides/03-generalities-space-slides.html#variable-nominale-asymétrique",
    "href": "slides/03-generalities-space-slides.html#variable-nominale-asymétrique",
    "title": "Généralités",
    "section": "4. Variable nominale asymétrique",
    "text": "4. Variable nominale asymétrique\n\n\n\nDéfinition\n\n\nUne variable qualitative où une modalité a un statut particulier (souvent “par défaut” ou plus fréquente).\nExemples :\n\nPrésence/absence de symptôme\nTransaction frauduleuse/normale\nDéfaut/bon fonctionnement"
  },
  {
    "objectID": "slides/03-generalities-space-slides.html#au-delà-des-variables-classiques",
    "href": "slides/03-generalities-space-slides.html#au-delà-des-variables-classiques",
    "title": "Généralités",
    "section": "Au-delà des variables classiques",
    "text": "Au-delà des variables classiques\n\nCourbes et signaux\nTextes et séquences de caractères\nImages et vidéos\nRéseaux et graphes\nDonnées géospatiales\n\n→ Le choix de représentation dépend de l’unité statistique."
  },
  {
    "objectID": "slides/03-generalities-space-slides.html#espaces-dobservation-mathcalx",
    "href": "slides/03-generalities-space-slides.html#espaces-dobservation-mathcalx",
    "title": "Généralités",
    "section": "Espaces d’observation \\(\\mathcal{X}\\)",
    "text": "Espaces d’observation \\(\\mathcal{X}\\)\n\n\n\nDéfinition\n\n\nEspace mathématique dans lequel nos variables prennent leurs valeurs.\n\nCadre formel de travail.\nGuide les choix méthodologiques.\n\n\n\n\nNotation : \\(\\mathcal{X}\\) = espace d’observation"
  },
  {
    "objectID": "slides/03-generalities-space-slides.html#variable-numérique-mathbbr",
    "href": "slides/03-generalities-space-slides.html#variable-numérique-mathbbr",
    "title": "Généralités",
    "section": "Variable numérique → \\(\\mathbb{R}\\)",
    "text": "Variable numérique → \\(\\mathbb{R}\\)\n\nCas général : \\(\\mathcal{X} = \\mathbb{R}\\)\n\nCas restreints :\n\nTaille d’une personne : \\(\\mathcal{X} = [0, +\\infty)\\)\nProbabilité : \\(\\mathcal{X} = [0, 1]\\)\nPourcentage : \\(\\mathcal{X} = [0, 100]\\)"
  },
  {
    "objectID": "slides/03-generalities-space-slides.html#variable-nominale-ensemble-fini",
    "href": "slides/03-generalities-space-slides.html#variable-nominale-ensemble-fini",
    "title": "Généralités",
    "section": "Variable nominale → Ensemble fini",
    "text": "Variable nominale → Ensemble fini\n\nExemple : Lancer de dé\n\\[\\mathcal{X} = \\{1, 2, 3, 4, 5, 6\\}\\]\nExemple : Filière d’étude\n\\[\\mathcal{X} = \\{\\text{Math}, \\text{Physique}, \\text{Bio}, \\text{Info}\\}\\]"
  },
  {
    "objectID": "slides/03-generalities-space-slides.html#espaces-plus-complexes",
    "href": "slides/03-generalities-space-slides.html#espaces-plus-complexes",
    "title": "Généralités",
    "section": "Espaces plus complexes",
    "text": "Espaces plus complexes\n\nAnalyse de courbes : fonctions continues sur \\([a,b]\\)\n\\[\\mathcal{X} = \\mathcal{C}([a,b])\\]\nAnalyse de texte : alphabet\n\\[\\mathcal{X} = \\{A, B, C, \\ldots, Z\\}\\]"
  },
  {
    "objectID": "slides/03-generalities-space-slides.html#variables-multiples-produit-cartésien",
    "href": "slides/03-generalities-space-slides.html#variables-multiples-produit-cartésien",
    "title": "Généralités",
    "section": "Variables multiples : Produit cartésien",
    "text": "Variables multiples : Produit cartésien\n\nObservation simultanée : taille + poids + sexe\n\\[\\mathcal{X} = \\mathcal{X}_1 \\times \\mathcal{X}_2 \\times \\mathcal{X}_3\\]\nCas particulier : \\(p\\) variables numériques \\[\\mathcal{X} = \\mathbb{R}^p\\]"
  },
  {
    "objectID": "slides/03-generalities-space-slides.html#exemple-concret-étude-détudiants",
    "href": "slides/03-generalities-space-slides.html#exemple-concret-étude-détudiants",
    "title": "Généralités",
    "section": "Exemple concret : étude d’étudiants",
    "text": "Exemple concret : étude d’étudiants\n\n\nAge (numérique) : \\(\\mathcal{X}_1 = [16, 30]\\).\nFilière (nominale symétrique) : \\(\\mathcal{X}_2 = \\{\\text{Math}, \\text{Physique}\\}\\).\nSatisfaction (ordinale) : \\(\\mathcal{X}_3 = \\{\\text{Faible}, \\text{Moyen}, \\text{Élevé}\\}\\).\n\n\n\nEspace d’observation :\n\n\\[\\mathcal{X} = [16, 30] \\times \\{\\text{Math}, \\text{Physique}\\} \\times \\{\\text{Faible}, \\text{Moyen}, \\text{Élevé}\\}\\]"
  },
  {
    "objectID": "slides/03-generalities-space-slides.html#récapitulatif",
    "href": "slides/03-generalities-space-slides.html#récapitulatif",
    "title": "Généralités",
    "section": "Récapitulatif",
    "text": "Récapitulatif\n\n\n\n\n\nType de variable\nExemple\nEspace typique\n\n\n\n\nNumérique\nÂge, revenu\n\\(\\mathbb{R}\\), \\([0,+\\infty)\\)\n\n\nOrdinale\nSatisfaction\nEnsemble ordonné fini\n\n\nNominale symétrique\nNationalité\nEnsemble fini\n\n\nNominale asymétrique\nPrésence/absence\n\\(\\{0, 1\\}\\)"
  },
  {
    "objectID": "slides/03-generalities-space-slides.html#conclusion",
    "href": "slides/03-generalities-space-slides.html#conclusion",
    "title": "Généralités",
    "section": "Conclusion",
    "text": "Conclusion\n\n\nL’unité statistique définit le niveau d’analyse.\nLe type de variable guide le choix de l’espace.\nL’espace d’observation \\(\\mathcal{X}\\) est le cadre mathématique.\nVariables multiples → produit cartésien d’ensemble.\n\n\n→ Prochaine étape : Définir des distances dans ces espaces !"
  },
  {
    "objectID": "slides/07-ethics-slides.html#lanalyse-de-données-aujourdhui",
    "href": "slides/07-ethics-slides.html#lanalyse-de-données-aujourdhui",
    "title": "Éthique",
    "section": "L’analyse de données aujourd’hui",
    "text": "L’analyse de données aujourd’hui\n\nOmniprésence dans tous les domaines :\n\nSanté\nÉducation\nPolitiques publiques\nJustice\nFinance\n\n\n→ Décisions automatisées aux conséquences réelles."
  },
  {
    "objectID": "slides/07-ethics-slides.html#plan",
    "href": "slides/07-ethics-slides.html#plan",
    "title": "Éthique",
    "section": "Plan",
    "text": "Plan\n\n\nGénéralités\nConfidentialité des données\nImplications sociales\nMythes à déconstruire\nSolutions pratiques"
  },
  {
    "objectID": "slides/07-ethics-slides.html#pourquoi-léthique-est-cruciale",
    "href": "slides/07-ethics-slides.html#pourquoi-léthique-est-cruciale",
    "title": "Éthique",
    "section": "Pourquoi l’éthique est cruciale ?",
    "text": "Pourquoi l’éthique est cruciale ?\n\n\nLes données ne sont pas neutres !\n\n\nElles sont le reflet d’un contexte :\n\nSocial\nInstitutionnel\nTechnique\n\n\n\nRisques → Renforcer les inégalités, reproduire des biais historiques, porter atteinte à la vie privée, …"
  },
  {
    "objectID": "slides/07-ethics-slides.html#questions-fondamentales",
    "href": "slides/07-ethics-slides.html#questions-fondamentales",
    "title": "Éthique",
    "section": "Questions fondamentales",
    "text": "Questions fondamentales\n\nAvant toute analyse, se demander :\n\nMes données représentent-elles fidèlement ma population ?\nQuelles décisions seront prises à partir de ces analyses ?\nQuelles sont les conséquences pour les individus concernés ?\nQui est affecté par ces décisions ?"
  },
  {
    "objectID": "slides/07-ethics-slides.html#confidentialité-enjeux",
    "href": "slides/07-ethics-slides.html#confidentialité-enjeux",
    "title": "Éthique",
    "section": "Confidentialité : Enjeux",
    "text": "Confidentialité : Enjeux\n\nLors de la manipulation de données personnelles :\n\nRéidentification des personnes\nExploitation abusive\nViolation de la vie privée\n\n→ Nécessité de prendre des mesures adéquates."
  },
  {
    "objectID": "slides/07-ethics-slides.html#anonymisation-des-données",
    "href": "slides/07-ethics-slides.html#anonymisation-des-données",
    "title": "Éthique",
    "section": "Anonymisation des données",
    "text": "Anonymisation des données\n\nSupprimer ou transformer les identifiants :\n\nDirects :\n\nNoms, adresses, numéros\n\nIndirects :\n\nDates de naissance, codes postaux\n\n\n\n⚠️ Attention : Le croisement de plusieurs variables peut permettre l’identification !"
  },
  {
    "objectID": "slides/07-ethics-slides.html#exemple-netflix-et-imdb",
    "href": "slides/07-ethics-slides.html#exemple-netflix-et-imdb",
    "title": "Éthique",
    "section": "Exemple : Netflix et IMDb",
    "text": "Exemple : Netflix et IMDb\n\nDes chercheurs de l’université du Texas ont pu identifier des utilisateurs Netflix en croisant leurs notes avec celles sur IMDb.\n\n Leçon → L’anonymisation simple ne suffit pas toujours !"
  },
  {
    "objectID": "slides/07-ethics-slides.html#techniques-de-protection",
    "href": "slides/07-ethics-slides.html#techniques-de-protection",
    "title": "Éthique",
    "section": "Techniques de protection",
    "text": "Techniques de protection\n\nRéduire la granularité :\n\nÂges → Tranches d’âge (25-34 ans au lieu de 28 ans)\nCodes postaux → Régions\nSalaires → Catégories\n\n\nRegrouper les modalités rares :\n\nÉviter les combinaisons uniques de caractéristiques\nFusionner les catégories peu fréquentes"
  },
  {
    "objectID": "slides/07-ethics-slides.html#techniques-de-protection-1",
    "href": "slides/07-ethics-slides.html#techniques-de-protection-1",
    "title": "Éthique",
    "section": "Techniques de protection",
    "text": "Techniques de protection\n\nAjout de bruit :\n\nPréserver les tendances globales\nRendre l’identification individuelle difficile\n⚠️ Ne pas compromettre la validité des analyses\n\n\nConfidentialité différentielle :\n\nGarantir qu’un individu ne peut être identifié\nMême en connaissant toutes les autres données"
  },
  {
    "objectID": "slides/07-ethics-slides.html#implications-sociales-sources-de-biais",
    "href": "slides/07-ethics-slides.html#implications-sociales-sources-de-biais",
    "title": "Éthique",
    "section": "Implications sociales : Sources de biais",
    "text": "Implications sociales : Sources de biais\n\nPlusieurs points de défaillance :\n\nBiais d’échantillonnage → Données non représentatives\nBiais dans la variable cible → Reproduction de discriminations historiques\nValidité variable selon les groupes → Performance inégale\nClasses mal représentées → Groupes minoritaires ignorés"
  },
  {
    "objectID": "slides/07-ethics-slides.html#biais-déchantillonnage",
    "href": "slides/07-ethics-slides.html#biais-déchantillonnage",
    "title": "Éthique",
    "section": "Biais d’échantillonnage",
    "text": "Biais d’échantillonnage\n\nProblème : Le jeu de données ne reflète pas fidèlement la population cible.\nExemples :\n\nSous-représentation de certaines communautés\nSurreprésentation de populations faciles d’accès\nExclusion de groupes vulnérables\n\n→ Modèles inéquitables"
  },
  {
    "objectID": "slides/07-ethics-slides.html#biais-dans-la-variable-cible",
    "href": "slides/07-ethics-slides.html#biais-dans-la-variable-cible",
    "title": "Éthique",
    "section": "Biais dans la variable cible",
    "text": "Biais dans la variable cible\n\nProblème : Reproduction de discriminations historiques.\nExemples :\n\nJustice prédictive : logiciel COMPAS utilisé aux USA.\nRecrutement : processus d’Amazon.\n\n→ Discrimination systématique de certains groupes."
  },
  {
    "objectID": "slides/07-ethics-slides.html#validité-variable-selon-les-groupes",
    "href": "slides/07-ethics-slides.html#validité-variable-selon-les-groupes",
    "title": "Éthique",
    "section": "Validité variable selon les groupes",
    "text": "Validité variable selon les groupes\n\nUn même modèle peut :\n\nTrès bien fonctionner pour un sous-groupe\nÊtre très mauvais pour un autre\n\n\nExemple (Krishnan, A., Almadan, A., & Rattani, A. (2020)):\n\nReconnaissance faciale : Haute précision pour hommes blancs.\nMême système : Faible précision pour femmes noires.\n\n→ Erreurs systématiques discriminatoires"
  },
  {
    "objectID": "slides/07-ethics-slides.html#classes-mal-représentées",
    "href": "slides/07-ethics-slides.html#classes-mal-représentées",
    "title": "Éthique",
    "section": "Classes mal représentées",
    "text": "Classes mal représentées\n\nConséquences :\n\nLes maladies rares sont mal prédites.\nLes groupes démographiques minoritaires sont ignorés.\nLa performance globale masque les échecs locaux.\nLes inégalités existantes sont renforcées."
  },
  {
    "objectID": "slides/07-ethics-slides.html#mythes-à-déconstruire",
    "href": "slides/07-ethics-slides.html#mythes-à-déconstruire",
    "title": "Éthique",
    "section": "Mythes à déconstruire",
    "text": "Mythes à déconstruire\n\n\nQuelques idées reçues :\n\n“La machine apprend toute seule.”\n“C’est objectif, c’est basé sur des données.”\n“Mon modèle ne peut pas être sexiste, je n’utilise pas le genre.”"
  },
  {
    "objectID": "slides/07-ethics-slides.html#mythe-1-la-machine-apprend-toute-seule.",
    "href": "slides/07-ethics-slides.html#mythe-1-la-machine-apprend-toute-seule.",
    "title": "Éthique",
    "section": "Mythe 1 : “La machine apprend toute seule.”",
    "text": "Mythe 1 : “La machine apprend toute seule.”\n\nEn réalité → Ce sont des choix faits par des humains :\n\nQuelles données collecter ?\nQuelles variables inclure ?\nQuelle variable à expliquer ?\nQuel algorithme utiliser ?\n\n→ L’“apprentissage” dépend entièrement des décisions humaines"
  },
  {
    "objectID": "slides/07-ethics-slides.html#mythe-2-cest-objectif-basé-sur-des-données.",
    "href": "slides/07-ethics-slides.html#mythe-2-cest-objectif-basé-sur-des-données.",
    "title": "Éthique",
    "section": "Mythe 2 : “C’est objectif, basé sur des données.”",
    "text": "Mythe 2 : “C’est objectif, basé sur des données.”\n\nRéalité → Les données ne sont jamais neutres !\nElles sont le produit de :\n\nContexte social\nChoix institutionnels\nDécisions méthodologiques\n\n→ Les modèles héritent des biais des données."
  },
  {
    "objectID": "slides/07-ethics-slides.html#mythe-3-sans-genre-pas-de-sexisme.",
    "href": "slides/07-ethics-slides.html#mythe-3-sans-genre-pas-de-sexisme.",
    "title": "Éthique",
    "section": "Mythe 3 : “Sans genre, pas de sexisme.”",
    "text": "Mythe 3 : “Sans genre, pas de sexisme.”\n\nRéalité → Le modèle peut inférer le genre via d’autres variables corrélées :\n\nProfession\nParcours scolaire\nHistorique salarial\n\n→ L’exclusion explicite ne garantit pas l’absence de biais."
  },
  {
    "objectID": "slides/07-ethics-slides.html#solutions-vue-densemble",
    "href": "slides/07-ethics-slides.html#solutions-vue-densemble",
    "title": "Éthique",
    "section": "Solutions : Vue d’ensemble",
    "text": "Solutions : Vue d’ensemble\n\nAgir à différentes étapes du processus :\n\nEn amont → Sur les données\nPendant → Sur la fonction de perte\nA posteriori → Sur les résultats"
  },
  {
    "objectID": "slides/07-ethics-slides.html#solution-1-agir-sur-les-données",
    "href": "slides/07-ethics-slides.html#solution-1-agir-sur-les-données",
    "title": "Éthique",
    "section": "Solution 1 : Agir sur les données",
    "text": "Solution 1 : Agir sur les données\n\nInterventions en amont :\n\nRééquilibrer l’échantillon → Représentation équitable\nSur-représenter certains groupes → Compenser sous-représentation\nCorriger les biais connus → Dans la variable cible\nAugmenter les données → Pour classes minoritaires"
  },
  {
    "objectID": "slides/07-ethics-slides.html#solution-2-modifier-la-fonction-de-perte",
    "href": "slides/07-ethics-slides.html#solution-2-modifier-la-fonction-de-perte",
    "title": "Éthique",
    "section": "Solution 2 : Modifier la fonction de perte",
    "text": "Solution 2 : Modifier la fonction de perte\n\nIntégrer l’équité dans l’apprentissage :\n\nPénalités pour inégalités de performance\nImposer équité en termes de faux positifs\nContraintes sur disparités entre groupes\nOptimisation multi-objectifs (performance + équité)"
  },
  {
    "objectID": "slides/07-ethics-slides.html#solution-3-corrections-a-posteriori",
    "href": "slides/07-ethics-slides.html#solution-3-corrections-a-posteriori",
    "title": "Éthique",
    "section": "Solution 3 : Corrections a posteriori",
    "text": "Solution 3 : Corrections a posteriori\n\nAjuster les résultats produits :\n\nRecalibrage des probabilités → Par groupe démographique\nAjustement des seuils → Seuils différents selon contexte\nPost-processing → Corrections statistiques\nAudits réguliers → Monitoring des performances"
  },
  {
    "objectID": "slides/07-ethics-slides.html#limites-des-solutions-techniques",
    "href": "slides/07-ethics-slides.html#limites-des-solutions-techniques",
    "title": "Éthique",
    "section": "Limites des solutions techniques",
    "text": "Limites des solutions techniques\n\n⚠️ Les solutions techniques ne suffisent pas !\nNécessité de :\n\nRéflexion éthique continue\nTransparence sur les choix\nImplication des parties prenantes\nÉvaluation des impacts sociaux"
  },
  {
    "objectID": "slides/07-ethics-slides.html#bonnes-pratiques",
    "href": "slides/07-ethics-slides.html#bonnes-pratiques",
    "title": "Éthique",
    "section": "Bonnes pratiques",
    "text": "Bonnes pratiques\n\nDémarche éthique complète :\n\nDocumentation → Origines et limites des données\nTransparence → Méthodes et hypothèses\nÉvaluation différenciée → Performance par sous-groupe\nImplication → Communautés affectées\nRévision → Continue et critique"
  },
  {
    "objectID": "slides/07-ethics-slides.html#cadres-réglementaires",
    "href": "slides/07-ethics-slides.html#cadres-réglementaires",
    "title": "Éthique",
    "section": "Cadres réglementaires",
    "text": "Cadres réglementaires\n\nProtection juridique croissante :\n\nRGPD (UE) → Droit à l’explication\nAI Act (UE) → Cadre réglementaire et juridique commun pour l’IA\nLIAD (Canada) → Protection des consommateurs\nAlgorithmic Accountability Act (USA) → Audits obligatoires\nLégislations nationales en développement"
  },
  {
    "objectID": "slides/07-ethics-slides.html#responsabilité-du-data-scientist",
    "href": "slides/07-ethics-slides.html#responsabilité-du-data-scientist",
    "title": "Éthique",
    "section": "Responsabilité du data scientist",
    "text": "Responsabilité du data scientist\n\nRôle et obligations :\n\nVigilance → Identifier les biais potentiels\nTransparence → Communiquer les limites\nPrudence → Évaluer les conséquences\nCourage → Refuser les projets problématiques\nFormation → Continue sur l’éthique"
  },
  {
    "objectID": "slides/07-ethics-slides.html#questions-à-se-poser",
    "href": "slides/07-ethics-slides.html#questions-à-se-poser",
    "title": "Éthique",
    "section": "Questions à se poser",
    "text": "Questions à se poser\n\nAvant de déployer un modèle :\n\nQui bénéficie de ce modèle ? Qui peut en pâtir ?\nLes données sont-elles représentatives ?\nY a-t-il des groupes désavantagés ?\nLes erreurs ont-elles les mêmes conséquences pour tous ?\nExiste-t-il des alternatives moins risquées ?"
  },
  {
    "objectID": "slides/07-ethics-slides.html#conclusion",
    "href": "slides/07-ethics-slides.html#conclusion",
    "title": "Éthique",
    "section": "Conclusion",
    "text": "Conclusion\n\nLes données et modèles ne sont jamais neutres.\nBiais multiples à toutes les étapes du processus.\nSolutions techniques nécessaires mais insuffisantes.\nRéflexion éthique indispensable à chaque projet.\nResponsabilité collective → Data scientists, organisations, société\n\n\n→ L’éthique n’est pas une contrainte, c’est une condition de qualité !\nProchaine étape → Classification supervisée"
  },
  {
    "objectID": "slides/04-dimension-ca-slides.html#limite-de-lacp",
    "href": "slides/04-dimension-ca-slides.html#limite-de-lacp",
    "title": "Dimension",
    "section": "Limite de l’ACP",
    "text": "Limite de l’ACP\n\nL’analyse en composantes principales est restreinte à des données quantitatives.\n\n→ Comment faire lorsque l’on a des données qualitatives ?"
  },
  {
    "objectID": "slides/04-dimension-ca-slides.html#plan",
    "href": "slides/04-dimension-ca-slides.html#plan",
    "title": "Dimension",
    "section": "Plan",
    "text": "Plan\n\n\nLa théorie de l’analyse factorielle des correspondances\nL’AFC en pratique\nExemple : le taux d’emploi au Canada"
  },
  {
    "objectID": "slides/04-dimension-ca-slides.html#analyse-factorielle-des-correspondances",
    "href": "slides/04-dimension-ca-slides.html#analyse-factorielle-des-correspondances",
    "title": "Dimension",
    "section": "Analyse factorielle des correspondances",
    "text": "Analyse factorielle des correspondances\n\nL’Analyse Factorielle des Correspondances (AFC) est une méthode d’analyse exploratoire pour représenter graphiquement les relations entre les modalités de deux variables qualitatives.\n\nObjectif → Trouver une représentation en faible dimension tout en gardant le maximum d’information possible."
  },
  {
    "objectID": "slides/04-dimension-ca-slides.html#notation",
    "href": "slides/04-dimension-ca-slides.html#notation",
    "title": "Dimension",
    "section": "Notation",
    "text": "Notation\n\nTableau de contingence:\n\\[K = (k_{ij}) \\text{ avec } k_{ij} = \\text{ effectif classe } i \\in \\{1, \\dots, n\\}, \\text{ catégorie } j \\in \\{1, \\dots, p\\}.\\]"
  },
  {
    "objectID": "slides/04-dimension-ca-slides.html#notation-1",
    "href": "slides/04-dimension-ca-slides.html#notation-1",
    "title": "Dimension",
    "section": "Notation",
    "text": "Notation\n\nTableau de fréquences relatives :\n\\[F = (f_{ij}) \\text{ où } f_{ij} = \\frac{k_{ij}}{k_{\\bullet\\bullet}}.\\]"
  },
  {
    "objectID": "slides/04-dimension-ca-slides.html#notation-2",
    "href": "slides/04-dimension-ca-slides.html#notation-2",
    "title": "Dimension",
    "section": "Notation",
    "text": "Notation\n\nMarges :\n\nLigne : \\(f_{i\\bullet} = \\sum_{j=1}^p f_{ij}\\)\nColonne : \\(f_{\\bullet j} = \\sum_{i=1}^n f_{ij}\\)\nTotal : \\(f_{\\bullet\\bullet} = 1\\)"
  },
  {
    "objectID": "slides/04-dimension-ca-slides.html#exemple-concret",
    "href": "slides/04-dimension-ca-slides.html#exemple-concret",
    "title": "Dimension",
    "section": "Exemple concret",
    "text": "Exemple concret\n\n\n\n\n\n\n\n\n\n\n\n\nCollège\nUniversité Laval\nAutre université\nHors Québec\n\\(f_{i \\bullet}\\)\n\n\n\n\nActuariat\n0.08\n0\n0\n0.04\n0.12\n\n\nStatistique\n0.08\n0.16\n0.04\n0\n0.28\n\n\nBio-info\n0.16\n0.08\n0\n0.08\n0.32\n\n\nFinance\n0.08\n0\n0\n0\n0.08\n\n\nMaths\n0.04\n0\n0\n0\n0.04\n\n\nInfo\n0.08\n0.04\n0\n0.04\n0.16\n\n\n\\(f_{\\bullet j}\\)\n0.52\n0.28\n0.04\n0.16\n1"
  },
  {
    "objectID": "slides/04-dimension-ca-slides.html#indépendance-statistique",
    "href": "slides/04-dimension-ca-slides.html#indépendance-statistique",
    "title": "Dimension",
    "section": "Indépendance statistique",
    "text": "Indépendance statistique\n\nTest d’indépendance : Si les variables sont indépendantes, \\[f_{ij} \\approx f_{i\\bullet} f_{\\bullet j}\\]"
  },
  {
    "objectID": "slides/04-dimension-ca-slides.html#indépendance-statistique-1",
    "href": "slides/04-dimension-ca-slides.html#indépendance-statistique-1",
    "title": "Dimension",
    "section": "Indépendance statistique",
    "text": "Indépendance statistique\n\nTest du \\(\\chi^2\\) :\n\\[T = \\sum_{i,j} \\frac{(k_{ij} - \\mathbb{E}(k_{ij}))^2}{\\mathbb{E}(k_{ij})} = \\sum_{i,j} \\frac{(k_{ij} - \\frac{k_{i\\bullet}k_{\\bullet j}}{k_{\\bullet\\bullet}})^2}{\\frac{k_{i\\bullet}k_{\\bullet j}}{k_{\\bullet\\bullet}}}\\]\nSi \\(T \\approx 0\\) → indépendance\nSi \\(T\\) grand → dépendance (intéressant pour l’AFC)"
  },
  {
    "objectID": "slides/04-dimension-ca-slides.html#profils-lignes-et-profils-colonnes",
    "href": "slides/04-dimension-ca-slides.html#profils-lignes-et-profils-colonnes",
    "title": "Dimension",
    "section": "Profils-lignes et profils-colonnes",
    "text": "Profils-lignes et profils-colonnes\nProfil-ligne \\(i\\) : Répartition de la modalité \\(i\\) parmi les colonnes \\[L_i = \\left(\\frac{f_{i1}}{f_{i\\bullet}}, \\ldots, \\frac{f_{ip}}{f_{i\\bullet}}\\right).\\]\n\nProfil-colonne \\(j\\) : Répartition de la modalité \\(j\\) parmi les lignes \\[C_j = \\left(\\frac{f_{1j}}{f_{\\bullet j}}, \\ldots, \\frac{f_{nj}}{f_{\\bullet j}}\\right).\\]\n\n\nLes profils sont des distributions conditionnelles."
  },
  {
    "objectID": "slides/04-dimension-ca-slides.html#profils-moyens",
    "href": "slides/04-dimension-ca-slides.html#profils-moyens",
    "title": "Dimension",
    "section": "Profils moyens",
    "text": "Profils moyens\n\nProfil-ligne moyen = fréquences marginales colonnes : \\[\\text{Profil moyen} = (f_{\\bullet 1}, \\ldots, f_{\\bullet p})\\]\nProfil-colonne moyen = fréquences marginales lignes : \\[\\text{Profil moyen} = (f_{1\\bullet}, \\ldots, f_{n\\bullet})\\]\nIndépendance \\(\\Longleftrightarrow\\) Tous les profils \\(\\approx\\) profils moyens."
  },
  {
    "objectID": "slides/04-dimension-ca-slides.html#distance-du-chi2",
    "href": "slides/04-dimension-ca-slides.html#distance-du-chi2",
    "title": "Dimension",
    "section": "Distance du \\(\\chi^2\\)",
    "text": "Distance du \\(\\chi^2\\)\nDistance entre profils-lignes : \\[d^2(L_i, L_{i'}) = \\sum_{j=1}^p \\frac{1}{f_{\\bullet j}} \\left(\\frac{f_{ij}}{f_{i\\bullet}} - \\frac{f_{i'j}}{f_{i'\\bullet}}\\right)^2\\]\nDistance entre profils-colonnes : \\[d^2(C_j, C_{j'}) = \\sum_{i=1}^n \\frac{1}{f_{i\\bullet}} \\left(\\frac{f_{ij}}{f_{\\bullet j}} - \\frac{f_{ij'}}{f_{\\bullet j'}}\\right)^2\\]"
  },
  {
    "objectID": "slides/04-dimension-ca-slides.html#formulation-matricielle",
    "href": "slides/04-dimension-ca-slides.html#formulation-matricielle",
    "title": "Dimension",
    "section": "Formulation matricielle",
    "text": "Formulation matricielle\nMatrices de poids :\n\n\\(D_n = \\text{diag}(f_{i\\bullet})\\) → poids des lignes\n\\(D_p = \\text{diag}(f_{\\bullet j})\\) → poids des colonnes\n\n\nDistance du \\(\\chi^2\\) matricielle :\n\nProfils-lignes : \\(d^2(L_i, L_{i'}) = (L_i - L_{i'})^\\top D_p^{-1} (L_i - L_{i'})\\)\nProfils-colonnes : \\(d^2(C_j, C_{j'}) = (C_j - C_{j'})^\\top D_n^{-1} (C_j - C_{j'})\\)"
  },
  {
    "objectID": "slides/04-dimension-ca-slides.html#analyse-directe-profils-lignes",
    "href": "slides/04-dimension-ca-slides.html#analyse-directe-profils-lignes",
    "title": "Dimension",
    "section": "Analyse directe (profils-lignes)",
    "text": "Analyse directe (profils-lignes)\nObjectif : Maximiser la variance projetée des profils-lignes\n\nProblème d’optimisation : \\[\\max_u u^\\top D_p^{-1} F^\\top D_n^{-1} F D_p^{-1} u \\quad \\text{s.c.} \\quad u^\\top D_p^{-1} u = 1\\]\n\n\nSolution : Vecteurs propres de la matrice \\[S = F^\\top D_n^{-1} F D_p^{-1}\\]\n\n\nPremier axe factoriel : \\(S u_1 = \\lambda_1 u_1\\)"
  },
  {
    "objectID": "slides/04-dimension-ca-slides.html#analyse-duale-profils-colonnes",
    "href": "slides/04-dimension-ca-slides.html#analyse-duale-profils-colonnes",
    "title": "Dimension",
    "section": "Analyse duale (profils-colonnes)",
    "text": "Analyse duale (profils-colonnes)\nObjectif : Maximiser la variance projetée des profils-colonnes\n\nProblème d’optimisation similaire avec matrice : \\[T = F D_p^{-1} F^\\top D_n^{-1}\\]\n\n\nPremier axe factoriel : \\(T v_1 = \\mu_1 v_1\\)"
  },
  {
    "objectID": "slides/04-dimension-ca-slides.html#propriété",
    "href": "slides/04-dimension-ca-slides.html#propriété",
    "title": "Dimension",
    "section": "Propriété",
    "text": "Propriété\n\n\n\n\n\n\nPropriété\n\n\n\\(S\\) et \\(T\\) ont les mêmes \\(r = \\min(n-1, p-1)\\) valeurs propres positives !"
  },
  {
    "objectID": "slides/04-dimension-ca-slides.html#relations-entre-analyses",
    "href": "slides/04-dimension-ca-slides.html#relations-entre-analyses",
    "title": "Dimension",
    "section": "Relations entre analyses",
    "text": "Relations entre analyses\nCohérence des représentations : \\(\\lambda_k = \\mu_k\\) pour \\(k = 1, \\ldots, r\\)\nRelations entre vecteurs propres : \\[u_k = \\frac{1}{\\sqrt{\\lambda_k}} F^\\top D_n^{-1} v_k.\\] \\[v_k = \\frac{1}{\\sqrt{\\lambda_k}} F D_p^{-1} u_k.\\]\nCette propriété garantit une représentation cohérente dans le même espace réduit."
  },
  {
    "objectID": "slides/04-dimension-ca-slides.html#afc-comme-double-acp",
    "href": "slides/04-dimension-ca-slides.html#afc-comme-double-acp",
    "title": "Dimension",
    "section": "AFC comme double ACP",
    "text": "AFC comme double ACP\n\n\n\n\n\n\nRemarque\n\n\nL’AFC peut être vue comme une double ACP pondérée :\n\nACP des profils-lignes dans \\(\\mathbb{R}^p\\),\nACP des profils-colonnes dans \\(\\mathbb{R}^n\\),\nAvec métriques du \\(\\chi^2\\) adaptées à chaque espace."
  },
  {
    "objectID": "slides/04-dimension-ca-slides.html#coordonnées-factorielles",
    "href": "slides/04-dimension-ca-slides.html#coordonnées-factorielles",
    "title": "Dimension",
    "section": "Coordonnées factorielles",
    "text": "Coordonnées factorielles\nCoordonnées des profils-lignes sur l’axe \\(k\\) :\n\\[\\Phi_k = D_n^{-1} F D_p^{-1} u_k.\\]\nCoordonnées des profils-colonnes sur l’axe \\(k\\) :\n\\[\\Psi_k = D_p^{-1} F^\\top D_n^{-1} v_k.\\]\nRelations entre coordonnées : \\[\\Phi_k = \\frac{1}{\\sqrt{\\lambda_k}} D_n^{-1} F \\Psi_k.\\]"
  },
  {
    "objectID": "slides/04-dimension-ca-slides.html#centrage-et-centre-de-gravité",
    "href": "slides/04-dimension-ca-slides.html#centrage-et-centre-de-gravité",
    "title": "Dimension",
    "section": "Centrage et centre de gravité",
    "text": "Centrage et centre de gravité\n\nConvention : Représentation centrée en \\((0,0)\\) dans les logiciels.\n\nCentres de gravité :\n\nLignes : \\(G_L = (f_{\\bullet 1}, \\ldots, f_{\\bullet p})^\\top\\).\nColonnes : \\(G_C = (f_{1\\bullet}, \\ldots, f_{n\\bullet})^\\top\\).\n\n\n\nCentrage : Soustraire les profils moyens\n\\[\\frac{f_{ij}}{f_{i\\bullet}} - f_{\\bullet j} = \\frac{f_{ij} - f_{i\\bullet}f_{\\bullet j}}{f_{i\\bullet}}.\\]"
  },
  {
    "objectID": "slides/04-dimension-ca-slides.html#afc-sur-matrice-centrée",
    "href": "slides/04-dimension-ca-slides.html#afc-sur-matrice-centrée",
    "title": "Dimension",
    "section": "AFC sur matrice centrée",
    "text": "AFC sur matrice centrée\n\nMatrice centrée : \\(S^* = (s_{jj'}^*)\\) avec\n\\[s_{jj'}^* = \\sum_{i=1}^n \\frac{(f_{ij} - f_{i\\bullet}f_{\\bullet j})(f_{ij'} - f_{i\\bullet}f_{\\bullet j'})}{f_{i\\bullet}f_{\\bullet j'}}.\\]"
  },
  {
    "objectID": "slides/04-dimension-ca-slides.html#inertie-totale",
    "href": "slides/04-dimension-ca-slides.html#inertie-totale",
    "title": "Dimension",
    "section": "Inertie totale",
    "text": "Inertie totale\n\nInertie totale :\n\\[\\text{tr}(S^*) = \\sum_{j=1}^p \\sum_{i=1}^n \\frac{(f_{ij} - f_{i\\bullet}f_{\\bullet j})^2}{f_{i\\bullet}f_{\\bullet j}}\\]\nCette inertie correspond à la statistique du \\(\\chi^2\\) normalisée."
  },
  {
    "objectID": "slides/04-dimension-ca-slides.html#représentation-barycentrique",
    "href": "slides/04-dimension-ca-slides.html#représentation-barycentrique",
    "title": "Dimension",
    "section": "Représentation barycentrique",
    "text": "Représentation barycentrique\nSur chaque axe factoriel,\n\\[[\\Phi_k]_i = \\frac{1}{\\sqrt{\\lambda_k}} \\sum_{j=1}^p \\frac{f_{ij}}{f_{i\\bullet}} [\\Psi_k]_j,\\]\n\\[[\\Psi_k]_j = \\frac{1}{\\sqrt{\\lambda_k}} \\sum_{i=1}^n \\frac{f_{ij}}{f_{\\bullet j}} [\\Phi_k]_i.\\]\nInterprétation : Chaque profil-ligne est au barycentre des profils-colonnes (et vice versa)."
  },
  {
    "objectID": "slides/04-dimension-ca-slides.html#double-représentation-barycentrique",
    "href": "slides/04-dimension-ca-slides.html#double-représentation-barycentrique",
    "title": "Dimension",
    "section": "Double représentation barycentrique",
    "text": "Double représentation barycentrique\n\n\n\n\n\n\nPropriété fondamentale de l’AFC :\n\n\nSur les axes factoriels, chaque point d’un nuage est au barycentre des points de l’autre nuage, pondéré par les contributions respectives.\n\n\n\nCette propriété permet une interprétation conjointe des modalités des deux variables dans le même graphique."
  },
  {
    "objectID": "slides/04-dimension-ca-slides.html#interprétation-des-résultats",
    "href": "slides/04-dimension-ca-slides.html#interprétation-des-résultats",
    "title": "Dimension",
    "section": "Interprétation des résultats",
    "text": "Interprétation des résultats\n\nProximité géométrique = Similarité des profils\nAxes factoriels : Directions de variation maximale.\nContribution des modalités : Importance dans la construction des axes.\nQualité de représentation : Proportion de l’inertie d’un point expliquée par les axes retenus."
  },
  {
    "objectID": "slides/04-dimension-ca-slides.html#exemple-le-taux-demploi-au-canada",
    "href": "slides/04-dimension-ca-slides.html#exemple-le-taux-demploi-au-canada",
    "title": "Dimension",
    "section": "Exemple : le taux d’emploi au Canada",
    "text": "Exemple : le taux d’emploi au Canada\nOn considère le jeu de données suivant :\n\n\n\n\n\nprovince\nfull-time\npart-time\nunemployed\n\n\n\n\nNewfoundland and Labrador\n205.0\n39.3\n27.3\n\n\nPrince Edward Island\n82.1\n11.9\n7.6\n\n\nNova Scotia\n437.7\n87.2\n34.0\n\n\nNew Brunswick\n346.8\n54.8\n30.0\n\n\nQuebec\n3762.6\n864.7\n279.6\n\n\nOntario\n6721.8\n1508.0\n667.2\n\n\nManitoba\n608.4\n122.6\n46.3\n\n\nSaskatchewan\n498.1\n115.4\n31.7\n\n\nAlberta\n2084.6\n466.2\n195.0\n\n\nBritish Columbia\n2357.1\n587.8\n190.1"
  },
  {
    "objectID": "slides/04-dimension-ca-slides.html#exemple-le-taux-demploi-au-canada-1",
    "href": "slides/04-dimension-ca-slides.html#exemple-le-taux-demploi-au-canada-1",
    "title": "Dimension",
    "section": "Exemple : le taux d’emploi au Canada",
    "text": "Exemple : le taux d’emploi au Canada\n\nCodePlotValeurs propres\n\n\n\nlibrary(FactoMineR)\nlibrary(factoextra)\nres_ca &lt;- df |&gt;\n  column_to_rownames(var = 'province') |&gt; \n  CA(graph = FALSE)\nfviz_eig(res_ca)\nget_eigenvalue(res_ca)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\neigenvalue\nvariance.percent\ncumulative.variance.percent\n\n\n\n\nDim.1\n0.0014335\n70.77104\n70.77104\n\n\nDim.2\n0.0005921\n29.22896\n100.00000"
  },
  {
    "objectID": "slides/04-dimension-ca-slides.html#exemple-le-taux-demploi-au-canada-2",
    "href": "slides/04-dimension-ca-slides.html#exemple-le-taux-demploi-au-canada-2",
    "title": "Dimension",
    "section": "Exemple : le taux d’emploi au Canada",
    "text": "Exemple : le taux d’emploi au Canada\n\nCodePlot\n\n\n\n# Résultat pour les lignes\nfviz_ca_row(res_ca, repel = TRUE)"
  },
  {
    "objectID": "slides/04-dimension-ca-slides.html#exemple-le-taux-demploi-au-canada-3",
    "href": "slides/04-dimension-ca-slides.html#exemple-le-taux-demploi-au-canada-3",
    "title": "Dimension",
    "section": "Exemple : le taux d’emploi au Canada",
    "text": "Exemple : le taux d’emploi au Canada\n\nCodeContribution à la première composante\n\n\n\n# Visualisation des lignes sur l'axe 1\nfviz_contrib(res_ca, choice =\"row\", axes = 1)"
  },
  {
    "objectID": "slides/04-dimension-ca-slides.html#exemple-le-taux-demploi-au-canada-4",
    "href": "slides/04-dimension-ca-slides.html#exemple-le-taux-demploi-au-canada-4",
    "title": "Dimension",
    "section": "Exemple : le taux d’emploi au Canada",
    "text": "Exemple : le taux d’emploi au Canada\n\nCodeContribution à la première composante\n\n\n\n# Visualisation des lignes sur l'axe 2\nfviz_contrib(res_ca, choice =\"row\", axes = 2)"
  },
  {
    "objectID": "slides/04-dimension-ca-slides.html#exemple-le-taux-demploi-au-canada-5",
    "href": "slides/04-dimension-ca-slides.html#exemple-le-taux-demploi-au-canada-5",
    "title": "Dimension",
    "section": "Exemple : le taux d’emploi au Canada",
    "text": "Exemple : le taux d’emploi au Canada\n\nCodeBiplot\n\n\n\nfviz_ca_biplot(res_ca, repel = TRUE)"
  },
  {
    "objectID": "slides/04-dimension-ca-slides.html#conclusion",
    "href": "slides/04-dimension-ca-slides.html#conclusion",
    "title": "Dimension",
    "section": "Conclusion",
    "text": "Conclusion\n\nL’AFC est un outil pour :\n\nExplorer les relations entre variables qualitatives\nVisualiser des structures complexes en 2D/3D\nIdentifier des associations non évidentes\nInterpréter conjointement lignes et colonnes\n\nProchaine étape → Que faire si on a plus que deux variables qualitatives ?"
  },
  {
    "objectID": "slides/04-dimension-mca-slides.html#introduction-à-lacm",
    "href": "slides/04-dimension-mca-slides.html#introduction-à-lacm",
    "title": "Dimension",
    "section": "Introduction à l’ACM",
    "text": "Introduction à l’ACM\n\nL’Analyse des Correspondances Multiples (ACM) est un prolongement de l’AFC pour analyser plus de deux variables qualitatives.\nPrincipe fondamental : Transformation préalable par codage disjonctif complet."
  },
  {
    "objectID": "slides/04-dimension-mca-slides.html#plan",
    "href": "slides/04-dimension-mca-slides.html#plan",
    "title": "Dimension",
    "section": "Plan",
    "text": "Plan\n\n\nLa théorie de l’analyse des correspondances multiples\nL’ACM en pratique\nExemple :"
  },
  {
    "objectID": "slides/04-dimension-mca-slides.html#notation-de-base",
    "href": "slides/04-dimension-mca-slides.html#notation-de-base",
    "title": "Dimension",
    "section": "Notation de base",
    "text": "Notation de base\n\n\n\\(n\\) : nombre d’individus (observations)\n\\(Q\\) : nombre de variables (questions)\n\\(J_q\\) : nombre de modalités de la variable \\(q\\)\n\\(J\\) : nombre total de modalités (\\(J = \\sum_{q=1}^Q J_q\\))\n\nObjectif : Représenter graphiquement les relations entre toutes ces modalités."
  },
  {
    "objectID": "slides/04-dimension-mca-slides.html#codage-disjonctif-complet",
    "href": "slides/04-dimension-mca-slides.html#codage-disjonctif-complet",
    "title": "Dimension",
    "section": "Codage disjonctif complet",
    "text": "Codage disjonctif complet\n\n\n\n\n\n\nDéfinition\n\n\nTableau binaire \\(Z\\) (\\(n \\times J\\)) contenant uniquement des 0 et des 1.\n\n\n\n\n\nChaque variable → ses modalités\nIndividu possède modalité → code 1\nIndividu ne possède pas modalité → code 0\nDisjonctif : au plus une modalité par variable\nComplet : au moins une modalité par variable"
  },
  {
    "objectID": "slides/04-dimension-mca-slides.html#exemple-concret---données",
    "href": "slides/04-dimension-mca-slides.html#exemple-concret---données",
    "title": "Dimension",
    "section": "Exemple concret - Données",
    "text": "Exemple concret - Données\n\nProduits avec Type et Prix:\n\n\n\nProduit\nType\nPrix ($)\n\n\n\n\nNike\nHoodie\n256.72\n\n\nPuma\nJoggers\n221.26\n\n\nOff-W\nHoodie\n198.45\n\n\nSupreme\nHoodie\n235.50\n\n\n\nVariable Prix → 3 classes : &lt;200$, [200$-250$], &gt;250$"
  },
  {
    "objectID": "slides/04-dimension-mca-slides.html#exemple---codage-disjonctif-complet",
    "href": "slides/04-dimension-mca-slides.html#exemple---codage-disjonctif-complet",
    "title": "Dimension",
    "section": "Exemple - Codage disjonctif complet",
    "text": "Exemple - Codage disjonctif complet\n\nTableau transformé \\(Z\\) :\n\n\n\n\n\n\n\n\n\n\n\n\nProduit\nHoodie\nJoggers\nSneakers\n&lt;200$\n[200$-250$]\n&gt;250$\n\n\n\n\nNike\n1\n0\n0\n0\n0\n1\n\n\nPuma\n0\n1\n0\n0\n1\n0\n\n\nOff-W\n1\n0\n0\n1\n0\n0\n\n\nSupreme\n1\n0\n0\n0\n1\n0\n\n\n\nChaque ligne somme à \\(Q = 2\\) (2 variables)."
  },
  {
    "objectID": "slides/04-dimension-mca-slides.html#propriétés-du-tableau-z",
    "href": "slides/04-dimension-mca-slides.html#propriétés-du-tableau-z",
    "title": "Dimension",
    "section": "Propriétés du tableau Z",
    "text": "Propriétés du tableau Z\n\n\n\n\n\n\n\nPropriétés\n\n\n\nSomme par ligne : \\(\\sum_{j=1}^J z_{ij} = Q\\) (constante)\nSomme totale : \\(\\sum_{i,j} z_{ij} = nQ\\)\nSomme par colonne : \\(\\sum_{i=1}^n z_{ij} = n_j\\) (effectif modalité \\(j\\))"
  },
  {
    "objectID": "slides/04-dimension-mca-slides.html#perte-dinformation",
    "href": "slides/04-dimension-mca-slides.html#perte-dinformation",
    "title": "Dimension",
    "section": "Perte d’information",
    "text": "Perte d’information\n\nVariables quantitatives → Classes : Perte d’information inévitable.\n\nValeur précise → Appartenance à une classe\nExemple : 256.72$ → &gt;250$\nCompromis : Granularité vs interprétabilité\n\nEnjeu : Choisir des classes pertinentes pour le problème étudié."
  },
  {
    "objectID": "slides/04-dimension-mca-slides.html#tableau-de-burt",
    "href": "slides/04-dimension-mca-slides.html#tableau-de-burt",
    "title": "Dimension",
    "section": "Tableau de Burt",
    "text": "Tableau de Burt\n\n\n\n\n\n\n\nDéfinition\n\n\n\\(B = Z^\\top Z\\) (matrice \\(J \\times J\\))"
  },
  {
    "objectID": "slides/04-dimension-mca-slides.html#propriétés-du-tableau-de-burt",
    "href": "slides/04-dimension-mca-slides.html#propriétés-du-tableau-de-burt",
    "title": "Dimension",
    "section": "Propriétés du tableau de Burt",
    "text": "Propriétés du tableau de Burt\n\nTaille : \\(J \\times J\\) (nombre total de modalités)\n\nBlocs diagonaux : Matrices diagonales\n→ Éléments = effectifs de chaque modalité\n\n\nBlocs non-diagonaux : Tableaux de contingence\n→ Croisement entre variables \\(q\\) et \\(q'\\)\n\n\nSymétrie : \\(B = B^\\top\\)"
  },
  {
    "objectID": "slides/04-dimension-mca-slides.html#équivalence-mathématique",
    "href": "slides/04-dimension-mca-slides.html#équivalence-mathématique",
    "title": "Dimension",
    "section": "Équivalence mathématique",
    "text": "Équivalence mathématique\n\nL’ACM peut s’effectuer sur :\n\nLe tableau disjonctif complet \\(Z\\)\nLe tableau de Burt \\(B = Z^\\top Z\\)\n\nRésultat → Les deux approches donnent les mêmes facteurs!\nCette équivalence offre une flexibilité computationnelle."
  },
  {
    "objectID": "slides/04-dimension-mca-slides.html#éléments-propres-de-z",
    "href": "slides/04-dimension-mca-slides.html#éléments-propres-de-z",
    "title": "Dimension",
    "section": "Éléments propres de Z",
    "text": "Éléments propres de Z\nAnalyse directe - Vecteurs propres de :\n\\[S = \\frac{1}{Q} Z^\\top Z D_J^{-1}\\] où \\(D_J = \\text{diag}(n_1, \\ldots, n_J)\\).\nCoordonnées profils-lignes : \\[\\Phi_k = n Z D_J^{-1} u_k\\]"
  },
  {
    "objectID": "slides/04-dimension-mca-slides.html#éléments-propres-de-z-1",
    "href": "slides/04-dimension-mca-slides.html#éléments-propres-de-z-1",
    "title": "Dimension",
    "section": "Éléments propres de Z",
    "text": "Éléments propres de Z\nAnalyse duale - Vecteurs propres de : \\[T = \\frac{1}{Q} Z D_J^{-1} Z^\\top\\] où \\(D_J = \\text{diag}(n_1, \\ldots, n_J)\\).\nCoordonnées profils-colonnes : \\[\\Psi_k = n D_J^{-1} Z^\\top v_k\\]"
  },
  {
    "objectID": "slides/04-dimension-mca-slides.html#éléments-propres-de-b",
    "href": "slides/04-dimension-mca-slides.html#éléments-propres-de-b",
    "title": "Dimension",
    "section": "Éléments propres de B",
    "text": "Éléments propres de B\nTableau de Burt symétrique → Analyse directe = duale\nVecteurs propres de : \\[S' = \\frac{1}{Q^2} B^\\top D_J^{-1} B D_J^{-1}\\]\n\nOn peut réécrire \\(S'\\) comme : \\[S' = \\frac{1}{Q^2} Z^\\top Z D_J^{-1} Z^\\top Z D_J^{-1}\\]"
  },
  {
    "objectID": "slides/04-dimension-mca-slides.html#relation-entre-valeurs-propres",
    "href": "slides/04-dimension-mca-slides.html#relation-entre-valeurs-propres",
    "title": "Dimension",
    "section": "Relation entre valeurs propres",
    "text": "Relation entre valeurs propres\n\nSi \\(\\lambda\\) est valeur propre de \\(S\\) (analyse de \\(Z\\)), alors \\(\\lambda^2\\) est valeur propre de \\(S'\\) (analyse de \\(B\\)).\nConséquence :\n\nMêmes vecteurs propres pour \\(Z\\) et \\(B\\).\nValeurs propres de \\(B\\) = carrés des valeurs propres de \\(Z\\)."
  },
  {
    "objectID": "slides/04-dimension-mca-slides.html#encodage-des-variables-quantitatives",
    "href": "slides/04-dimension-mca-slides.html#encodage-des-variables-quantitatives",
    "title": "Dimension",
    "section": "Encodage des variables quantitatives",
    "text": "Encodage des variables quantitatives\nChoix des bornes crucial pour la qualité de l’analyse.\n\nApproches recommandées :\n\nAnalyser la distribution (histogrammes)\nBornes pertinentes au domaine d’étude\nÉviter les classes peu informatives\n\n\n\nApproche déconseillée :\n\nEffectifs égaux automatiques\nClasses sans sens métier"
  },
  {
    "objectID": "slides/04-dimension-mca-slides.html#encodage-des-variables-qualitatives",
    "href": "slides/04-dimension-mca-slides.html#encodage-des-variables-qualitatives",
    "title": "Dimension",
    "section": "Encodage des variables qualitatives",
    "text": "Encodage des variables qualitatives\nModalités “naturelles” mais problèmes potentiels.\n\nEffectifs déséquilibrés :\n\nModalités très rares vs très fréquentes\nImpact sur la représentation\n\n\n\nSolutions potentielles :\n\nRegroupements pertinents (connaissance du domaine)\nÉviter la répartition aléatoire\nPréserver le sens des modalités"
  },
  {
    "objectID": "slides/04-dimension-mca-slides.html#exemple-le-succès-des-étudiants",
    "href": "slides/04-dimension-mca-slides.html#exemple-le-succès-des-étudiants",
    "title": "Dimension",
    "section": "Exemple : le succès des étudiants",
    "text": "Exemple : le succès des étudiants\nOn considère le jeu de données suivant :\n\n\n\n\n\n\n\n\n\n\n\n\n\nHours_Studied\nAttendance\nParental_Involvement\nExtracurricular_Activities\nSleep_Hours\nSchool_Type\n\n\n\n\n21-30\n71-90\nLow\nNo\n&lt;=7\nPublic\n\n\n11-20\n0-70\nLow\nNo\n&gt;7\nPublic\n\n\n21-30\n91-100\nMedium\nYes\n&lt;=7\nPublic\n\n\n21-30\n71-90\nLow\nYes\n&gt;7\nPublic\n\n\n11-20\n91-100\nMedium\nYes\n&lt;=7\nPublic\n\n\n11-20\n71-90\nMedium\nYes\n&gt;7\nPublic"
  },
  {
    "objectID": "slides/04-dimension-mca-slides.html#exemple-le-succès-des-étudiants-1",
    "href": "slides/04-dimension-mca-slides.html#exemple-le-succès-des-étudiants-1",
    "title": "Dimension",
    "section": "Exemple : le succès des étudiants",
    "text": "Exemple : le succès des étudiants\n\nCodePlotValeurs propres\n\n\n\nlibrary(FactoMineR)\nlibrary(factoextra)\nres_mca &lt;- df |&gt;\n  MCA(graph = FALSE)\nfviz_eig(res_mca)\nget_eigenvalue(res_mca)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\neigenvalue\nvariance.percent\ncumulative.variance.percent\n\n\n\n\nDim.1\n0.1984565\n11.907391\n11.90739\n\n\nDim.2\n0.1898757\n11.392541\n23.29993\n\n\nDim.3\n0.1814721\n10.888326\n34.18826\n\n\nDim.4\n0.1773147\n10.638882\n44.82714\n\n\nDim.5\n0.1672860\n10.037161\n54.86430\n\n\nDim.6\n0.1651723\n9.910338\n64.77464\n\n\nDim.7\n0.1561348\n9.368089\n74.14273\n\n\nDim.8\n0.1500984\n9.005905\n83.14863\n\n\nDim.9\n0.1415169\n8.491017\n91.63965\n\n\nDim.10\n0.1393392\n8.360350\n100.00000"
  },
  {
    "objectID": "slides/04-dimension-mca-slides.html#exemple-le-succès-des-étudiants-2",
    "href": "slides/04-dimension-mca-slides.html#exemple-le-succès-des-étudiants-2",
    "title": "Dimension",
    "section": "Exemple : le succès des étudiants",
    "text": "Exemple : le succès des étudiants\n\nCodePlot\n\n\n\nfviz_mca_biplot(\n  res_mca, \n  repel = TRUE,\n  label = \"none\",\n  ggtheme = theme_minimal()\n)"
  },
  {
    "objectID": "slides/04-dimension-mca-slides.html#exemple-le-succès-des-étudiants-3",
    "href": "slides/04-dimension-mca-slides.html#exemple-le-succès-des-étudiants-3",
    "title": "Dimension",
    "section": "Exemple : le succès des étudiants",
    "text": "Exemple : le succès des étudiants\n\nCodePlot\n\n\n\nfviz_mca_var(\n  res_mca,\n  choice = \"mca.cor\", \n  repel = TRUE,\n  ggtheme = theme_minimal()\n)"
  },
  {
    "objectID": "slides/04-dimension-mca-slides.html#exemple-le-succès-des-étudiants-4",
    "href": "slides/04-dimension-mca-slides.html#exemple-le-succès-des-étudiants-4",
    "title": "Dimension",
    "section": "Exemple : le succès des étudiants",
    "text": "Exemple : le succès des étudiants\n\nCodePlot\n\n\n\nfviz_mca_var(\n  res_mca,\n  repel = TRUE,\n  ggtheme = theme_minimal()\n)"
  },
  {
    "objectID": "slides/04-dimension-mca-slides.html#exemple-le-succès-des-étudiants-5",
    "href": "slides/04-dimension-mca-slides.html#exemple-le-succès-des-étudiants-5",
    "title": "Dimension",
    "section": "Exemple : le succès des étudiants",
    "text": "Exemple : le succès des étudiants\n\nCodePlot\n\n\n\nfviz_mca_var(\n  res_mca,\n  col.var = \"cos2\",\n  gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"), \n  repel = TRUE,\n  ggtheme = theme_minimal()\n)"
  },
  {
    "objectID": "slides/04-dimension-mca-slides.html#exemple-le-succès-des-étudiants-6",
    "href": "slides/04-dimension-mca-slides.html#exemple-le-succès-des-étudiants-6",
    "title": "Dimension",
    "section": "Exemple : le succès des étudiants",
    "text": "Exemple : le succès des étudiants\n\nCodePlot\n\n\n\nfviz_mca_ind(\n  res_mca,\n  label = \"none\",\n  col.ind = \"cos2\", \n  gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n  repel = TRUE,\n  ggtheme = theme_minimal()\n)"
  },
  {
    "objectID": "slides/04-dimension-mca-slides.html#exemple-le-succès-des-étudiants-7",
    "href": "slides/04-dimension-mca-slides.html#exemple-le-succès-des-étudiants-7",
    "title": "Dimension",
    "section": "Exemple : le succès des étudiants",
    "text": "Exemple : le succès des étudiants\n\nCodePlotPlot\n\n\n\nfviz_mca_ind(\n  res_mca, \n  label = \"none\",\n  habillage = \"Hours_Studied\",\n  palette = c(\"#D81B60\", \"#1E88E5\", \"#FFC107\", \"#004D40\"),\n  addEllipses = TRUE, ellipse.type = \"confidence\",\n  ggtheme = theme_minimal()\n)"
  },
  {
    "objectID": "slides/04-dimension-mca-slides.html#avantages-de-lacm",
    "href": "slides/04-dimension-mca-slides.html#avantages-de-lacm",
    "title": "Dimension",
    "section": "Avantages de l’ACM",
    "text": "Avantages de l’ACM\n\n\nVisualisation de relations complexes multi-variables\nTraitement unifié de variables hétérogènes\nRéduction de dimension préservant les associations\nInterprétation intuitive des proximités\nFlexibilité dans l’encodage des variables"
  },
  {
    "objectID": "slides/04-dimension-mca-slides.html#limites-et-précautions",
    "href": "slides/04-dimension-mca-slides.html#limites-et-précautions",
    "title": "Dimension",
    "section": "Limites et précautions",
    "text": "Limites et précautions\n\nPerte d’information :\n\nVariables continues → classes\nChoix d’encodage critique\n\nInterprétation :\n\nProximités entre modalités de variables différentes\nAttention aux artefacts d’encodage"
  },
  {
    "objectID": "slides/04-dimension-mca-slides.html#conclusion",
    "href": "slides/04-dimension-mca-slides.html#conclusion",
    "title": "Dimension",
    "section": "Conclusion",
    "text": "Conclusion\n\nL’ACM étend l’AFC au cas multi-variables grâce au codage disjonctif complet :\n\nTransformation binaire préalable\nÉquivalence \\(Z\\) ↔︎ Tableau de Burt \\(B\\)\nChoix d’encodage\nVisualisation des patterns complexes\n\nProchaine étape → éthique de l’analyse de données"
  },
  {
    "objectID": "slides/02-revisions-algebre-slides-annotated.html#plan-de-la-présentation",
    "href": "slides/02-revisions-algebre-slides-annotated.html#plan-de-la-présentation",
    "title": "Révisions",
    "section": "Plan de la présentation",
    "text": "Plan de la présentation\n\n\nPropriétés matricielles\nValeurs et vecteurs propres\nDiagonalisation de matrices"
  },
  {
    "objectID": "slides/02-revisions-algebre-slides-annotated.html#notations-de-base",
    "href": "slides/02-revisions-algebre-slides-annotated.html#notations-de-base",
    "title": "Révisions",
    "section": "Notations de base",
    "text": "Notations de base\n\n\n\\(M_{n, m}(\\mathbb{R})\\) : matrices à \\(n\\) lignes et \\(m\\) colonnes\n\\(M_{n}(\\mathbb{R})\\) : matrices carrées de taille \\(n\\)\n\\(I_n\\) : matrice identité de taille \\(n\\)\n\\(u, v \\in \\mathbb{R}^n\\) : vecteurs colonnes"
  },
  {
    "objectID": "slides/02-revisions-algebre-slides-annotated.html#propriétés-du-déterminant",
    "href": "slides/02-revisions-algebre-slides-annotated.html#propriétés-du-déterminant",
    "title": "Révisions",
    "section": "Propriétés du déterminant",
    "text": "Propriétés du déterminant\n\n\n\n\nPropriétés\n\n\n\n\\(\\det(A^\\top) = \\det(A)\\)\n\\(\\det(AB) = \\det(A)\\det(B)\\)\n\\(\\det(A^{-1}) = \\frac{1}{\\det(A)}\\)"
  },
  {
    "objectID": "slides/02-revisions-algebre-slides-annotated.html#propriétés-du-déterminant-1",
    "href": "slides/02-revisions-algebre-slides-annotated.html#propriétés-du-déterminant-1",
    "title": "Révisions",
    "section": "Propriétés du déterminant",
    "text": "Propriétés du déterminant\n\nCas particulier où \\(A \\in M_{2}(\\mathbb{R})\\).\nSoit \\(A = \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}\\). Alors\n\\[\\det(A) = ad - bc.\\]"
  },
  {
    "objectID": "slides/02-revisions-algebre-slides-annotated.html#propriété-du-déterminant-exemple-wooclap-kffpdj",
    "href": "slides/02-revisions-algebre-slides-annotated.html#propriété-du-déterminant-exemple-wooclap-kffpdj",
    "title": "Révisions",
    "section": "Propriété du déterminant : exemple (Wooclap: KFFPDJ)",
    "text": "Propriété du déterminant : exemple (Wooclap: KFFPDJ)\n\n\\[A = \\begin{pmatrix} 1 & 2 \\\\ 3 & 4 \\end{pmatrix}, \\quad B = \\begin{pmatrix} 2 & 0 \\\\ 1 & 1 \\end{pmatrix}\\]\n\nOn trouve que \\(\\det(A) = 4 - 6 = -2\\), \\(\\det(B) = 2 - 0 = 2\\). Donc\n\\[\\det(AB) = \\det(A) \\det(B) = -4\\]"
  },
  {
    "objectID": "slides/02-revisions-algebre-slides-annotated.html#inverse-de-matrices",
    "href": "slides/02-revisions-algebre-slides-annotated.html#inverse-de-matrices",
    "title": "Révisions",
    "section": "Inverse de matrices",
    "text": "Inverse de matrices\n\n\n\n\nPropriété\n\n\nSi \\(A\\) et \\(B\\) sont inversibles, alors : \\((AB)^{-1} = B^{-1} A^{-1}\\)."
  },
  {
    "objectID": "slides/02-revisions-algebre-slides-annotated.html#inverse-de-matrices-1",
    "href": "slides/02-revisions-algebre-slides-annotated.html#inverse-de-matrices-1",
    "title": "Révisions",
    "section": "Inverse de matrices",
    "text": "Inverse de matrices\n\nCas particulier où \\(A \\in M_{2}(\\mathbb{R})\\).\nSoit \\(A = \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}\\). Alors\n\\[A^{-1} = \\frac{1}{ad - bc}\\begin{pmatrix} d & -b \\\\ -c & a \\end{pmatrix}.\\]"
  },
  {
    "objectID": "slides/02-revisions-algebre-slides-annotated.html#inverse-de-matrices-exemple-wooclap-kffpdj",
    "href": "slides/02-revisions-algebre-slides-annotated.html#inverse-de-matrices-exemple-wooclap-kffpdj",
    "title": "Révisions",
    "section": "Inverse de matrices : exemple (Wooclap: KFFPDJ)",
    "text": "Inverse de matrices : exemple (Wooclap: KFFPDJ)\n\n\\[A = \\begin{pmatrix} 2 & 0 \\\\ 0 & 3 \\end{pmatrix}, \\quad B = \\begin{pmatrix} 1 & 1 \\\\ 0 & 1 \\end{pmatrix}\\]\n\nOn a : \\(AB = \\begin{pmatrix} 2 & 2 \\\\ 0 & 3 \\end{pmatrix},\\) donc \\((AB)^{-1} = \\begin{pmatrix} 1/2 & -1/3 \\\\ 0 & 1/3 \\end{pmatrix}\\).\nEt effectivement : \\(B^{-1}A^{-1} = \\begin{pmatrix} 1 & -1 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1/2 & 0 \\\\ 0 & 1/3 \\end{pmatrix} = \\begin{pmatrix} 1/2 & -1/3 \\\\ 0 & 1/3 \\end{pmatrix}\\)."
  },
  {
    "objectID": "slides/02-revisions-algebre-slides-annotated.html#propriétés-de-la-trace",
    "href": "slides/02-revisions-algebre-slides-annotated.html#propriétés-de-la-trace",
    "title": "Révisions",
    "section": "Propriétés de la trace",
    "text": "Propriétés de la trace\n\nLa trace de la matrice \\(A \\in M_{n}(\\mathbb{R})\\) est la somme des éléments diagonaux \\[\\text{tr}(A) = \\sum_{i=1}^{n} a_{ii}.\\]"
  },
  {
    "objectID": "slides/02-revisions-algebre-slides-annotated.html#propriétés-de-la-trace-1",
    "href": "slides/02-revisions-algebre-slides-annotated.html#propriétés-de-la-trace-1",
    "title": "Révisions",
    "section": "Propriétés de la trace",
    "text": "Propriétés de la trace\n\n\n\n\nPropriétés\n\n\n\n\\(\\text{tr}(A) = \\text{tr}(A^{\\top})\\)\n\\(\\text{tr}(A + B) = \\text{tr}(A) + \\text{tr}(B)\\)\n\\(\\text{tr}(MN^{\\top}) = \\text{tr}(N^{\\top}M)\\)"
  },
  {
    "objectID": "slides/02-revisions-algebre-slides-annotated.html#propriétés-de-la-trace-exemple-wooclap-kffpdj",
    "href": "slides/02-revisions-algebre-slides-annotated.html#propriétés-de-la-trace-exemple-wooclap-kffpdj",
    "title": "Révisions",
    "section": "Propriétés de la trace : exemple (Wooclap: KFFPDJ)",
    "text": "Propriétés de la trace : exemple (Wooclap: KFFPDJ)\n\nSoit \\(X \\in M_{n, p}(\\mathbb{R})\\) avec \\(p &lt; n\\) et de rang \\(p\\), et donc telle que \\(X^\\top X\\) est de dimension \\(p \\times p\\) et inversible. Quelle est la valeur de la trace de \\(X (X^\\top X)^{-1} X^\\top\\) ?\n\n\\[\\begin{align*}\n\\text{tr}(X (X^\\top X)^{-1} X^\\top) &= \\text{tr}(X^\\top X (X^\\top X)^{-1})\\\\\n&= \\text{tr}(I_p) \\\\\n&= p.\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/02-revisions-algebre-slides-annotated.html#matrices-spéciales",
    "href": "slides/02-revisions-algebre-slides-annotated.html#matrices-spéciales",
    "title": "Révisions",
    "section": "Matrices spéciales",
    "text": "Matrices spéciales\n\n\n\nDéfinitions\n\n\nMatrice définie positive : \\(A\\) est définie positive si \\[\\text{pour tout } u \\neq 0,\\quad u^\\top A u &gt; 0.\\]\nMatrice orthogonale : \\(A\\) est orthogonale si \\[A^\\top A = A A^\\top = I_n.\\]"
  },
  {
    "objectID": "slides/02-revisions-algebre-slides-annotated.html#comment-montrer-quune-matrice-est-définie-positive",
    "href": "slides/02-revisions-algebre-slides-annotated.html#comment-montrer-quune-matrice-est-définie-positive",
    "title": "Révisions",
    "section": "Comment montrer qu’une matrice est définie positive ?",
    "text": "Comment montrer qu’une matrice est définie positive ?\n\nSoit \\(A = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix}\\) et pour tout \\(u = \\begin{pmatrix} x \\\\ y \\end{pmatrix} \\neq 0\\), on a : \\[\\begin{align*}\nu^\\top A u &= \\begin{pmatrix} x & y \\end{pmatrix} \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix} \\\\\n  &= 2x^2 + 2xy + 2y^2 \\\\\n  &= x^2 + y^2 + (x+y)^2 &gt; 0.\n\\end{align*}\\]\nDonc \\(A\\) est définie positive."
  },
  {
    "objectID": "slides/02-revisions-algebre-slides-annotated.html#valeurs-et-vecteurs-propres-1",
    "href": "slides/02-revisions-algebre-slides-annotated.html#valeurs-et-vecteurs-propres-1",
    "title": "Révisions",
    "section": "Valeurs et vecteurs propres",
    "text": "Valeurs et vecteurs propres\n\nFaire un dessin pour Au = lu.\n\n\n\n\nDéfinitions\n\n\nSoit \\(A \\in M_{n}(\\mathbb{R})\\). Le scalaire \\(\\lambda \\in \\mathbb{R}\\) est une valeur propre de \\(A\\) s’il existe \\(u \\neq 0\\) tel que : \\(Au = \\lambda u\\).\n\\(u\\) est le vecteur propre correspondant à \\(\\lambda\\).\nLe spectre de \\(A\\), noté \\(\\text{sp}(A)\\), est l’ensemble des valeurs propres."
  },
  {
    "objectID": "slides/02-revisions-algebre-slides-annotated.html#valeurs-et-vecteurs-propres-exemple",
    "href": "slides/02-revisions-algebre-slides-annotated.html#valeurs-et-vecteurs-propres-exemple",
    "title": "Révisions",
    "section": "Valeurs et vecteurs propres : exemple",
    "text": "Valeurs et vecteurs propres : exemple\n\\[A = \\begin{pmatrix} 3 & 1 \\\\ 0 & 2 \\end{pmatrix}\\]\n\nCalcul des valeurs propres : \\[\\det(A - \\lambda I) = \\det\\begin{pmatrix} 3-\\lambda & 1 \\\\ 0 & 2-\\lambda \\end{pmatrix} = (3-\\lambda)(2-\\lambda)\\]\nDonc \\(\\lambda_1 = 3\\) et \\(\\lambda_2 = 2\\)."
  },
  {
    "objectID": "slides/02-revisions-algebre-slides-annotated.html#valeurs-et-vecteurs-propres-exemple-1",
    "href": "slides/02-revisions-algebre-slides-annotated.html#valeurs-et-vecteurs-propres-exemple-1",
    "title": "Révisions",
    "section": "Valeurs et vecteurs propres : exemple",
    "text": "Valeurs et vecteurs propres : exemple\n\\[A = \\begin{pmatrix} 3 & 1 \\\\ 0 & 2 \\end{pmatrix}\\]\n\nCalcul des vecteurs propres :\n\nPour \\(\\lambda_1 = 3\\) : \\(u_1 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\\).\nPour \\(\\lambda_2 = 2\\) : \\(u_2 = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}\\)."
  },
  {
    "objectID": "slides/02-revisions-algebre-slides-annotated.html#propriétés-des-vecteurs-propres",
    "href": "slides/02-revisions-algebre-slides-annotated.html#propriétés-des-vecteurs-propres",
    "title": "Révisions",
    "section": "Propriétés des vecteurs propres",
    "text": "Propriétés des vecteurs propres\n\n\n\nPropriétés\n\n\n\nSi \\(u\\) est vecteur propre pour \\(\\lambda\\), alors \\(cu\\) l’est aussi (pour \\(c \\neq 0\\)).\nPour matrices symétriques : Si \\(u_1\\) et \\(u_2\\) sont deux vecteurs propres correspondent à des valeurs propres différentes, alors : \\[u_1^\\top u_2 = 0 \\text{ (orthogonalité)}.\\]"
  },
  {
    "objectID": "slides/02-revisions-algebre-slides-annotated.html#preuve-de-lorthogonalité",
    "href": "slides/02-revisions-algebre-slides-annotated.html#preuve-de-lorthogonalité",
    "title": "Révisions",
    "section": "Preuve de l’orthogonalité",
    "text": "Preuve de l’orthogonalité\nSoient \\(\\lambda_1 \\neq \\lambda_2\\) et \\(Au_1 = \\lambda_1 u_1\\), \\(Au_2 = \\lambda_2 u_2\\).\n\nOn a \\[\\lambda_1 u_1^\\top u_2 = u_1^\\top A u_2 = \\lambda_2 u_1^\\top u_2.\\]\n\n\nDonc \\[(\\lambda_1 - \\lambda_2)u_1^\\top u_2 = 0\\]\n\n\nComme \\(\\lambda_1 \\neq \\lambda_2\\), on a nécessairement \\(u_1^\\top u_2 = 0\\)\n→ Cette propriété est cruciale pour l’ACP !"
  },
  {
    "objectID": "slides/02-revisions-algebre-slides-annotated.html#caractérisation-des-valeurs-propres",
    "href": "slides/02-revisions-algebre-slides-annotated.html#caractérisation-des-valeurs-propres",
    "title": "Révisions",
    "section": "Caractérisation des valeurs propres",
    "text": "Caractérisation des valeurs propres\n\n\n\n\nPropriétés\n\n\n\nMatrice symétrique → toutes les valeurs propres sont réelles.\nMatrice définie positive → toutes les valeurs propres sont strictement positives."
  },
  {
    "objectID": "slides/02-revisions-algebre-slides-annotated.html#caractérisation-des-valeurs-propres-exemple",
    "href": "slides/02-revisions-algebre-slides-annotated.html#caractérisation-des-valeurs-propres-exemple",
    "title": "Révisions",
    "section": "Caractérisation des valeurs propres : exemple",
    "text": "Caractérisation des valeurs propres : exemple\n\nSoit \\(A = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix} \\text{ (symétrique et définie positive)}\\).\n\nValeurs propres : \\(\\lambda_1 = 3, \\lambda_2 = 1\\) (toutes réelles et &gt; 0).\nVecteurs propres : \\(u_1 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}, u_2 = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}\\).\nVérifions l’orthogonalité : \\(u_1^\\top u_2 = 1 \\times 1 + 1 \\times (-1) = 0\\)."
  },
  {
    "objectID": "slides/02-revisions-algebre-slides-annotated.html#diagonalisation-de-matrices",
    "href": "slides/02-revisions-algebre-slides-annotated.html#diagonalisation-de-matrices",
    "title": "Révisions",
    "section": "Diagonalisation de matrices",
    "text": "Diagonalisation de matrices\n\n\n\n\nDéfinition\n\n\nUne matrice \\(A\\) est diagonalisable s’il existe \\(P\\) inversible et \\(D\\) diagonale telles que : \\[A = P D P^{-1} \\Leftrightarrow P^{-1} A P = D.\\]\n\n\n\n\nInterprétation : On peut “simplifier” \\(A\\) en changeant de base."
  },
  {
    "objectID": "slides/02-revisions-algebre-slides-annotated.html#théorème-de-décomposition-spectrale",
    "href": "slides/02-revisions-algebre-slides-annotated.html#théorème-de-décomposition-spectrale",
    "title": "Révisions",
    "section": "Théorème de décomposition spectrale",
    "text": "Théorème de décomposition spectrale\n\n\n\n\nThéorème\n\n\nPour toute matrice symétrique \\(A\\), il existe une matrice orthogonale \\(P\\) telle que : \\(A = P \\Lambda P^\\top\\) où \\(\\Lambda = \\text{diag}(\\lambda_1, \\ldots, \\lambda_n)\\).\n\n\n\nLa matrice \\(P\\) est formée des vecteurs propres de \\(A\\) et \\(\\Lambda\\) est la matrice diagonale des valeurs propres associées."
  },
  {
    "objectID": "slides/02-revisions-algebre-slides-annotated.html#exemple",
    "href": "slides/02-revisions-algebre-slides-annotated.html#exemple",
    "title": "Révisions",
    "section": "Exemple",
    "text": "Exemple\nEn reprenant l’example précédent.\nSoit \\(A = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix}\\). On a \\(P = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 & 1 \\\\ 1 & -1 \\end{pmatrix}\\) et \\(\\Lambda = \\begin{pmatrix} 3 & 0 \\\\ 0 & 1 \\end{pmatrix}.\\)\nOn peut vérifier le théorème de décomposition spectrale :\n\\[P \\Lambda P^\\top = \\frac{1}{2}\\begin{pmatrix} 1 & 1 \\\\ 1 & -1 \\end{pmatrix}\\begin{pmatrix} 3 & 0 \\\\ 0 & 1 \\end{pmatrix}\\begin{pmatrix} 1 & 1 \\\\ 1 & -1 \\end{pmatrix} = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix} = A.\\]"
  },
  {
    "objectID": "slides/02-revisions-algebre-slides-annotated.html#lien-avec-le-déterminant-et-la-trace",
    "href": "slides/02-revisions-algebre-slides-annotated.html#lien-avec-le-déterminant-et-la-trace",
    "title": "Révisions",
    "section": "Lien avec le déterminant et la trace",
    "text": "Lien avec le déterminant et la trace\n\n\n\n\nPropriétés\n\n\nSi la matrice \\(A\\) a pour valeurs propres \\(\\lambda_1, \\ldots, \\lambda_n\\) :\n\n\\(\\det(A) = \\prod_{i=1}^{n} \\lambda_i\\).\n\\(\\text{tr}(A) = \\sum_{i=1}^{n} \\lambda_i\\)."
  },
  {
    "objectID": "slides/02-revisions-algebre-slides-annotated.html#lien-avec-le-déterminant-et-la-trace-exemple",
    "href": "slides/02-revisions-algebre-slides-annotated.html#lien-avec-le-déterminant-et-la-trace-exemple",
    "title": "Révisions",
    "section": "Lien avec le déterminant et la trace : exemple",
    "text": "Lien avec le déterminant et la trace : exemple\n\nSoit \\(A = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix}\\). On a vu que ses valeurs propres sont \\(\\lambda_1 = 3\\) et \\(\\lambda_2 = 1\\).\nOn peut donc vérifier les formules pour le déterminant et la trace :\n\\[\\det(A) = 4 - 1 = 3 = 3 \\times 1 = \\lambda_1 \\lambda_2,\\]\n\\[\\text{tr}(A) = 2 + 2 = 4 = 3 + 1 = \\lambda_1 + \\lambda_2.\\]"
  },
  {
    "objectID": "slides/01-introduction-slides.html#qui-suis-je",
    "href": "slides/01-introduction-slides.html#qui-suis-je",
    "title": "STT-2200: Analyse de données",
    "section": "Qui suis-je ?",
    "text": "Qui suis-je ?\n\n\nDiplôme d’ingénieur en statistique + Maitrise en analyse de données massives à l’ENSAI, France\nDoctorat en maths appliqués, statistiques à l’ENSAI, France\nPostdoc en statistique à University of Limerick, Ireland\nActuellement, professeur adjoint à ULaval\nRecherche en analyse de données fonctionnelles"
  },
  {
    "objectID": "slides/01-introduction-slides.html#qui-êtes-vous",
    "href": "slides/01-introduction-slides.html#qui-êtes-vous",
    "title": "STT-2200: Analyse de données",
    "section": "Qui êtes-vous ?",
    "text": "Qui êtes-vous ?\n\n\n\n\n\nAllez sur wooclap.com\nEntrez le code KFFPDJ"
  },
  {
    "objectID": "slides/01-introduction-slides.html#quest-ce-que-lanalyse-de-données",
    "href": "slides/01-introduction-slides.html#quest-ce-que-lanalyse-de-données",
    "title": "STT-2200: Analyse de données",
    "section": "Qu’est ce que l’analyse de données ?",
    "text": "Qu’est ce que l’analyse de données ?\n\n\n\n\n\nAllez sur wooclap.com\nEntrez le code KFFPDJ"
  },
  {
    "objectID": "slides/01-introduction-slides.html#quest-ce-que-lanalyse-de-données-1",
    "href": "slides/01-introduction-slides.html#quest-ce-que-lanalyse-de-données-1",
    "title": "STT-2200: Analyse de données",
    "section": "Qu’est ce que l’analyse de données ?",
    "text": "Qu’est ce que l’analyse de données ?\n\nL’analyse de données est un ensemble de méthodes permettant de retirer de l’information d’un jeu de données. On parle aussi d’apprentissage statistique (statistical learning).\n\n\n\n\nExemple\n\n\nVous travaillez pour l’ONU. Votre mission est d’avoir de l’information sur l’espérance de vie à travers le monde. Pour cela, vous disposez de diverse données sur les différents pays du monde."
  },
  {
    "objectID": "slides/01-introduction-slides.html#ce-quon-ne-fait-pas-dans-ce-cours",
    "href": "slides/01-introduction-slides.html#ce-quon-ne-fait-pas-dans-ce-cours",
    "title": "STT-2200: Analyse de données",
    "section": "Ce qu’on ne fait PAS dans ce cours",
    "text": "Ce qu’on ne fait PAS dans ce cours\n\nmais qui pourrait être inclus dans un cours d’analyse de données.\n\nVisualisation de données\nDéveloppement algorithmique\nDeep learning"
  },
  {
    "objectID": "slides/01-introduction-slides.html#ce-quon-fait-dans-ce-cours",
    "href": "slides/01-introduction-slides.html#ce-quon-fait-dans-ce-cours",
    "title": "STT-2200: Analyse de données",
    "section": "Ce qu’on fait dans ce cours",
    "text": "Ce qu’on fait dans ce cours\n\n\nCompréhension des concepts d’analyse de données\nDéveloppement mathématique des méthodes classiques\nPractique de l’analyse de données\nUtilisation des librairies classiques"
  },
  {
    "objectID": "slides/01-introduction-slides.html#à-quoi-ça-sert",
    "href": "slides/01-introduction-slides.html#à-quoi-ça-sert",
    "title": "STT-2200: Analyse de données",
    "section": "À quoi ça sert ?",
    "text": "À quoi ça sert ?\n\nLes méthodes enseignées servent à\n\nréduire la dimension des données\nclassifier des individus automatiquement\nformer des groupes d’individus"
  },
  {
    "objectID": "slides/01-introduction-slides.html#contenu-du-cours",
    "href": "slides/01-introduction-slides.html#contenu-du-cours",
    "title": "STT-2200: Analyse de données",
    "section": "Contenu du cours",
    "text": "Contenu du cours\n\n\nRévisions / Rappels\nGénéralité sur l’analyse de données\nRéduction de la dimension\nClassication supervisée\nClassification non-supervisée\nÉthique"
  },
  {
    "objectID": "slides/01-introduction-slides.html#format-du-cours",
    "href": "slides/01-introduction-slides.html#format-du-cours",
    "title": "STT-2200: Analyse de données",
    "section": "Format du cours",
    "text": "Format du cours\n\n\nMardi : Laboratoire 1h (mais la salle est réservée pour 2h)\nVendredi : Cours magistral 2h\nLes labs du mardi porteront sur le cours vu le vendredi. Vous pouvez ammener votre propre laptop pour les labs, cependant les ordinateurs de l’université seront utilisable.\nLes labs consisteront en des exercices (théoriques et pratiques) à faire en autonomie et je serais là pour répondre à vos questions."
  },
  {
    "objectID": "slides/01-introduction-slides.html#évaluations",
    "href": "slides/01-introduction-slides.html#évaluations",
    "title": "STT-2200: Analyse de données",
    "section": "Évaluations",
    "text": "Évaluations\n\n\nIntra le 24 octobre \\(\\longrightarrow\\) QCM (30%)\n\nReprise le 30 octobre\n\nExamen final le 12 décembre (50%)\n\nReprise le 18 décembre\n\nProjet en équipe à rendre le 2 décembre à 17h00 (20%)\nPrésentation du projet le 9 décembre au matin."
  },
  {
    "objectID": "slides/01-introduction-slides.html#logiciels",
    "href": "slides/01-introduction-slides.html#logiciels",
    "title": "STT-2200: Analyse de données",
    "section": "Logiciels",
    "text": "Logiciels\n\nLibre à vous de choisir votre logiciel préféré (ou celui que vous maitrisez le mieux)\n\n\n\n\n\nAllez sur wooclap.com\nEntrez le code KFFPDJ"
  },
  {
    "objectID": "slides/01-introduction-slides.html#communications",
    "href": "slides/01-introduction-slides.html#communications",
    "title": "STT-2200: Analyse de données",
    "section": "Communications",
    "text": "Communications\n\n\nSite du cours MonPortail\nNotes de cours : stt-2200.netlify.app\nUne question : forum ou courriel\nMon bureau : VCH-2209"
  },
  {
    "objectID": "slides/01-introduction-slides.html#mon-rôle",
    "href": "slides/01-introduction-slides.html#mon-rôle",
    "title": "STT-2200: Analyse de données",
    "section": "Mon rôle",
    "text": "Mon rôle\n\n\nMon but est de vous aider dans votre apprentissage.\nJ’essaie de vous poser des questions, de vous présenter des exemples et de vous aider à utiliser les ressources disponibles pour vous assurer d’avoir bien compris.\nMon objectif est de vous donner les bases et les compétences nécessaires pour continuer à apprendre sur ces sujets de manière indépendante, tout en ayant du recul sur l’utilisation des différentes méthodes."
  },
  {
    "objectID": "slides/01-introduction-slides.html#politique-sur-la-sécurité-dans-les-laboratoires-informatiques",
    "href": "slides/01-introduction-slides.html#politique-sur-la-sécurité-dans-les-laboratoires-informatiques",
    "title": "STT-2200: Analyse de données",
    "section": "Politique sur la sécurité dans les laboratoires informatiques",
    "text": "Politique sur la sécurité dans les laboratoires informatiques\n\nPour utiliser les laboratoires informatiques, il faut avoir pris connaissance de la Politique de sécurité et vous engagez à vous y conformer :\n\nwww.fsg.ulaval.ca/securite-labo-info"
  },
  {
    "objectID": "slides/01-introduction-slides.html#pour-finir-lintroduction",
    "href": "slides/01-introduction-slides.html#pour-finir-lintroduction",
    "title": "STT-2200: Analyse de données",
    "section": "Pour finir l’introduction",
    "text": "Pour finir l’introduction\n\n\nStatistics courses […] tend to resemble horoscopes.\n\nR. McElreath - Statistical Rethinking"
  },
  {
    "objectID": "slides/05-supervised-tree-slides.html#plan",
    "href": "slides/05-supervised-tree-slides.html#plan",
    "title": "Supervisée",
    "section": "Plan",
    "text": "Plan\n\n\nLa théorie des arbres de décision\nLes arbres en pratique\nExemple"
  },
  {
    "objectID": "slides/05-supervised-tree-slides.html#cart",
    "href": "slides/05-supervised-tree-slides.html#cart",
    "title": "Supervisée",
    "section": "CART",
    "text": "CART\n\nVariables : \\(X_1, \\ldots, X_p\\) avec \\(n\\) observations dans \\(\\mathbb{R}^p\\).\n\n\nClassification : Prédire l’appartenance à l’un des \\(K\\) groupes.\n\n\n\nRégression : Prédire une variable numérique continue \\(Y\\)."
  },
  {
    "objectID": "slides/05-supervised-tree-slides.html#objectif-principal",
    "href": "slides/05-supervised-tree-slides.html#objectif-principal",
    "title": "Supervisée",
    "section": "Objectif principal",
    "text": "Objectif principal\n\nPartitionner l’espace des observations \\(\\mathbb{R}^p\\) en sous-ensembles homogènes."
  },
  {
    "objectID": "slides/05-supervised-tree-slides.html#algorithme-récursif-de-découpage-binaire",
    "href": "slides/05-supervised-tree-slides.html#algorithme-récursif-de-découpage-binaire",
    "title": "Supervisée",
    "section": "Algorithme récursif de découpage binaire",
    "text": "Algorithme récursif de découpage binaire\n\nDécouper selon une variable \\(X_j\\) à un seuil \\(s\\).\n\n\nCréer des sous-régions rectangulaires (hyper-rectangles).\n\n\n\nChaque sous-ensemble = une feuille avec une prédiction.\n\n\n\n⚠️ Algorithme glouton: Optimisation locale, pas globale !"
  },
  {
    "objectID": "slides/05-supervised-tree-slides.html#prédiction-par-feuille",
    "href": "slides/05-supervised-tree-slides.html#prédiction-par-feuille",
    "title": "Supervisée",
    "section": "Prédiction par feuille",
    "text": "Prédiction par feuille\n\nClassification : Classe majoritaire\n\n\nRégression : Moyenne des \\(Y\\)"
  },
  {
    "objectID": "slides/05-supervised-tree-slides.html#algorithme",
    "href": "slides/05-supervised-tree-slides.html#algorithme",
    "title": "Supervisée",
    "section": "Algorithme",
    "text": "Algorithme\n\nPoint de départ: L’ensemble complet des observations\n\nPour chaque nœud:\n\nÉvaluer tous les couples \\((X_j, s)\\) possibles.\nCalculer la réduction du critère d’homogénéité.\nSélectionner le couple \\((X_j, s)\\) qui maximise le gain.\nDiviser le nœud selon \\(X_j \\leq s\\) vs \\(X_j &gt; s\\).\nRépéter jusqu’au critère d’arrêt."
  },
  {
    "objectID": "slides/05-supervised-tree-slides.html#critères-dhomogénéité",
    "href": "slides/05-supervised-tree-slides.html#critères-dhomogénéité",
    "title": "Supervisée",
    "section": "Critères d’homogénéité",
    "text": "Critères d’homogénéité\n\nSoit \\(\\widehat{p}_{jk}\\) la proportion d’observations de la région \\(R_j\\) appartenant à la classe \\(k\\).\n\n\nTaux d’erreur de classification\n\\[E_j = 1 - \\max_k \\widehat{p}_{jk}\\]\n\n\nFacile à interpréter.\nPeu sensible → rarement utilisé pour la construction."
  },
  {
    "objectID": "slides/05-supervised-tree-slides.html#critères-dhomogénéité-1",
    "href": "slides/05-supervised-tree-slides.html#critères-dhomogénéité-1",
    "title": "Supervisée",
    "section": "Critères d’homogénéité",
    "text": "Critères d’homogénéité\n\nSoit \\(\\widehat{p}_{jk}\\) la proportion d’observations de la région \\(R_j\\) appartenant à la classe \\(k\\).\n\nIndice de Gini (le plus utilisé)\n\\[G_j = \\sum_{k=1}^{K} \\widehat{p}_{jk}(1 - \\widehat{p}_{jk})\\]\n\nMinimal (\\(= 0\\)) quand homogénéité parfaite.\nOn cherche un indice faible."
  },
  {
    "objectID": "slides/05-supervised-tree-slides.html#critères-dhomogénéité-2",
    "href": "slides/05-supervised-tree-slides.html#critères-dhomogénéité-2",
    "title": "Supervisée",
    "section": "Critères d’homogénéité",
    "text": "Critères d’homogénéité\n\nSoit \\(\\widehat{p}_{jk}\\) la proportion d’observations de la région \\(R_j\\) appartenant à la classe \\(k\\).\n\nEntropie croisée (théorie de l’information)\n\\[D_j = -\\sum_{k=1}^{K} \\widehat{p}_{jk} \\log(\\widehat{p}_{jk})\\]\n\nBasé sur l’information de Shannon.\nMinimal pour classe parfaitement prédite."
  },
  {
    "objectID": "slides/05-supervised-tree-slides.html#gain-dinformation",
    "href": "slides/05-supervised-tree-slides.html#gain-dinformation",
    "title": "Supervisée",
    "section": "Gain d’information",
    "text": "Gain d’information\n\nMesure de la réduction de l’hétérogénéité après division.\n\nObjectif : Maximiser le gain d’information.\n\n\nExemple avec l’indice de Gini:\n\\[\\Delta G = G_{\\text{avant}} - \\left(\\frac{n_1}{n} G_1 + \\frac{n_2}{n} G_2\\right)\\]\noù:\n\n\\(n_1, n_2\\) : nombres d’observations dans les sous-ensembles\n\\(n = n_1 + n_2\\)"
  },
  {
    "objectID": "slides/05-supervised-tree-slides.html#exemple",
    "href": "slides/05-supervised-tree-slides.html#exemple",
    "title": "Supervisée",
    "section": "Exemple",
    "text": "Exemple\n\nCodePrintPlot\n\n\n\nlibrary(tidyverse)\n\ncarseats &lt;- as_tibble(ISLR::Carseats)\ntree_carseat &lt;- rpart::rpart(Sales ~ ., data = carseats)\n\n\n\n\nprint(tree_carseat)\n\nn= 400 \n\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n 1) root 400 3182.27500  7.496325  \n   2) ShelveLoc=Bad,Medium 315 1859.56000  6.762984  \n     4) Price&gt;=105.5 207  956.57240  6.018792  \n       8) ShelveLoc=Bad 61  240.81970  4.722459  \n        16) Population&lt; 196.5 25   88.22930  3.767200 *\n        17) Population&gt;=196.5 36  113.93510  5.385833 *\n       9) ShelveLoc=Medium 146  570.41420  6.560411  \n        18) Advertising&lt; 5.5 77  280.11340  5.902468  \n          36) Price&gt;=127 34  133.53970  4.986765 *\n          37) Price&lt; 127 43   95.52198  6.626512 *\n        19) Advertising&gt;=5.5 69  219.77110  7.294638  \n          38) CompPrice&lt; 121.5 19   40.33360  6.230000 *\n          39) CompPrice&gt;=121.5 50  149.71840  7.699200  \n            78) Price&gt;=127 28   71.99441  6.731786 *\n            79) Price&lt; 127 22   18.16730  8.930455 *\n     5) Price&lt; 105.5 108  568.61750  8.189352  \n      10) Age&gt;=54.5 65  303.05690  7.380154  \n        20) Income&lt; 105.5 56  203.03290  6.946071  \n          40) ShelveLoc=Bad 20   76.96006  5.786500 *\n          41) ShelveLoc=Medium 36   84.24070  7.590278 *\n        21) Income&gt;=105.5 9   23.81549 10.081110 *\n      11) Age&lt; 54.5 43  158.66040  9.412558  \n        22) Income&lt; 57.5 13   19.24283  7.987692 *\n        23) Income&gt;=57.5 30  101.58740 10.030000  \n          46) ShelveLoc=Bad 9   22.75640  8.396667 *\n          47) ShelveLoc=Medium 21   44.53100 10.730000 *\n   3) ShelveLoc=Good 85  525.52220 10.214000  \n     6) Price&gt;=109.5 57  277.26520  9.244386  \n      12) Advertising&lt; 13.5 48  185.42030  8.742500  \n        24) Price&gt;=142.5 12   36.64722  7.152500 *\n        25) Price&lt; 142.5 36  108.32350  9.272500  \n          50) Income&lt; 40.5 9    9.82780  7.603333 *\n          51) Income&gt;=40.5 27   65.06227  9.828889 *\n      13) Advertising&gt;=13.5 9   15.27049 11.921110 *\n     7) Price&lt; 109.5 28   85.57727 12.187860 *\n\n\n\n\n\nplot(tree_carseat)\ntext(tree_carseat, use.n = TRUE)"
  },
  {
    "objectID": "slides/05-supervised-tree-slides.html#problème-de-lajustement",
    "href": "slides/05-supervised-tree-slides.html#problème-de-lajustement",
    "title": "Supervisée",
    "section": "Problème de l’ajustement",
    "text": "Problème de l’ajustement\n\nDilemme:\n\nArbre trop profond → Sur-ajustement aux données d’apprentissage\nArbre trop simple → Sous-ajustement, classification trop simpliste\n\n\nConséquence → Perte de capacité de généralisation à de nouvelles données.\n\n\nSolution → Élagage (pruning)"
  },
  {
    "objectID": "slides/05-supervised-tree-slides.html#élagage-de-larbre",
    "href": "slides/05-supervised-tree-slides.html#élagage-de-larbre",
    "title": "Supervisée",
    "section": "Élagage de l’arbre",
    "text": "Élagage de l’arbre\n\nStratégie en deux temps :\n\nCroissance complète jusqu’aux feuilles pures\nÉlagage des branches peu performantes\n\n\nCritère coût-complexité :\n\\[\\sum_{j=1}^{|T|} c_j + \\alpha |T|\\]\noù \\(|T|\\) est le nombre de feuilles, \\(c_j\\) est le coût dans la feuille \\(j\\) et \\(\\alpha\\) est un paramètre de régularisation."
  },
  {
    "objectID": "slides/05-supervised-tree-slides.html#choix-du-paramètre-α",
    "href": "slides/05-supervised-tree-slides.html#choix-du-paramètre-α",
    "title": "Supervisée",
    "section": "Choix du paramètre α",
    "text": "Choix du paramètre α\n\nInterprétation\n\n\\(\\alpha\\) élevé → Favorise les arbres simples\n\\(\\alpha\\) faible → Arbres plus complexes\n\n\nMéthode de sélection → Validation croisée pour comparer les performances sur différentes valeurs de \\(\\alpha\\)"
  },
  {
    "objectID": "slides/05-supervised-tree-slides.html#exemple-1",
    "href": "slides/05-supervised-tree-slides.html#exemple-1",
    "title": "Supervisée",
    "section": "Exemple",
    "text": "Exemple\n\nCodePrintPlot\n\n\n\nlibrary(tidyverse)\n\ncarseats &lt;- as_tibble(ISLR::Carseats)\ntree_carseat &lt;- rpart::rpart(Sales ~ ., data = carseats)\ntree_prune &lt;- rpart::prune(tree_carseat, cp = 0.1)\n\n\n\n\nprint(tree_prune)\n\nn= 400 \n\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n1) root 400 3182.2750  7.496325  \n  2) ShelveLoc=Bad,Medium 315 1859.5600  6.762984  \n    4) Price&gt;=105.5 207  956.5724  6.018792 *\n    5) Price&lt; 105.5 108  568.6175  8.189352 *\n  3) ShelveLoc=Good 85  525.5222 10.214000 *\n\n\n\n\n\nplot(tree_prune)\ntext(tree_prune, use.n = TRUE)"
  },
  {
    "objectID": "slides/05-supervised-tree-slides.html#avantages-des-arbres",
    "href": "slides/05-supervised-tree-slides.html#avantages-des-arbres",
    "title": "Supervisée",
    "section": "Avantages des arbres",
    "text": "Avantages des arbres\n\nTrès interprétables : Chemin explicite pour chaque prédiction.\n\nRobustes aux valeurs extrêmes et transformations monotones.\n\n\nGestion naturelle des interactions entre variables.\n\n\nVariables mixtes : Quantitatives et qualitatives.\n\n\nAucune hypothèse sur la distribution des données.\n\n\nSélection automatique des variables importantes.\n\n\nBase des méthodes modernes (forêts aléatoires, boosting)."
  },
  {
    "objectID": "slides/05-supervised-tree-slides.html#limites-des-arbres",
    "href": "slides/05-supervised-tree-slides.html#limites-des-arbres",
    "title": "Supervisée",
    "section": "Limites des arbres",
    "text": "Limites des arbres\n\nInstabilité : Sensibles aux petites modifications des données\n\nPerformance limitée seuls, surtout avec données bruitées.\n\n\nDivisions rectangulaires inadaptées aux frontières non-linéaires.\n\n\nTendance au sur-ajustement sans élagage.\n\n\nSensibilité au déséquilibre des classes.\n\n\nSolution : Méthodes ensemblistes (forêts, boosting)."
  },
  {
    "objectID": "slides/05-supervised-tree-slides.html#conclusion",
    "href": "slides/05-supervised-tree-slides.html#conclusion",
    "title": "Supervisée",
    "section": "Conclusion",
    "text": "Conclusion\n\nLes arbres :\n\nMéthode de partitionnement récursif\nAlgorithme glouton simple et intuitif\nNécessitent un élagage pour éviter le sur-ajustement\n\n\nProchaine étape → Les modèles ensemblistes"
  },
  {
    "objectID": "tp/04-dimension-tp.html",
    "href": "tp/04-dimension-tp.html",
    "title": "TP: Dimension",
    "section": "",
    "text": "Vous pouvez faire les exercices dans le langage de votre choix."
  },
  {
    "objectID": "tp/04-dimension-tp.html#exercice-1-des-coquillages",
    "href": "tp/04-dimension-tp.html#exercice-1-des-coquillages",
    "title": "TP: Dimension",
    "section": "Exercice 1: Des coquillages",
    "text": "Exercice 1: Des coquillages\nDans cet exercice, on se propose de faire une analyse de données de coquillages. Pour cela, on a accès à 7 mesures physiques des coquillages, ainsi que leur genre et leur âge.\n\nTélécharger le jeu de données suivant: lien. Des informations sur le jeu de données sont disponibles ici.\nFaire une rapide analyse descriptive des données.\nFaire une analyse en composantes principales.\nCombien gardez-vous d’axes factoriels en fonction des différentes règles vu en cours ?\nQuelles sont les deux variables qui contribuent le plus à la deuxième dimension ? Et quelles sont les deux observations qui contribuent le plus à la troisième dimension ?\nComment interprétez la première dimension ?\nQuelle est le pourcentage de la variabilité expliquée par les trois premiers axes factoriels ?\nDans ce contexte, était-il pertinent de standardiser les données pour faire l’analyse en composantes principales ? Justifiez."
  },
  {
    "objectID": "tp/04-dimension-tp.html#exercice-2-écoute-radio-au-canada",
    "href": "tp/04-dimension-tp.html#exercice-2-écoute-radio-au-canada",
    "title": "TP: Dimension",
    "section": "Exercice 2: Écoute radio au Canada",
    "text": "Exercice 2: Écoute radio au Canada\nDans cet exercice, on cherche à savoir si certains types de radio sont préférés par des adoslescents, des hommes ou bien des femmes.\n\nTélécharger le jeu de données suivant: lien. Ces données sont tirés de Statistique Canada et correspondent aux types de radio écoutées par les adoslescents, les hommes et les femmes adultes au Canada.\nFaire une analyse factorielle des correspondances.\nInterpréter le premier axe factoriel.\nQuelle est la qualité de représentation des adoslescents, des hommes et des femmes adultes. Expliquer."
  },
  {
    "objectID": "tp/04-dimension-tp.html#exercice-3-le-café",
    "href": "tp/04-dimension-tp.html#exercice-3-le-café",
    "title": "TP: Dimension",
    "section": "Exercice 3: Le café",
    "text": "Exercice 3: Le café\nDans cet exercice, on s’intéresse à la relation entre le type de café commandé et la période de la journée.\n\nTélécharger le jeu de données suivant: lien.\nConstruire la tableau des fréquences relatives croisant Types de café et Période de la journée.\nFaire une analyse factorielle des correspondances de ce tableau de fréquences.\nInterpréter les axes factoriels.\nLes deux variables sont-elles indépendantes ? Justifier."
  },
  {
    "objectID": "tp/04-dimension-tp.html#exercice-4-dépression-post-partum",
    "href": "tp/04-dimension-tp.html#exercice-4-dépression-post-partum",
    "title": "TP: Dimension",
    "section": "Exercice 4: Dépression post-partum",
    "text": "Exercice 4: Dépression post-partum\nDans cet exercice, on s’intéresse à l’analyse d’un questionnaire sur la dépression post-partum après l’accouchement.\n\nTélécharger le jeu de données suivant: lien. Des informations sur le jeu de données sont disponibles ici.\nConstuire le tableau disjonctif complet de ce jeu de données.\nConstruire le tableau de Burt de ce jeu de données.\nFaire une analyse des correspondances multiples sur le tableau disjonctif complet. Vérifier que les résultats sont cohérents avec ue ACM faites sur le tableau de Burt.\nInterpréter les axes factoriels.\nQuelles associations pouvez-vous faire à l’aide des résultats de l’ACM ?"
  },
  {
    "objectID": "tp/03-generalities-tp.html",
    "href": "tp/03-generalities-tp.html",
    "title": "TP: Généralités",
    "section": "",
    "text": "Vous pouvez faire les exercices dans le langage de votre choix."
  },
  {
    "objectID": "tp/03-generalities-tp.html#exercice-1-nettoyage-et-exploration-de-données",
    "href": "tp/03-generalities-tp.html#exercice-1-nettoyage-et-exploration-de-données",
    "title": "TP: Généralités",
    "section": "Exercice 1 : Nettoyage et exploration de données",
    "text": "Exercice 1 : Nettoyage et exploration de données\n\nTélécharger le jeu de données suivant: lien\nNettoyer le jeu de données. En particulier, on s’intéressera aux points suivant :\n\n\nencodage des valeurs manquantes;\ngestion des valeurs extrêmes;\ngestion des valeurs abberantes;\ngestion des doublons;\nharmonisation des dates;\nsuppression des tirets, points, … dans les numéros de téléphone.\ngestion des titres (Mr., Ms., Dr., …) dans les noms;\nharmonisation des pays;\nconversion des Oui/Non en TRUE/FALSE;\netc.\n\n\nFaire une exploration unidimensionelle/bidimensionelle. En particulier, on pourra faire :\n\n\ngraphiques des données quantitatives;\ncalcul de corrélations;\ntableaux de fréquences pour les variables qualitatives;\netc."
  },
  {
    "objectID": "tp/03-generalities-tp.html#exercice-2-compromis-biais-variance",
    "href": "tp/03-generalities-tp.html#exercice-2-compromis-biais-variance",
    "title": "TP: Généralités",
    "section": "Exercice 2 : Compromis biais-variance",
    "text": "Exercice 2 : Compromis biais-variance\nDans cet exercice, on se propose d’illustrer le compromis biais-variance à l’aide de données simulées et d’un modèle de régression. On fait l’hypothèse que le vrai modèle de nos données est \\[f(x) = 3 + 8 x + 2 x^2.\\]\n\nSimuler un ensemble de données \\((X, Y)\\) tel que \\(Y = f(X) + \\epsilon\\), avec \\(X \\sim \\mathcal{N}(0, 1)\\), \\(\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)\\) et \\(\\sigma^2 = 5\\).\nAjuster un modèle linéaire de la forme \\(Y = \\beta_{0} + \\beta_{1} X\\). En R, la fonction standard est lm. En Python, vous pouvez utiliser la fonction ols du package statsmodels.\nCalculer \\(\\widehat{Y}\\) en utilisant \\(X\\) généré à la question 1.\nCalculer l’erreur quadratique moyenne sur le jeu d’entraînement.\nSimuler un jeu de données de validation et calculer l’erreur quadratique moyenne, le biais et la variance du modèle sur ce jeu de validation.\nRefaire les questions 2 à 5, mais avec en ajustant un modèle linéaire avec un polynôme d’ordre 2.\nRefaire les questions 2 à 5, mais avec en ajustant un modèle linéaire avec un polynôme d’ordre 10.\nConclure quant au compromis biais-variance et la complexité du modèle."
  },
  {
    "objectID": "tp/03-generalities-tp.html#exercice-3-validation-croisée",
    "href": "tp/03-generalities-tp.html#exercice-3-validation-croisée",
    "title": "TP: Généralités",
    "section": "Exercice 3 : Validation croisée",
    "text": "Exercice 3 : Validation croisée\nEn pratique, nous ne connaissons pas la vraie fonction \\(f(x)\\). Pour évaluer la qualité de notre modèle, on peut utiliser la validation croisée. Notez que le but de cet exercice est de faire votre propre code et non d’utiliser des fonctions déjà faites.\n\nSimuler un jeu de données en utilisant le modèle de l’exercice 2.\nFaire une validation croisée avec \\(K = 3\\) pour évaluer le modèle de régression linéaire simple (sans polynôme d’ordre supérieur) en utilisant l’erreur quadratique moyenne.\nFaire la même chose avec le modèle de régression linéaire avec un polynôme d’ordre 2.\nFaire la même chose avec le modèle de régression linéaire avec un polynôme d’ordre 10.\nConclure quant à l’utilisation de la validation croisée."
  },
  {
    "objectID": "tp/02-revision-tp.html",
    "href": "tp/02-revision-tp.html",
    "title": "TP: Révision",
    "section": "",
    "text": "Vous pouvez faire les exercices dans le langage de votre choix."
  },
  {
    "objectID": "tp/02-revision-tp.html#exercice-1-estimer-pi",
    "href": "tp/02-revision-tp.html#exercice-1-estimer-pi",
    "title": "TP: Révision",
    "section": "Exercice 1: Estimer \\(\\pi\\)",
    "text": "Exercice 1: Estimer \\(\\pi\\)\nDans cet exercice, on se propose d’estimer \\(\\pi\\) grâce à la méthode de Monte-Carlo. Le méthode de Monte-Carlo est une méthode algorithmique permettant d’estimer des quantités en utilisant des tirages aléatoires. Pour estimer \\(\\pi\\), l’idée est de générer des points dans un carré de façon uniforme et ensuite de compter la proportion de ces points qui sont dans le cercle unité.\n\nGénérer un nombre \\(n\\) de points \\((x, y)\\) dans un carré de longueur \\(2\\) centré à l’origine, i.e. \\(x \\in [-1, 1]\\) et \\(y \\in [-1, 1]\\).\nPour chaque point \\(n\\), déterminer si le point appartient au cercle unité.\nCalculer la proportion du nombre de points dans le cercle unité.\nÀ partir du résultat précédent, estimer \\(\\pi\\).\nNotons \\(\\widehat{\\pi}(n)\\), l’estimateur de \\(\\pi\\) utilisant \\(n\\) points générés. Tracer l’erreur d’estimation \\(\\left| \\pi - \\widehat{\\pi}(n) \\right|\\) en fonction de \\(n\\)."
  },
  {
    "objectID": "tp/02-revision-tp.html#exercice-2-estimation-dintégrale",
    "href": "tp/02-revision-tp.html#exercice-2-estimation-dintégrale",
    "title": "TP: Révision",
    "section": "Exercice 2: Estimation d’intégrale",
    "text": "Exercice 2: Estimation d’intégrale\nDans cette exercice, on se propose d’estimer \\[I = \\int_{0}^{1} \\sqrt{1 - x^2} dx .\\]\n\nCalculer \\(I\\) en utilisant une primitive.\nIl est possible d’estimer \\(I = \\int_{0}^{1} f(x) dx\\) à l’aide des sommes de Riemann \\(\\widehat{I}(n) = \\frac{1}{n}\\sum_{i = 1}^{n} f(\\frac{i}{n}), i = 1, \\dots, n\\). Estimer \\(I\\) en utilisant les sommes de Riemann.\nL’intégrale \\(I\\) peut être vu comme l’espérance d’un variable aléatoire. \\(I = \\mathbb{E}[f(U)]\\), où \\(U \\sim \\mathcal{U}(0, 1)\\). On peut estimer \\(I\\) en utilisant \\(\\widetilde{I}(n) = \\frac{1}{n} \\sum_{i = 1}^{n} f(u_i)\\), où les \\(u_i\\) sont des réalisations de la variable aléatoire \\(U\\). Estimer \\(I\\) en utilisant cette méthode.\nComparer la qualité de ces deux estimateurs de \\(I\\) en fonction de \\(n\\)."
  },
  {
    "objectID": "tp/02-revision-tp.html#exercice-3-la-loi-des-gaz-parfaits",
    "href": "tp/02-revision-tp.html#exercice-3-la-loi-des-gaz-parfaits",
    "title": "TP: Révision",
    "section": "Exercice 3: La loi des gaz parfaits",
    "text": "Exercice 3: La loi des gaz parfaits\nDans cet exercice, on se propose de vérifier la loi des gaz parfaits à partir des données. On rappelle que la loi des gaz parfaits est donnée par \\(PV = nRT\\) où :\n\n\\(P\\) est la pression à l’intérieur du volume considéré en Pascal (Pa);\n\\(V\\) est le volume du gaz en m\\(^3\\);\n\\(n\\) est la quantité de matière en mole (mol);\n\\(R = 8.314\\) est la constante universelle des gaz parfaits en J.mol\\(^{-1}\\).K\\(^{-1}\\);\n\\(T\\) est la température à l’intérieur du volume considéré en Kelvin (K).\n\nOn conduit une expérience consistant à chauffer une quantité fixe de gaz dans un récipient fermé de volume fixé. La température \\(T\\) en Kelvin et la pression \\(P\\) en kPa sont enregistrées. On trouve les résultats suivants :\n\ntemperature = [\n  406, 296, 272, 449, 483, 439, 460, 276, 321, 462, 408, 322, 285,\n  411, 491, 359, 453, 486, 413, 350, 263, 456, 390, 462, 389, 494,\n  303, 496, 336, 460\n]\n\npression = [\n  1365, 982, 898, 1486, 1596, 1481, 1506, 906, 1085, 1542, 1367,\n  1072, 955, 1379, 1633, 1186, 1499, 1606, 1378, 1156, 867, 1514,\n  1306, 1525, 1287, 1665, 1020, 1635, 1118, 1529\n]\n\n\nTracer la pression en fonction de la température. Est-ce que le graphique est linéaire ?\nConstruire la matrice \\(X = (1 | \\text{temperature})\\). La première colonne de \\(X\\) est une colonne de \\(1\\) et la deuxième colonne de \\(X\\) est le vecteur des températures.\nCalculer le vecteur \\(\\beta = (X^{\\top} X)^{-1} X^{\\top} Y\\) où \\(Y\\) est le vecteur des pressions.\nQuelle est l’interprétation physique des éléments du vecteur \\(\\beta\\) ?\nÀ quelle valeur de \\(\\beta_0\\), le premier coefficient de \\(\\beta\\), devrait-on s’attendre dans le cadre d’un gaz parfait ? Est-ce le cas ici ? Pourquoi ?\nSupposons que le volume de gaz est de \\(10\\)dm\\(^3\\). Estimer la quantité de matière \\(n\\) en mole utilisé pour avoir les données."
  },
  {
    "objectID": "td/03-generalities-td.html",
    "href": "td/03-generalities-td.html",
    "title": "TD: Généralités",
    "section": "",
    "text": "Dans cette exercice, on se propose de finir la preuve que la distance de Jaccard est bien une distance. On s’intéresse en particulier à la preuve de l’inégalité triangulaire. Pour cela, l’idée est de réécrire la distance de Jaccard comme une distance entre des ensembles.\nConsidérons deux observations \\(x\\) et \\(y\\) de \\(K\\) variables binaires. Notons \\(X = \\{ i \\in \\{ 1, \\dots, K \\} \\mid x_i = 1 \\}\\) et \\(Y = \\{ i \\in \\{ 1, \\dots, K \\} \\mid y_i = 1 \\}\\).\n\nMontrer les trois premières propriétés des distances pour la distance de Jaccard.\nFaire un dessin montrant \\(M_{11}\\), \\(M_{10}\\) et \\(M_{01}\\) à l’aide des ensembles \\(X\\) et \\(Y\\).\nÉcrire l’indice de Jaccard entre \\(x\\) et \\(y\\) à l’aide des ensembles \\(X\\) et \\(Y\\). En déduire la distance de Jaccard en fonction des ensembles \\(X\\) et \\(Y\\). On pourra noter \\(X \\Delta Y = (X \\backslash Y) \\cup (Y \\backslash X)\\).\nSoit \\(A = \\{ 1, 2 \\}\\), \\(B = \\{ 2, 3 \\}\\) et \\(C = \\{ 3, 4 \\}\\). Calculer les distances de Jaccard entre \\(A\\) et \\(B\\), \\(B\\) et \\(C\\) et \\(A\\) et \\(C\\) en utilisant la définition ensembliste de cette distance.\nSoit une troisième observation \\(z\\), notons \\(Z = \\{ i \\in \\{ 1, \\dots, K \\} \\mid z_i = 1 \\}\\). Réécrire l’inégalité triangulaire pour la distance de Jaccard en utilisant la définition ensembliste.\nMontrer que \\(X \\Delta Y \\subseteq (X \\Delta Z) \\cup (Y \\Delta Z)\\).\nMontrer que \\(X \\cup Y \\subseteq (X \\cup Z) \\cup (Y \\cup Z)\\).\nConclure sur l’inégalité triangulaire. On pourra supposer que, pour tous nombres positifs \\(a_{1}, a_{2}, b_{1}, b_{2}\\), on a \\(\\frac{a_{1} + a_{2}}{b_{1} + b_{2}} \\leq \\frac{a_{1}}{b_{1}} + \\frac{a_{2}}{b_{2}}\\)."
  },
  {
    "objectID": "td/03-generalities-td.html#exercice-1-preuve-de-la-distance-de-jaccard",
    "href": "td/03-generalities-td.html#exercice-1-preuve-de-la-distance-de-jaccard",
    "title": "TD: Généralités",
    "section": "",
    "text": "Dans cette exercice, on se propose de finir la preuve que la distance de Jaccard est bien une distance. On s’intéresse en particulier à la preuve de l’inégalité triangulaire. Pour cela, l’idée est de réécrire la distance de Jaccard comme une distance entre des ensembles.\nConsidérons deux observations \\(x\\) et \\(y\\) de \\(K\\) variables binaires. Notons \\(X = \\{ i \\in \\{ 1, \\dots, K \\} \\mid x_i = 1 \\}\\) et \\(Y = \\{ i \\in \\{ 1, \\dots, K \\} \\mid y_i = 1 \\}\\).\n\nMontrer les trois premières propriétés des distances pour la distance de Jaccard.\nFaire un dessin montrant \\(M_{11}\\), \\(M_{10}\\) et \\(M_{01}\\) à l’aide des ensembles \\(X\\) et \\(Y\\).\nÉcrire l’indice de Jaccard entre \\(x\\) et \\(y\\) à l’aide des ensembles \\(X\\) et \\(Y\\). En déduire la distance de Jaccard en fonction des ensembles \\(X\\) et \\(Y\\). On pourra noter \\(X \\Delta Y = (X \\backslash Y) \\cup (Y \\backslash X)\\).\nSoit \\(A = \\{ 1, 2 \\}\\), \\(B = \\{ 2, 3 \\}\\) et \\(C = \\{ 3, 4 \\}\\). Calculer les distances de Jaccard entre \\(A\\) et \\(B\\), \\(B\\) et \\(C\\) et \\(A\\) et \\(C\\) en utilisant la définition ensembliste de cette distance.\nSoit une troisième observation \\(z\\), notons \\(Z = \\{ i \\in \\{ 1, \\dots, K \\} \\mid z_i = 1 \\}\\). Réécrire l’inégalité triangulaire pour la distance de Jaccard en utilisant la définition ensembliste.\nMontrer que \\(X \\Delta Y \\subseteq (X \\Delta Z) \\cup (Y \\Delta Z)\\).\nMontrer que \\(X \\cup Y \\subseteq (X \\cup Z) \\cup (Y \\cup Z)\\).\nConclure sur l’inégalité triangulaire. On pourra supposer que, pour tous nombres positifs \\(a_{1}, a_{2}, b_{1}, b_{2}\\), on a \\(\\frac{a_{1} + a_{2}}{b_{1} + b_{2}} \\leq \\frac{a_{1}}{b_{1}} + \\frac{a_{2}}{b_{2}}\\)."
  },
  {
    "objectID": "td/03-generalities-td.html#exercice-2-différent-scénarios",
    "href": "td/03-generalities-td.html#exercice-2-différent-scénarios",
    "title": "TD: Généralités",
    "section": "Exercice 2 : Différent scénarios",
    "text": "Exercice 2 : Différent scénarios\nPour chacun des scénarios suivants, dire si c’est un problème de classification ou de régression et si c’est un problème d’inférence ou de prédiction, donner le nombre d’observations et l’ensemble mathématique dans lequel ces observations vivent, donner une distance possible pour comparer les observations.\n\nOn voudrait savoir si une personne hospitalisée au Québec à une chance de développer une complication respiratoire. Pour cela, on se rend dans 5 hôpitaux de la province et on demande à 100 patients dans chacun des hôpitaux s’ils ont eu une complication respiratoire, ainsi que leur âge, leur IMC et leur statut vaccinal.\nOn cherche à modéliser le prix de vente des maisons à Québec en fonction de leur surface, du nombres de chambres et de la présence d’un garage. Il y a \\(2000\\) ventes dans la base de données.\nOn s’intéresse à la concentration quotidienne de particules fines (PM2.5) dans l’air à Québec sur une année. De plus, on mesure la température, le vent et l’humidité.\nUn sondage est mené auprès de \\(2000\\) résidents canadiens pour évaluer leur niveau de satisfaction vis-à-vis des services publics (santé, éducation, transport, etc.). On recueille les réponses sur une échelle de 1 (très insatisfait) à 5 (très satisfait), ainsi que des informations démographiques comme l’âge, la province de résidence, le revenu et le niveau d’éducation.\nOn veut identifier l’espèce d’un animal à partir de mesures morphologiques (longeur des pattes, poids, longueur du museau) collectées dans les parcs nationaux. On observe des castors, des ours noirs, des cerfs et des wapitis. En tout, on observe \\(1000\\) animaux."
  },
  {
    "objectID": "td/03-generalities-td.html#exercice-3-une-mise-en-situation",
    "href": "td/03-generalities-td.html#exercice-3-une-mise-en-situation",
    "title": "TD: Généralités",
    "section": "Exercice 3 : Une mise en situation",
    "text": "Exercice 3 : Une mise en situation\nUne chercheuse en sciences politiques souhaite étudier les caractéristiques de certains élus canadiens. Elle a monté une base de données avec \\(151\\) élus. Pour chaque élus, la base de données contient des variables qualitatives (genre, langue, parti politique, …) et ordinales (niveau de scolarité, tranche d’âge au moment de l’élection, …). Elle souhaite visualiser ses données pour mieux comprendre les caractéristiques des élus et les regrouper selon leurs caractéristiques.\nProposer un plan de travail : identifier toutes les étapes de l’analyse (en commençant par la définition de l’objectif) et associer un nombre d’heures approximatif à chaque étape."
  },
  {
    "objectID": "td/03-generalities-td.html#exercice-4-calcul-de-distance",
    "href": "td/03-generalities-td.html#exercice-4-calcul-de-distance",
    "title": "TD: Généralités",
    "section": "Exercice 4 : Calcul de distance",
    "text": "Exercice 4 : Calcul de distance\n\nUn robot commence à la position \\((0, 0)\\). Calculer la distance la plus courte que le robot doive parcourir pour aller à la position \\((8, 6)\\).\nUn taxi New-Yorkais dois aller de l’angle de la 5th avenue et 42nd street (New York public library) à l’angle de la 1st avenue et 114th street (Thomas Jefferson park). Calculer la distance que le taxi va faire.\nUne entreprise de transport utilise les coordonnées GPS pour calculer les distances entre son entrepôt et deux de ses clients. Les coordonnées de l’entrepôt sont \\((10, 15)\\), les coordonnées du premier client sont \\((18, 22)\\) et les coordonnées du deuxième client sont \\((5, 8)\\). Quel est le client le plus proche de l’entrepôt à vol d’oiseau. Est-ce le même client qui est le plus proche de l’entrepôt pour un camion qui suivrait des routes, réparties en grille.\nConsidérons un système de communication qui envoie des messages encodés sur 4 bits. Ce système envoie les messages suivants: “1001” et “1101”. Quelle est la distance de Hamming entre ces deux messages ? Si un système de détection d’erreurs peut détecter des erreurs de 2 bits, est-ce que la différence entre les deux messages est détectable ?\nTrois utilisateurs ont aimé les films suivants: \\(\\{ \\text{Titanic}, \\text{Avatar}, \\text{Star Wars}, \\text{Matrix}, \\text{Inception} \\}\\), \\(\\{ \\text{Avatar}, \\text{Matrix}, \\text{Batman}, \\text{Superman}, \\text{Inception} \\}\\) et \\(\\{ \\text{Inception}, \\text{Avengers}, \\text{Spiderman}, \\text{Superman}, \\text{Batman} \\}\\). Quels sont les utilisateurs les plus proches basés sur la distance de Hamming ?"
  },
  {
    "objectID": "td/04-dimension-td.html",
    "href": "td/04-dimension-td.html",
    "title": "TD: Dimension",
    "section": "",
    "text": "Soit le vecteur aléatoire \\(X = (X_{1}, X_{2}, X_{3}, X_{4})^{\\top}\\) dont l’espérance et la variance sont, respectivement, données par\n\\[\\mu = \\begin{pmatrix}\n  0 \\\\ 1 \\\\ -1 \\\\ 0\n\\end{pmatrix} \\text{ et } \\Sigma = \\begin{pmatrix}\n  9 &  1 & -1 & 2 \\\\\n  1 &  4 & -1 & 1 \\\\\n-1 & -1 & 16 & 0 \\\\\n  2 &  1 &  0 & 9\n\\end{pmatrix}.\\]\nLes valeurs et vecteurs propres (normés) correspondants de \\(\\Sigma\\) sont\n\\[\\begin{align}\n\\lambda_{1} &= 16.27, \\quad & \\alpha_1 &= (0.165, 0.098, -0.980, 0.059)^{\\top}, \\\\\n\\lambda_{2} &= 11.12, \\quad & \\alpha_1 &= (0.665, 0.169, 0.171, 0.707)^{\\top}, \\\\\n\\lambda_{3} &= 6.95, \\quad & \\alpha_1 &= (0.718, -0.017, 0.077, -0.691)^{\\top}, \\\\\n\\lambda_{4} &= 3.67, \\quad & \\alpha_1 &= (0.118, -0.981, -0.070, 0.139)^{\\top}. \\\\\n\\end{align}\\]\n\nTrouvez une combinaison linéaire de \\(X_{1}, X_{2}, X_{3}\\) et \\(X_{4}\\) dont la variance est \\(6.95\\).\nSoit \\(Z = a_{0} + a_{1} X_{1} + a_{2} X_{2} + a_{3} X_{3} + a_{4} X_{4}\\). Trouvez les valeurs de \\(a_{0}, a_{1}, a_{2}, a_{3}, a_{4}\\) telles que \\(Z\\) est de moyenne \\(0\\) et de variance maximale. Quelle est la valeur de cette variance ? Quelle est la covariance entre cette combinaison linéaire et celle trouvée dans la question 1.\nDonnez une matrice diagonale \\(M\\) telle que \\(M \\Sigma M = R\\), où \\(R\\) est la matrice des corrélations de \\(X\\)."
  },
  {
    "objectID": "td/04-dimension-td.html#exercice-1-compréhension-de-lacp",
    "href": "td/04-dimension-td.html#exercice-1-compréhension-de-lacp",
    "title": "TD: Dimension",
    "section": "",
    "text": "Soit le vecteur aléatoire \\(X = (X_{1}, X_{2}, X_{3}, X_{4})^{\\top}\\) dont l’espérance et la variance sont, respectivement, données par\n\\[\\mu = \\begin{pmatrix}\n  0 \\\\ 1 \\\\ -1 \\\\ 0\n\\end{pmatrix} \\text{ et } \\Sigma = \\begin{pmatrix}\n  9 &  1 & -1 & 2 \\\\\n  1 &  4 & -1 & 1 \\\\\n-1 & -1 & 16 & 0 \\\\\n  2 &  1 &  0 & 9\n\\end{pmatrix}.\\]\nLes valeurs et vecteurs propres (normés) correspondants de \\(\\Sigma\\) sont\n\\[\\begin{align}\n\\lambda_{1} &= 16.27, \\quad & \\alpha_1 &= (0.165, 0.098, -0.980, 0.059)^{\\top}, \\\\\n\\lambda_{2} &= 11.12, \\quad & \\alpha_1 &= (0.665, 0.169, 0.171, 0.707)^{\\top}, \\\\\n\\lambda_{3} &= 6.95, \\quad & \\alpha_1 &= (0.718, -0.017, 0.077, -0.691)^{\\top}, \\\\\n\\lambda_{4} &= 3.67, \\quad & \\alpha_1 &= (0.118, -0.981, -0.070, 0.139)^{\\top}. \\\\\n\\end{align}\\]\n\nTrouvez une combinaison linéaire de \\(X_{1}, X_{2}, X_{3}\\) et \\(X_{4}\\) dont la variance est \\(6.95\\).\nSoit \\(Z = a_{0} + a_{1} X_{1} + a_{2} X_{2} + a_{3} X_{3} + a_{4} X_{4}\\). Trouvez les valeurs de \\(a_{0}, a_{1}, a_{2}, a_{3}, a_{4}\\) telles que \\(Z\\) est de moyenne \\(0\\) et de variance maximale. Quelle est la valeur de cette variance ? Quelle est la covariance entre cette combinaison linéaire et celle trouvée dans la question 1.\nDonnez une matrice diagonale \\(M\\) telle que \\(M \\Sigma M = R\\), où \\(R\\) est la matrice des corrélations de \\(X\\)."
  },
  {
    "objectID": "td/04-dimension-td.html#exercice-2-matrice-de-covariance-et-de-corrélation",
    "href": "td/04-dimension-td.html#exercice-2-matrice-de-covariance-et-de-corrélation",
    "title": "TD: Dimension",
    "section": "Exercice 2: Matrice de covariance et de corrélation",
    "text": "Exercice 2: Matrice de covariance et de corrélation\nDémontrer que la matrice de variance-covariance échantillonnnale des données centrées et réduites est égale à la matrice de corrélation échantillonnale des données initiales."
  },
  {
    "objectID": "td/04-dimension-td.html#exercice-3-pratique-de-lacp",
    "href": "td/04-dimension-td.html#exercice-3-pratique-de-lacp",
    "title": "TD: Dimension",
    "section": "Exercice 3: Pratique de l’ACP",
    "text": "Exercice 3: Pratique de l’ACP\nSoit un vecteur de trois variables aléatoires \\(X = (X_{1}, X_{2}, X_{3})^{\\top}\\) dont la matrice de variance est donnée par \\(\\mathrm{Var}(X) = \\Sigma\\). On vous dit que les valeurs propres de \\(\\Sigma\\) sont \\(3, 2\\) et \\(1\\) et que les vecteurs propres normés correspondants sont, respectivement, \\(v_{1} = (0, - 1 / \\sqrt{2}, 1 / \\sqrt{2})^{\\top}\\), \\(v_{2} = (1, 0, 0)^{\\top}\\) et \\(v_{3} = (0, 1 / \\sqrt{2}, 1 / \\sqrt{2})^{\\top}\\).\n\nEn fait, \\(X_{1}, X_{2}\\) et \\(X_{3}\\) représentent, respectivement, la circonférence du poignet droit, un score de capacité pulmonaire et l’indice de masse corporelle des individus d’une certaine population. Les médecins aimeraient résumer ces trois mesures en un seul score, noté \\(C\\), qui est une combinaison linéaire des trois variables originales, i.e. \\(C = c_{1} X_{1} + c_{2} X_{2} + c_{3} X_{3}\\) pour des nombres réels \\(c_{1}, c_{2}\\) et \\(c_{3}\\)) et qui capture le plus de variabilité possible. Trouvez les valeurs de \\(c_{1}, c_{2}\\) et \\(c_{3}\\) telles que \\(\\sum_{j = 1}^{3} c_j^2 = 1\\) et que la variance de \\(C\\) sera maximale. Quelle est la valeur de cette variance maximale ?\nQuelle proportion de la variabilité totale de \\(X_{1}, X_{2}\\) et \\(X_{3}\\) est capturée par \\(C\\) ?\nRetrouvez la matrice \\(\\Sigma\\) à partir des informations données dans l’énoncé de la question."
  },
  {
    "objectID": "td/04-dimension-td.html#exercice-4-taux-de-survie-de-patients",
    "href": "td/04-dimension-td.html#exercice-4-taux-de-survie-de-patients",
    "title": "TD: Dimension",
    "section": "Exercice 4: Taux de survie de patients",
    "text": "Exercice 4: Taux de survie de patients\nLa tableau suivant donne la répartition de patients selon l’issue observée en ligne (survie ou décès codés respectivement 0 et 1) en fonction de l’observation de troubles cognitifs en phase initiale en colonne (absence codée 0, présence de troubles codée 1 et troubles sévères codée 2).\n\n\n\n\nAbscence\nTroubles\nTroubles sévères\nTotal\n\n\n\n\nSurvie\n108\n61\n13\n182\n\n\nDécès\n8\n4\n2\n14\n\n\nTotal\n116\n65\n15\n196\n\n\n\n\nQuelle méthode vous semble la mieux adaptée à l’analyse de ce tableau ?\nDonner le tableau des fréquences relatives.\nDonner le profil moyen des lignes. À quoi correspond-il ?\nDonner le profil moyen des colonnes. À quoi correspond-il ?\nCombien d’axes obtient-on en effectuant l’analyse donnée en question 1 ? Justifier.\nQuel test vous permet de détecter une éventuelle indépendance entre l’issue observée et la présence ou l’absence de troubles cognitifs en phase initiale ? Sachant que l’inertie totale du nuage est égale à 0.0050, donner la valeur de la statistique du test correspondant."
  },
  {
    "objectID": "td/04-dimension-td.html#exercice-5-théorie-de-lacm",
    "href": "td/04-dimension-td.html#exercice-5-théorie-de-lacm",
    "title": "TD: Dimension",
    "section": "Exercice 5: Théorie de l’ACM",
    "text": "Exercice 5: Théorie de l’ACM\n\nParmi les tableaux suivants, quels sont ceux qui peuvent être étudiés par une ACM ? Justifier.\n\n\nUn tableau issu d’une enquête où toutes les questions engendrent des réponses qualitatives.\nUn tableau où les individus réponsent pour chaque question par “trop peu”, “peu”, “assez”, “trop”, “beaucoup trop”.\nUn tableau dans lequel on dénombre le nombre de personnes ayant répondu à deux questions particulières.\nUn tableau contenant diverses caractéristiques d’individus (taille, poids, couleur des yeux, couleur des cheveux, etc.).\n\n\nQuels sont les énoncés vrais dans le cadre de l’ACM ? Justifier.\n\n\nLa tableau de données brut contient autant de colonnes que de variables.\nLe tableau disjonctif complet contient autant de colonnes que de variables.\nLe tableau de Burt contient autant de colonnes que de variables.\nLe tableau disjonctif complet contient autant de colonnes que de modalités (toutes variables confondues).\n\n\nQuels sont les énoncés vrais dans le cadre de l’ACM ? Justifier. La distance entre deux points représentant les individus \\(i\\) et \\(j\\) est\n\n\nd’autant plus petite que ces deux individus prennent à peu près les mêmes modalités.\nd’autant plus grande que les individus prennent des modalités différentes.\nd’autant plus grande que les individus prennent des modalités différentes et que ces modalités sont fréquentes.\nd’autant plus grande que les individus prennent des modalités différentes et que ces modalités sont rares.\n\n\nQuels sont les énoncés vrais dans le cadre de l’ACM ? Justifier.\n\n\nLa distance entre deux points représentant les modalités \\(k\\) et \\(l\\) est d’autant plus petite que ces deux modalités ont en commun un grand nombre d’individus.\nLa distance entre deux points représentant les modalités \\(k\\) et \\(l\\) est d’autant plus grande que ces deux modalités sont rares.\nLa distance entre deux modalités ayant de gros effectifs est nécessairement grande.\nLa distance entre deux modalités rares est nécessairement grande.\n\n\nQuels sont les énoncés vrais dans le cadre de l’ACM ? Justifier.\n\n\nLe graphe biplot fournit une représentation optimale du nuage des individus.\nLe graphe biplot fournit une représentation optimale du nuage des modalités.\nSi un individu est proche d’une modalité alors il prend cette modalité.\nUn individu est du côté des modalités qu’il a choisi.\nUne modalité est du côté des individus qui ont choisi cette modalité."
  },
  {
    "objectID": "informations/materials.html",
    "href": "informations/materials.html",
    "title": "Matériel",
    "section": "",
    "text": "Cette page regroupe différentes ressources utiles pour le cours.\nThe Elements of Statistical Learning: Data Mining, Inference and Prediction, 2nd Edition\nAuteurs: Trevor Hastie, Robert Tibshirani et Jerome Friedman\nÉditeur: Springer (New York, 2009)\nISBN: 0-387-84857-0\nLien: site web des authors\nAn Introduction to Statistical Learning: with application in R, 2nd Edition\nAuteurs: Gareth James, Daniela Witten, Trevor Hastie et Robert Tibshirani\nÉditeur: Springer (New York, 2021)\nISBN: 1-0716-1417-4\nLien: site web des authors\nAn Introduction to Statistical Learning: with application in Python\nAuteurs: Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani et Jonathan Taylor\nÉditeur: Springer (Suisse, 2023)\nISBN: 3-031-38746-3\nLien: site web des authors",
    "crumbs": [
      "Informations",
      "Matériel"
    ]
  },
  {
    "objectID": "informations/description.html",
    "href": "informations/description.html",
    "title": "Description du cours",
    "section": "",
    "text": "Dans ce cours, nous visons à introduire des méthodes qui permettront aux étudiantes et étudiants d’étudier un jeu de données de “haute dimension” (ici, “haute” est pris dans le sens où l’on ne peut pas faire un simple graphique de l’ensemble des variables pour toutes les observations) sans avoir recours à un modèle probabiliste. Les techniques que l’on y enseigne servent à réduire la dimension des données, identifier certains liens entre les variables, visualiser les données ou à diviser le jeu de données en groupes/classes.\nSans négliger la théorie, l’accent sera mis sur l’aspect pratique de l’analyse des données et l’utilisation d’un langage de programmation, que ce soit R, Python ou autre.",
    "crumbs": [
      "Informations",
      "Description"
    ]
  },
  {
    "objectID": "informations/description.html#objectifs",
    "href": "informations/description.html#objectifs",
    "title": "Description du cours",
    "section": "",
    "text": "Dans ce cours, nous visons à introduire des méthodes qui permettront aux étudiantes et étudiants d’étudier un jeu de données de “haute dimension” (ici, “haute” est pris dans le sens où l’on ne peut pas faire un simple graphique de l’ensemble des variables pour toutes les observations) sans avoir recours à un modèle probabiliste. Les techniques que l’on y enseigne servent à réduire la dimension des données, identifier certains liens entre les variables, visualiser les données ou à diviser le jeu de données en groupes/classes.\nSans négliger la théorie, l’accent sera mis sur l’aspect pratique de l’analyse des données et l’utilisation d’un langage de programmation, que ce soit R, Python ou autre.",
    "crumbs": [
      "Informations",
      "Description"
    ]
  },
  {
    "objectID": "informations/description.html#place-du-cours-dans-le-programme",
    "href": "informations/description.html#place-du-cours-dans-le-programme",
    "title": "Description du cours",
    "section": "Place du cours dans le programme",
    "text": "Place du cours dans le programme\nCe cours est généralement suivi par des étudiants du baccalauréat en statistique lors de leur deuxième année de formation. C’est aussi un cours à option en actuariat, en mathématique ainsi que dans certains programmes de génie et d’administration.\nTous les étudiants doivent s’assurer d’avoir suivi au minimum un cours d’algèbre (ex. MAT-1200) et un cours de statistique de base (ex. STT-1000), car la majorité des méthodes d’analyse de données s’appuient sur ces notions.\nLes étudiants devraient aussi avoir une certaine familiarité avec les concepts algorithmiques de base, ainsi qu’avec (au moins) un langage de programmation.",
    "crumbs": [
      "Informations",
      "Description"
    ]
  },
  {
    "objectID": "informations/description.html#objectifs-spécifiques",
    "href": "informations/description.html#objectifs-spécifiques",
    "title": "Description du cours",
    "section": "Objectifs spécifiques",
    "text": "Objectifs spécifiques\nÀ la fin du cours, l’étudiant ou l’étudiante devrait être capable :\n\nde comprendre et décrire les fondements théoriques des méthodes d’analyse de données étudiées;\nd’identifier correctement les situations où l’emploi de ces méthodes est indiqué;\nd’utiliser efficacement un langage de programmation pour mettre en oeuvre ces méthodes;\nd’analyser et d’interpréter judicieusement les résultats découlant de l’analyse;\nde formuler par écrit les conclusions de l’analyse, dans le respect des limites de la méthodologie.",
    "crumbs": [
      "Informations",
      "Description"
    ]
  },
  {
    "objectID": "informations/description.html#équipement-informatique",
    "href": "informations/description.html#équipement-informatique",
    "title": "Description du cours",
    "section": "Équipement informatique",
    "text": "Équipement informatique\nVous pourriez avoir besoin d’un ordinateur, de haut-parleurs ou d’un casque d’écoute, d’un microphone, d’une webcam et d’une connexion Internet avec fil à large bande ou sans fil. Pour vérifier les paramètres de configuration minimaux selon le système d’exploitation, nous vous invitons à visiter cette page.\nDe plus, ce cours peut nécessiter des besoins logiciels particuliers qui seront alors décrits dans d’autres sections du plan de cours, le cas échéant.",
    "crumbs": [
      "Informations",
      "Description"
    ]
  },
  {
    "objectID": "informations/description.html#fonctionnement",
    "href": "informations/description.html#fonctionnement",
    "title": "Description du cours",
    "section": "Fonctionnement",
    "text": "Fonctionnement\nLes cours et examens auront lieu en présentiel les mardi et vendredi matin, mais le matériel sera disponible en ligne. De façon générale, la séance du vendredi sera une séance magistrale et la séance du mardi sera dédiée à des exercices pratiques réalisés de façon autonome par les étudiants avec le soutien de l’enseignant.",
    "crumbs": [
      "Informations",
      "Description"
    ]
  },
  {
    "objectID": "informations/description.html#approches-pédagogiques",
    "href": "informations/description.html#approches-pédagogiques",
    "title": "Description du cours",
    "section": "Approches pédagogiques",
    "text": "Approches pédagogiques\nL’approche pédagogique privilégiée est l’exposé interactif (vendredi) en alternance avec des périodes de laboratoires (mardi). L’approche pédagogique est très axée sur l’apprentissage actif et exige un engagement soutenu des étudiants tout au long de la session.",
    "crumbs": [
      "Informations",
      "Description"
    ]
  },
  {
    "objectID": "informations/general.html",
    "href": "informations/general.html",
    "title": "Informations générales",
    "section": "",
    "text": "Faculté des sciences et de génie\nDépartement de mathématiques et de statistique",
    "crumbs": [
      "Informations",
      "Générales"
    ]
  },
  {
    "objectID": "informations/general.html#stt-2200-analyse-de-données",
    "href": "informations/general.html#stt-2200-analyse-de-données",
    "title": "Informations générales",
    "section": "STT-2200: Analyse de données",
    "text": "STT-2200: Analyse de données\nFormule d’enseignement: Présentiel\nTemps consacré:\n\nCours: 2h\nLaboratoire: 1h\nTravail personnel: 6h\nTotal: 9h\n\nCrédits: 3\nPréalables:\n\nACT-2000 ou STT-1000 ou STT-1300 ou STT-1900\nMAT-1200 ou ACT-2002\nSTT-1100 ou IFT-4902 ou ECN-2090 ou GLO-1901 ou IFT-1004\n\nPlage horaire:\n\n\n\nType\nJour\nHoraire\nSalle\n\n\n\n\nLabo\nMardi\n10h30-12h20\nPLT-3928\n\n\nClasse\nVendredi\n08h30-10h20\nVCH-3830",
    "crumbs": [
      "Informations",
      "Générales"
    ]
  },
  {
    "objectID": "informations/general.html#coordonnées-et-disponibilités",
    "href": "informations/general.html#coordonnées-et-disponibilités",
    "title": "Informations générales",
    "section": "Coordonnées et disponibilités",
    "text": "Coordonnées et disponibilités\nEnseignant: Steven Golovkine\nLocal: VCH-2209\nE-mail: steven.golovkine@mat.ulaval.ca\nDisponibilités: Je suis disponible pour vous rencontrer sur rendez-vous par couriel ou sans rendez-vous du lundi au jeudi entre 8h et 10h.\n\nL’Université reconnaît le droit à la déconnexion des professeures et professeurs, des personnes chargées de cours et des autres membres du personnel enseignant. Cela signifie que ces personnes ne sont pas tenues de consulter les messages qui leur sont envoyés (courriel, boîte vocale, message dans un forum, etc.) pendant les soirs, fins de semaine, jours fériés et vacances. La personne qui aura envoyé un message durant ces périodes devra donc s’attendre à recevoir une réponse dans un délai raisonnable, calculé à partir de la reprise des heures normales de travail.",
    "crumbs": [
      "Informations",
      "Générales"
    ]
  },
  {
    "objectID": "informations/evaluations.html",
    "href": "informations/evaluations.html",
    "title": "Évaluations",
    "section": "",
    "text": "Voici les différents examens prévus pour ce module.\n\n\n\n\n\n\n\n\n\nTitre\nDate\nMode de travail\nPondération\n\n\n\n\nExamen 1\n24 oct. 2025\nIndividuel\n30%\n\n\nExamen 2\n12 déc. 2025\nIndividuel\n50%\n\n\nProjet d’analyse de données\n02 déc. 2025\nEn équipe\n20%\n\n\n\n\nIdentification. Lors d’un examen, une carte d’identité avec photo admissible doit être déposée sur le coin de votre table. Les cartes admissibles sont la carte de l’Université Laval en plastique, un permis de conduire canadien, une carte d’assurance-maladie avec photo émise par une province canadienne ou un passeport canadien ou étranger.",
    "crumbs": [
      "Informations",
      "Évaluations"
    ]
  },
  {
    "objectID": "informations/evaluations.html#modalités-dévaluations",
    "href": "informations/evaluations.html#modalités-dévaluations",
    "title": "Évaluations",
    "section": "",
    "text": "Voici les différents examens prévus pour ce module.\n\n\n\n\n\n\n\n\n\nTitre\nDate\nMode de travail\nPondération\n\n\n\n\nExamen 1\n24 oct. 2025\nIndividuel\n30%\n\n\nExamen 2\n12 déc. 2025\nIndividuel\n50%\n\n\nProjet d’analyse de données\n02 déc. 2025\nEn équipe\n20%\n\n\n\n\nIdentification. Lors d’un examen, une carte d’identité avec photo admissible doit être déposée sur le coin de votre table. Les cartes admissibles sont la carte de l’Université Laval en plastique, un permis de conduire canadien, une carte d’assurance-maladie avec photo émise par une province canadienne ou un passeport canadien ou étranger.",
    "crumbs": [
      "Informations",
      "Évaluations"
    ]
  },
  {
    "objectID": "informations/evaluations.html#informations-détaillées-sur-les-évaluations",
    "href": "informations/evaluations.html#informations-détaillées-sur-les-évaluations",
    "title": "Évaluations",
    "section": "Informations détaillées sur les évaluations",
    "text": "Informations détaillées sur les évaluations\nExamen 1\nDate et lieu: 24 octobre 2025 de 08h30 à 09h30\nMode de travail: Individuel\nPondération: 30%\nRemise de l’évaluation: Remise en main propre à l’enseignant\nDirectives de l’évaluation: Tout le contenu du cours (diapos, présentations en classe, lectures, …) est susceptible d’être à l’examen.\nMatériel autorisé: Tout matériel non-connecté à internet est autorisé. Calculatrice autorisée. L’utilisation d’outils d’IA n’est PAS autorisée pour cette examen.\nInformation supplementaire: lien.\nExamen 2\nDate et lieu: 12 décembre 2025 de 08h30 à 10h20\nMode de travail: Individuel\nPondération: 50%\nRemise de l’évaluation: Remise en main propre à l’enseignant\nDirectives de l’évaluation: Tout le contenu du cours (diapos, présentations en classe, lectures, …) est susceptible d’être à l’examen.\nMatériel autorisé: Tout matériel non-connecté à internet est autorisé. Calculatrice autorisée. L’utilisation d’outils d’IA n’est PAS autorisée pour cette examen.\nProjet d’analyse de données\nDate et lieu: dû le 02 décembre 2025 à 17h00\nMode de travail: En équipe\nPondération: 20%\nRemise de l’évaluation: Via monPortail\nDirectives de l’évaluation: Tout retard sur la remise du projet entrainera la note de 0 pour cette partie. Il y aura une présentation des posters le 9 décembre entre 10h30 et 12h30. Les modalités exactes des présentations restent à définir. Dans le cas où l’affichage des posters ne puissent pas se faire au pavillon VCH, les présentations se feront dans la salle de classe PLT-3928. Le projet d’analyse de données consiste en la création d’un poster présentant une analyse de données. Le choix du sujet et de la problématique est laissé à votre discrétion. Il est recommandé d’utiliser les méthodes présentées en cours, mais ce n’est pas obligatoire. Dans tous les cas, attendez vous à avoir des questions sur les méthodes que vous présentez. Je suis bien entendu à votre disposition pour répondre à toutes questions (où trouver des données, comment faire un poster, …). Merci de déposer un dossier .zip connant votre projet. Ce dossier doit contenir votre poster au format .pdf, votre code, ainsi que vos données. Si la taille du fichier de vos données est trop importante, merci de m’envoyer un email. Je m’occuperai de l’impression des posters.\nMatériel autorisé : L’utilisation d’outils d’IA est autorisée pour cette examen. Il convient cependant de proprement documenter son utilisation.",
    "crumbs": [
      "Informations",
      "Évaluations"
    ]
  },
  {
    "objectID": "informations/evaluations.html#échelle-des-cotes",
    "href": "informations/evaluations.html#échelle-des-cotes",
    "title": "Évaluations",
    "section": "Échelle des cotes",
    "text": "Échelle des cotes\n\n\n\nCote\n% minimum\n% maximum\n\n\n\n\nA+\n92\n100\n\n\nA\n88\n91.99\n\n\nA-\n84\n87.99\n\n\nB+\n80\n83.99\n\n\nB\n75\n79.99\n\n\nB-\n70\n74.99\n\n\nC+\n65\n69.99\n\n\nC\n60\n64.99\n\n\nC-\n55\n59.99\n\n\nD+\n52\n54.99\n\n\nD\n50\n51.99\n\n\nE\n0\n49.99",
    "crumbs": [
      "Informations",
      "Évaluations"
    ]
  },
  {
    "objectID": "informations/evaluations.html#détails-sur-les-modalités-dévaluation",
    "href": "informations/evaluations.html#détails-sur-les-modalités-dévaluation",
    "title": "Évaluations",
    "section": "Détails sur les modalités d’évaluation",
    "text": "Détails sur les modalités d’évaluation\nConformément à la politique du Département de mathématiques et de statistique en matière d’amélioration et de consolidation de la connaissance du français, la qualité de l’écrit sera sanctionnée dans tous les travaux et examens. Un maximum de 10% des points pourra être enlevé pour la qualité de la langue et de la rédaction.\nAucun retard n’est accepté pour la remise des travaux. Un retard entraîne automatiquement une note de zéro.\nToute reprise d’évaluation accordée par la politique de reprise d’évaluation du Département aura lieu lors des dates de reprises officielles de la faculté. Pour toute demande de révision de note, vous devez suivre la procédure du chapitre 4 du Réglement des études. Vous trouverez un formulaire à remplir à l’adresse suivante: lien.\nLes séances d’examen de reprise FSG pour la session d’automne 2025 auront lieu :\n\nLe jeudi 30 octobre 2025, local à déterminer, de 9h à 12h et/ou de 13h30 à 16h30.\nLe jeudi 18 décembre 2025, local à déterminer, de 9h à 12h et/ou de 13h30 à 16h30.",
    "crumbs": [
      "Informations",
      "Évaluations"
    ]
  },
  {
    "objectID": "informations/evaluations.html#absence-à-une-activité-obligatoire",
    "href": "informations/evaluations.html#absence-à-une-activité-obligatoire",
    "title": "Évaluations",
    "section": "Absence à une activité obligatoire",
    "text": "Absence à une activité obligatoire\nCes modalités s’appliquent en vertu des articles 4.41 et 4.42 du Réglement des études de l’Université Laval:\n\nArticle 4.41: Tout défaut de se soumettre à une activité d’évaluation entraîne la note de zéro pour cette activité d’évaluation, à moins que l’étudiante ou l’étudiant ne démontre que cette omission est attribuable à des motifs sérieux.\nArticle 4.42: La reprise d’une évaluation est possible pour des motifs sérieux. Elle se fait selon les modalités prévues par l’unité responsable de l’activité de formation.\n\nLa reprise d’une évaluation peut donc execptionnellement être autorisée pour des motifs jugés sérieux, dans la mesure où la procédure décrite ci-dessous est respectée.\nMotifs d’absence jugés sérieux\nLes motifs suivants sont jugés sérieux et donc acceptables pour demander une reprise d’évaluation:\n\nmaladie ou accident empêchant de se déplacer;\nhospitalisation;\nmaladie grave ou décès d’un proche;\nparticipation à une activité sportive de haut niveau;\nconvocation en cour de justice.\n\nProcédure à suivre\nDès que possible et au plus tard cinq (5) jours ouvrables après la date de l’évaluation (ou dans certains cas, avant la date de l’évaluation, dès que le motif sera connu), l’étudiante ou l’étudiant qui veut faire une demande de reprise d’évaluation doit remplir et soumettre le formulaire électronique “Demande de reprise d’une évaluation” en prenant soin d’y joindre les pièces justificatives requises.\nPour avoir plus de détail sur les procédures à suivre et les motifs sérieux pouvant donner droit à une reprise d’évaluation, consulter le document “Modalités et procédure de reprise d’une évaluation sommative à la Faculté des sciences et de génie” disponible sur le site web de la FSG.\nDans certains cas, la ou le responsable du cours pourrait adopter une procédure simplifiée de gestion des demandes de reprises d’évaluation, tout en respectant les critères décrits dans cette politique. Dans ces cas, des explications particulières seront données à cet effet dans le plan de cours et présentées lors de la première séance.",
    "crumbs": [
      "Informations",
      "Évaluations"
    ]
  },
  {
    "objectID": "informations/evaluations.html#politique-sur-la-sécurité-dans-les-laboratoires-informatiques",
    "href": "informations/evaluations.html#politique-sur-la-sécurité-dans-les-laboratoires-informatiques",
    "title": "Évaluations",
    "section": "Politique sur la sécurité dans les laboratoires informatiques",
    "text": "Politique sur la sécurité dans les laboratoires informatiques\nPour utiliser les laboratoires informatiques, il faut avoir pris connaissance de la Politique de sécurité et vous engagez à vous y conformer : lien.\nPour toute assistance technique, veuillez écrire à [aide@fsg.ulaval.ca]. En cas d’urgence, composez le 418-656-5555, poste 555 ou utilisez un téléphone rouge situé à proximité.",
    "crumbs": [
      "Informations",
      "Évaluations"
    ]
  },
  {
    "objectID": "informations/evaluations.html#politique-sur-lutilisation-dappareils-électroniques",
    "href": "informations/evaluations.html#politique-sur-lutilisation-dappareils-électroniques",
    "title": "Évaluations",
    "section": "Politique sur l’utilisation d’appareils électroniques",
    "text": "Politique sur l’utilisation d’appareils électroniques\nLa politique sur l’utilisation d’appareils électroniques de la Faculté des Sciences et de Génie peut être consultée à l’adresse: lien.",
    "crumbs": [
      "Informations",
      "Évaluations"
    ]
  },
  {
    "objectID": "informations/evaluations.html#politique-sur-le-plagiat-et-la-fraude-académique",
    "href": "informations/evaluations.html#politique-sur-le-plagiat-et-la-fraude-académique",
    "title": "Évaluations",
    "section": "Politique sur le plagiat et la fraude académique",
    "text": "Politique sur le plagiat et la fraude académique\nRègles disciplinaires\nTout étudiant qui commet une infraction au Règlement disciplinaire à l’intention des étudiants de l’Université Laval dans le cadre du présent cours, notamment en matière de plagiat, est passible des sanctions qui sont prévues dans ce règlement. Il est très important pour tout étudiant de prendre connaissance des articles 23 à 46 du Règlement disciplinaire. Celui-ci peut être consulté à l’adresse suivante: lien.\nPlagiat\nTout étudiant est tenu de respecter les règles relatives au plagiat. Constitue notamment du plagiat le fait de:\n\ncopier textuellement un ou plusieurs passages provenant d’un ouvrage sous format papier ou électronique sans mettre ces passages entre guillemets et sans en mentionner la source;\nrésumer l’idée originale d’un auteur en l’exprimant dans ses propres mots (paraphraser) sans en mentionner la source;\ntraduire partiellement ou totalement un texte sans en mentionner la provenance;\nremettre un travail copié d’un autre étudiant (avec ou sans l’accord de cet autre étudiant);\nremettre un travail téléchargé d’un site d’achat ou d’échange de travaux scolaires.\n\nL’Université Laval étant abonnée à un service de détection de plagiat, il est possible que l’enseignant soumette vos travaux pour analyse.",
    "crumbs": [
      "Informations",
      "Évaluations"
    ]
  },
  {
    "objectID": "informations/evaluations.html#étudiants-ayant-une-situation-de-handicap-liée-à-une-limitation-fonctionnelle",
    "href": "informations/evaluations.html#étudiants-ayant-une-situation-de-handicap-liée-à-une-limitation-fonctionnelle",
    "title": "Évaluations",
    "section": "Étudiants ayant une situation de handicap liée à une limitation fonctionnelle",
    "text": "Étudiants ayant une situation de handicap liée à une limitation fonctionnelle\nAfin de bénéficier de mesures d’accommodement pour les cours ou les examens, un rendez-vous avec une conseillère ou un conseiller du Centre d’aide aux étudiants travaillant en Accueil et soutien aux étudiants en situation de handicap (ACSESH) est nécessaire. Pour ce faire, les étudiants présentant une situation de handicap liée à une limitation fonctionnelle permanente doivent visiter le site “Accommodement” et prendre un rendez-vous, le plus tôt possible.\nAu cours de la semaine qui suit l’autorisation des mesures, leur activation doit être effectuée dans “Accommodement” pour assurer leur mise en place.\nLes étudiants ayant déjà obtenu des mesures d’accommodements scolaires doivent procéder à l’activation de leurs mesures pour les cours et/ou les examens dans “Accommodement” afin que celles-ci puissent être mises en place. Il est à noter que l’activation doit s’effectuer au cours de deux premières semaines de cours.\nLes étudiants concernés recevront par la suite des facultés et départements responsables de leurs cours les informations détaillées sur les modalités permettant d’appliquer les mesures d’accommodement identifiées.",
    "crumbs": [
      "Informations",
      "Évaluations"
    ]
  },
  {
    "objectID": "informations/mcq.html",
    "href": "informations/mcq.html",
    "title": "QCM",
    "section": "",
    "text": "Le premier examen est un questionaire à choix multiples avec quantification de l’incertitude.\nL’ utilisation de degrés de certitude permet à l’étudiant et à l’étudiante d’évaluer le niveau de connaissance de la réponse donnée. La barème des notes, basé sur la théorie des décision (Leclercq et al. 1993), peut sembler bizarre mais il a été fait de tel manière que:\n\ndire la vérité soit la stratégie qui rapporte le plus de points;\nceux qui s’auto-évaluent bien gagnent plus de points que si l’on appliquait un barème correctif tenant compte des probabilités d’avoir la réponse correcte aléatoirement.\n\n\n\n\nSi vous condidérez que votre réponse a une probabilité d’être correcte comprise entre…\n\n\nChoisissez le degré de certitude…\n\n\nVous obtiendrez les points suivants en case de réponse…\n\n\n\n\n\n\n\n\ncorrecte\n\n\nincorrecte\n\n\n\n\n0% et 25%\n\n\n0\n\n\n+13\n\n\n+4\n\n\n\n\n25% et 50%\n\n\n1\n\n\n+16\n\n\n+3\n\n\n\n\n50% et 70%\n\n\n2\n\n\n+17\n\n\n+2\n\n\n\n\n70% et 85%\n\n\n3\n\n\n+18\n\n\n0\n\n\n\n\n85% et 95%\n\n\n4\n\n\n+19\n\n\n-6\n\n\n\n\n95% et 100%\n\n\n5\n\n\n+20\n\n\n-10\n\n\n\nGénéralement, les étudiants s’auto-estiment avec réalisme, et sont avantagés par les degrés de certitude. Leur score obtenu est meilleur que s’il avait été calculé uniquement sur la base du nombre de réponses justes.",
    "crumbs": [
      "Informations",
      "Évaluations",
      "QCM"
    ]
  },
  {
    "objectID": "informations/mcq.html#information-sur-le-premier-examen",
    "href": "informations/mcq.html#information-sur-le-premier-examen",
    "title": "QCM",
    "section": "",
    "text": "Le premier examen est un questionaire à choix multiples avec quantification de l’incertitude.\nL’ utilisation de degrés de certitude permet à l’étudiant et à l’étudiante d’évaluer le niveau de connaissance de la réponse donnée. La barème des notes, basé sur la théorie des décision (Leclercq et al. 1993), peut sembler bizarre mais il a été fait de tel manière que:\n\ndire la vérité soit la stratégie qui rapporte le plus de points;\nceux qui s’auto-évaluent bien gagnent plus de points que si l’on appliquait un barème correctif tenant compte des probabilités d’avoir la réponse correcte aléatoirement.\n\n\n\n\nSi vous condidérez que votre réponse a une probabilité d’être correcte comprise entre…\n\n\nChoisissez le degré de certitude…\n\n\nVous obtiendrez les points suivants en case de réponse…\n\n\n\n\n\n\n\n\ncorrecte\n\n\nincorrecte\n\n\n\n\n0% et 25%\n\n\n0\n\n\n+13\n\n\n+4\n\n\n\n\n25% et 50%\n\n\n1\n\n\n+16\n\n\n+3\n\n\n\n\n50% et 70%\n\n\n2\n\n\n+17\n\n\n+2\n\n\n\n\n70% et 85%\n\n\n3\n\n\n+18\n\n\n0\n\n\n\n\n85% et 95%\n\n\n4\n\n\n+19\n\n\n-6\n\n\n\n\n95% et 100%\n\n\n5\n\n\n+20\n\n\n-10\n\n\n\nGénéralement, les étudiants s’auto-estiment avec réalisme, et sont avantagés par les degrés de certitude. Leur score obtenu est meilleur que s’il avait été calculé uniquement sur la base du nombre de réponses justes.",
    "crumbs": [
      "Informations",
      "Évaluations",
      "QCM"
    ]
  },
  {
    "objectID": "informations/cours.html",
    "href": "informations/cours.html",
    "title": "Cours",
    "section": "",
    "text": "Voici les enregistrements des cours pour le semestre Automne 2025:\n\nCours du 02 septembre 2025: lien.",
    "crumbs": [
      "Informations",
      "Cours"
    ]
  },
  {
    "objectID": "informations/schedule.html",
    "href": "informations/schedule.html",
    "title": "Plan",
    "section": "",
    "text": "Le plan est donné à titre indicatif.\n\n\n\nSemaine\nPlan\nÉvaluations\n\n\n\n\n1er septembre\nPrésentation du cours et rappels\n\n\n\n8 septembre\nRappels\n\n\n\n15 septembre\nGénéralités\n\n\n\n22 septembre\nGénéralités\n\n\n\n29 septembre\nACP\n\n\n\n6 octobre\nAFC\n\n\n\n13 octobre\nACM\n\n\n\n20 octobre\nAnalyse discriminante\nExamen intra\n\n\n27 octobre\nSemaine de lecture\nExamen de reprise\n\n\n3 novembre\nArbres\n\n\n\n10 novembre\nModèles ensemblistes\n\n\n\n17 novembre\n\\(k\\)-moyennes\n\n\n\n24 novembre\nMéthodes hiérarchiques + Modèles de mélanges\n\n\n\n1er décembre\nRévision + Fête de l’université\nRemise du poster\n\n\n8 décembre\n\nPrésentation du poster + Examen final\n\n\n15 décembre\n\nExamen de reprise",
    "crumbs": [
      "Informations",
      "Plan"
    ]
  },
  {
    "objectID": "td/02-revision-td.html",
    "href": "td/02-revision-td.html",
    "title": "TD: Révision",
    "section": "",
    "text": "Dans ce problème, on cherche à modéliser la répartition des vélos en libre service dans les différentes stations de l’université Laval. Le campus comporte cinq stations pouvant acceuillir un total de \\(136\\) vélos (lien): Abitibi-Prince (ABP), Alphone-Desjardins (ADJ), Charles-De Koninck (DKN), Ferdinand-Vandry (VND) et PEPS.\nSupposons que tous les vélos sont retournés à une des cinq stations à la fin de la journée, i.e. pour chaque jour, il y a un moment dans la journée (par exemple, à minuit), où tous les vélos sont à une certaine station. Nous pouvons donc nous intéresser à ces stations à ce moment de la journée pour chaque jour. Nous cherchons à modéliser les movements des vélos de minuit pour un jour donné jusqu’à minuit du jour suivant. Nous trouvons que :\n\npour les vélos empruntés à ABP, \\(50\\%\\) y retourne, \\(10\\%\\) vont à ADJ, \\(20\\%\\) vont à DKN et \\(20\\%\\) vont à VND.\npour les vélos empruntés à ADJ, \\(30\\%\\) y retourne, \\(10\\%\\) vont à ABP, \\(10\\%\\) vont à DKN, \\(10\\%\\) vont à VND et \\(40\\%\\) vont au PEPS.\npour les vélos empruntés à DKN, \\(80\\%\\) y retourne, \\(5\\%\\) vont à ADJ, et \\(15\\%\\) vont au PEPS.\npour les vélos empruntés à VND, \\(50\\%\\) y retourne, \\(20\\%\\) vont à ABP, \\(5\\%\\) vont à ADJ, \\(15\\%\\) vont à DKN et \\(10\\%\\) vont au PEPS.\npour les vélos empruntés au PEPS, \\(0\\%\\) y retourne, \\(25\\%\\) vont à ABP, \\(25\\%\\) vont à ADJ, \\(25\\%\\) vont à DKN et \\(25\\%\\) vont à VND.\n\n\nFaire un graphe de la situation.\nÉcrire la matrice \\(T\\) tel que chaque entrée \\(t_{ij}\\) corresponde à la probabilité de passer de la station \\(i\\) à la station \\(j\\), \\(i, j \\in \\{ \\text{ABP}, \\text{ADJ}, \\text{DKN}, \\text{VND}, \\text{PEPS} \\}\\). On appelle cette matrice, la matrice de transition.\nQuelle est la probabilité qu’un vélo soit à la station PEPS au jour 2 sachant qu’il était à la station ADJ au début ?\nSupposons qu’il y ait \\(20\\) vélos à ABP, \\(35\\) vélos à ADJ, \\(26\\) vélos à DKN, \\(45\\) à VND et \\(10\\) au PEPS. En utilisant une diagonalisation de la matrice \\(T\\), donner la répartition des vélos après \\(10\\) jours."
  },
  {
    "objectID": "td/02-revision-td.html#exercice-1-un-problème-de-vélo",
    "href": "td/02-revision-td.html#exercice-1-un-problème-de-vélo",
    "title": "TD: Révision",
    "section": "",
    "text": "Dans ce problème, on cherche à modéliser la répartition des vélos en libre service dans les différentes stations de l’université Laval. Le campus comporte cinq stations pouvant acceuillir un total de \\(136\\) vélos (lien): Abitibi-Prince (ABP), Alphone-Desjardins (ADJ), Charles-De Koninck (DKN), Ferdinand-Vandry (VND) et PEPS.\nSupposons que tous les vélos sont retournés à une des cinq stations à la fin de la journée, i.e. pour chaque jour, il y a un moment dans la journée (par exemple, à minuit), où tous les vélos sont à une certaine station. Nous pouvons donc nous intéresser à ces stations à ce moment de la journée pour chaque jour. Nous cherchons à modéliser les movements des vélos de minuit pour un jour donné jusqu’à minuit du jour suivant. Nous trouvons que :\n\npour les vélos empruntés à ABP, \\(50\\%\\) y retourne, \\(10\\%\\) vont à ADJ, \\(20\\%\\) vont à DKN et \\(20\\%\\) vont à VND.\npour les vélos empruntés à ADJ, \\(30\\%\\) y retourne, \\(10\\%\\) vont à ABP, \\(10\\%\\) vont à DKN, \\(10\\%\\) vont à VND et \\(40\\%\\) vont au PEPS.\npour les vélos empruntés à DKN, \\(80\\%\\) y retourne, \\(5\\%\\) vont à ADJ, et \\(15\\%\\) vont au PEPS.\npour les vélos empruntés à VND, \\(50\\%\\) y retourne, \\(20\\%\\) vont à ABP, \\(5\\%\\) vont à ADJ, \\(15\\%\\) vont à DKN et \\(10\\%\\) vont au PEPS.\npour les vélos empruntés au PEPS, \\(0\\%\\) y retourne, \\(25\\%\\) vont à ABP, \\(25\\%\\) vont à ADJ, \\(25\\%\\) vont à DKN et \\(25\\%\\) vont à VND.\n\n\nFaire un graphe de la situation.\nÉcrire la matrice \\(T\\) tel que chaque entrée \\(t_{ij}\\) corresponde à la probabilité de passer de la station \\(i\\) à la station \\(j\\), \\(i, j \\in \\{ \\text{ABP}, \\text{ADJ}, \\text{DKN}, \\text{VND}, \\text{PEPS} \\}\\). On appelle cette matrice, la matrice de transition.\nQuelle est la probabilité qu’un vélo soit à la station PEPS au jour 2 sachant qu’il était à la station ADJ au début ?\nSupposons qu’il y ait \\(20\\) vélos à ABP, \\(35\\) vélos à ADJ, \\(26\\) vélos à DKN, \\(45\\) à VND et \\(10\\) au PEPS. En utilisant une diagonalisation de la matrice \\(T\\), donner la répartition des vélos après \\(10\\) jours."
  },
  {
    "objectID": "td/02-revision-td.html#exercice-2-paradoxe-des-deux-enfants",
    "href": "td/02-revision-td.html#exercice-2-paradoxe-des-deux-enfants",
    "title": "TD: Révision",
    "section": "Exercice 2: Paradoxe des deux enfants",
    "text": "Exercice 2: Paradoxe des deux enfants\nOn cherche à modéliser les probabilités d’avoir des enfants d’un certain sexe. Dans toutes les questions, les enfants sont soit de sexe masculin, soit de sexe féminin, de façon équiprobable.\n\nM. Gagnon a deux enfants. L’enfant aîné est une fille. Quelle est la probabilité que son deuxième enfant soit aussi une fille ?\nMme Tremblay a deux enfants. On lui pose la question suivante : “Avez-vous au moins un garçon ?” et elle répond : “Oui”. Quelle est la probabilité que les deux enfants soient des garçons ?\nEst-ce que la probabilité de Mme Tremblay d’avoir deux garçons change si on lui pose la question suivante : “Indiquez moi le sexe de l’un de vos enfants.” et qu’elle répond : “J’ai (au moins) un garçon.” ?\nOn croise M. Gagnon dans la rue en train de se balader avec sa fille. On lui demande quel jour de la semaine elle est née. Elle nous répond: “Vendredi”. Quelle est la probabilité que son deuxième enfant soit aussi une fille ?"
  },
  {
    "objectID": "td/02-revision-td.html#exercice-3-la-loi-dohm",
    "href": "td/02-revision-td.html#exercice-3-la-loi-dohm",
    "title": "TD: Révision",
    "section": "Exercice 3: La loi d’Ohm",
    "text": "Exercice 3: La loi d’Ohm\nOn cherche à calculer la valeur d’un résistance. Pour cela, on lui envoie un courant électrique (intensité en ampères, A) et on mesure la différence de potentiel entre les bornes de la résistance (tension en volt, V). On trouve les valeurs suivantes :\n\n\n\nIntensité (A)\nVoltage (V)\n\n\n\n\n0.2\n4.0\n\n\n0.5\n10.4\n\n\n0.9\n18.7\n\n\n1.0\n21.1\n\n\n1.2\n25.1\n\n\n1.3\n27.4\n\n\n1.8\n37.8\n\n\n\n\nCalculer l’intensité moyenne et le voltage moyen.\nCalculer les variances de l’intensité et du voltage.\nCalculer la covariance entre l’intensité et le voltage.\nLa loi d’Ohm nous dit que l’intensité et le voltage sont proportionnelle suivant la relation \\(V = R A\\) où \\(R\\) est la valeur de la résistance en Ohm. Cette relation est équivalente à estimer un modèle linéaire entre l’intensité et le voltage. La résistance \\(R\\) est donc donnée par le ratio entre la covariance entre l’intensité et le voltage et la variance de l’intensité (cf. cours de régression pour une preuve). Calculer la valeur de \\(R\\).\nEn reprenant les valeurs du tableau, vérifier la valeur de \\(R\\). Pourquoi est-ce que l’on ne retrouve pas exactement les valeurs données ?"
  },
  {
    "objectID": "tp/06-unsupervised-tp.html",
    "href": "tp/06-unsupervised-tp.html",
    "title": "TP: Non-supervisée",
    "section": "",
    "text": "Vous pouvez faire les exercices dans le langage de votre choix."
  },
  {
    "objectID": "tp/07-ethics-tp.html",
    "href": "tp/07-ethics-tp.html",
    "title": "TP: Dimension",
    "section": "",
    "text": "Vous pouvez faire les exercices dans le langage de votre choix."
  },
  {
    "objectID": "tp/05-supervised-tp.html",
    "href": "tp/05-supervised-tp.html",
    "title": "TP: Supervisée",
    "section": "",
    "text": "Vous pouvez faire les exercices dans le langage de votre choix."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STT-2200: Analyse de données",
    "section": "",
    "text": "Bienvenue à tous au cours STT-2200 de l’automne 2025.\nVous trouverez sur ce site toute les informations nécessaire pour les cours, les différents travaux et les examens. Les contenus seront mis à jour au cours de la session avec les corrigés de certains travaux. Je vous invite à regarder les différents modules avant les cours.\nCe site a été créé en s’aidant des notes de cours de Aurélien Nicosia et de Thierry Duchesne.\nCe site a été créé avec Quarto. Des larges modèles de langages (LLM), ChatGPT et Claude, ont été utilisés pour améliorer la fluidité des notes, pour corriger les typos et pour la mise en forme des figures. Toutes erreurs restantes me sont strictement et directement imputable.\n\n\n\nData trap (xkcd:2582)."
  },
  {
    "objectID": "slides/05-supervised-ensemble-slides.html#principe-fondamental",
    "href": "slides/05-supervised-ensemble-slides.html#principe-fondamental",
    "title": "Supervisée",
    "section": "Principe fondamental",
    "text": "Principe fondamental\n\nIdée centrale Combiner les prédictions de plusieurs modèles simples (classificateurs faibles) plutôt qu’un unique modèle complexe\n\n\nMeilleure performance globale\nPlus grande robustesse au bruit\nRéduction du sur-ajustement\nStabilité des prédictions améliorée"
  },
  {
    "objectID": "slides/05-supervised-ensemble-slides.html#stratégies-de-combinaison",
    "href": "slides/05-supervised-ensemble-slides.html#stratégies-de-combinaison",
    "title": "Supervisée",
    "section": "Stratégies de combinaison",
    "text": "Stratégies de combinaison\n\nMoyenne des probabilités prédites par chaque modèle\nVote de majorité : Classe la plus souvent prédite\nSélection de modèles : Analogie avec la sélection de variables\nMéthodes automatisées : Bagging, boosting, forêts aléatoires"
  },
  {
    "objectID": "slides/05-supervised-ensemble-slides.html#bootstrap-aggregating",
    "href": "slides/05-supervised-ensemble-slides.html#bootstrap-aggregating",
    "title": "Supervisée",
    "section": "Bootstrap Aggregating",
    "text": "Bootstrap Aggregating\n\nPrincipe Construire plusieurs versions d’un même modèle sur des échantillons différents\n\n\nGénérer \\(B\\) échantillons bootstrap (tirage avec remise)\nConstruire un arbre pour chaque échantillon\nObtenir \\(B\\) arbres de classification\nPrédire par vote de majorité ou moyenne des probabilités"
  },
  {
    "objectID": "slides/05-supervised-ensemble-slides.html#avantages-et-évaluation",
    "href": "slides/05-supervised-ensemble-slides.html#avantages-et-évaluation",
    "title": "Supervisée",
    "section": "Avantages et évaluation",
    "text": "Avantages et évaluation\n\nAvantages principaux - Réduction de la variance des prédictions - Augmentation de la stabilité\n\n\nInconvénient Perte d’interprétabilité des variables individuelles\n\n\nÉvaluation de l’importance des variables - Sommer les réductions d’indice de Gini par variable - Moyenner sur tous les arbres - Exprimer en pourcentage relatif"
  },
  {
    "objectID": "slides/05-supervised-ensemble-slides.html#extension-du-bagging",
    "href": "slides/05-supervised-ensemble-slides.html#extension-du-bagging",
    "title": "Supervisée",
    "section": "Extension du bagging",
    "text": "Extension du bagging\n\nProblème du bagging Les arbres se ressemblent (dominance des classes fréquentes)\n\n\nSolution des forêts aléatoires Introduction d’une source supplémentaire d’aléa\n\n\nMécanisme À chaque division, sélectionner aléatoirement \\(m\\) variables parmi \\(p\\) disponibles\nChoix usuel: \\(m \\approx \\sqrt{p}\\)"
  },
  {
    "objectID": "slides/05-supervised-ensemble-slides.html#bénéfices-de-la-randomisation",
    "href": "slides/05-supervised-ensemble-slides.html#bénéfices-de-la-randomisation",
    "title": "Supervisée",
    "section": "Bénéfices de la randomisation",
    "text": "Bénéfices de la randomisation\n\nDécorrélation des arbres construits\nDiversification des modèles\nAmélioration de la performance globale\nRobustesse accrue\n\n\nRésultat Meilleure généralisation que le bagging simple"
  },
  {
    "objectID": "slides/05-supervised-ensemble-slides.html#approche-séquentielle",
    "href": "slides/05-supervised-ensemble-slides.html#approche-séquentielle",
    "title": "Supervisée",
    "section": "Approche séquentielle",
    "text": "Approche séquentielle\n\nPhilosophie opposée au bagging Construction séquentielle au lieu de parallèle\n\n\nPrincipe 1. Construire une série de modèles simples 2. Accorder progressivement plus d’importance aux erreurs 3. Corriger les erreurs du modèle précédent 4. Atteindre une très haute précision globale"
  },
  {
    "objectID": "slides/05-supervised-ensemble-slides.html#condition-de-succès",
    "href": "slides/05-supervised-ensemble-slides.html#condition-de-succès",
    "title": "Supervisée",
    "section": "Condition de succès",
    "text": "Condition de succès\n\nExigence fondamentale Utiliser des modèles faibles (weak learners)\n\n\nRaison Laisser au processus de boosting le soin de combiner optimalement les prédictions"
  },
  {
    "objectID": "slides/05-supervised-ensemble-slides.html#variantes-principales",
    "href": "slides/05-supervised-ensemble-slides.html#variantes-principales",
    "title": "Supervisée",
    "section": "Variantes principales",
    "text": "Variantes principales\n\n1. AdaBoost (Adaptive Boosting) - Ajuste les poids des observations à chaque itération - Méthode historique et bien établie\n\n\n2. Gradient Boosting - Minimisation d’une fonction de perte - Procédure itérative inspirée de la descente du gradient - Très utilisé aujourd’hui"
  },
  {
    "objectID": "slides/05-supervised-ensemble-slides.html#nécessité-de-loptimisation",
    "href": "slides/05-supervised-ensemble-slides.html#nécessité-de-loptimisation",
    "title": "Supervisée",
    "section": "Nécessité de l’optimisation",
    "text": "Nécessité de l’optimisation\n\nObservation empirique Plus une méthode est puissante, plus l’optimisation des hyper-paramètres est complexe et sensible\n\n\nObjectif Atteindre un bon compromis entre performance et robustesse"
  },
  {
    "objectID": "slides/05-supervised-ensemble-slides.html#paramètres-par-méthode",
    "href": "slides/05-supervised-ensemble-slides.html#paramètres-par-méthode",
    "title": "Supervisée",
    "section": "Paramètres par méthode",
    "text": "Paramètres par méthode\n\nBagging - Nombre d’échantillons bootstrap \\(B\\)\n\n\nForêts aléatoires - Nombre d’échantillons bootstrap \\(B\\) - Nombre de variables \\(m\\) à considérer à chaque division\n\n\nBoosting - Taille des arbres - Fraction du jeu de données échantillonné - Nombre d’itérations - Paramètres de régularisation"
  },
  {
    "objectID": "slides/05-supervised-ensemble-slides.html#méthode-doptimisation",
    "href": "slides/05-supervised-ensemble-slides.html#méthode-doptimisation",
    "title": "Supervisée",
    "section": "Méthode d’optimisation",
    "text": "Méthode d’optimisation\n\nValidation croisée Nécessaire pour chaque paramètre afin d’éviter le sur-ajustement\n\n\nProcessus 1. Définir une grille de valeurs pour chaque paramètre 2. Évaluer par validation croisée 3. Sélectionner la combinaison optimale 4. Tester sur un jeu de données indépendant"
  },
  {
    "objectID": "slides/05-supervised-ensemble-slides.html#récapitulatif-des-méthodes",
    "href": "slides/05-supervised-ensemble-slides.html#récapitulatif-des-méthodes",
    "title": "Supervisée",
    "section": "Récapitulatif des méthodes",
    "text": "Récapitulatif des méthodes\n\n\n\n\nMéthode\nApproche\nAvantage principal\n\n\n\n\nBagging\nParallèle\nRéduction variance\n\n\nForêts aléatoires\nParallèle + aléa\nDécorrélation\n\n\nBoosting\nSéquentielle\nCorrection d’erreurs"
  },
  {
    "objectID": "slides/05-supervised-ensemble-slides.html#points-clés-à-retenir",
    "href": "slides/05-supervised-ensemble-slides.html#points-clés-à-retenir",
    "title": "Supervisée",
    "section": "Points clés à retenir",
    "text": "Points clés à retenir\n\nEnsemble &gt; Individuel : Combinaison de modèles faibles\nDiversité : Clé du succès des méthodes ensemblistes\nTrade-off : Performance vs Interprétabilité\nOptimisation : Cruciale pour la performance finale\nApplications : Base de nombreux algorithmes modernes (XGBoost, LightGBM, etc.)"
  },
  {
    "objectID": "slides/05-supervised-ensemble-slides.html#merci-pour-votre-attention",
    "href": "slides/05-supervised-ensemble-slides.html#merci-pour-votre-attention",
    "title": "Supervisée",
    "section": "Merci pour votre attention",
    "text": "Merci pour votre attention\nQuestions ?\n\nLes méthodes ensemblistes : quand l’union fait la force"
  },
  {
    "objectID": "slides/02-revisions-proba-slides-annotated.html#plan-de-la-présentation",
    "href": "slides/02-revisions-proba-slides-annotated.html#plan-de-la-présentation",
    "title": "Révisions",
    "section": "Plan de la présentation",
    "text": "Plan de la présentation\n\n\nModéliser le hasard\nVariables aléatoires\nVecteurs aléatoires\nEstimation statistique"
  },
  {
    "objectID": "slides/02-revisions-proba-slides-annotated.html#espace-dévénements-et-événements",
    "href": "slides/02-revisions-proba-slides-annotated.html#espace-dévénements-et-événements",
    "title": "Révisions",
    "section": "Espace d’événements et événements",
    "text": "Espace d’événements et événements\n\n\n\n\nDéfinitions\n\n\n\nEspace d’événements \\(S\\) : ensemble de tous les résultats possibles d’une expérience.\nÉvénement : sous-ensemble de \\(S\\)."
  },
  {
    "objectID": "slides/02-revisions-proba-slides-annotated.html#espace-dévénements-et-événements-exemples",
    "href": "slides/02-revisions-proba-slides-annotated.html#espace-dévénements-et-événements-exemples",
    "title": "Révisions",
    "section": "Espace d’événements et événements : exemples",
    "text": "Espace d’événements et événements : exemples\n\nExemple 1 : Lancer de pièce\n\n\\(S = \\{0, 1\\}\\) (pile = 0, face = 1).\nÉvénement \\(E = \\{1\\}\\) : “obtenir face”.\n\n\nExemple 2 : Durée de vie d’un téléphone\n\n\\(S = \\mathbb{R}_+ = [0, +\\infty)\\) (en années).\nÉvénement \\(E = [10, \\infty)\\) : “durée de vie &gt; 10 ans”."
  },
  {
    "objectID": "slides/02-revisions-proba-slides-annotated.html#mesure-de-probabilité",
    "href": "slides/02-revisions-proba-slides-annotated.html#mesure-de-probabilité",
    "title": "Révisions",
    "section": "Mesure de probabilité",
    "text": "Mesure de probabilité\n\n\n\nDéfinition\n\n\nUne mesure de probabilité \\(\\mathbb{P}\\) satisfait :\n\n\\(\\mathbb{P}(E) \\in [0, 1]\\) pour tout événement \\(E\\);\n\\(\\mathbb{P}(S) = 1\\) (certitude totale);\nPour des événements disjoints : \\(\\mathbb{P}(E_1 \\cup E_2 \\cup \\cdots) = \\mathbb{P}(E_1) + \\mathbb{P}(E_2) + \\cdots\\)."
  },
  {
    "objectID": "slides/02-revisions-proba-slides-annotated.html#mesure-de-probabilité-exemple-wooclap-kffpdj",
    "href": "slides/02-revisions-proba-slides-annotated.html#mesure-de-probabilité-exemple-wooclap-kffpdj",
    "title": "Révisions",
    "section": "Mesure de probabilité : exemple (Wooclap: KFFPDJ)",
    "text": "Mesure de probabilité : exemple (Wooclap: KFFPDJ)\n\nExemple : Lancer d’un dé équilibré\n\nEspace d’événements : \\(S = \\{1, 2, 3, 4, 5, 6\\}\\).\n\\(\\mathbb{P}(\\{i\\}) = 1/6\\) pour \\(i = 1, \\ldots, 6\\).\n\\(\\mathbb{P}(\\{2, 4, 6\\}) = 1/6 + 1/6 + 1/6 = 1/2\\) (nombre pair)."
  },
  {
    "objectID": "slides/02-revisions-proba-slides-annotated.html#subjectivité-de-la-probabilité",
    "href": "slides/02-revisions-proba-slides-annotated.html#subjectivité-de-la-probabilité",
    "title": "Révisions",
    "section": "Subjectivité de la probabilité",
    "text": "Subjectivité de la probabilité\n\nContexte : Nombre de jours sans neige à Québec dans l’année\n\nEspace d’événements : \\(S = \\{0, 1, 2, \\ldots, 365\\}\\).\nÉvénement \\(E_1 = [0, 100]\\) : “moins de 100 jours sans neige”.\nÉvénement \\(E_2 = [100, 200]\\) : “entre 100 et 200 jours sans neige”.\n\nPerspective subjective :\n\nNouvel arrivant : probabilités uniformes sur \\(S\\).\nQuébécois : probabilités plus élevées pour \\(E_1\\) que \\(E_2\\)."
  },
  {
    "objectID": "slides/02-revisions-proba-slides-annotated.html#indépendance-et-probabilité-conditionnelle",
    "href": "slides/02-revisions-proba-slides-annotated.html#indépendance-et-probabilité-conditionnelle",
    "title": "Révisions",
    "section": "Indépendance et probabilité conditionnelle",
    "text": "Indépendance et probabilité conditionnelle\n\n\n\nDéfinitions\n\n\nIndépendance : Les événements \\(E\\) et \\(F\\) sont indépendants si \\[\\mathbb{P}(E \\cap F) = \\mathbb{P}(E) \\times \\mathbb{P}(F).\\] Probabilité conditionnelle : La probabilité de \\(E\\) sachant \\(F\\) est \\[\\mathbb{P}(E \\mid F) = \\frac{\\mathbb{P}(E \\cap F)}{\\mathbb{P}(F)}.\\]"
  },
  {
    "objectID": "slides/02-revisions-proba-slides-annotated.html#exemple-cartes-à-jouer",
    "href": "slides/02-revisions-proba-slides-annotated.html#exemple-cartes-à-jouer",
    "title": "Révisions",
    "section": "Exemple : Cartes à jouer",
    "text": "Exemple : Cartes à jouer\n\nContexte : Tirage d’une carte dans un jeu standard (52 cartes)\n\n\\(E\\) : “tirer un roi” → \\(\\mathbb{P}(E) = 4/52 = 1/13\\).\n\\(F\\) : “tirer un cœur” → \\(\\mathbb{P}(F) = 13/52 = 1/4\\).\n\nTest d’indépendance :\n\n\\(E \\cap F\\) : “tirer le roi de cœur” → \\(\\mathbb{P}(E \\cap F) = 1/52\\).\n\\(\\mathbb{P}(E) \\times \\mathbb{P}(F) = (1/13) \\times (1/4) = 1/52\\).\n\nConclusion : \\(E\\) et \\(F\\) sont indépendants !"
  },
  {
    "objectID": "slides/02-revisions-proba-slides-annotated.html#variables-discrètes",
    "href": "slides/02-revisions-proba-slides-annotated.html#variables-discrètes",
    "title": "Révisions",
    "section": "Variables discrètes",
    "text": "Variables discrètes\n\n\n\n\nDéfinition\n\n\nUne variable aléatoire discrète prend au plus un nombre dénombrable de valeurs.\nSa distribution donnée par \\(\\mathbb{P}(X = x)\\) pour chaque valeur \\(x\\)."
  },
  {
    "objectID": "slides/02-revisions-proba-slides-annotated.html#variables-discrètes-exemple",
    "href": "slides/02-revisions-proba-slides-annotated.html#variables-discrètes-exemple",
    "title": "Révisions",
    "section": "Variables discrètes : exemple",
    "text": "Variables discrètes : exemple\n\nExemple : Lancer de deux dés\n\nVariable aléatoire \\(X\\) = somme des deux dés.\nEspace d’événements : \\(S = \\{2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12\\}\\).\n\\(\\mathbb{P}(X = 7) = 6/36 = 1/6\\) (combinaisons : 1+6, 2+5, 3+4, 4+3, 5+2, 6+1).\n\\(\\mathbb{P}(X = 2) = 1/36\\) (une seule combinaison : 1+1)."
  },
  {
    "objectID": "slides/02-revisions-proba-slides-annotated.html#variables-continues",
    "href": "slides/02-revisions-proba-slides-annotated.html#variables-continues",
    "title": "Révisions",
    "section": "Variables continues",
    "text": "Variables continues\n\n\n\nDéfinition\n\n\nLes probabilités d’une variable aléatoire continue sont données par les intégrales \\[\\mathbb{P}(X \\in A) = \\int_A f(x) dx\\] où \\(f(x) \\geq 0\\) et \\(\\int_{-\\infty}^{\\infty} f(x) dx = 1\\).\nImportant : \\(\\mathbb{P}(X = x) = 0\\) pour tout \\(x\\) fixé !"
  },
  {
    "objectID": "slides/02-revisions-proba-slides-annotated.html#variables-continues-exemple",
    "href": "slides/02-revisions-proba-slides-annotated.html#variables-continues-exemple",
    "title": "Révisions",
    "section": "Variables continues : exemple",
    "text": "Variables continues : exemple\nVariable uniforme sur \\([0, 1]\\) : \\[f(x) = \\begin{cases} 1 & \\text{si } x \\in [0, 1] \\\\ 0 & \\text{sinon} \\end{cases}.\\]\nCalculs de probabilités :\n\n\\(\\mathbb{P}(X \\leq 0.5) = \\int_0^{0.5} 1 \\, dx = 0.5\\).\n\\(\\mathbb{P}(0.2 &lt; X &lt; 0.7) = \\int_{0.2}^{0.7} 1 \\, dx = 0.5\\).\n\\(\\mathbb{P}(X = 0.3) = 0\\) (point isolé)."
  },
  {
    "objectID": "slides/02-revisions-proba-slides-annotated.html#espérance-mathématique",
    "href": "slides/02-revisions-proba-slides-annotated.html#espérance-mathématique",
    "title": "Révisions",
    "section": "Espérance mathématique",
    "text": "Espérance mathématique\n\n\n\nDéfinition\n\n\nL’espérance \\(\\mathbb{E}(X)\\) est la valeur moyenne de \\(X\\).\n\nCas discret : \\(\\mathbb{E}(X) = \\sum_x x \\mathbb{P}(X = x)\\).\nCas continu : \\(\\mathbb{E}(X) = \\int_{-\\infty}^{\\infty} x  f(x) dx\\)."
  },
  {
    "objectID": "slides/02-revisions-proba-slides-annotated.html#espérance-mathématique-exemple-wooclap-kffpdj",
    "href": "slides/02-revisions-proba-slides-annotated.html#espérance-mathématique-exemple-wooclap-kffpdj",
    "title": "Révisions",
    "section": "Espérance mathématique : exemple (Wooclap: KFFPDJ)",
    "text": "Espérance mathématique : exemple (Wooclap: KFFPDJ)\nExemple 1 : Dé équilibré\n\n\\[\\mathbb{E}(X) = 1 \\cdot \\frac{1}{6} + 2 \\cdot \\frac{1}{6} + \\cdots + 6 \\cdot \\frac{1}{6} = \\frac{21}{6} = 3.5.\\]\n\n\nExemple 2 : Loi uniforme sur \\([0, 1]\\)\n\n\n\\[\\mathbb{E}(X) = \\int_0^1 x \\cdot 1 \\, dx = \\left[\\frac{x^2}{2}\\right]_0^1 = \\frac{1}{2}.\\]"
  },
  {
    "objectID": "slides/02-revisions-proba-slides-annotated.html#théorème-de-transfert",
    "href": "slides/02-revisions-proba-slides-annotated.html#théorème-de-transfert",
    "title": "Révisions",
    "section": "Théorème de transfert",
    "text": "Théorème de transfert\n\n\n\nThéorème\n\n\nPour une fonction \\(g\\), l’espérance de \\(g(X)\\) est :\n\nCas discret : \\(\\mathbb{E}[g(X)] = \\sum_x g(x) \\mathbb{P}(X = x)\\).\nCas continu : \\(\\mathbb{E}[g(X)] = \\int_{-\\infty}^{\\infty} g(x) f(x) dx\\)."
  },
  {
    "objectID": "slides/02-revisions-proba-slides-annotated.html#théorème-de-transfert-exemple-wooclap-kffpdj",
    "href": "slides/02-revisions-proba-slides-annotated.html#théorème-de-transfert-exemple-wooclap-kffpdj",
    "title": "Révisions",
    "section": "Théorème de transfert : exemple (Wooclap: KFFPDJ)",
    "text": "Théorème de transfert : exemple (Wooclap: KFFPDJ)\n\nPour \\(X\\) suivant une loi uniforme sur \\([0, 1]\\) et \\(g(x) = x^2\\) :\n\n\\[\\mathbb{E}[X^2] = \\int_0^1 x^2 \\cdot 1 \\, dx = \\left[\\frac{x^3}{3}\\right]_0^1 = \\frac{1}{3}.\\]"
  },
  {
    "objectID": "slides/02-revisions-proba-slides-annotated.html#variance-et-écart-type",
    "href": "slides/02-revisions-proba-slides-annotated.html#variance-et-écart-type",
    "title": "Révisions",
    "section": "Variance et écart-type",
    "text": "Variance et écart-type\n\n\n\n\nDéfinitions\n\n\nVariance : \\(\\text{Var}(X) = \\mathbb{E}[(X - \\mathbb{E}(X))^2] = \\mathbb{E}[X^2] - \\mathbb{E}[X]^2\\)\nÉcart-type : \\(\\sigma(X) = \\sqrt{\\text{Var}(X)}\\)"
  },
  {
    "objectID": "slides/02-revisions-proba-slides-annotated.html#variance-et-écart-type-exemple",
    "href": "slides/02-revisions-proba-slides-annotated.html#variance-et-écart-type-exemple",
    "title": "Révisions",
    "section": "Variance et écart-type : exemple",
    "text": "Variance et écart-type : exemple\n\nLoi uniforme sur \\([0, 1]\\)\n\n\\(\\mathbb{E}(X) = 1/2\\)\n\\(\\mathbb{E}(X^2) = 1/3\\)\n\\(\\text{Var}(X) = 1/3 - (1/2)^2 = 1/3 - 1/4 = 1/12\\)\n\\(\\sigma(X) = \\sqrt{1/12} \\approx 0.289\\)"
  },
  {
    "objectID": "slides/02-revisions-proba-slides-annotated.html#fonction-de-répartition",
    "href": "slides/02-revisions-proba-slides-annotated.html#fonction-de-répartition",
    "title": "Révisions",
    "section": "Fonction de répartition",
    "text": "Fonction de répartition\n\n\n\n\nDéfinition\n\n\nLa fonction de répartition est donnée par : \\[F(t) = \\mathbb{P}(X \\leq t).\\]"
  },
  {
    "objectID": "slides/02-revisions-proba-slides-annotated.html#fonction-de-répartition-exemple",
    "href": "slides/02-revisions-proba-slides-annotated.html#fonction-de-répartition-exemple",
    "title": "Révisions",
    "section": "Fonction de répartition : exemple",
    "text": "Fonction de répartition : exemple\nLoi uniforme sur \\([0, 1]\\)\n\\[F(t) = \\begin{cases}\n0 & \\text{si } t &lt; 0 \\\\\nt & \\text{si } 0 \\leq t \\leq 1 \\\\\n1 & \\text{si } t &gt; 1\n\\end{cases}.\\]\nCalcul : \\(\\mathbb{P}(0.2 &lt; X \\leq 0.7) = F(0.7) - F(0.2) = 0.7 - 0.2 = 0.5\\)."
  },
  {
    "objectID": "slides/02-revisions-proba-slides-annotated.html#densités-marginales-et-indépendance",
    "href": "slides/02-revisions-proba-slides-annotated.html#densités-marginales-et-indépendance",
    "title": "Révisions",
    "section": "Densités marginales et indépendance",
    "text": "Densités marginales et indépendance\nVecteur aléatoire : \\(X = (X_1, X_2)^\\top\\) avec densité conjointe \\(f_X(x, y)\\).\nDensités marginales :\n\n\\(f_{X_1}(x) = \\int_{-\\infty}^{\\infty} f_X(x, y) dy\\)\n\\(f_{X_2}(y) = \\int_{-\\infty}^{\\infty} f_X(x, y) dx\\)\n\nIndépendance : \\(f_X(x, y) = f_{X_1}(x) \\cdot f_{X_2}(y)\\)."
  },
  {
    "objectID": "slides/02-revisions-proba-slides-annotated.html#exemple-variables-indépendantes",
    "href": "slides/02-revisions-proba-slides-annotated.html#exemple-variables-indépendantes",
    "title": "Révisions",
    "section": "Exemple : Variables indépendantes",
    "text": "Exemple : Variables indépendantes\nContexte : Soient \\(X_1\\) et \\(X_2\\) uniformes indépendantes sur \\([0, 1]\\).\n\\[f_X(x, y) = \\begin{cases} 1 & \\text{si } (x,y) \\in [0,1]^2 \\\\ 0 & \\text{sinon} \\end{cases}\\]\n\nCalculs de probabilités :\n\n\\(\\mathbb{P}(X_1 \\leq 0.5, X_2 \\leq 0.8) = \\int_0^{0.5} \\int_0^{0.8} 1 \\, dy \\, dx = 0.5 \\times 0.8 = 0.4\\).\n\\(\\mathbb{P}(X_1 + X_2 \\leq 1) = \\int \\int_{x+y \\leq 1} 1 \\, dy \\, dx = 0.5\\) (aire du triangle)."
  },
  {
    "objectID": "slides/02-revisions-proba-slides-annotated.html#loi-normale-multivariée",
    "href": "slides/02-revisions-proba-slides-annotated.html#loi-normale-multivariée",
    "title": "Révisions",
    "section": "Loi normale multivariée",
    "text": "Loi normale multivariée\n\n\n\nLoi normale \\(p\\)-dimensionnelle\n\n\nSoit \\(X\\) un vecteur aléatoire suivant une loi normale \\(p\\)-dimensionnelle. Sa densité est donnée par : \\[f_X(x) = \\frac{1}{(2\\pi)^{p/2}} \\frac{1}{(\\det \\Sigma)^{1/2}} \\exp\\left\\{-\\frac{1}{2}(x-\\mu)^T \\Sigma^{-1} (x-\\mu)\\right\\}.\\]\nNotation : \\(X \\sim \\mathcal{N}_p(\\mu, \\Sigma)\\)."
  },
  {
    "objectID": "slides/02-revisions-proba-slides-annotated.html#exemple-loi-normale-bivariée",
    "href": "slides/02-revisions-proba-slides-annotated.html#exemple-loi-normale-bivariée",
    "title": "Révisions",
    "section": "Exemple : Loi normale bivariée",
    "text": "Exemple : Loi normale bivariée\nCas simple : \\(X \\sim \\mathcal{N}_2(\\mu, I_2)\\) avec \\(\\mu = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\\)\n\\[f_X(x, y) = \\frac{1}{2\\pi} \\exp\\left\\{-\\frac{x^2 + y^2}{2}\\right\\}\\]"
  },
  {
    "objectID": "slides/02-revisions-proba-slides-annotated.html#covariance-et-corrélation",
    "href": "slides/02-revisions-proba-slides-annotated.html#covariance-et-corrélation",
    "title": "Révisions",
    "section": "Covariance et corrélation",
    "text": "Covariance et corrélation\n\n\n\nDéfinitions\n\n\nSoit \\(X = (X_1, X_2)^\\top\\) un vecteur aléatoire.\nCovariance : \\(\\text{Cov}(X_1, X_2) = \\mathbb{E}[(X_1 - \\mathbb{E}(X_1))(X_2 - \\mathbb{E}(X_2))].\\)\nCorrélation : \\(\\text{Corr}(X_1, X_2) = \\frac{\\text{Cov}(X_1, X_2)}{\\sigma(X_1) \\sigma(X_2)}.\\)"
  },
  {
    "objectID": "slides/02-revisions-proba-slides-annotated.html#interprétation-des-signes",
    "href": "slides/02-revisions-proba-slides-annotated.html#interprétation-des-signes",
    "title": "Révisions",
    "section": "Interprétation des signes",
    "text": "Interprétation des signes\n\nCorrélation positive (\\(&gt; 0\\)) : \\(X_1 \\nearrow ~\\Rightarrow ~ X_2 \\nearrow\\).\nCorrélation négative (\\(&lt; 0\\)) : \\(X_1 \\searrow ~\\Rightarrow ~ X_2 \\searrow\\).\nCorrélation nulle (\\(= 0\\)) : pas de relation linéaire (variables orthogonales)"
  },
  {
    "objectID": "slides/02-revisions-proba-slides-annotated.html#interprétation-des-signes-exemple-wooclap-kffpdj",
    "href": "slides/02-revisions-proba-slides-annotated.html#interprétation-des-signes-exemple-wooclap-kffpdj",
    "title": "Révisions",
    "section": "Interprétation des signes : exemple (Wooclap: KFFPDJ)",
    "text": "Interprétation des signes : exemple (Wooclap: KFFPDJ)\nSoit \\(X = (X_1, X_2, X_3)^\\top\\) avec \\[\\text{Var}(X) = \\begin{pmatrix} 4 & 0 & 3 \\\\ 0 & 2 & 0 \\\\ 3 & 0 & 9 \\end{pmatrix}.\\] Calculer \\(\\text{Corr}(X_1, X_3)\\).\n\n\\[\\text{Corr}(X_1, X_3) = \\frac{\\text{Cov}(X_1, X_3)}{\\sigma(X_1)\\sigma(X_3)}\n= \\frac{3}{\\sqrt{4} \\times \\sqrt{9}} = \\frac{1}{2}.\\]"
  },
  {
    "objectID": "slides/02-revisions-proba-slides-annotated.html#propriétés-de-la-covariance",
    "href": "slides/02-revisions-proba-slides-annotated.html#propriétés-de-la-covariance",
    "title": "Révisions",
    "section": "Propriétés de la covariance",
    "text": "Propriétés de la covariance\n\n\n\nPropriétés\n\n\n\n\\(\\text{Cov}(X_1, X_2) = \\mathbb{E}(X_1 X_2) - \\mathbb{E}(X_1)\\mathbb{E}(X_2)\\).\n\\(\\text{Cov}(X_1, X_2) = \\text{Cov}(X_2, X_1)\\) (symétrie).\n\\(\\text{Cov}(X_1 + \\lambda Y_1, X_2) = \\text{Cov}(X_1, X_2) + \\lambda \\text{Cov}(Y_1, X_2)\\) (linéarité)."
  },
  {
    "objectID": "slides/02-revisions-proba-slides-annotated.html#le-problème-de-lestimation",
    "href": "slides/02-revisions-proba-slides-annotated.html#le-problème-de-lestimation",
    "title": "Révisions",
    "section": "Le problème de l’estimation",
    "text": "Le problème de l’estimation\n\n\nRéalité : On ne connaît pas exactement la distribution de \\(X\\).\nDonnées : Échantillon \\(x_1, x_2, \\ldots, x_n\\) (observations).\nObjectif : Estimer les paramètres inconnus (\\(\\mu\\), \\(\\Sigma\\), etc.)."
  },
  {
    "objectID": "slides/02-revisions-proba-slides-annotated.html#estimation-de-la-moyenne",
    "href": "slides/02-revisions-proba-slides-annotated.html#estimation-de-la-moyenne",
    "title": "Révisions",
    "section": "Estimation de la moyenne",
    "text": "Estimation de la moyenne\n\n\n\nEstimateur de la moyenne\n\n\n\\[\\widehat{\\mu} = \\overline{X} = \\frac{1}{n} \\sum_{i=1}^n x_i.\\]\n\n\n\n\nExemple pratique : Tailles en cm : 170, 175, 168, 180, 172\n\\[\\widehat{\\mu} = \\frac{170 + 175 + 168 + 180 + 172}{5} = \\frac{865}{5} = 173 \\text{cm.}\\]"
  },
  {
    "objectID": "slides/02-revisions-proba-slides-annotated.html#estimation-de-la-variance",
    "href": "slides/02-revisions-proba-slides-annotated.html#estimation-de-la-variance",
    "title": "Révisions",
    "section": "Estimation de la variance",
    "text": "Estimation de la variance\n\n\n\nEstimateur de la variance\n\n\n\\[\\widehat{\\Sigma} = \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\widehat{\\mu})(x_i - \\widehat{\\mu})^T\\]\nPourquoi diviser par \\(n-1\\) ? Correction du biais dû à l’utilisation de \\(\\widehat{\\mu}\\)."
  },
  {
    "objectID": "slides/02-revisions-proba-slides-annotated.html#exemple-complet-de-calcul",
    "href": "slides/02-revisions-proba-slides-annotated.html#exemple-complet-de-calcul",
    "title": "Révisions",
    "section": "Exemple complet de calcul",
    "text": "Exemple complet de calcul\nDonnées : 170, 175, 168, 180, 172 (tailles en cm)\n\n\\(\\widehat{\\mu} = 173\\)\nÉcarts au carré : \\((170-173)^2 = 9\\), \\((175-173)^2 = 4\\), \\((168-173)^2 = 25\\), \\((180-173)^2 = 49\\), \\((172-173)^2 = 1\\).\n\nVariance estimée :\n\\[\\widehat{\\sigma^2} = \\frac{9 + 4 + 25 + 49 + 1}{5-1} = \\frac{88}{4} = 22.\\]"
  },
  {
    "objectID": "slides/02-revisions-proba-slides-annotated.html#estimation-de-la-corrélation",
    "href": "slides/02-revisions-proba-slides-annotated.html#estimation-de-la-corrélation",
    "title": "Révisions",
    "section": "Estimation de la corrélation",
    "text": "Estimation de la corrélation\nMatrice de corrélation estimée : \\(\\widehat{R} = D^{-1} \\widehat{\\Sigma} D^{-1}\\) où \\(D\\) est la matrice diagonale des écarts-types.\n\nExemple 2D : Si \\(\\widehat{\\Sigma} = \\begin{pmatrix} 4 & 1.5 \\\\ 1.5 & 9 \\end{pmatrix}\\). Alors,\n\\[D = \\begin{pmatrix} 2 & 0 \\\\ 0 & 3 \\end{pmatrix}, \\quad\\text{et}\\quad\\widehat{R} = \\begin{pmatrix} 1 & 0.25 \\\\ 0.25 & 1 \\end{pmatrix}.\\]\nInterprétation : corrélation faible positive (0.25) entre les variables."
  },
  {
    "objectID": "slides/03-generalities-stat-slides.html#structure-dun-projet",
    "href": "slides/03-generalities-stat-slides.html#structure-dun-projet",
    "title": "Généralités",
    "section": "Structure d’un projet",
    "text": "Structure d’un projet\n\n5 étapes principales :\n\nDéfinition des objectifs\nCollecte et préparation des données\nÉlaboration et validation des modèles\nImplémentation et mise en production\nSuivi de la performance et amélioration continue"
  },
  {
    "objectID": "slides/03-generalities-stat-slides.html#répartition-temps-vs-importance-pyle-1999",
    "href": "slides/03-generalities-stat-slides.html#répartition-temps-vs-importance-pyle-1999",
    "title": "Généralités",
    "section": "Répartition temps vs importance (Pyle, 1999)",
    "text": "Répartition temps vs importance (Pyle, 1999)\n\n\n\n\nÉtape\nTemps\nImportance\n\n\n\n\nComprendre le problème\n10%\n15%\n\n\nExplorer la solution\n9%\n14%\n\n\nImplementer la solution\n1%\n51%\n\n\nPréparer les données\n60%\n15%\n\n\nAnalyse descriptive des données\n15%\n3%\n\n\nModéliser les données\n5%\n2%\n\n\n\n\nConstat: importance ≠ temps passé !"
  },
  {
    "objectID": "slides/03-generalities-stat-slides.html#définition-des-objectifs",
    "href": "slides/03-generalities-stat-slides.html#définition-des-objectifs",
    "title": "Généralités",
    "section": "1. Définition des objectifs",
    "text": "1. Définition des objectifs\n\nPourquoi c’est crucial ?\n\nGuide la collecte des données.\nDéfinit le modèle adéquat.\nFacilite l’interprétation.\nÉvite les explorations aveugles."
  },
  {
    "objectID": "slides/03-generalities-stat-slides.html#définition-des-objectifs-1",
    "href": "slides/03-generalities-stat-slides.html#définition-des-objectifs-1",
    "title": "Généralités",
    "section": "1. Définition des objectifs",
    "text": "1. Définition des objectifs\n\nComment bien formuler ?\n\nClarifier les termes.\nIdentifier les utilisateurs.\nDéfinir la population cible.\nPréciser les décisions qui dépendent des résultats."
  },
  {
    "objectID": "slides/03-generalities-stat-slides.html#exemples-dobjectifs",
    "href": "slides/03-generalities-stat-slides.html#exemples-dobjectifs",
    "title": "Généralités",
    "section": "1. Exemples d’objectifs",
    "text": "1. Exemples d’objectifs\n\n❌ Mauvais objectifs\n\nAnalyser les données clients.\nAnalyser les données des adversaires.\nAnalyser les données du médicament.\n\n\n✅ Bons objectifs\n\nPrédire quels clients achèteront le produit d’épargne.\nCaractériser le style de jeu pour identifier les faiblesses d’une équipe.\nConcevoir un protocole pour tester l’efficacité d’un médicament."
  },
  {
    "objectID": "slides/03-generalities-stat-slides.html#sources-de-données",
    "href": "slides/03-generalities-stat-slides.html#sources-de-données",
    "title": "Généralités",
    "section": "2. Sources de données",
    "text": "2. Sources de données\nPlateformes génériques :\n\nGoogle datasets\nKaggle\nUC Irvine ML Repository\n\nSources officielles :\n\nCanada: StatCan\nFrance: data.gouv.fr\nUSA: data.gov\n\nSources internes : bases clients, transactions, production, web analytics"
  },
  {
    "objectID": "slides/03-generalities-stat-slides.html#qualité-des-données",
    "href": "slides/03-generalities-stat-slides.html#qualité-des-données",
    "title": "Généralités",
    "section": "2. Qualité des données",
    "text": "2. Qualité des données\n\n\n“Garbage in, garbage out”\n\n\nQuelques questions importantes :\n\nLes données sont-elles représentatives ?\nSont-elles exactes, complètes, pertinentes ?\nY a-t-il des valeurs manquantes, doublons, incohérences ?"
  },
  {
    "objectID": "slides/03-generalities-stat-slides.html#format-tidy-data",
    "href": "slides/03-generalities-stat-slides.html#format-tidy-data",
    "title": "Généralités",
    "section": "2. Format “Tidy Data”",
    "text": "2. Format “Tidy Data”\n\n3 principes fondamentaux :\n\nChaque variable = une colonne.\nChaque observation = une ligne.\nChaque cellule = une valeur unique.\n\n\n\nR : tidyr, readr, readxl, haven\n\nPython : pandas, polars"
  },
  {
    "objectID": "slides/03-generalities-stat-slides.html#exploration-préliminaire---nettoyage",
    "href": "slides/03-generalities-stat-slides.html#exploration-préliminaire---nettoyage",
    "title": "Généralités",
    "section": "2. Exploration préliminaire - Nettoyage",
    "text": "2. Exploration préliminaire - Nettoyage\n\n\nSupprimer les doublons.\nUniformiser les modalités.\nVérifier les formats des valeurs spéciales."
  },
  {
    "objectID": "slides/03-generalities-stat-slides.html#exploration-préliminaire---exploration",
    "href": "slides/03-generalities-stat-slides.html#exploration-préliminaire---exploration",
    "title": "Généralités",
    "section": "2. Exploration préliminaire - Exploration",
    "text": "2. Exploration préliminaire - Exploration\n\n\nModalités rares/nombreuses.\nValeurs extrêmes ou aberrantes.\nCorrélations fortes.\nClasses déséquilibrées.\nValeurs manquantes."
  },
  {
    "objectID": "slides/03-generalities-stat-slides.html#exploration-préliminaire---exploration-1",
    "href": "slides/03-generalities-stat-slides.html#exploration-préliminaire---exploration-1",
    "title": "Généralités",
    "section": "2. Exploration préliminaire - Exploration",
    "text": "2. Exploration préliminaire - Exploration\n\nPour faire l’exploration, on peut utiliser :\n\ndes statistiques descriptives unidimensionnelles\n\nnumérique: moyenne, extremum, tableau de fréquences, …\ngraphique: histogramme, boxplot"
  },
  {
    "objectID": "slides/03-generalities-stat-slides.html#exploration-préliminaire---exploration-2",
    "href": "slides/03-generalities-stat-slides.html#exploration-préliminaire---exploration-2",
    "title": "Généralités",
    "section": "2. Exploration préliminaire - Exploration",
    "text": "2. Exploration préliminaire - Exploration\n\nPour faire l’exploration, on peut utiliser :\n\ndes statistiques descriptives bidimensionnelles\n\nnumérique: tableaux croisés, corrélation, …\ngraphique: diagramme de dispersion, …"
  },
  {
    "objectID": "slides/03-generalities-stat-slides.html#exploration-préliminaire---exploration-3",
    "href": "slides/03-generalities-stat-slides.html#exploration-préliminaire---exploration-3",
    "title": "Généralités",
    "section": "2. Exploration préliminaire - Exploration",
    "text": "2. Exploration préliminaire - Exploration\n\nPour faire l’exploration, on peut utiliser :\n\ndes transformations de variables\n\ntransformation simple: passer au \\(\\log\\), standardisation, …\nregrouper des modalités\ncombiner des variables"
  },
  {
    "objectID": "slides/03-generalities-stat-slides.html#exploration-préliminaire---exploration-4",
    "href": "slides/03-generalities-stat-slides.html#exploration-préliminaire---exploration-4",
    "title": "Généralités",
    "section": "2. Exploration préliminaire - Exploration",
    "text": "2. Exploration préliminaire - Exploration\n\nUne valeur extrême est une valeur éloignée des autres valeurs de la population.\n🚨 Peut correspondre à un profil particulier et plus rare dans la population.\n\nQue faire ?\n\n\n\nUtiliser des méthodes robustes\nTransformer la variable, e.g. avec un \\(\\log\\)."
  },
  {
    "objectID": "slides/03-generalities-stat-slides.html#exploration-préliminaire---exploration-5",
    "href": "slides/03-generalities-stat-slides.html#exploration-préliminaire---exploration-5",
    "title": "Généralités",
    "section": "2. Exploration préliminaire - Exploration",
    "text": "2. Exploration préliminaire - Exploration\n\nUne valeur aberrantes est une valeur erronée causée par une erreur de saisie, une erreur de calcul, une mauvaise mesure ou une fausse déclaration.\n\nQue faire ?\n\n\n\nLes remplacer par la bonne valeur.\nLes traiter comme des valeurs manquantes\nRetirer l’individu."
  },
  {
    "objectID": "slides/03-generalities-stat-slides.html#modélisation-4-composantes",
    "href": "slides/03-generalities-stat-slides.html#modélisation-4-composantes",
    "title": "Généralités",
    "section": "3. Modélisation : 4 composantes",
    "text": "3. Modélisation : 4 composantes\n\n\n\nEspace de représentation (cadre mathématique).\nDistance ou similarité (comparer les observations).\nModèle ou algorithme (méthode d’apprentissage).\nFonction de coût (mesure de qualité)."
  },
  {
    "objectID": "slides/03-generalities-stat-slides.html#mise-en-production-data-engineering",
    "href": "slides/03-generalities-stat-slides.html#mise-en-production-data-engineering",
    "title": "Généralités",
    "section": "4. Mise en production : Data Engineering",
    "text": "4. Mise en production : Data Engineering\n\n\nAutomatiser la collecte et le nettoyage.\nIntégrer le modèle dans l’application.\nGénérer rapports/prédictions en temps réel.\nMaintenir la pipeline de données.\n\nRôle du Data Engineer : conception et maintenance de bout en bout."
  },
  {
    "objectID": "slides/03-generalities-stat-slides.html#suivi-et-amélioration",
    "href": "slides/03-generalities-stat-slides.html#suivi-et-amélioration",
    "title": "Généralités",
    "section": "5. Suivi et amélioration",
    "text": "5. Suivi et amélioration\n\nDéfis :\n\nEvolution des données (data drift).\nHypothèses initiales invalidées.\nNouvelles variables disponibles.\n\n\nSolutions :\n\nMonitoring régulier des performances.\nRéentraînement avec données récentes.\nAmélioration continue du modèle."
  },
  {
    "objectID": "slides/03-generalities-stat-slides.html#conclusion",
    "href": "slides/03-generalities-stat-slides.html#conclusion",
    "title": "Généralités",
    "section": "Conclusion",
    "text": "Conclusion\n\nUn bon modèle :\n✅ Performant à un instant donné.\n✅ Robuste dans le temps.\n✅ Adaptable aux changements.\n→ Équilibre entre toutes les étapes, pas seulement la modélisation !"
  },
  {
    "objectID": "slides/04-dimension-mca-slides-annotated.html#introduction-à-lacm",
    "href": "slides/04-dimension-mca-slides-annotated.html#introduction-à-lacm",
    "title": "Dimension",
    "section": "Introduction à l’ACM",
    "text": "Introduction à l’ACM\n\nL’Analyse des Correspondances Multiples (ACM) est un prolongement de l’AFC pour analyser plus de deux variables qualitatives.\nPrincipe fondamental : Transformation préalable par codage disjonctif complet."
  },
  {
    "objectID": "slides/04-dimension-mca-slides-annotated.html#plan",
    "href": "slides/04-dimension-mca-slides-annotated.html#plan",
    "title": "Dimension",
    "section": "Plan",
    "text": "Plan\n\n\nLa théorie de l’analyse des correspondances multiples\nL’ACM en pratique\nExemple :"
  },
  {
    "objectID": "slides/04-dimension-mca-slides-annotated.html#notation-de-base",
    "href": "slides/04-dimension-mca-slides-annotated.html#notation-de-base",
    "title": "Dimension",
    "section": "Notation de base",
    "text": "Notation de base\n\n\n\\(n\\) : nombre d’individus (observations)\n\\(Q\\) : nombre de variables (questions)\n\\(J_q\\) : nombre de modalités de la variable \\(q\\)\n\\(J\\) : nombre total de modalités (\\(J = \\sum_{q=1}^Q J_q\\))\n\nObjectif : Représenter graphiquement les relations entre toutes ces modalités."
  },
  {
    "objectID": "slides/04-dimension-mca-slides-annotated.html#codage-disjonctif-complet",
    "href": "slides/04-dimension-mca-slides-annotated.html#codage-disjonctif-complet",
    "title": "Dimension",
    "section": "Codage disjonctif complet",
    "text": "Codage disjonctif complet\n\n\n\n\n\n\nDéfinition\n\n\nTableau binaire \\(Z\\) (\\(n \\times J\\)) contenant uniquement des 0 et des 1.\n\n\n\n\n\nChaque variable → ses modalités\nIndividu possède modalité → code 1\nIndividu ne possède pas modalité → code 0\nDisjonctif : au plus une modalité par variable\nComplet : au moins une modalité par variable"
  },
  {
    "objectID": "slides/04-dimension-mca-slides-annotated.html#exemple-concret---données",
    "href": "slides/04-dimension-mca-slides-annotated.html#exemple-concret---données",
    "title": "Dimension",
    "section": "Exemple concret - Données",
    "text": "Exemple concret - Données\n\nProduits avec Type et Prix:\n\n\n\nProduit\nType\nPrix ($)\n\n\n\n\nNike\nHoodie\n256.72\n\n\nPuma\nJoggers\n221.26\n\n\nOff-W\nHoodie\n198.45\n\n\nSupreme\nHoodie\n235.50\n\n\n\nVariable Prix → 3 classes : &lt;200$, [200$-250$], &gt;250$"
  },
  {
    "objectID": "slides/04-dimension-mca-slides-annotated.html#exemple---codage-disjonctif-complet",
    "href": "slides/04-dimension-mca-slides-annotated.html#exemple---codage-disjonctif-complet",
    "title": "Dimension",
    "section": "Exemple - Codage disjonctif complet",
    "text": "Exemple - Codage disjonctif complet\n\nTableau transformé \\(Z\\) :\n\n\n\n\n\n\n\n\n\n\n\n\nProduit\nHoodie\nJoggers\nSneakers\n&lt;200$\n[200$-250$]\n&gt;250$\n\n\n\n\nNike\n1\n0\n0\n0\n0\n1\n\n\nPuma\n0\n1\n0\n0\n1\n0\n\n\nOff-W\n1\n0\n0\n1\n0\n0\n\n\nSupreme\n1\n0\n0\n0\n1\n0\n\n\n\nChaque ligne somme à \\(Q = 2\\) (2 variables)."
  },
  {
    "objectID": "slides/04-dimension-mca-slides-annotated.html#propriétés-du-tableau-z",
    "href": "slides/04-dimension-mca-slides-annotated.html#propriétés-du-tableau-z",
    "title": "Dimension",
    "section": "Propriétés du tableau Z",
    "text": "Propriétés du tableau Z\n\n\n\n\n\n\n\nPropriétés\n\n\n\nSomme par ligne : \\(\\sum_{j=1}^J z_{ij} = Q\\) (constante)\nSomme totale : \\(\\sum_{i,j} z_{ij} = nQ\\)\nSomme par colonne : \\(\\sum_{i=1}^n z_{ij} = n_j\\) (effectif modalité \\(j\\))"
  },
  {
    "objectID": "slides/04-dimension-mca-slides-annotated.html#perte-dinformation",
    "href": "slides/04-dimension-mca-slides-annotated.html#perte-dinformation",
    "title": "Dimension",
    "section": "Perte d’information",
    "text": "Perte d’information\n\nVariables quantitatives → Classes : Perte d’information inévitable.\n\nValeur précise → Appartenance à une classe\nExemple : 256.72$ → &gt;250$\nCompromis : Granularité vs interprétabilité\n\nEnjeu : Choisir des classes pertinentes pour le problème étudié."
  },
  {
    "objectID": "slides/04-dimension-mca-slides-annotated.html#tableau-de-burt",
    "href": "slides/04-dimension-mca-slides-annotated.html#tableau-de-burt",
    "title": "Dimension",
    "section": "Tableau de Burt",
    "text": "Tableau de Burt\n\n\n\n\n\n\n\nDéfinition\n\n\n\\(B = Z^\\top Z\\) (matrice \\(J \\times J\\))"
  },
  {
    "objectID": "slides/04-dimension-mca-slides-annotated.html#propriétés-du-tableau-de-burt",
    "href": "slides/04-dimension-mca-slides-annotated.html#propriétés-du-tableau-de-burt",
    "title": "Dimension",
    "section": "Propriétés du tableau de Burt",
    "text": "Propriétés du tableau de Burt\n\nTaille : \\(J \\times J\\) (nombre total de modalités)\n\nBlocs diagonaux : Matrices diagonales\n→ Éléments = effectifs de chaque modalité\n\n\nBlocs non-diagonaux : Tableaux de contingence\n→ Croisement entre variables \\(q\\) et \\(q'\\)\n\n\nSymétrie : \\(B = B^\\top\\)"
  },
  {
    "objectID": "slides/04-dimension-mca-slides-annotated.html#équivalence-mathématique",
    "href": "slides/04-dimension-mca-slides-annotated.html#équivalence-mathématique",
    "title": "Dimension",
    "section": "Équivalence mathématique",
    "text": "Équivalence mathématique\n\nL’ACM peut s’effectuer sur :\n\nLe tableau disjonctif complet \\(Z\\)\nLe tableau de Burt \\(B = Z^\\top Z\\)\n\nRésultat → Les deux approches donnent les mêmes facteurs!\nCette équivalence offre une flexibilité computationnelle."
  },
  {
    "objectID": "slides/04-dimension-mca-slides-annotated.html#éléments-propres-de-z",
    "href": "slides/04-dimension-mca-slides-annotated.html#éléments-propres-de-z",
    "title": "Dimension",
    "section": "Éléments propres de Z",
    "text": "Éléments propres de Z\nAnalyse directe - Vecteurs propres de :\n\\[S = \\frac{1}{Q} Z^\\top Z D_J^{-1}\\] où \\(D_J = \\text{diag}(n_1, \\ldots, n_J)\\).\nCoordonnées profils-lignes : \\[\\Phi_k = n Z D_J^{-1} u_k\\]"
  },
  {
    "objectID": "slides/04-dimension-mca-slides-annotated.html#éléments-propres-de-z-1",
    "href": "slides/04-dimension-mca-slides-annotated.html#éléments-propres-de-z-1",
    "title": "Dimension",
    "section": "Éléments propres de Z",
    "text": "Éléments propres de Z\nAnalyse duale - Vecteurs propres de : \\[T = \\frac{1}{Q} Z D_J^{-1} Z^\\top\\] où \\(D_J = \\text{diag}(n_1, \\ldots, n_J)\\).\nCoordonnées profils-colonnes : \\[\\Psi_k = n D_J^{-1} Z^\\top v_k\\]"
  },
  {
    "objectID": "slides/04-dimension-mca-slides-annotated.html#éléments-propres-de-b",
    "href": "slides/04-dimension-mca-slides-annotated.html#éléments-propres-de-b",
    "title": "Dimension",
    "section": "Éléments propres de B",
    "text": "Éléments propres de B\nTableau de Burt symétrique → Analyse directe = duale\nVecteurs propres de : \\[S' = \\frac{1}{Q^2} B^\\top D_J^{-1} B D_J^{-1}\\]\n\nOn peut réécrire \\(S'\\) comme : \\[S' = \\frac{1}{Q^2} Z^\\top Z D_J^{-1} Z^\\top Z D_J^{-1}\\]"
  },
  {
    "objectID": "slides/04-dimension-mca-slides-annotated.html#relation-entre-valeurs-propres",
    "href": "slides/04-dimension-mca-slides-annotated.html#relation-entre-valeurs-propres",
    "title": "Dimension",
    "section": "Relation entre valeurs propres",
    "text": "Relation entre valeurs propres\n\nSi \\(\\lambda\\) est valeur propre de \\(S\\) (analyse de \\(Z\\)), alors \\(\\lambda^2\\) est valeur propre de \\(S'\\) (analyse de \\(B\\)).\nConséquence :\n\nMêmes vecteurs propres pour \\(Z\\) et \\(B\\).\nValeurs propres de \\(B\\) = carrés des valeurs propres de \\(Z\\)."
  },
  {
    "objectID": "slides/04-dimension-mca-slides-annotated.html#encodage-des-variables-quantitatives",
    "href": "slides/04-dimension-mca-slides-annotated.html#encodage-des-variables-quantitatives",
    "title": "Dimension",
    "section": "Encodage des variables quantitatives",
    "text": "Encodage des variables quantitatives\nChoix des bornes crucial pour la qualité de l’analyse.\n\nApproches recommandées :\n\nAnalyser la distribution (histogrammes)\nBornes pertinentes au domaine d’étude\nÉviter les classes peu informatives\n\n\n\nApproche déconseillée :\n\nEffectifs égaux automatiques\nClasses sans sens métier"
  },
  {
    "objectID": "slides/04-dimension-mca-slides-annotated.html#encodage-des-variables-qualitatives",
    "href": "slides/04-dimension-mca-slides-annotated.html#encodage-des-variables-qualitatives",
    "title": "Dimension",
    "section": "Encodage des variables qualitatives",
    "text": "Encodage des variables qualitatives\nModalités “naturelles” mais problèmes potentiels.\n\nEffectifs déséquilibrés :\n\nModalités très rares vs très fréquentes\nImpact sur la représentation\n\n\n\nSolutions potentielles :\n\nRegroupements pertinents (connaissance du domaine)\nÉviter la répartition aléatoire\nPréserver le sens des modalités"
  },
  {
    "objectID": "slides/04-dimension-mca-slides-annotated.html#exemple-le-succès-des-étudiants",
    "href": "slides/04-dimension-mca-slides-annotated.html#exemple-le-succès-des-étudiants",
    "title": "Dimension",
    "section": "Exemple : le succès des étudiants",
    "text": "Exemple : le succès des étudiants\nOn considère le jeu de données suivant :\n\n\n\n\n\n\n\n\n\n\n\n\n\nHours_Studied\nAttendance\nParental_Involvement\nExtracurricular_Activities\nSleep_Hours\nSchool_Type\n\n\n\n\n21-30\n71-90\nLow\nNo\n&lt;=7\nPublic\n\n\n11-20\n0-70\nLow\nNo\n&gt;7\nPublic\n\n\n21-30\n91-100\nMedium\nYes\n&lt;=7\nPublic\n\n\n21-30\n71-90\nLow\nYes\n&gt;7\nPublic\n\n\n11-20\n91-100\nMedium\nYes\n&lt;=7\nPublic\n\n\n11-20\n71-90\nMedium\nYes\n&gt;7\nPublic"
  },
  {
    "objectID": "slides/04-dimension-mca-slides-annotated.html#exemple-le-succès-des-étudiants-1",
    "href": "slides/04-dimension-mca-slides-annotated.html#exemple-le-succès-des-étudiants-1",
    "title": "Dimension",
    "section": "Exemple : le succès des étudiants",
    "text": "Exemple : le succès des étudiants\n\nCodePlotValeurs propres\n\n\n\nlibrary(FactoMineR)\nlibrary(factoextra)\nres_mca &lt;- df |&gt;\n  MCA(graph = FALSE)\nfviz_eig(res_mca)\nget_eigenvalue(res_mca)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\neigenvalue\nvariance.percent\ncumulative.variance.percent\n\n\n\n\nDim.1\n0.1984565\n11.907391\n11.90739\n\n\nDim.2\n0.1898757\n11.392541\n23.29993\n\n\nDim.3\n0.1814721\n10.888326\n34.18826\n\n\nDim.4\n0.1773147\n10.638882\n44.82714\n\n\nDim.5\n0.1672860\n10.037161\n54.86430\n\n\nDim.6\n0.1651723\n9.910338\n64.77464\n\n\nDim.7\n0.1561348\n9.368089\n74.14273\n\n\nDim.8\n0.1500984\n9.005905\n83.14863\n\n\nDim.9\n0.1415169\n8.491017\n91.63965\n\n\nDim.10\n0.1393392\n8.360350\n100.00000"
  },
  {
    "objectID": "slides/04-dimension-mca-slides-annotated.html#exemple-le-succès-des-étudiants-2",
    "href": "slides/04-dimension-mca-slides-annotated.html#exemple-le-succès-des-étudiants-2",
    "title": "Dimension",
    "section": "Exemple : le succès des étudiants",
    "text": "Exemple : le succès des étudiants\n\nCodePlot\n\n\n\nfviz_mca_biplot(\n  res_mca, \n  repel = TRUE,\n  label = \"none\",\n  ggtheme = theme_minimal()\n)"
  },
  {
    "objectID": "slides/04-dimension-mca-slides-annotated.html#exemple-le-succès-des-étudiants-3",
    "href": "slides/04-dimension-mca-slides-annotated.html#exemple-le-succès-des-étudiants-3",
    "title": "Dimension",
    "section": "Exemple : le succès des étudiants",
    "text": "Exemple : le succès des étudiants\n\nCodePlot\n\n\n\nfviz_mca_var(\n  res_mca,\n  choice = \"mca.cor\", \n  repel = TRUE,\n  ggtheme = theme_minimal()\n)"
  },
  {
    "objectID": "slides/04-dimension-mca-slides-annotated.html#exemple-le-succès-des-étudiants-4",
    "href": "slides/04-dimension-mca-slides-annotated.html#exemple-le-succès-des-étudiants-4",
    "title": "Dimension",
    "section": "Exemple : le succès des étudiants",
    "text": "Exemple : le succès des étudiants\n\nCodePlot\n\n\n\nfviz_mca_var(\n  res_mca,\n  repel = TRUE,\n  ggtheme = theme_minimal()\n)"
  },
  {
    "objectID": "slides/04-dimension-mca-slides-annotated.html#exemple-le-succès-des-étudiants-5",
    "href": "slides/04-dimension-mca-slides-annotated.html#exemple-le-succès-des-étudiants-5",
    "title": "Dimension",
    "section": "Exemple : le succès des étudiants",
    "text": "Exemple : le succès des étudiants\n\nCodePlot\n\n\n\nfviz_mca_var(\n  res_mca,\n  col.var = \"cos2\",\n  gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"), \n  repel = TRUE,\n  ggtheme = theme_minimal()\n)"
  },
  {
    "objectID": "slides/04-dimension-mca-slides-annotated.html#exemple-le-succès-des-étudiants-6",
    "href": "slides/04-dimension-mca-slides-annotated.html#exemple-le-succès-des-étudiants-6",
    "title": "Dimension",
    "section": "Exemple : le succès des étudiants",
    "text": "Exemple : le succès des étudiants\n\nCodePlot\n\n\n\nfviz_mca_ind(\n  res_mca,\n  label = \"none\",\n  col.ind = \"cos2\", \n  gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n  repel = TRUE,\n  ggtheme = theme_minimal()\n)"
  },
  {
    "objectID": "slides/04-dimension-mca-slides-annotated.html#exemple-le-succès-des-étudiants-7",
    "href": "slides/04-dimension-mca-slides-annotated.html#exemple-le-succès-des-étudiants-7",
    "title": "Dimension",
    "section": "Exemple : le succès des étudiants",
    "text": "Exemple : le succès des étudiants\n\nCodePlotPlot\n\n\n\nfviz_mca_ind(\n  res_mca, \n  label = \"none\",\n  habillage = \"Hours_Studied\",\n  palette = c(\"#D81B60\", \"#1E88E5\", \"#FFC107\", \"#004D40\"),\n  addEllipses = TRUE, ellipse.type = \"confidence\",\n  ggtheme = theme_minimal()\n)"
  },
  {
    "objectID": "slides/04-dimension-mca-slides-annotated.html#avantages-de-lacm",
    "href": "slides/04-dimension-mca-slides-annotated.html#avantages-de-lacm",
    "title": "Dimension",
    "section": "Avantages de l’ACM",
    "text": "Avantages de l’ACM\n\n\nVisualisation de relations complexes multi-variables\nTraitement unifié de variables hétérogènes\nRéduction de dimension préservant les associations\nInterprétation intuitive des proximités\nFlexibilité dans l’encodage des variables"
  },
  {
    "objectID": "slides/04-dimension-mca-slides-annotated.html#limites-et-précautions",
    "href": "slides/04-dimension-mca-slides-annotated.html#limites-et-précautions",
    "title": "Dimension",
    "section": "Limites et précautions",
    "text": "Limites et précautions\n\nPerte d’information :\n\nVariables continues → classes\nChoix d’encodage critique\n\nInterprétation :\n\nProximités entre modalités de variables différentes\nAttention aux artefacts d’encodage"
  },
  {
    "objectID": "slides/04-dimension-mca-slides-annotated.html#conclusion",
    "href": "slides/04-dimension-mca-slides-annotated.html#conclusion",
    "title": "Dimension",
    "section": "Conclusion",
    "text": "Conclusion\n\nL’ACM étend l’AFC au cas multi-variables grâce au codage disjonctif complet :\n\nTransformation binaire préalable\nÉquivalence \\(Z\\) ↔︎ Tableau de Burt \\(B\\)\nChoix d’encodage\nVisualisation des patterns complexes\n\nProchaine étape → éthique de l’analyse de données"
  },
  {
    "objectID": "slides/03-generalities-distance-slides-annotated.html#pourquoi-les-distances",
    "href": "slides/03-generalities-distance-slides-annotated.html#pourquoi-les-distances",
    "title": "Généralités",
    "section": "Pourquoi les distances ?",
    "text": "Pourquoi les distances ?\n\nDans tout projet d’analyse de données, il faut pouvoir quantifier la ressemblance ou la dissemblance entre observations.\n\nImpact direct sur :\n\nAlgorithmes d’apprentissage\nMéthodes de regroupement\nTechniques de visualisation"
  },
  {
    "objectID": "slides/03-generalities-distance-slides-annotated.html#plan",
    "href": "slides/03-generalities-distance-slides-annotated.html#plan",
    "title": "Généralités",
    "section": "Plan",
    "text": "Plan\n\n\nNotion de distance - Définition mathématique\nDistances numériques - \\(L_q\\), euclidienne, Manhattan\nNotion de similarité - Concept opposé\nVariables qualitatives - Encodages et distances adaptées\nDistance de Jaccard - Cas des variables binaires"
  },
  {
    "objectID": "slides/03-generalities-distance-slides-annotated.html#définition-mathématique",
    "href": "slides/03-generalities-distance-slides-annotated.html#définition-mathématique",
    "title": "Généralités",
    "section": "Définition mathématique",
    "text": "Définition mathématique\n\n\n\nDistance sur \\(\\mathcal{X}\\)\n\n\nUne fonction \\(d: \\mathcal{X} \\times \\mathcal{X} \\to \\mathbb{R}\\) est une distance si :\n\nNon-négativité : \\(d(x, y) \\geq 0\\)\nSéparation : \\(d(x, y) = 0 \\Leftrightarrow x = y\\)\nSymétrie : \\(d(x, y) = d(y, x)\\)\nInégalité triangulaire : \\(d(x, y) \\leq d(x, z) + d(y, z)\\)"
  },
  {
    "objectID": "slides/03-generalities-distance-slides-annotated.html#distance-euclidienne",
    "href": "slides/03-generalities-distance-slides-annotated.html#distance-euclidienne",
    "title": "Généralités",
    "section": "Distance euclidienne",
    "text": "Distance euclidienne\n\n\n\nDéfinition\n\n\nPour \\(x, y \\in \\mathbb{R}^p\\) :\n\\[d(x, y) = \\left\\| x - y \\right\\|_2 = \\sqrt{\\sum_{i=1}^{p} (x_i - y_i)^2}\\]\nQuand l’utiliser : Variables numériques de même ordre de grandeur."
  },
  {
    "objectID": "slides/03-generalities-distance-slides-annotated.html#distance-euclidienne-preuve",
    "href": "slides/03-generalities-distance-slides-annotated.html#distance-euclidienne-preuve",
    "title": "Généralités",
    "section": "Distance euclidienne (preuve)",
    "text": "Distance euclidienne (preuve)"
  },
  {
    "objectID": "slides/03-generalities-distance-slides-annotated.html#distance-l_q-minkowski",
    "href": "slides/03-generalities-distance-slides-annotated.html#distance-l_q-minkowski",
    "title": "Généralités",
    "section": "Distance \\(L_q\\) (Minkowski)",
    "text": "Distance \\(L_q\\) (Minkowski)\n\n\n\nFormule générale :\n\n\n\\[d(x, y) = \\left( \\sum_{i=1}^{p} |x_i - y_i|^q \\right)^{1/q}\\]\nCas particuliers :\n\n\\(q = 1\\) : Distance de Manhattan → \\(d(x, y) = \\sum_{i=1}^{p} |x_i - y_i|\\)\n\\(q = 2\\) : Distance euclidienne"
  },
  {
    "objectID": "slides/03-generalities-distance-slides-annotated.html#exemple-concret-wooclap-kffpdj",
    "href": "slides/03-generalities-distance-slides-annotated.html#exemple-concret-wooclap-kffpdj",
    "title": "Généralités",
    "section": "Exemple concret (WOOCLAP: KFFPDJ)",
    "text": "Exemple concret (WOOCLAP: KFFPDJ)\n\n\n\nNom\nTaille (cm)\nPoids (kg)\n\n\n\n\nAlice\n165\n70\n\n\nBob\n177\n75"
  },
  {
    "objectID": "slides/03-generalities-distance-slides-annotated.html#problème-déchelle",
    "href": "slides/03-generalities-distance-slides-annotated.html#problème-déchelle",
    "title": "Généralités",
    "section": "Problème d’échelle",
    "text": "Problème d’échelle\n\n\n\nAttention !\n\n\nLes distances \\(L_q\\) ne sont pas invariantes aux changements d’échelle."
  },
  {
    "objectID": "slides/03-generalities-distance-slides-annotated.html#problème-déchelle---solution",
    "href": "slides/03-generalities-distance-slides-annotated.html#problème-déchelle---solution",
    "title": "Généralités",
    "section": "Problème d’échelle - solution",
    "text": "Problème d’échelle - solution\n\nStandardiser les variables !\n\nDistance euclidienne standardisée :\n\\[d(x, y) = \\sum_{i=1}^{p} \\left( \\frac{x_i - y_i}{\\sigma_i} \\right)^2\\]\noù \\(\\sigma_i\\) = écart-type de la variable \\(i\\)."
  },
  {
    "objectID": "slides/03-generalities-distance-slides-annotated.html#notion-de-similarité",
    "href": "slides/03-generalities-distance-slides-annotated.html#notion-de-similarité",
    "title": "Généralités",
    "section": "Notion de similarité",
    "text": "Notion de similarité\n\n\n\n\nMesure de similarité sur \\(\\mathcal{X}\\)\n\n\nUne fonction \\(s: \\mathcal{X} \\times \\mathcal{X} \\to \\mathbb{R}\\) telle que :\n\n\\(s(x, y) \\geq 0\\)\n\\(s(x, y) = s(y, x)\\) (symétrie)\n\\(s(x, x) = 1 \\geq s(x, y)\\) (maximum pour soi-même)"
  },
  {
    "objectID": "slides/03-generalities-distance-slides-annotated.html#conversion-distance-similarité",
    "href": "slides/03-generalities-distance-slides-annotated.html#conversion-distance-similarité",
    "title": "Généralités",
    "section": "Conversion distance ↔︎ similarité",
    "text": "Conversion distance ↔︎ similarité\n\nDistance → Similarité :\n\\[s(x, y) = \\frac{1}{1 + d(x, y)}\\]\n\n⚠️ Une mesure de similarité ne respecte généralement pas l’inégalité triangulaire.\n\n\nSimilarité → Dissemblance :\n\\[d^*(x, y) = 1 - s(x, y)\\]"
  },
  {
    "objectID": "slides/03-generalities-distance-slides-annotated.html#variables-qualitatives",
    "href": "slides/03-generalities-distance-slides-annotated.html#variables-qualitatives",
    "title": "Généralités",
    "section": "Variables qualitatives",
    "text": "Variables qualitatives\n\nProblème : pour \\(\\mathcal{X} = \\{\\text{Rouge}, \\text{Vert}, \\text{Bleu}\\}\\).\n\n\n❌ Mauvaise pratique :\nRouge = 1, Vert = 2, Bleu = 3 → Introduit un ordre artificiel !\n\n\n\n✅ Bonne pratique : Encodage adapté"
  },
  {
    "objectID": "slides/03-generalities-distance-slides-annotated.html#encodage-1-parmi-k-one-hot-encoding",
    "href": "slides/03-generalities-distance-slides-annotated.html#encodage-1-parmi-k-one-hot-encoding",
    "title": "Généralités",
    "section": "Encodage “1 parmi K” (One-hot encoding)",
    "text": "Encodage “1 parmi K” (One-hot encoding)\n\n\nRouge → \\((1, 0, 0)\\)\nVert → \\((0, 1, 0)\\)\nBleu → \\((0, 0, 1)\\)\n\n\nAvantages : Pas d’ordre artificiel\nInconvénients : Grande dimension si beaucoup de modalités"
  },
  {
    "objectID": "slides/03-generalities-distance-slides-annotated.html#distance-discrète",
    "href": "slides/03-generalities-distance-slides-annotated.html#distance-discrète",
    "title": "Généralités",
    "section": "Distance discrète",
    "text": "Distance discrète\n\n\n\nDistance de Hamming\n\n\nPour des vecteurs :\n\\[d(x, y) = \\sum_{i=1}^{p} \\mathbb{1}(x_i \\neq y_i)\\]"
  },
  {
    "objectID": "slides/03-generalities-distance-slides-annotated.html#exemple-distance-discrète-wooclap-kffpdj",
    "href": "slides/03-generalities-distance-slides-annotated.html#exemple-distance-discrète-wooclap-kffpdj",
    "title": "Généralités",
    "section": "Exemple : Distance discrète (WOOCLAP: KFFPDJ)",
    "text": "Exemple : Distance discrète (WOOCLAP: KFFPDJ)\n\n\n\nNom\nCouleur\nYeux\nCheveux\n\n\n\n\nAlice\nRouge\nVert\nBlond\n\n\nBob\nVert\nBleu\nRoux\n\n\nChris\nRouge\nVert\nBlond"
  },
  {
    "objectID": "slides/03-generalities-distance-slides-annotated.html#similarité-par-accords",
    "href": "slides/03-generalities-distance-slides-annotated.html#similarité-par-accords",
    "title": "Généralités",
    "section": "Similarité par accords",
    "text": "Similarité par accords\n\nCompter les accords normalisés :\n\\[s(x, y) = \\frac{1}{p}\\sum_{i=1}^{p} \\mathbb{1}(x_i = y_i)\\]\n\nExemple précédent :\n\n\\(s(\\text{Alice}, \\text{Bob}) = 0\\)\n\\(s(\\text{Alice}, \\text{Chris}) = 1\\)\n\\(s(\\text{Bob}, \\text{Chris}) = 0\\)"
  },
  {
    "objectID": "slides/03-generalities-distance-slides-annotated.html#problème-des-variables-binaires",
    "href": "slides/03-generalities-distance-slides-annotated.html#problème-des-variables-binaires",
    "title": "Généralités",
    "section": "Problème des variables binaires",
    "text": "Problème des variables binaires\n\nAvec l’encodage “1 parmi K” :\n\nBeaucoup de variables binaires\nBeaucoup de 0, peu de 1\nDistance discrète peu informative\n\n\nSolution : Se concentrer sur les attributs à 1 → Distance de Jaccard"
  },
  {
    "objectID": "slides/03-generalities-distance-slides-annotated.html#indice-de-jaccard-définition",
    "href": "slides/03-generalities-distance-slides-annotated.html#indice-de-jaccard-définition",
    "title": "Généralités",
    "section": "Indice de Jaccard : Définition",
    "text": "Indice de Jaccard : Définition\n\nPour deux observations de \\(K\\) variables binaires :\n\n\\(M_{11}\\) : nombre de variables à 1 pour x et y\n\\(M_{10}\\) : nombre de variables à 1 pour x, 0 pour y\n\\(M_{01}\\) : nombre de variables à 0 pour x, 1 pour y\n\\(M_{00}\\) : nombre de variables à 0 pour x et y\n\n\n\\[J(x, y) = \\frac{M_{11}}{M_{10} + M_{01} + M_{11}} = \\frac{M_{11}}{K - M_{00}}\\]"
  },
  {
    "objectID": "slides/03-generalities-distance-slides-annotated.html#distance-de-jaccard",
    "href": "slides/03-generalities-distance-slides-annotated.html#distance-de-jaccard",
    "title": "Généralités",
    "section": "Distance de Jaccard",
    "text": "Distance de Jaccard\n\n\n\n\nDistance de Jaccard :\n\n\n\\[d(x, y) = 1 - J(x, y) = \\frac{M_{10} + M_{01}}{M_{01} + M_{10} + M_{11}}\\]\n\n\n\nIntuition : Proportion de désaccords parmi les variables “actives”."
  },
  {
    "objectID": "slides/03-generalities-distance-slides-annotated.html#exemple-distance-de-jaccard",
    "href": "slides/03-generalities-distance-slides-annotated.html#exemple-distance-de-jaccard",
    "title": "Généralités",
    "section": "Exemple : Distance de Jaccard",
    "text": "Exemple : Distance de Jaccard\n\n\n\nNom\nQ1\nQ2\nQ3\nQ4\nQ5\n\n\n\n\nAlice\n1\n0\n1\n0\n1\n\n\nBob\n1\n0\n0\n1\n0"
  },
  {
    "objectID": "slides/03-generalities-distance-slides-annotated.html#récapitulatif-des-distances",
    "href": "slides/03-generalities-distance-slides-annotated.html#récapitulatif-des-distances",
    "title": "Généralités",
    "section": "Récapitulatif des distances",
    "text": "Récapitulatif des distances\n\n\n\n\nType de données\nDistance utilisable\n\n\n\n\nNumériques\nEuclidienne (standardisée)\n\n\nQualitatives\nDiscrète (Hamming)\n\n\nBinaires nombreuses\nJaccard\n\n\nMixtes\nCombinaison adaptée"
  },
  {
    "objectID": "slides/03-generalities-distance-slides-annotated.html#choix-pratique-de-la-distance",
    "href": "slides/03-generalities-distance-slides-annotated.html#choix-pratique-de-la-distance",
    "title": "Généralités",
    "section": "Choix pratique de la distance",
    "text": "Choix pratique de la distance\n\nQuestions à se poser :\n\nQuel type de variables ? (numériques/qualitatives)\nMême échelle ou standardisation nécessaire ?\nBeaucoup de variables binaires ?\nLes double-absences sont-elles informatives ?\n\n\n→ Le choix de distance influence directement les résultats !"
  },
  {
    "objectID": "slides/03-generalities-distance-slides-annotated.html#conclusion",
    "href": "slides/03-generalities-distance-slides-annotated.html#conclusion",
    "title": "Généralités",
    "section": "Conclusion",
    "text": "Conclusion\n\n\nDistance → mesure de dissemblance (4 propriétés mathématiques)\nSimilarité → mesure de ressemblance (concept opposé)\nVariables numériques → Distance euclidienne (standardisée)\nVariables qualitatives → Encodage + distance adaptée\nVariables binaires → Distance de Jaccard\n\n\nProchaine étape → Compromis biais/variance"
  },
  {
    "objectID": "slides/05-supervised-generalities.html#plan",
    "href": "slides/05-supervised-generalities.html#plan",
    "title": "Supervisée",
    "section": "Plan",
    "text": "Plan\n\n\nQu’est-ce que la classification supervisée ?\nExemples\nApproche"
  },
  {
    "objectID": "slides/05-supervised-generalities.html#principe",
    "href": "slides/05-supervised-generalities.html#principe",
    "title": "Supervisée",
    "section": "Principe",
    "text": "Principe\n\nOn considère une population comportant \\(K\\) groupes.\nOn connaît le nombre des groupes.\nOn observe \\(p\\) variables \\(X_1, \\dots, X_p\\) pour chaque individu / observation de la population.\nOn connaît le groupe d’appartenance de chaque individu / observation."
  },
  {
    "objectID": "slides/05-supervised-generalities.html#objectif",
    "href": "slides/05-supervised-generalities.html#objectif",
    "title": "Supervisée",
    "section": "Objectif",
    "text": "Objectif\n\n\\[Y = f(X) + \\epsilon\\]\n\nObtenir un modèle / algorithme pour classer de nouveaux individus / nouvelles observations dans les bons groupes.\n\n\nPrédire \\(Y\\) à partir de \\(X_1, \\dots, X_p\\)."
  },
  {
    "objectID": "slides/05-supervised-generalities.html#quelques-méthodes",
    "href": "slides/05-supervised-generalities.html#quelques-méthodes",
    "title": "Supervisée",
    "section": "Quelques méthodes",
    "text": "Quelques méthodes\n\nClassificateur naïf de Bayes\n\nAnalyse discrimante\n\n\nArbres de classification\n\n\n\\(k\\) plus proches voisins (\\(k\\)-NN)\n\n\nRégression logistiques et dérivée (GLM, GAM, GLMM, GAMM, …)\n\n\nMéthodes ensemblistes (bagging, boosting, forêt aléatoire, …)\n\n\nMachine de vecteur de soutien (support vector machine, SVM)\n\n\nRéseaux de neurones"
  },
  {
    "objectID": "slides/05-supervised-generalities.html#exemples",
    "href": "slides/05-supervised-generalities.html#exemples",
    "title": "Supervisée",
    "section": "Exemples",
    "text": "Exemples\n\nFiltrage de courriels indésirables\nSystèmes de recommendation\nPrévision des gagnants lors d’événements sportifs\nTraduction automatisée de textes\nReconnaissance d’images"
  },
  {
    "objectID": "slides/05-supervised-generalities.html#approche-générale",
    "href": "slides/05-supervised-generalities.html#approche-générale",
    "title": "Supervisée",
    "section": "Approche générale",
    "text": "Approche générale\n\nSélectionner un certain nombre d’individus / d’observations dont on connaît le groupe d’appartenance.\nMesurer \\(p\\) caractéristiques \\(X_1, \\dots, X_p\\) sur ces individus.\nDivisier ce jeu de données en deux :\n\n\nUn jeu de données pour la modélisation (train)\nUn jeu de données pour la vérification (test)\n\n\nDévelopper un algorithme pour classer le mieux possible les individus du jeu de données d’entraînement.\nÉvaluer notre algorithme sur le jeu de données de validation."
  },
  {
    "objectID": "slides/05-supervised-generalities.html#conclusion",
    "href": "slides/05-supervised-generalities.html#conclusion",
    "title": "Supervisée",
    "section": "Conclusion",
    "text": "Conclusion\n\nModèle général:\n\\[Y = f(X) + \\epsilon\\]\nClassification supervisée: connaissant \\(X\\) et \\(Y\\), on cherche à estimer \\(f\\) (en supposant éventuellement une certaine forme pour \\(f\\)).\n\n\nProchaine étape → Analyse discriminante"
  },
  {
    "objectID": "slides/06-unsupervised-hierarchy-slides.html#motivation",
    "href": "slides/06-unsupervised-hierarchy-slides.html#motivation",
    "title": "Non-supervisée",
    "section": "Motivation",
    "text": "Motivation\n\nLimites du \\(k\\)-means - Nécessite les données originales - Problématique avec matrice de similarité/distance uniquement\n\n\nSolution : Classification hiérarchique - Série de partitions imbriquées - De \\(n\\) groupes (chaque observation isolée) à 1 groupe (toutes ensemble) - Représentation : Dendrogramme"
  },
  {
    "objectID": "slides/06-unsupervised-hierarchy-slides.html#types-dalgorithmes",
    "href": "slides/06-unsupervised-hierarchy-slides.html#types-dalgorithmes",
    "title": "Non-supervisée",
    "section": "Types d’algorithmes",
    "text": "Types d’algorithmes\n\nAlgorithmes ascendants\n\nDépart : \\(n\\) groupes individuels\nProcessus : Fusions successives\n\nAlgorithmes descendants\n\nDépart : 1 groupe de toutes les observations\nProcessus : Divisions successives\n\n\n\nRésultat commun : \\(n\\) partitions hiérarchiques de 1 à \\(n\\) groupes"
  },
  {
    "objectID": "slides/06-unsupervised-hierarchy-slides.html#algorithmes-descendants",
    "href": "slides/06-unsupervised-hierarchy-slides.html#algorithmes-descendants",
    "title": "Non-supervisée",
    "section": "Algorithmes descendants",
    "text": "Algorithmes descendants\n\nPrincipe - Démarrage avec toutes les observations dans un groupe - Division du groupe le moins homogène à chaque étape - Maximisation de la dissimilarité entre sous-groupes\n\n\nInconvénient majeur Coût computationnel très élevé (évaluation de toutes les divisions possibles)\n\n\nUsage : Rarement utilisé en pratique"
  },
  {
    "objectID": "slides/06-unsupervised-hierarchy-slides.html#algorithmes-ascendants",
    "href": "slides/06-unsupervised-hierarchy-slides.html#algorithmes-ascendants",
    "title": "Non-supervisée",
    "section": "Algorithmes ascendants",
    "text": "Algorithmes ascendants\n\nPrincipe - Démarrage avec \\(n\\) groupes distincts - Fusion des deux groupes les plus similaires à chaque étape - Continuation jusqu’à un seul groupe\n\n\nAvantage : Plus efficace computationnellement\n\n\nEnjeu clé : Définir la distance entre groupes"
  },
  {
    "objectID": "slides/06-unsupervised-hierarchy-slides.html#plus-proche-voisin-single-linkage",
    "href": "slides/06-unsupervised-hierarchy-slides.html#plus-proche-voisin-single-linkage",
    "title": "Non-supervisée",
    "section": "Plus proche voisin (Single Linkage)",
    "text": "Plus proche voisin (Single Linkage)\n\nDéfinition \\[d(A, B) = \\min \\{d_{ij}: i \\in A, j \\in B\\}\\] Distance = plus petite distance entre individus des deux groupes\n\n\nAvantages - Bon avec variables de types différents - Groupes de formes irrégulières - Robuste aux données aberrantes - Bonnes propriétés théoriques\n\n\nInconvénients - Groupes déséquilibrés (effet “chaîne”) - Moins performant pour groupes réguliers"
  },
  {
    "objectID": "slides/06-unsupervised-hierarchy-slides.html#voisin-le-plus-distant-complete-linkage",
    "href": "slides/06-unsupervised-hierarchy-slides.html#voisin-le-plus-distant-complete-linkage",
    "title": "Non-supervisée",
    "section": "Voisin le plus distant (Complete Linkage)",
    "text": "Voisin le plus distant (Complete Linkage)\n\nDéfinition \\[d(A, B) = \\max \\{d_{ij}: i \\in A, j \\in B\\}\\] Distance = plus grande distance entre individus\n\n\nAvantages - Groupes réguliers et homogènes en taille - Adapté aux variables de types différents\n\n\nInconvénients - Extrêmement sensible aux données aberrantes - Force des groupes de même taille"
  },
  {
    "objectID": "slides/06-unsupervised-hierarchy-slides.html#méthode-de-la-moyenne-average-linkage",
    "href": "slides/06-unsupervised-hierarchy-slides.html#méthode-de-la-moyenne-average-linkage",
    "title": "Non-supervisée",
    "section": "Méthode de la moyenne (Average Linkage)",
    "text": "Méthode de la moyenne (Average Linkage)\n\nDéfinition \\[d(A, B) = \\frac{1}{n_A n_B} \\sum_{i \\in A} \\sum_{j \\in B} d(X_i, X_j)\\] Distance = moyenne de toutes les distances inter-groupes\n\n\nCaractéristiques - Considère toutes les interactions - Produit des groupes homogènes (variance faible) - Privilégie des groupes de variance similaire"
  },
  {
    "objectID": "slides/06-unsupervised-hierarchy-slides.html#méthode-du-centroïde",
    "href": "slides/06-unsupervised-hierarchy-slides.html#méthode-du-centroïde",
    "title": "Non-supervisée",
    "section": "Méthode du centroïde",
    "text": "Méthode du centroïde\n\nDéfinition \\[d(A, B) = d(\\overline{X}_A, \\overline{X}_B)\\] Distance entre les moyennes des groupes\n\n\nMise à jour après fusion \\[\\overline{X}_{AB} = \\frac{n_A \\overline{X}_A + n_B \\overline{X}_B}{n_A + n_B}\\]\n\n\nPropriété : Assez robuste aux données aberrantes, mais peu performant sans elles"
  },
  {
    "objectID": "slides/06-unsupervised-hierarchy-slides.html#méthode-de-la-médiane",
    "href": "slides/06-unsupervised-hierarchy-slides.html#méthode-de-la-médiane",
    "title": "Non-supervisée",
    "section": "Méthode de la médiane",
    "text": "Méthode de la médiane\n\nPrincipe : Mise à jour récursive des distances\n\n\nFormule de mise à jour \\[d(AB, C) = \\frac{d(A, C) + d(B, C)}{2} - \\frac{d(A, B)}{4}\\]\n\n\nCaractéristiques - Très robuste aux données aberrantes - Peu efficace sans valeurs extrêmes"
  },
  {
    "objectID": "slides/06-unsupervised-hierarchy-slides.html#méthode-de-ward",
    "href": "slides/06-unsupervised-hierarchy-slides.html#méthode-de-ward",
    "title": "Non-supervisée",
    "section": "Méthode de Ward",
    "text": "Méthode de Ward\n\nPrincipe : Minimisation de l’augmentation d’inertie intra-groupe\n\n\nCritère d’optimisation \\[I_{AB} = SC_{AB} - SC_A - SC_B = \\frac{d^2(\\overline{X}_A, \\overline{X}_B)}{\\frac{1}{n_A} + \\frac{1}{n_B}}\\]\n\n\nConditions optimales - Observations gaussiennes multivariées - Même matrice de variance-covariance - Moyennes différentes"
  },
  {
    "objectID": "slides/06-unsupervised-hierarchy-slides.html#ward-avantages-et-limites",
    "href": "slides/06-unsupervised-hierarchy-slides.html#ward-avantages-et-limites",
    "title": "Non-supervisée",
    "section": "Ward : Avantages et limites",
    "text": "Ward : Avantages et limites\n\nAvantages - Très efficace sous hypothèses gaussiennes - Groupes homogènes et de taille comparable - Méthode très populaire en pratique\n\n\nInconvénients - Sensible aux données aberrantes - Tend à former des groupes de même taille - Hypothèses gaussiennes restrictives"
  },
  {
    "objectID": "slides/06-unsupervised-hierarchy-slides.html#méthode-flexible",
    "href": "slides/06-unsupervised-hierarchy-slides.html#méthode-flexible",
    "title": "Non-supervisée",
    "section": "Méthode flexible",
    "text": "Méthode flexible\n\nFormule générale \\[d(C, AB) = \\alpha_A d(C, A) + \\alpha_B d(C, B) + \\beta d(A, B) + \\gamma |d(C, A) - d(C, B)|\\]\n\n\nFlexibilité Selon \\((\\alpha_A, \\alpha_B, \\beta, \\gamma)\\), on retrouve toutes les méthodes précédentes\n\n\nParamétrage standard - \\(\\alpha_A = \\alpha_B\\), \\(\\alpha_A + \\alpha_B + \\beta = 1\\), \\(\\gamma = 0\\) - Choix courant : \\(\\beta = -0.25\\) - Données aberrantes : \\(\\beta = -0.5\\)"
  },
  {
    "objectID": "slides/06-unsupervised-hierarchy-slides.html#le-problème-du-choix",
    "href": "slides/06-unsupervised-hierarchy-slides.html#le-problème-du-choix",
    "title": "Non-supervisée",
    "section": "Le problème du choix",
    "text": "Le problème du choix\n\nQuestion centrale Parmi les \\(n\\) partitions possibles, laquelle choisir ?\n\n\nCritères pratiques - Interprétabilité scientifique ou métier - Pertinence opérationnelle - Nombre de groupes \\(K\\) déterminé à l’avance\n\n\nCritères statistiques Quand les considérations pratiques ne suffisent pas"
  },
  {
    "objectID": "slides/06-unsupervised-hierarchy-slides.html#outils-disponibles",
    "href": "slides/06-unsupervised-hierarchy-slides.html#outils-disponibles",
    "title": "Non-supervisée",
    "section": "Outils disponibles",
    "text": "Outils disponibles\n\nLibrairies spécialisées - R NbClust : ~30 critères - Python sklearn : ~10 critères\n\n\nAdaptation Critères surtout adaptés aux variables continues"
  },
  {
    "objectID": "slides/06-unsupervised-hierarchy-slides.html#décomposition-de-linertie",
    "href": "slides/06-unsupervised-hierarchy-slides.html#décomposition-de-linertie",
    "title": "Non-supervisée",
    "section": "Décomposition de l’inertie",
    "text": "Décomposition de l’inertie\n\nInertie totale \\[I_{\\text{tot}} = \\frac{1}{n}\\sum_{i=1}^{n} d(X_i, G)\\]\n\n\nDécomposition fondamentale \\[I_{\\text{tot}} = I_{\\text{intra-groupe}} + I_{\\text{inter-groupe}}\\]\n\n\nObjectif : Maximiser \\(I_{\\text{inter-groupe}}\\) (séparation des groupes)"
  },
  {
    "objectID": "slides/06-unsupervised-hierarchy-slides.html#inertie-inter-groupe-et-intra-groupe",
    "href": "slides/06-unsupervised-hierarchy-slides.html#inertie-inter-groupe-et-intra-groupe",
    "title": "Non-supervisée",
    "section": "Inertie inter-groupe et intra-groupe",
    "text": "Inertie inter-groupe et intra-groupe\n\nInertie inter-groupe (séparation) \\[I_{\\text{inter-groupe}} = \\frac{1}{n}\\sum_{k=1}^{K} n_k d(G_k, G)\\]\n\n\nInertie intra-groupe (compacité) \\[I_{\\text{intra-groupe}} = \\frac{1}{n}\\sum_{k=1}^{K} \\sum_{i \\in C_k} d(X_i, G_k)\\]\n\n\nPrincipe : Groupes compacts \\(\\Rightarrow\\) faible inertie intra-groupe"
  },
  {
    "objectID": "slides/06-unsupervised-hierarchy-slides.html#critères-dérivés",
    "href": "slides/06-unsupervised-hierarchy-slides.html#critères-dérivés",
    "title": "Non-supervisée",
    "section": "Critères dérivés",
    "text": "Critères dérivés\n\nPseudo-\\(R^2\\) \\[\\text{pseudo-}R^2 = \\frac{I_{\\text{inter-groupe}}}{I_{\\text{tot}}}\\] Proportion d’inertie expliquée par la partition\n\n\nStatistique Calinski-Harabasz (CH) \\[\\text{CH} = \\frac{I_{\\text{inter-groupe}} / (K-1)}{I_{\\text{intra-groupe}} / (n-K)}\\] Équilibre entre compacité et séparation"
  },
  {
    "objectID": "slides/06-unsupervised-hierarchy-slides.html#indice-de-dunn",
    "href": "slides/06-unsupervised-hierarchy-slides.html#indice-de-dunn",
    "title": "Non-supervisée",
    "section": "Indice de Dunn",
    "text": "Indice de Dunn\n\nDéfinition \\[D = \\frac{\\text{Distance minimale entre 2 groupes}}{\\text{Distance maximale dans un groupe}}\\]\n\n\nObjectif : Maximiser \\(D\\) - Groupes denses (faible distance interne) - Groupes bien séparés (grande distance externe)"
  },
  {
    "objectID": "slides/06-unsupervised-hierarchy-slides.html#indice-de-silhouette",
    "href": "slides/06-unsupervised-hierarchy-slides.html#indice-de-silhouette",
    "title": "Non-supervisée",
    "section": "Indice de silhouette",
    "text": "Indice de silhouette\n\nPour une observation \\(X_i\\) \\[S(X_i) = \\frac{b_i - a_i}{\\max(b_i, a_i)}\\] où : - \\(a_i\\) : distance moyenne aux observations du même groupe - \\(b_i\\) : distance moyenne au groupe le plus proche\n\n\nInterprétation - \\(S(X_i) \\approx 1\\) : Bien classée - \\(S(X_i) \\approx 0\\) : À la frontière - \\(S(X_i) \\approx -1\\) : Mal classée"
  },
  {
    "objectID": "slides/06-unsupervised-hierarchy-slides.html#utilisation-de-la-silhouette",
    "href": "slides/06-unsupervised-hierarchy-slides.html#utilisation-de-la-silhouette",
    "title": "Non-supervisée",
    "section": "Utilisation de la silhouette",
    "text": "Utilisation de la silhouette\n\nSilhouette moyenne Indicateur de cohérence globale de la partition\n\n\nAvantage : Intuitive et facilement interprétable\n\n\nUsage : Souvent utilisée pour choisir le nombre optimal de groupes"
  },
  {
    "objectID": "slides/06-unsupervised-hierarchy-slides.html#recommandations-pratiques",
    "href": "slides/06-unsupervised-hierarchy-slides.html#recommandations-pratiques",
    "title": "Non-supervisée",
    "section": "Recommandations pratiques",
    "text": "Recommandations pratiques\n\nCroiser les approches\n\nConnaissance métier\nVisualisation (dendrogramme)\nCritères statistiques multiples\n\nAucun critère n’est parfait\n\nComplémentarité des indicateurs\nDécision éclairée et contextualisée\n\nValidation\n\nInterprétabilité des groupes obtenus\nStabilité des résultats"
  },
  {
    "objectID": "slides/06-unsupervised-hierarchy-slides.html#récapitulatif",
    "href": "slides/06-unsupervised-hierarchy-slides.html#récapitulatif",
    "title": "Non-supervisée",
    "section": "Récapitulatif",
    "text": "Récapitulatif\n\nClassification hiérarchique - Alternative robuste au \\(k\\)-means - Exploration complète de la hiérarchie de groupes - Choix de méthode de distance crucial\n\n\nMéthodes populaires - Ward (sous hypothèses gaussiennes) - Average linkage (bon compromis) - Complete linkage (groupes réguliers)\n\n\nChoix du nombre de groupes Combinaison de critères pratiques et statistiques"
  },
  {
    "objectID": "slides/06-unsupervised-hierarchy-slides.html#merci-pour-votre-attention",
    "href": "slides/06-unsupervised-hierarchy-slides.html#merci-pour-votre-attention",
    "title": "Non-supervisée",
    "section": "Merci pour votre attention",
    "text": "Merci pour votre attention\nQuestions ?\n\nClassification hiérarchique : explorer la structure naturelle des données"
  },
  {
    "objectID": "slides/03-generalities-distance-slides.html#pourquoi-les-distances",
    "href": "slides/03-generalities-distance-slides.html#pourquoi-les-distances",
    "title": "Généralités",
    "section": "Pourquoi les distances ?",
    "text": "Pourquoi les distances ?\n\nDans tout projet d’analyse de données, il faut pouvoir quantifier la ressemblance ou la dissemblance entre observations.\n\nImpact direct sur :\n\nAlgorithmes d’apprentissage\nMéthodes de regroupement\nTechniques de visualisation"
  },
  {
    "objectID": "slides/03-generalities-distance-slides.html#plan",
    "href": "slides/03-generalities-distance-slides.html#plan",
    "title": "Généralités",
    "section": "Plan",
    "text": "Plan\n\n\nNotion de distance - Définition mathématique\nDistances numériques - \\(L_q\\), euclidienne, Manhattan\nNotion de similarité - Concept opposé\nVariables qualitatives - Encodages et distances adaptées\nDistance de Jaccard - Cas des variables binaires"
  },
  {
    "objectID": "slides/03-generalities-distance-slides.html#définition-mathématique",
    "href": "slides/03-generalities-distance-slides.html#définition-mathématique",
    "title": "Généralités",
    "section": "Définition mathématique",
    "text": "Définition mathématique\n\n\n\nDistance sur \\(\\mathcal{X}\\)\n\n\nUne fonction \\(d: \\mathcal{X} \\times \\mathcal{X} \\to \\mathbb{R}\\) est une distance si :\n\nNon-négativité : \\(d(x, y) \\geq 0\\)\nSéparation : \\(d(x, y) = 0 \\Leftrightarrow x = y\\)\nSymétrie : \\(d(x, y) = d(y, x)\\)\nInégalité triangulaire : \\(d(x, y) \\leq d(x, z) + d(y, z)\\)"
  },
  {
    "objectID": "slides/03-generalities-distance-slides.html#distance-euclidienne",
    "href": "slides/03-generalities-distance-slides.html#distance-euclidienne",
    "title": "Généralités",
    "section": "Distance euclidienne",
    "text": "Distance euclidienne\n\n\n\nDéfinition\n\n\nPour \\(x, y \\in \\mathbb{R}^p\\) :\n\\[d(x, y) = \\left\\| x - y \\right\\|_2 = \\sqrt{\\sum_{i=1}^{p} (x_i - y_i)^2}\\]\nQuand l’utiliser : Variables numériques de même ordre de grandeur."
  },
  {
    "objectID": "slides/03-generalities-distance-slides.html#distance-euclidienne-preuve",
    "href": "slides/03-generalities-distance-slides.html#distance-euclidienne-preuve",
    "title": "Généralités",
    "section": "Distance euclidienne (preuve)",
    "text": "Distance euclidienne (preuve)"
  },
  {
    "objectID": "slides/03-generalities-distance-slides.html#distance-l_q-minkowski",
    "href": "slides/03-generalities-distance-slides.html#distance-l_q-minkowski",
    "title": "Généralités",
    "section": "Distance \\(L_q\\) (Minkowski)",
    "text": "Distance \\(L_q\\) (Minkowski)\n\n\n\nFormule générale :\n\n\n\\[d(x, y) = \\left( \\sum_{i=1}^{p} |x_i - y_i|^q \\right)^{1/q}\\]\nCas particuliers :\n\n\\(q = 1\\) : Distance de Manhattan → \\(d(x, y) = \\sum_{i=1}^{p} |x_i - y_i|\\)\n\\(q = 2\\) : Distance euclidienne"
  },
  {
    "objectID": "slides/03-generalities-distance-slides.html#exemple-concret-wooclap-kffpdj",
    "href": "slides/03-generalities-distance-slides.html#exemple-concret-wooclap-kffpdj",
    "title": "Généralités",
    "section": "Exemple concret (WOOCLAP: KFFPDJ)",
    "text": "Exemple concret (WOOCLAP: KFFPDJ)\n\n\n\nNom\nTaille (cm)\nPoids (kg)\n\n\n\n\nAlice\n165\n70\n\n\nBob\n177\n75"
  },
  {
    "objectID": "slides/03-generalities-distance-slides.html#problème-déchelle",
    "href": "slides/03-generalities-distance-slides.html#problème-déchelle",
    "title": "Généralités",
    "section": "Problème d’échelle",
    "text": "Problème d’échelle\n\n\n\nAttention !\n\n\nLes distances \\(L_q\\) ne sont pas invariantes aux changements d’échelle."
  },
  {
    "objectID": "slides/03-generalities-distance-slides.html#problème-déchelle---solution",
    "href": "slides/03-generalities-distance-slides.html#problème-déchelle---solution",
    "title": "Généralités",
    "section": "Problème d’échelle - solution",
    "text": "Problème d’échelle - solution\n\nStandardiser les variables !\n\nDistance euclidienne standardisée :\n\\[d(x, y) = \\sum_{i=1}^{p} \\left( \\frac{x_i - y_i}{\\sigma_i} \\right)^2\\]\noù \\(\\sigma_i\\) = écart-type de la variable \\(i\\)."
  },
  {
    "objectID": "slides/03-generalities-distance-slides.html#notion-de-similarité",
    "href": "slides/03-generalities-distance-slides.html#notion-de-similarité",
    "title": "Généralités",
    "section": "Notion de similarité",
    "text": "Notion de similarité\n\n\n\n\nMesure de similarité sur \\(\\mathcal{X}\\)\n\n\nUne fonction \\(s: \\mathcal{X} \\times \\mathcal{X} \\to \\mathbb{R}\\) telle que :\n\n\\(s(x, y) \\geq 0\\)\n\\(s(x, y) = s(y, x)\\) (symétrie)\n\\(s(x, x) = 1 \\geq s(x, y)\\) (maximum pour soi-même)"
  },
  {
    "objectID": "slides/03-generalities-distance-slides.html#conversion-distance-similarité",
    "href": "slides/03-generalities-distance-slides.html#conversion-distance-similarité",
    "title": "Généralités",
    "section": "Conversion distance ↔︎ similarité",
    "text": "Conversion distance ↔︎ similarité\n\nDistance → Similarité :\n\\[s(x, y) = \\frac{1}{1 + d(x, y)}\\]\n\n⚠️ Une mesure de similarité ne respecte généralement pas l’inégalité triangulaire.\n\n\nSimilarité → Dissemblance :\n\\[d^*(x, y) = 1 - s(x, y)\\]"
  },
  {
    "objectID": "slides/03-generalities-distance-slides.html#variables-qualitatives",
    "href": "slides/03-generalities-distance-slides.html#variables-qualitatives",
    "title": "Généralités",
    "section": "Variables qualitatives",
    "text": "Variables qualitatives\n\nProblème : pour \\(\\mathcal{X} = \\{\\text{Rouge}, \\text{Vert}, \\text{Bleu}\\}\\).\n\n\n❌ Mauvaise pratique :\nRouge = 1, Vert = 2, Bleu = 3 → Introduit un ordre artificiel !\n\n\n\n✅ Bonne pratique : Encodage adapté"
  },
  {
    "objectID": "slides/03-generalities-distance-slides.html#encodage-1-parmi-k-one-hot-encoding",
    "href": "slides/03-generalities-distance-slides.html#encodage-1-parmi-k-one-hot-encoding",
    "title": "Généralités",
    "section": "Encodage “1 parmi K” (One-hot encoding)",
    "text": "Encodage “1 parmi K” (One-hot encoding)\n\n\nRouge → \\((1, 0, 0)\\)\nVert → \\((0, 1, 0)\\)\nBleu → \\((0, 0, 1)\\)\n\n\nAvantages : Pas d’ordre artificiel\nInconvénients : Grande dimension si beaucoup de modalités"
  },
  {
    "objectID": "slides/03-generalities-distance-slides.html#distance-discrète",
    "href": "slides/03-generalities-distance-slides.html#distance-discrète",
    "title": "Généralités",
    "section": "Distance discrète",
    "text": "Distance discrète\n\n\n\nDistance de Hamming\n\n\nPour des vecteurs :\n\\[d(x, y) = \\sum_{i=1}^{p} \\mathbb{1}(x_i \\neq y_i)\\]"
  },
  {
    "objectID": "slides/03-generalities-distance-slides.html#exemple-distance-discrète-wooclap-kffpdj",
    "href": "slides/03-generalities-distance-slides.html#exemple-distance-discrète-wooclap-kffpdj",
    "title": "Généralités",
    "section": "Exemple : Distance discrète (WOOCLAP: KFFPDJ)",
    "text": "Exemple : Distance discrète (WOOCLAP: KFFPDJ)\n\n\n\nNom\nCouleur\nYeux\nCheveux\n\n\n\n\nAlice\nRouge\nVert\nBlond\n\n\nBob\nVert\nBleu\nRoux\n\n\nChris\nRouge\nVert\nBlond"
  },
  {
    "objectID": "slides/03-generalities-distance-slides.html#similarité-par-accords",
    "href": "slides/03-generalities-distance-slides.html#similarité-par-accords",
    "title": "Généralités",
    "section": "Similarité par accords",
    "text": "Similarité par accords\n\nCompter les accords normalisés :\n\\[s(x, y) = \\frac{1}{p}\\sum_{i=1}^{p} \\mathbb{1}(x_i = y_i)\\]\n\nExemple précédent :\n\n\\(s(\\text{Alice}, \\text{Bob}) = 0\\)\n\\(s(\\text{Alice}, \\text{Chris}) = 1\\)\n\\(s(\\text{Bob}, \\text{Chris}) = 0\\)"
  },
  {
    "objectID": "slides/03-generalities-distance-slides.html#problème-des-variables-binaires",
    "href": "slides/03-generalities-distance-slides.html#problème-des-variables-binaires",
    "title": "Généralités",
    "section": "Problème des variables binaires",
    "text": "Problème des variables binaires\n\nAvec l’encodage “1 parmi K” :\n\nBeaucoup de variables binaires\nBeaucoup de 0, peu de 1\nDistance discrète peu informative\n\n\nSolution : Se concentrer sur les attributs à 1 → Distance de Jaccard"
  },
  {
    "objectID": "slides/03-generalities-distance-slides.html#indice-de-jaccard-définition",
    "href": "slides/03-generalities-distance-slides.html#indice-de-jaccard-définition",
    "title": "Généralités",
    "section": "Indice de Jaccard : Définition",
    "text": "Indice de Jaccard : Définition\n\nPour deux observations de \\(K\\) variables binaires :\n\n\\(M_{11}\\) : nombre de variables à 1 pour x et y\n\\(M_{10}\\) : nombre de variables à 1 pour x, 0 pour y\n\\(M_{01}\\) : nombre de variables à 0 pour x, 1 pour y\n\\(M_{00}\\) : nombre de variables à 0 pour x et y\n\n\n\\[J(x, y) = \\frac{M_{11}}{M_{10} + M_{01} + M_{11}} = \\frac{M_{11}}{K - M_{00}}\\]"
  },
  {
    "objectID": "slides/03-generalities-distance-slides.html#distance-de-jaccard",
    "href": "slides/03-generalities-distance-slides.html#distance-de-jaccard",
    "title": "Généralités",
    "section": "Distance de Jaccard",
    "text": "Distance de Jaccard\n\n\n\n\nDistance de Jaccard :\n\n\n\\[d(x, y) = 1 - J(x, y) = \\frac{M_{10} + M_{01}}{M_{01} + M_{10} + M_{11}}\\]\n\n\n\nIntuition : Proportion de désaccords parmi les variables “actives”."
  },
  {
    "objectID": "slides/03-generalities-distance-slides.html#exemple-distance-de-jaccard",
    "href": "slides/03-generalities-distance-slides.html#exemple-distance-de-jaccard",
    "title": "Généralités",
    "section": "Exemple : Distance de Jaccard",
    "text": "Exemple : Distance de Jaccard\n\n\n\nNom\nQ1\nQ2\nQ3\nQ4\nQ5\n\n\n\n\nAlice\n1\n0\n1\n0\n1\n\n\nBob\n1\n0\n0\n1\n0"
  },
  {
    "objectID": "slides/03-generalities-distance-slides.html#récapitulatif-des-distances",
    "href": "slides/03-generalities-distance-slides.html#récapitulatif-des-distances",
    "title": "Généralités",
    "section": "Récapitulatif des distances",
    "text": "Récapitulatif des distances\n\n\n\n\nType de données\nDistance utilisable\n\n\n\n\nNumériques\nEuclidienne (standardisée)\n\n\nQualitatives\nDiscrète (Hamming)\n\n\nBinaires nombreuses\nJaccard\n\n\nMixtes\nCombinaison adaptée"
  },
  {
    "objectID": "slides/03-generalities-distance-slides.html#choix-pratique-de-la-distance",
    "href": "slides/03-generalities-distance-slides.html#choix-pratique-de-la-distance",
    "title": "Généralités",
    "section": "Choix pratique de la distance",
    "text": "Choix pratique de la distance\n\nQuestions à se poser :\n\nQuel type de variables ? (numériques/qualitatives)\nMême échelle ou standardisation nécessaire ?\nBeaucoup de variables binaires ?\nLes double-absences sont-elles informatives ?\n\n\n→ Le choix de distance influence directement les résultats !"
  },
  {
    "objectID": "slides/03-generalities-distance-slides.html#conclusion",
    "href": "slides/03-generalities-distance-slides.html#conclusion",
    "title": "Généralités",
    "section": "Conclusion",
    "text": "Conclusion\n\n\nDistance → mesure de dissemblance (4 propriétés mathématiques)\nSimilarité → mesure de ressemblance (concept opposé)\nVariables numériques → Distance euclidienne (standardisée)\nVariables qualitatives → Encodage + distance adaptée\nVariables binaires → Distance de Jaccard\n\n\nProchaine étape → Compromis biais/variance"
  },
  {
    "objectID": "slides/06-unsupervised-kmeans-slides.html#objectif-de-la-classification-non-supervisée",
    "href": "slides/06-unsupervised-kmeans-slides.html#objectif-de-la-classification-non-supervisée",
    "title": "Non-supervisée",
    "section": "Objectif de la classification non-supervisée",
    "text": "Objectif de la classification non-supervisée\n\nDonnées \\(n\\) observations \\(X_1, \\ldots, X_n\\) décrites par \\(p\\) variables numériques\n\n\nPréparation Variables généralement standardisées pour éviter les effets d’échelle\n\n\nObjectif Regrouper en \\(K\\) groupes tels que : 1. Intra-groupe : Observations similaires 2. Inter-groupe : Groupes distincts"
  },
  {
    "objectID": "slides/06-unsupervised-kmeans-slides.html#fonction-de-classification",
    "href": "slides/06-unsupervised-kmeans-slides.html#fonction-de-classification",
    "title": "Non-supervisée",
    "section": "Fonction de classification",
    "text": "Fonction de classification\n\nDéfinition \\[C: \\{1, \\ldots, n\\} \\to \\{1, \\ldots, K\\}\\] Associe à chaque observation \\(i\\) une étiquette de groupe \\(C(i)\\)\n\n\nFonction de coût \\[W(C) = \\sum_{k=1}^{K} \\sum_{i: C(i)=k} \\sum_{j: C(j)=k} d(X_i, X_j)\\] où \\(d(X_i, X_j)\\) mesure la dissimilarité entre observations\n\n\nObjectif : Minimiser \\(W(C)\\) (cohésion intra-groupe)"
  },
  {
    "objectID": "slides/06-unsupervised-kmeans-slides.html#complexité-du-problème",
    "href": "slides/06-unsupervised-kmeans-slides.html#complexité-du-problème",
    "title": "Non-supervisée",
    "section": "Complexité du problème",
    "text": "Complexité du problème\n\nProblème d’optimisation combinatoire - \\(K^n\\) combinaisons possibles - Exploration exhaustive impossible même pour petites valeurs\n\n\nSolution pratique : Algorithmes gloutons - Exploration d’un sous-ensemble restreint - Amélioration itérative progressive - Convergence vers un minimum local\n\n\nHypothèses 1. Variables numériques, souvent centrées-réduites 2. Nombre de groupes \\(K\\) fixé à l’avance"
  },
  {
    "objectID": "slides/06-unsupervised-kmeans-slides.html#principe-général",
    "href": "slides/06-unsupervised-kmeans-slides.html#principe-général",
    "title": "Non-supervisée",
    "section": "Principe général",
    "text": "Principe général\n\nObjectif Regrouper \\(n\\) observations en \\(K\\) groupes homogènes\n\n\nMesure de dissimilarité Généralement la distance euclidienne pour données quantitatives\n\n\nStratégie - Observations proches au sein d’un groupe - Groupes éloignés entre eux"
  },
  {
    "objectID": "slides/06-unsupervised-kmeans-slides.html#algorithme-détaillé",
    "href": "slides/06-unsupervised-kmeans-slides.html#algorithme-détaillé",
    "title": "Non-supervisée",
    "section": "Algorithme détaillé",
    "text": "Algorithme détaillé\n\nChoix de \\(K\\) : Nombre de groupes fixé à l’avance\nInitialisation : Partition aléatoire ou centres initiaux aléatoires\nCalcul des centroïdes : \\[\\mu_k = \\frac{1}{N_k} \\sum_{i: C(i)=k} X_i\\]\nRéaffectation : Chaque observation au centre le plus proche\nItération : Répéter 3-4 jusqu’à stabilisation"
  },
  {
    "objectID": "slides/06-unsupervised-kmeans-slides.html#convergence",
    "href": "slides/06-unsupervised-kmeans-slides.html#convergence",
    "title": "Non-supervisée",
    "section": "Convergence",
    "text": "Convergence\n\nGarantie de convergence En nombre fini d’itérations\n\n\nMécanisme Chaque étape réduit l’inertie intra-groupe (somme des distances observations ↔︎ centroïdes)\n\n\n⚠️ Limitation importante Convergence vers un minimum local, pas nécessairement global"
  },
  {
    "objectID": "slides/06-unsupervised-kmeans-slides.html#limites-des-k-moyennes",
    "href": "slides/06-unsupervised-kmeans-slides.html#limites-des-k-moyennes",
    "title": "Non-supervisée",
    "section": "Limites des \\(k\\)-moyennes",
    "text": "Limites des \\(k\\)-moyennes\n\nSensibilité à l’initialisation\n\nRésultats variables selon centres initiaux\nSolution : Algorithme \\(k\\)-means++\n\nChoix de \\(K\\) à l’avance\n\nPas toujours évident\nCritères : Silhouette, méthode du coude, BIC/AIC\n\nCoût computationnel\n\nRecalcul de toutes les distances à chaque itération\nProblématique pour grand \\(n\\) ou \\(p\\)"
  },
  {
    "objectID": "slides/06-unsupervised-kmeans-slides.html#limites-suite",
    "href": "slides/06-unsupervised-kmeans-slides.html#limites-suite",
    "title": "Non-supervisée",
    "section": "Limites (suite)",
    "text": "Limites (suite)\n\n\nSensibilité aux valeurs extrêmes\n\nLa moyenne influencée par observations atypiques\nCentroïdes faussés\nRegroupements incohérents\n\n\n\n\nConséquence Nécessité de prétraiter les données (détection d’outliers)"
  },
  {
    "objectID": "slides/06-unsupervised-kmeans-slides.html#principe-des-k-médoïdes",
    "href": "slides/06-unsupervised-kmeans-slides.html#principe-des-k-médoïdes",
    "title": "Non-supervisée",
    "section": "Principe des \\(k\\)-médoïdes",
    "text": "Principe des \\(k\\)-médoïdes\n\nDifférence fondamentale Utilise des observations réelles comme représentants (pas des moyennes)\n\n\nMédoïde Observation qui minimise la somme des distances aux autres observations du même groupe\n\n\nAvantage conceptuel Représentants concrets et interprétables"
  },
  {
    "objectID": "slides/06-unsupervised-kmeans-slides.html#avantages-des-k-médoïdes",
    "href": "slides/06-unsupervised-kmeans-slides.html#avantages-des-k-médoïdes",
    "title": "Non-supervisée",
    "section": "Avantages des \\(k\\)-médoïdes",
    "text": "Avantages des \\(k\\)-médoïdes\n\nRobustesse aux valeurs extrêmes\n\nMédoïdes moins sensibles que moyennes\nMeilleure résistance aux outliers\n\nCompatibilité étendue\n\nVariables ordinales ou catégorielles\nMesures de dissimilarité personnalisées\n\nFlexibilité\n\nMatrice de dissimilarité sur mesure\nAdaptée à la nature des données"
  },
  {
    "objectID": "slides/06-unsupervised-kmeans-slides.html#inconvénients-des-k-médoïdes",
    "href": "slides/06-unsupervised-kmeans-slides.html#inconvénients-des-k-médoïdes",
    "title": "Non-supervisée",
    "section": "Inconvénients des \\(k\\)-médoïdes",
    "text": "Inconvénients des \\(k\\)-médoïdes\n\nProblèmes partagés avec \\(k\\)-moyennes - Choix de \\(K\\) à l’avance - Convergence vers minimum local\n\n\nCoût computationnel plus élevé - Calculs plus complexes pour identifier médoïdes - Particulièrement avec distances non-euclidiennes - Problématique pour grands jeux de données"
  },
  {
    "objectID": "slides/06-unsupervised-kmeans-slides.html#comparaison-des-méthodes",
    "href": "slides/06-unsupervised-kmeans-slides.html#comparaison-des-méthodes",
    "title": "Non-supervisée",
    "section": "Comparaison des méthodes",
    "text": "Comparaison des méthodes\n\n\n\n\nAspect\n\\(k\\)-moyennes\n\\(k\\)-médoïdes\n\n\n\n\nReprésentants\nCentroïdes (moyennes)\nObservations réelles\n\n\nRobustesse\nSensible aux outliers\nPlus robuste\n\n\nTypes de variables\nNumériques\nNumériques + ordinales\n\n\nCoût computationnel\nModéré\nPlus élevé\n\n\nInterprétabilité\nAbstraite\nConcrète"
  },
  {
    "objectID": "slides/06-unsupervised-kmeans-slides.html#critères-de-choix-de-k",
    "href": "slides/06-unsupervised-kmeans-slides.html#critères-de-choix-de-k",
    "title": "Non-supervisée",
    "section": "Critères de choix de \\(K\\)",
    "text": "Critères de choix de \\(K\\)\n\nMéthode du coude (Elbow method)\n\nGraphique inertie vs \\(K\\)\nRecherche du “coude”\n\nCoefficient de silhouette\n\nMesure qualité des groupes\nValeurs entre -1 et 1\n\nCritères d’information\n\nBIC, AIC en cadre probabiliste\nPénalisent la complexité"
  },
  {
    "objectID": "slides/06-unsupervised-kmeans-slides.html#points-clés-à-retenir",
    "href": "slides/06-unsupervised-kmeans-slides.html#points-clés-à-retenir",
    "title": "Non-supervisée",
    "section": "Points clés à retenir",
    "text": "Points clés à retenir\n\nClassification non-supervisée : Découverte de structure sans étiquettes\n\\(k\\)-moyennes : Méthode classique, efficace mais limitations\n\\(k\\)-médoïdes : Alternative robuste, coût plus élevé\nChoix de \\(K\\) : Crucial, plusieurs critères disponibles\nInitialisation : Importante pour éviter minima locaux\nPrétraitement : Standardisation et gestion des outliers essentielles"
  },
  {
    "objectID": "slides/06-unsupervised-kmeans-slides.html#merci-pour-votre-attention",
    "href": "slides/06-unsupervised-kmeans-slides.html#merci-pour-votre-attention",
    "title": "Non-supervisée",
    "section": "Merci pour votre attention",
    "text": "Merci pour votre attention\nQuestions ?\n\nClassification non-supervisée : découvrir la structure cachée des données"
  },
  {
    "objectID": "slides/03-generalities-bias-variance-slides.html#objectif-de-modélisation",
    "href": "slides/03-generalities-bias-variance-slides.html#objectif-de-modélisation",
    "title": "Généralités",
    "section": "Objectif de modélisation",
    "text": "Objectif de modélisation\n\nModéliser la relation entre :\n\nVariable réponse \\(Y\\) (quantitative, qualitative, etc.).\nVariables explicatives \\(X = (X_1, \\ldots, X_p)\\)."
  },
  {
    "objectID": "slides/03-generalities-bias-variance-slides.html#modèle-général",
    "href": "slides/03-generalities-bias-variance-slides.html#modèle-général",
    "title": "Généralités",
    "section": "Modèle général",
    "text": "Modèle général\n\n\\[Y = f(X) + \\varepsilon\\]\n\n\\(f\\) : fonction déterministe (information systématique).\n\\(\\varepsilon\\) : terme d’erreur aléatoire."
  },
  {
    "objectID": "slides/03-generalities-bias-variance-slides.html#hypothèses-du-modèle",
    "href": "slides/03-generalities-bias-variance-slides.html#hypothèses-du-modèle",
    "title": "Généralités",
    "section": "Hypothèses du modèle",
    "text": "Hypothèses du modèle\n\nSur le terme d’erreur \\(\\varepsilon\\) :\n\nIndépendant des variables explicatives \\(X\\).\n\\(\\mathbb{E}[\\varepsilon] = 0\\) (espérance nulle).\n\\(\\mathrm{Var}(\\varepsilon) = \\sigma^2\\) (variance constante).\n\n\n→ Cadre général pour toutes les méthodes du cours."
  },
  {
    "objectID": "slides/03-generalities-bias-variance-slides.html#plan",
    "href": "slides/03-generalities-bias-variance-slides.html#plan",
    "title": "Généralités",
    "section": "Plan",
    "text": "Plan\n\n\nMesures de qualité - MSE et taux d’erreur\nCompromis biais/variance - Décomposition fondamentale\nFlexibilité des modèles - Rigidité vs adaptabilité\nOptimisation pratique - Trouver l’équilibre"
  },
  {
    "objectID": "slides/03-generalities-bias-variance-slides.html#comment-mesurer-la-qualité",
    "href": "slides/03-generalities-bias-variance-slides.html#comment-mesurer-la-qualité",
    "title": "Généralités",
    "section": "Comment mesurer la qualité ?",
    "text": "Comment mesurer la qualité ?\n\nUne fois \\(\\hat{f}\\) estimé, comment évaluer \\(\\hat{Y} = \\hat{f}(X)\\) ?\n\n\nIdée : Mesurer à quel point \\(\\hat{Y}\\) est proche de la vraie valeur \\(Y\\)."
  },
  {
    "objectID": "slides/03-generalities-bias-variance-slides.html#variables-quantitatives-eqm-mse",
    "href": "slides/03-generalities-bias-variance-slides.html#variables-quantitatives-eqm-mse",
    "title": "Généralités",
    "section": "Variables quantitatives : EQM (MSE)",
    "text": "Variables quantitatives : EQM (MSE)\n\n\n\nErreur Quadratique Moyenne (MSE)\n\n\n\\[\\text{MSE}(Y, \\hat{Y}) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{f}(x_i))^2\\]\n\n\n\n\nInterprétation → Distance moyenne entre valeurs observées et prédites.\n\n\nMSE faible → Prédictions proches des observations."
  },
  {
    "objectID": "slides/03-generalities-bias-variance-slides.html#variables-qualitatives-taux-derreur-er",
    "href": "slides/03-generalities-bias-variance-slides.html#variables-qualitatives-taux-derreur-er",
    "title": "Généralités",
    "section": "Variables qualitatives : Taux d’erreur (ER)",
    "text": "Variables qualitatives : Taux d’erreur (ER)\n\n\n\nTaux d’erreur (ER)\n\n\n\\[\\text{ER}(Y, \\hat{Y}) = \\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{1}(y_i \\neq \\hat{f}(x_i))\\]\n\n\n\n\nInterprétation → Proportion de mauvaises prédictions.\n\n\nER faible → Prédictions proches des observations."
  },
  {
    "objectID": "slides/03-generalities-bias-variance-slides.html#exactitude-vs-interprétabilité",
    "href": "slides/03-generalities-bias-variance-slides.html#exactitude-vs-interprétabilité",
    "title": "Généralités",
    "section": "Exactitude vs Interprétabilité",
    "text": "Exactitude vs Interprétabilité\n\nModèles simples (ex: régression linéaire) :\n\nFaciles à interpréter\nRelations complexes mal captées\n\nModèles flexibles (ex: forêt aléatoire) :\n\nMeilleures prédictions\nDifficiles à interpréter\n\n\nLe choix dépend de l’objectif : compréhension ou performance ?"
  },
  {
    "objectID": "slides/03-generalities-bias-variance-slides.html#no-free-lunch-in-statistics",
    "href": "slides/03-generalities-bias-variance-slides.html#no-free-lunch-in-statistics",
    "title": "Généralités",
    "section": "“No Free Lunch in Statistics”",
    "text": "“No Free Lunch in Statistics”\n\n\n\n\nIl n’existe pas de méthode universellement optimale !\n\n\nUne méthode performante dans un contexte peut échouer ailleurs.\n→ Toujours adapter l’approche au problème."
  },
  {
    "objectID": "slides/03-generalities-bias-variance-slides.html#erreur-de-prédiction",
    "href": "slides/03-generalities-bias-variance-slides.html#erreur-de-prédiction",
    "title": "Généralités",
    "section": "Erreur de prédiction",
    "text": "Erreur de prédiction\n\nNotre vrai objectif : Minimiser l’erreur sur de nouvelles données\n\\[\\mathbb{E}[(Y - \\hat{Y})^2] = \\mathbb{E}[(Y - \\hat{f}(X))^2]\\]\n\n\nCette erreur se décompose en 3 parties…"
  },
  {
    "objectID": "slides/03-generalities-bias-variance-slides.html#décomposition-biaisvariance",
    "href": "slides/03-generalities-bias-variance-slides.html#décomposition-biaisvariance",
    "title": "Généralités",
    "section": "Décomposition biais/variance",
    "text": "Décomposition biais/variance\n\n\n\nDécomposition biais/variance\n\n\n\\[\\mathbb{E}[(Y - \\hat{f}(X))^2] = \\text{Biais}(\\hat{f}(X))^2 + \\text{Var}(\\hat{f}(X)) + \\sigma^2\\]\n\n\n\nTrois composantes :\n\nBiais\\(^2\\) → Erreur systématique d’approximation.\nVariance → Sensibilité aux fluctuations d’échantillon.\n\\(\\sigma^2\\) → Erreur irréductible (bruit intrinsèque)."
  },
  {
    "objectID": "slides/03-generalities-bias-variance-slides.html#décomposition-biaisvariance-2",
    "href": "slides/03-generalities-bias-variance-slides.html#décomposition-biaisvariance-2",
    "title": "Généralités",
    "section": "Décomposition biais/variance",
    "text": "Décomposition biais/variance\n\nModèle peu flexible (ex: régression linéaire)\n\n✅ Variance faible\n❌ Biais élevé\n\nModèle très flexible (ex: régression avec beaucoup de polynômes)\n\n❌ Variance élevée\n✅ Biais faible\n\n\n→ Besoin d’un équilibre optimal !"
  },
  {
    "objectID": "slides/03-generalities-bias-variance-slides.html#décomposition-biaisvariance-3",
    "href": "slides/03-generalities-bias-variance-slides.html#décomposition-biaisvariance-3",
    "title": "Généralités",
    "section": "Décomposition biais/variance",
    "text": "Décomposition biais/variance\n\n\nPar Bigbossfarin - Own work, CC0, Link."
  },
  {
    "objectID": "slides/03-generalities-bias-variance-slides.html#stratégies-doptimisation",
    "href": "slides/03-generalities-bias-variance-slides.html#stratégies-doptimisation",
    "title": "Généralités",
    "section": "Stratégies d’optimisation",
    "text": "Stratégies d’optimisation\n\nComment trouver l’équilibre ?\n\nValidation croisée → Estimation de l’erreur de prédiction.\nSélection de modèle → Comparer différentes des modèles avec différentes flexibilités.\nMéthodes d’ensembles → Combiner plusieurs modèles.\nRégularisation → Contrôler la complexité du modèle."
  },
  {
    "objectID": "slides/03-generalities-bias-variance-slides.html#implications-pour-la-pratique",
    "href": "slides/03-generalities-bias-variance-slides.html#implications-pour-la-pratique",
    "title": "Généralités",
    "section": "Implications pour la pratique",
    "text": "Implications pour la pratique\n\nQuestions à se poser :\n\nAi-je assez de données pour un modèle flexible ?\nMon objectif est-il la prédiction ou la compréhension ?\nQuelle est la complexité réelle du phénomène étudié ?\nComment évaluer la performance sur nouvelles données ?"
  },
  {
    "objectID": "slides/03-generalities-bias-variance-slides.html#conclusion",
    "href": "slides/03-generalities-bias-variance-slides.html#conclusion",
    "title": "Généralités",
    "section": "Conclusion",
    "text": "Conclusion\n\nErreur totale = Biais\\(^2\\) + Variance + Bruit irréductible.\nCompromis entre biais et variance.\nModèles rigides → Fort biais, faible variance.\nModèles flexibles → Faible biais, forte variance.\nOptimum → Équilibre minimisant l’erreur totale.\nPas de solution universelle → Il faut s’adapter au contexte.\n\n\nProchaine étape → Validation et sélection de modèles."
  },
  {
    "objectID": "slides/04-dimension-ca-slides-annotated.html#limite-de-lacp",
    "href": "slides/04-dimension-ca-slides-annotated.html#limite-de-lacp",
    "title": "Dimension",
    "section": "Limite de l’ACP",
    "text": "Limite de l’ACP\n\nL’analyse en composantes principales est restreinte à des données quantitatives.\n\n→ Comment faire lorsque l’on a des données qualitatives ?"
  },
  {
    "objectID": "slides/04-dimension-ca-slides-annotated.html#plan",
    "href": "slides/04-dimension-ca-slides-annotated.html#plan",
    "title": "Dimension",
    "section": "Plan",
    "text": "Plan\n\n\nLa théorie de l’analyse factorielle des correspondances\nL’AFC en pratique\nExemple : le taux d’emploi au Canada"
  },
  {
    "objectID": "slides/04-dimension-ca-slides-annotated.html#analyse-factorielle-des-correspondances",
    "href": "slides/04-dimension-ca-slides-annotated.html#analyse-factorielle-des-correspondances",
    "title": "Dimension",
    "section": "Analyse factorielle des correspondances",
    "text": "Analyse factorielle des correspondances\n\nL’Analyse Factorielle des Correspondances (AFC) est une méthode d’analyse exploratoire pour représenter graphiquement les relations entre les modalités de deux variables qualitatives.\n\nObjectif → Trouver une représentation en faible dimension tout en gardant le maximum d’information possible."
  },
  {
    "objectID": "slides/04-dimension-ca-slides-annotated.html#notation",
    "href": "slides/04-dimension-ca-slides-annotated.html#notation",
    "title": "Dimension",
    "section": "Notation",
    "text": "Notation\n\nTableau de contingence:\n\\[K = (k_{ij}) \\text{ avec } k_{ij} = \\text{ effectif classe } i \\in \\{1, \\dots, n\\}, \\text{ catégorie } j \\in \\{1, \\dots, p\\}.\\]"
  },
  {
    "objectID": "slides/04-dimension-ca-slides-annotated.html#notation-1",
    "href": "slides/04-dimension-ca-slides-annotated.html#notation-1",
    "title": "Dimension",
    "section": "Notation",
    "text": "Notation\n\nTableau de fréquences relatives :\n\\[F = (f_{ij}) \\text{ où } f_{ij} = \\frac{k_{ij}}{k_{\\bullet\\bullet}}.\\]"
  },
  {
    "objectID": "slides/04-dimension-ca-slides-annotated.html#notation-2",
    "href": "slides/04-dimension-ca-slides-annotated.html#notation-2",
    "title": "Dimension",
    "section": "Notation",
    "text": "Notation\n\nMarges :\n\nLigne : \\(f_{i\\bullet} = \\sum_{j=1}^p f_{ij}\\)\nColonne : \\(f_{\\bullet j} = \\sum_{i=1}^n f_{ij}\\)\nTotal : \\(f_{\\bullet\\bullet} = 1\\)"
  },
  {
    "objectID": "slides/04-dimension-ca-slides-annotated.html#exemple-concret",
    "href": "slides/04-dimension-ca-slides-annotated.html#exemple-concret",
    "title": "Dimension",
    "section": "Exemple concret",
    "text": "Exemple concret\n\n\n\n\n\n\n\n\n\n\n\n\nCollège\nUniversité Laval\nAutre université\nHors Québec\n\\(f_{i \\bullet}\\)\n\n\n\n\nActuariat\n0.08\n0\n0\n0.04\n0.12\n\n\nStatistique\n0.08\n0.16\n0.04\n0\n0.28\n\n\nBio-info\n0.16\n0.08\n0\n0.08\n0.32\n\n\nFinance\n0.08\n0\n0\n0\n0.08\n\n\nMaths\n0.04\n0\n0\n0\n0.04\n\n\nInfo\n0.08\n0.04\n0\n0.04\n0.16\n\n\n\\(f_{\\bullet j}\\)\n0.52\n0.28\n0.04\n0.16\n1"
  },
  {
    "objectID": "slides/04-dimension-ca-slides-annotated.html#indépendance-statistique",
    "href": "slides/04-dimension-ca-slides-annotated.html#indépendance-statistique",
    "title": "Dimension",
    "section": "Indépendance statistique",
    "text": "Indépendance statistique\n\nTest d’indépendance : Si les variables sont indépendantes, \\[f_{ij} \\approx f_{i\\bullet} f_{\\bullet j}\\]"
  },
  {
    "objectID": "slides/04-dimension-ca-slides-annotated.html#indépendance-statistique-1",
    "href": "slides/04-dimension-ca-slides-annotated.html#indépendance-statistique-1",
    "title": "Dimension",
    "section": "Indépendance statistique",
    "text": "Indépendance statistique\n\nTest du \\(\\chi^2\\) :\n\\[T = \\sum_{i,j} \\frac{(k_{ij} - \\mathbb{E}(k_{ij}))^2}{\\mathbb{E}(k_{ij})} = \\sum_{i,j} \\frac{(k_{ij} - \\frac{k_{i\\bullet}k_{\\bullet j}}{k_{\\bullet\\bullet}})^2}{\\frac{k_{i\\bullet}k_{\\bullet j}}{k_{\\bullet\\bullet}}}\\]\nSi \\(T \\approx 0\\) → indépendance\nSi \\(T\\) grand → dépendance (intéressant pour l’AFC)"
  },
  {
    "objectID": "slides/04-dimension-ca-slides-annotated.html#profils-lignes-et-profils-colonnes",
    "href": "slides/04-dimension-ca-slides-annotated.html#profils-lignes-et-profils-colonnes",
    "title": "Dimension",
    "section": "Profils-lignes et profils-colonnes",
    "text": "Profils-lignes et profils-colonnes\nProfil-ligne \\(i\\) : Répartition de la modalité \\(i\\) parmi les colonnes \\[L_i = \\left(\\frac{f_{i1}}{f_{i\\bullet}}, \\ldots, \\frac{f_{ip}}{f_{i\\bullet}}\\right).\\]\n\nProfil-colonne \\(j\\) : Répartition de la modalité \\(j\\) parmi les lignes \\[C_j = \\left(\\frac{f_{1j}}{f_{\\bullet j}}, \\ldots, \\frac{f_{nj}}{f_{\\bullet j}}\\right).\\]\n\n\nLes profils sont des distributions conditionnelles."
  },
  {
    "objectID": "slides/04-dimension-ca-slides-annotated.html#profils-moyens",
    "href": "slides/04-dimension-ca-slides-annotated.html#profils-moyens",
    "title": "Dimension",
    "section": "Profils moyens",
    "text": "Profils moyens\n\nProfil-ligne moyen = fréquences marginales colonnes : \\[\\text{Profil moyen} = (f_{\\bullet 1}, \\ldots, f_{\\bullet p})\\]\nProfil-colonne moyen = fréquences marginales lignes : \\[\\text{Profil moyen} = (f_{1\\bullet}, \\ldots, f_{n\\bullet})\\]\nIndépendance \\(\\Longleftrightarrow\\) Tous les profils \\(\\approx\\) profils moyens."
  },
  {
    "objectID": "slides/04-dimension-ca-slides-annotated.html#distance-du-chi2",
    "href": "slides/04-dimension-ca-slides-annotated.html#distance-du-chi2",
    "title": "Dimension",
    "section": "Distance du \\(\\chi^2\\)",
    "text": "Distance du \\(\\chi^2\\)\nDistance entre profils-lignes : \\[d^2(L_i, L_{i'}) = \\sum_{j=1}^p \\frac{1}{f_{\\bullet j}} \\left(\\frac{f_{ij}}{f_{i\\bullet}} - \\frac{f_{i'j}}{f_{i'\\bullet}}\\right)^2\\]\nDistance entre profils-colonnes : \\[d^2(C_j, C_{j'}) = \\sum_{i=1}^n \\frac{1}{f_{i\\bullet}} \\left(\\frac{f_{ij}}{f_{\\bullet j}} - \\frac{f_{ij'}}{f_{\\bullet j'}}\\right)^2\\]"
  },
  {
    "objectID": "slides/04-dimension-ca-slides-annotated.html#formulation-matricielle",
    "href": "slides/04-dimension-ca-slides-annotated.html#formulation-matricielle",
    "title": "Dimension",
    "section": "Formulation matricielle",
    "text": "Formulation matricielle\nMatrices de poids :\n\n\\(D_n = \\text{diag}(f_{i\\bullet})\\) → poids des lignes\n\\(D_p = \\text{diag}(f_{\\bullet j})\\) → poids des colonnes\n\n\nDistance du \\(\\chi^2\\) matricielle :\n\nProfils-lignes : \\(d^2(L_i, L_{i'}) = (L_i - L_{i'})^\\top D_p^{-1} (L_i - L_{i'})\\)\nProfils-colonnes : \\(d^2(C_j, C_{j'}) = (C_j - C_{j'})^\\top D_n^{-1} (C_j - C_{j'})\\)"
  },
  {
    "objectID": "slides/04-dimension-ca-slides-annotated.html#analyse-directe-profils-lignes",
    "href": "slides/04-dimension-ca-slides-annotated.html#analyse-directe-profils-lignes",
    "title": "Dimension",
    "section": "Analyse directe (profils-lignes)",
    "text": "Analyse directe (profils-lignes)\nObjectif : Maximiser la variance projetée des profils-lignes\n\nProblème d’optimisation : \\[\\max_u u^\\top D_p^{-1} F^\\top D_n^{-1} F D_p^{-1} u \\quad \\text{s.c.} \\quad u^\\top D_p^{-1} u = 1\\]\n\n\nSolution : Vecteurs propres de la matrice \\[S = F^\\top D_n^{-1} F D_p^{-1}\\]\n\n\nPremier axe factoriel : \\(S u_1 = \\lambda_1 u_1\\)"
  },
  {
    "objectID": "slides/04-dimension-ca-slides-annotated.html#analyse-duale-profils-colonnes",
    "href": "slides/04-dimension-ca-slides-annotated.html#analyse-duale-profils-colonnes",
    "title": "Dimension",
    "section": "Analyse duale (profils-colonnes)",
    "text": "Analyse duale (profils-colonnes)\nObjectif : Maximiser la variance projetée des profils-colonnes\n\nProblème d’optimisation similaire avec matrice : \\[T = F D_p^{-1} F^\\top D_n^{-1}\\]\n\n\nPremier axe factoriel : \\(T v_1 = \\mu_1 v_1\\)"
  },
  {
    "objectID": "slides/04-dimension-ca-slides-annotated.html#propriété",
    "href": "slides/04-dimension-ca-slides-annotated.html#propriété",
    "title": "Dimension",
    "section": "Propriété",
    "text": "Propriété\n\n\n\n\n\n\nPropriété\n\n\n\\(S\\) et \\(T\\) ont les mêmes \\(r = \\min(n-1, p-1)\\) valeurs propres positives !"
  },
  {
    "objectID": "slides/04-dimension-ca-slides-annotated.html#relations-entre-analyses",
    "href": "slides/04-dimension-ca-slides-annotated.html#relations-entre-analyses",
    "title": "Dimension",
    "section": "Relations entre analyses",
    "text": "Relations entre analyses\nCohérence des représentations : \\(\\lambda_k = \\mu_k\\) pour \\(k = 1, \\ldots, r\\)\nRelations entre vecteurs propres : \\[u_k = \\frac{1}{\\sqrt{\\lambda_k}} F^\\top D_n^{-1} v_k.\\] \\[v_k = \\frac{1}{\\sqrt{\\lambda_k}} F D_p^{-1} u_k.\\]\nCette propriété garantit une représentation cohérente dans le même espace réduit."
  },
  {
    "objectID": "slides/04-dimension-ca-slides-annotated.html#afc-comme-double-acp",
    "href": "slides/04-dimension-ca-slides-annotated.html#afc-comme-double-acp",
    "title": "Dimension",
    "section": "AFC comme double ACP",
    "text": "AFC comme double ACP\n\n\n\n\n\n\nRemarque\n\n\nL’AFC peut être vue comme une double ACP pondérée :\n\nACP des profils-lignes dans \\(\\mathbb{R}^p\\),\nACP des profils-colonnes dans \\(\\mathbb{R}^n\\),\nAvec métriques du \\(\\chi^2\\) adaptées à chaque espace."
  },
  {
    "objectID": "slides/04-dimension-ca-slides-annotated.html#coordonnées-factorielles",
    "href": "slides/04-dimension-ca-slides-annotated.html#coordonnées-factorielles",
    "title": "Dimension",
    "section": "Coordonnées factorielles",
    "text": "Coordonnées factorielles\nCoordonnées des profils-lignes sur l’axe \\(k\\) :\n\\[\\Phi_k = D_n^{-1} F D_p^{-1} u_k.\\]\nCoordonnées des profils-colonnes sur l’axe \\(k\\) :\n\\[\\Psi_k = D_p^{-1} F^\\top D_n^{-1} v_k.\\]\nRelations entre coordonnées : \\[\\Phi_k = \\frac{1}{\\sqrt{\\lambda_k}} D_n^{-1} F \\Psi_k.\\]"
  },
  {
    "objectID": "slides/04-dimension-ca-slides-annotated.html#centrage-et-centre-de-gravité",
    "href": "slides/04-dimension-ca-slides-annotated.html#centrage-et-centre-de-gravité",
    "title": "Dimension",
    "section": "Centrage et centre de gravité",
    "text": "Centrage et centre de gravité\n\nConvention : Représentation centrée en \\((0,0)\\) dans les logiciels.\n\nCentres de gravité :\n\nLignes : \\(G_L = (f_{\\bullet 1}, \\ldots, f_{\\bullet p})^\\top\\).\nColonnes : \\(G_C = (f_{1\\bullet}, \\ldots, f_{n\\bullet})^\\top\\).\n\n\n\nCentrage : Soustraire les profils moyens\n\\[\\frac{f_{ij}}{f_{i\\bullet}} - f_{\\bullet j} = \\frac{f_{ij} - f_{i\\bullet}f_{\\bullet j}}{f_{i\\bullet}}.\\]"
  },
  {
    "objectID": "slides/04-dimension-ca-slides-annotated.html#afc-sur-matrice-centrée",
    "href": "slides/04-dimension-ca-slides-annotated.html#afc-sur-matrice-centrée",
    "title": "Dimension",
    "section": "AFC sur matrice centrée",
    "text": "AFC sur matrice centrée\n\nMatrice centrée : \\(S^* = (s_{jj'}^*)\\) avec\n\\[s_{jj'}^* = \\sum_{i=1}^n \\frac{(f_{ij} - f_{i\\bullet}f_{\\bullet j})(f_{ij'} - f_{i\\bullet}f_{\\bullet j'})}{f_{i\\bullet}f_{\\bullet j'}}.\\]"
  },
  {
    "objectID": "slides/04-dimension-ca-slides-annotated.html#inertie-totale",
    "href": "slides/04-dimension-ca-slides-annotated.html#inertie-totale",
    "title": "Dimension",
    "section": "Inertie totale",
    "text": "Inertie totale\n\nInertie totale :\n\\[\\text{tr}(S^*) = \\sum_{j=1}^p \\sum_{i=1}^n \\frac{(f_{ij} - f_{i\\bullet}f_{\\bullet j})^2}{f_{i\\bullet}f_{\\bullet j}}\\]\nCette inertie correspond à la statistique du \\(\\chi^2\\) normalisée."
  },
  {
    "objectID": "slides/04-dimension-ca-slides-annotated.html#représentation-barycentrique",
    "href": "slides/04-dimension-ca-slides-annotated.html#représentation-barycentrique",
    "title": "Dimension",
    "section": "Représentation barycentrique",
    "text": "Représentation barycentrique\nSur chaque axe factoriel,\n\\[[\\Phi_k]_i = \\frac{1}{\\sqrt{\\lambda_k}} \\sum_{j=1}^p \\frac{f_{ij}}{f_{i\\bullet}} [\\Psi_k]_j,\\]\n\\[[\\Psi_k]_j = \\frac{1}{\\sqrt{\\lambda_k}} \\sum_{i=1}^n \\frac{f_{ij}}{f_{\\bullet j}} [\\Phi_k]_i.\\]\nInterprétation : Chaque profil-ligne est au barycentre des profils-colonnes (et vice versa)."
  },
  {
    "objectID": "slides/04-dimension-ca-slides-annotated.html#double-représentation-barycentrique",
    "href": "slides/04-dimension-ca-slides-annotated.html#double-représentation-barycentrique",
    "title": "Dimension",
    "section": "Double représentation barycentrique",
    "text": "Double représentation barycentrique\n\n\n\n\n\n\nPropriété fondamentale de l’AFC :\n\n\nSur les axes factoriels, chaque point d’un nuage est au barycentre des points de l’autre nuage, pondéré par les contributions respectives.\n\n\n\nCette propriété permet une interprétation conjointe des modalités des deux variables dans le même graphique."
  },
  {
    "objectID": "slides/04-dimension-ca-slides-annotated.html#interprétation-des-résultats",
    "href": "slides/04-dimension-ca-slides-annotated.html#interprétation-des-résultats",
    "title": "Dimension",
    "section": "Interprétation des résultats",
    "text": "Interprétation des résultats\n\nProximité géométrique = Similarité des profils\nAxes factoriels : Directions de variation maximale.\nContribution des modalités : Importance dans la construction des axes.\nQualité de représentation : Proportion de l’inertie d’un point expliquée par les axes retenus."
  },
  {
    "objectID": "slides/04-dimension-ca-slides-annotated.html#exemple-le-taux-demploi-au-canada",
    "href": "slides/04-dimension-ca-slides-annotated.html#exemple-le-taux-demploi-au-canada",
    "title": "Dimension",
    "section": "Exemple : le taux d’emploi au Canada",
    "text": "Exemple : le taux d’emploi au Canada\nOn considère le jeu de données suivant :\n\n\n\n\n\nprovince\nfull-time\npart-time\nunemployed\n\n\n\n\nNewfoundland and Labrador\n205.0\n39.3\n27.3\n\n\nPrince Edward Island\n82.1\n11.9\n7.6\n\n\nNova Scotia\n437.7\n87.2\n34.0\n\n\nNew Brunswick\n346.8\n54.8\n30.0\n\n\nQuebec\n3762.6\n864.7\n279.6\n\n\nOntario\n6721.8\n1508.0\n667.2\n\n\nManitoba\n608.4\n122.6\n46.3\n\n\nSaskatchewan\n498.1\n115.4\n31.7\n\n\nAlberta\n2084.6\n466.2\n195.0\n\n\nBritish Columbia\n2357.1\n587.8\n190.1"
  },
  {
    "objectID": "slides/04-dimension-ca-slides-annotated.html#exemple-le-taux-demploi-au-canada-1",
    "href": "slides/04-dimension-ca-slides-annotated.html#exemple-le-taux-demploi-au-canada-1",
    "title": "Dimension",
    "section": "Exemple : le taux d’emploi au Canada",
    "text": "Exemple : le taux d’emploi au Canada\n\nCodePlotValeurs propres\n\n\n\nlibrary(FactoMineR)\nlibrary(factoextra)\nres_ca &lt;- df |&gt;\n  column_to_rownames(var = 'province') |&gt; \n  CA(graph = FALSE)\nfviz_eig(res_ca)\nget_eigenvalue(res_ca)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\neigenvalue\nvariance.percent\ncumulative.variance.percent\n\n\n\n\nDim.1\n0.0014335\n70.77104\n70.77104\n\n\nDim.2\n0.0005921\n29.22896\n100.00000"
  },
  {
    "objectID": "slides/04-dimension-ca-slides-annotated.html#exemple-le-taux-demploi-au-canada-2",
    "href": "slides/04-dimension-ca-slides-annotated.html#exemple-le-taux-demploi-au-canada-2",
    "title": "Dimension",
    "section": "Exemple : le taux d’emploi au Canada",
    "text": "Exemple : le taux d’emploi au Canada\n\nCodePlot\n\n\n\n# Résultat pour les lignes\nfviz_ca_row(res_ca, repel = TRUE)"
  },
  {
    "objectID": "slides/04-dimension-ca-slides-annotated.html#exemple-le-taux-demploi-au-canada-3",
    "href": "slides/04-dimension-ca-slides-annotated.html#exemple-le-taux-demploi-au-canada-3",
    "title": "Dimension",
    "section": "Exemple : le taux d’emploi au Canada",
    "text": "Exemple : le taux d’emploi au Canada\n\nCodeContribution à la première composante\n\n\n\n# Visualisation des lignes sur l'axe 1\nfviz_contrib(res_ca, choice =\"row\", axes = 1)"
  },
  {
    "objectID": "slides/04-dimension-ca-slides-annotated.html#exemple-le-taux-demploi-au-canada-4",
    "href": "slides/04-dimension-ca-slides-annotated.html#exemple-le-taux-demploi-au-canada-4",
    "title": "Dimension",
    "section": "Exemple : le taux d’emploi au Canada",
    "text": "Exemple : le taux d’emploi au Canada\n\nCodeContribution à la première composante\n\n\n\n# Visualisation des lignes sur l'axe 2\nfviz_contrib(res_ca, choice =\"row\", axes = 2)"
  },
  {
    "objectID": "slides/04-dimension-ca-slides-annotated.html#exemple-le-taux-demploi-au-canada-5",
    "href": "slides/04-dimension-ca-slides-annotated.html#exemple-le-taux-demploi-au-canada-5",
    "title": "Dimension",
    "section": "Exemple : le taux d’emploi au Canada",
    "text": "Exemple : le taux d’emploi au Canada\n\nCodeBiplot\n\n\n\nfviz_ca_biplot(res_ca, repel = TRUE)"
  },
  {
    "objectID": "slides/04-dimension-ca-slides-annotated.html#conclusion",
    "href": "slides/04-dimension-ca-slides-annotated.html#conclusion",
    "title": "Dimension",
    "section": "Conclusion",
    "text": "Conclusion\n\nL’AFC est un outil pour :\n\nExplorer les relations entre variables qualitatives\nVisualiser des structures complexes en 2D/3D\nIdentifier des associations non évidentes\nInterpréter conjointement lignes et colonnes\n\nProchaine étape → Que faire si on a plus que deux variables qualitatives ?"
  },
  {
    "objectID": "slides/02-revisions-programming-slides.html#philosophie-du-cours",
    "href": "slides/02-revisions-programming-slides.html#philosophie-du-cours",
    "title": "Révisions",
    "section": "Philosophie du cours",
    "text": "Philosophie du cours\n\n\n\nLiberté de choix\n\n\nCe cours n’impose aucun langage spécifique !\nVous êtes libres de choisir :\n\nR, Python, Julia, ou même SAS ou C++.\nLe langage qui vous convient le mieux.\n\n\n\n\n\nObjectif commun : Code lisible, compréhensible et réutilisable."
  },
  {
    "objectID": "slides/02-revisions-programming-slides.html#plan-de-la-présentation",
    "href": "slides/02-revisions-programming-slides.html#plan-de-la-présentation",
    "title": "Révisions",
    "section": "Plan de la présentation",
    "text": "Plan de la présentation\n\n\nBonnes pratiques universelles\nComment choisir ?"
  },
  {
    "objectID": "slides/02-revisions-programming-slides.html#principes-fondamentaux",
    "href": "slides/02-revisions-programming-slides.html#principes-fondamentaux",
    "title": "Révisions",
    "section": "Principes fondamentaux",
    "text": "Principes fondamentaux\n\n\nCode lisible : les autres (et vous dans 6 mois !) doivent comprendre\nDocumentation : commentaires clairs et concis\nStructure : organisation logique des fichiers et fonctions\nReproductibilité : mêmes données → mêmes résultats\nGestion des versions : Git pour suivre les modifications"
  },
  {
    "objectID": "slides/02-revisions-programming-slides.html#exemple-code-mal-écrit-vs.-bien-écrit",
    "href": "slides/02-revisions-programming-slides.html#exemple-code-mal-écrit-vs.-bien-écrit",
    "title": "Révisions",
    "section": "Exemple : Code mal écrit vs. bien écrit",
    "text": "Exemple : Code mal écrit vs. bien écrit\n\n❌ Code difficile à comprendre :\nimport pandas as pd\nd=pd.read_csv('data.csv')\nr=d.groupby('A').mean()['B']\nprint(r)\n\n✅ Code clair et documenté :\nimport pandas as pd\n\n# Chargement des données de ventes\nsales_data = pd.read_csv('monthly_sales.csv')\n\n# Calcul de la moyenne des ventes par région\naverage_sales_by_region = sales_data.groupby('region').mean()['sales_amount']\n\nprint(f\"Ventes moyennes par région : {average_sales_by_region}\")"
  },
  {
    "objectID": "slides/02-revisions-programming-slides.html#structure-de-projet-recommandée",
    "href": "slides/02-revisions-programming-slides.html#structure-de-projet-recommandée",
    "title": "Révisions",
    "section": "Structure de projet recommandée",
    "text": "Structure de projet recommandée\n\nmon_projet/\n├── data/           # Données brutes et nettoyées\n├── scripts/        # Scripts d'analyse\n├── functions/      # Fonctions réutilisables\n├── results/        # Graphiques, tableaux, documents\n├── docs/           # Documentation\n└── README.md       # Description du projet\n\nAvantages :\n\nOrganisation claire\nFacilite la collaboration\nProjet reproductible"
  },
  {
    "objectID": "slides/02-revisions-programming-slides.html#critères-de-décision",
    "href": "slides/02-revisions-programming-slides.html#critères-de-décision",
    "title": "Révisions",
    "section": "Critères de décision",
    "text": "Critères de décision\n\n\nVotre expérience : Quel langage connaissez-vous déjà ?\nType de projet : Statistiques pures vs. ML vs. applications web\nPerformance : Avez-vous besoin de calculs très rapides ?\nÉcosystème : Quels packages/bibliothèques vous faut-il ?\nÉquipe : Quel langage utilise votre groupe/entreprise ?"
  },
  {
    "objectID": "slides/02-revisions-programming-slides.html#apprentissage-recommandé",
    "href": "slides/02-revisions-programming-slides.html#apprentissage-recommandé",
    "title": "Révisions",
    "section": "Apprentissage recommandé",
    "text": "Apprentissage recommandé\n\n\n\n\n\nMaîtrisez un langage à fond plutôt que plusieurs superficiellement\nApprenez les concepts (statistiques, ML) avant la syntaxe\nPratiquez régulièrement : projets personnels\nLisez du code des autres : GitHub, Stack Overflow\nSuivez les bonnes pratiques dès le début"
  },
  {
    "objectID": "slides/02-revisions-programming-slides.html#ressources-dapprentissage",
    "href": "slides/02-revisions-programming-slides.html#ressources-dapprentissage",
    "title": "Révisions",
    "section": "Ressources d’apprentissage",
    "text": "Ressources d’apprentissage\n\nPour tous les langages :\n\nLearn X in Y minutes (guides rapides)\nStack Overflow (questions/réponses)\nDocumentation officielle"
  },
  {
    "objectID": "slides/02-revisions-programming-slides.html#limportant-cest",
    "href": "slides/02-revisions-programming-slides.html#limportant-cest",
    "title": "Révisions",
    "section": "L’important c’est…",
    "text": "L’important c’est…\n\n\n\n\nComprendre les concepts mathématiques et statistiques\nÉcrire du code lisible et documenté\nReproduire ses analyses\nContinuer d’apprendre et s’adapter\n\n\n\n\nLe langage de programmation n’est qu’un outil pour exprimer vos idées !"
  },
  {
    "objectID": "slides/02-revisions-algebre-slides.html#plan-de-la-présentation",
    "href": "slides/02-revisions-algebre-slides.html#plan-de-la-présentation",
    "title": "Révisions",
    "section": "Plan de la présentation",
    "text": "Plan de la présentation\n\n\nPropriétés matricielles\nValeurs et vecteurs propres\nDiagonalisation de matrices"
  },
  {
    "objectID": "slides/02-revisions-algebre-slides.html#notations-de-base",
    "href": "slides/02-revisions-algebre-slides.html#notations-de-base",
    "title": "Révisions",
    "section": "Notations de base",
    "text": "Notations de base\n\n\n\\(M_{n, m}(\\mathbb{R})\\) : matrices à \\(n\\) lignes et \\(m\\) colonnes\n\\(M_{n}(\\mathbb{R})\\) : matrices carrées de taille \\(n\\)\n\\(I_n\\) : matrice identité de taille \\(n\\)\n\\(u, v \\in \\mathbb{R}^n\\) : vecteurs colonnes"
  },
  {
    "objectID": "slides/02-revisions-algebre-slides.html#propriétés-du-déterminant",
    "href": "slides/02-revisions-algebre-slides.html#propriétés-du-déterminant",
    "title": "Révisions",
    "section": "Propriétés du déterminant",
    "text": "Propriétés du déterminant\n\n\n\n\nPropriétés\n\n\n\n\\(\\det(A^\\top) = \\det(A)\\)\n\\(\\det(AB) = \\det(A)\\det(B)\\)\n\\(\\det(A^{-1}) = \\frac{1}{\\det(A)}\\)"
  },
  {
    "objectID": "slides/02-revisions-algebre-slides.html#propriétés-du-déterminant-1",
    "href": "slides/02-revisions-algebre-slides.html#propriétés-du-déterminant-1",
    "title": "Révisions",
    "section": "Propriétés du déterminant",
    "text": "Propriétés du déterminant\n\nCas particulier où \\(A \\in M_{2}(\\mathbb{R})\\).\nSoit \\(A = \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}\\). Alors\n\\[\\det(A) = ad - bc.\\]"
  },
  {
    "objectID": "slides/02-revisions-algebre-slides.html#propriété-du-déterminant-exemple-wooclap-kffpdj",
    "href": "slides/02-revisions-algebre-slides.html#propriété-du-déterminant-exemple-wooclap-kffpdj",
    "title": "Révisions",
    "section": "Propriété du déterminant : exemple (Wooclap: KFFPDJ)",
    "text": "Propriété du déterminant : exemple (Wooclap: KFFPDJ)\n\n\\[A = \\begin{pmatrix} 1 & 2 \\\\ 3 & 4 \\end{pmatrix}, \\quad B = \\begin{pmatrix} 2 & 0 \\\\ 1 & 1 \\end{pmatrix}\\]\n\nOn trouve que \\(\\det(A) = 4 - 6 = -2\\), \\(\\det(B) = 2 - 0 = 2\\). Donc\n\\[\\det(AB) = \\det(A) \\det(B) = -4\\]"
  },
  {
    "objectID": "slides/02-revisions-algebre-slides.html#inverse-de-matrices",
    "href": "slides/02-revisions-algebre-slides.html#inverse-de-matrices",
    "title": "Révisions",
    "section": "Inverse de matrices",
    "text": "Inverse de matrices\n\n\n\n\nPropriété\n\n\nSi \\(A\\) et \\(B\\) sont inversibles, alors : \\((AB)^{-1} = B^{-1} A^{-1}\\)."
  },
  {
    "objectID": "slides/02-revisions-algebre-slides.html#inverse-de-matrices-1",
    "href": "slides/02-revisions-algebre-slides.html#inverse-de-matrices-1",
    "title": "Révisions",
    "section": "Inverse de matrices",
    "text": "Inverse de matrices\n\nCas particulier où \\(A \\in M_{2}(\\mathbb{R})\\).\nSoit \\(A = \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}\\). Alors\n\\[A^{-1} = \\frac{1}{ad - bc}\\begin{pmatrix} d & -b \\\\ -c & a \\end{pmatrix}.\\]"
  },
  {
    "objectID": "slides/02-revisions-algebre-slides.html#inverse-de-matrices-exemple-wooclap-kffpdj",
    "href": "slides/02-revisions-algebre-slides.html#inverse-de-matrices-exemple-wooclap-kffpdj",
    "title": "Révisions",
    "section": "Inverse de matrices : exemple (Wooclap: KFFPDJ)",
    "text": "Inverse de matrices : exemple (Wooclap: KFFPDJ)\n\n\\[A = \\begin{pmatrix} 2 & 0 \\\\ 0 & 3 \\end{pmatrix}, \\quad B = \\begin{pmatrix} 1 & 1 \\\\ 0 & 1 \\end{pmatrix}\\]\n\nOn a : \\(AB = \\begin{pmatrix} 2 & 2 \\\\ 0 & 3 \\end{pmatrix},\\) donc \\((AB)^{-1} = \\begin{pmatrix} 1/2 & -1/3 \\\\ 0 & 1/3 \\end{pmatrix}\\).\nEt effectivement : \\(B^{-1}A^{-1} = \\begin{pmatrix} 1 & -1 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1/2 & 0 \\\\ 0 & 1/3 \\end{pmatrix} = \\begin{pmatrix} 1/2 & -1/3 \\\\ 0 & 1/3 \\end{pmatrix}\\)."
  },
  {
    "objectID": "slides/02-revisions-algebre-slides.html#propriétés-de-la-trace",
    "href": "slides/02-revisions-algebre-slides.html#propriétés-de-la-trace",
    "title": "Révisions",
    "section": "Propriétés de la trace",
    "text": "Propriétés de la trace\n\nLa trace de la matrice \\(A \\in M_{n}(\\mathbb{R})\\) est la somme des éléments diagonaux \\[\\text{tr}(A) = \\sum_{i=1}^{n} a_{ii}.\\]"
  },
  {
    "objectID": "slides/02-revisions-algebre-slides.html#propriétés-de-la-trace-1",
    "href": "slides/02-revisions-algebre-slides.html#propriétés-de-la-trace-1",
    "title": "Révisions",
    "section": "Propriétés de la trace",
    "text": "Propriétés de la trace\n\n\n\n\nPropriétés\n\n\n\n\\(\\text{tr}(A) = \\text{tr}(A^{\\top})\\)\n\\(\\text{tr}(A + B) = \\text{tr}(A) + \\text{tr}(B)\\)\n\\(\\text{tr}(MN^{\\top}) = \\text{tr}(N^{\\top}M)\\)"
  },
  {
    "objectID": "slides/02-revisions-algebre-slides.html#propriétés-de-la-trace-exemple-wooclap-kffpdj",
    "href": "slides/02-revisions-algebre-slides.html#propriétés-de-la-trace-exemple-wooclap-kffpdj",
    "title": "Révisions",
    "section": "Propriétés de la trace : exemple (Wooclap: KFFPDJ)",
    "text": "Propriétés de la trace : exemple (Wooclap: KFFPDJ)\n\nSoit \\(X \\in M_{n, p}(\\mathbb{R})\\) avec \\(p &lt; n\\) et de rang \\(p\\), et donc telle que \\(X^\\top X\\) est de dimension \\(p \\times p\\) et inversible. Quelle est la valeur de la trace de \\(X (X^\\top X)^{-1} X^\\top\\) ?\n\n\\[\\begin{align*}\n\\text{tr}(X (X^\\top X)^{-1} X^\\top) &= \\text{tr}(X^\\top X (X^\\top X)^{-1})\\\\\n&= \\text{tr}(I_p) \\\\\n&= p.\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/02-revisions-algebre-slides.html#matrices-spéciales",
    "href": "slides/02-revisions-algebre-slides.html#matrices-spéciales",
    "title": "Révisions",
    "section": "Matrices spéciales",
    "text": "Matrices spéciales\n\n\n\nDéfinitions\n\n\nMatrice définie positive : \\(A\\) est définie positive si \\[\\text{pour tout } u \\neq 0,\\quad u^\\top A u &gt; 0.\\]\nMatrice orthogonale : \\(A\\) est orthogonale si \\[A^\\top A = A A^\\top = I_n.\\]"
  },
  {
    "objectID": "slides/02-revisions-algebre-slides.html#comment-montrer-quune-matrice-est-définie-positive",
    "href": "slides/02-revisions-algebre-slides.html#comment-montrer-quune-matrice-est-définie-positive",
    "title": "Révisions",
    "section": "Comment montrer qu’une matrice est définie positive ?",
    "text": "Comment montrer qu’une matrice est définie positive ?\n\nSoit \\(A = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix}\\) et pour tout \\(u = \\begin{pmatrix} x \\\\ y \\end{pmatrix} \\neq 0\\), on a : \\[\\begin{align*}\nu^\\top A u &= \\begin{pmatrix} x & y \\end{pmatrix} \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix} \\\\\n  &= 2x^2 + 2xy + 2y^2 \\\\\n  &= x^2 + y^2 + (x+y)^2 &gt; 0.\n\\end{align*}\\]\nDonc \\(A\\) est définie positive."
  },
  {
    "objectID": "slides/02-revisions-algebre-slides.html#valeurs-et-vecteurs-propres-1",
    "href": "slides/02-revisions-algebre-slides.html#valeurs-et-vecteurs-propres-1",
    "title": "Révisions",
    "section": "Valeurs et vecteurs propres",
    "text": "Valeurs et vecteurs propres\n\nFaire un dessin pour Au = lu.\n\n\n\n\nDéfinitions\n\n\nSoit \\(A \\in M_{n}(\\mathbb{R})\\). Le scalaire \\(\\lambda \\in \\mathbb{R}\\) est une valeur propre de \\(A\\) s’il existe \\(u \\neq 0\\) tel que : \\(Au = \\lambda u\\).\n\\(u\\) est le vecteur propre correspondant à \\(\\lambda\\).\nLe spectre de \\(A\\), noté \\(\\text{sp}(A)\\), est l’ensemble des valeurs propres."
  },
  {
    "objectID": "slides/02-revisions-algebre-slides.html#valeurs-et-vecteurs-propres-exemple",
    "href": "slides/02-revisions-algebre-slides.html#valeurs-et-vecteurs-propres-exemple",
    "title": "Révisions",
    "section": "Valeurs et vecteurs propres : exemple",
    "text": "Valeurs et vecteurs propres : exemple\n\\[A = \\begin{pmatrix} 3 & 1 \\\\ 0 & 2 \\end{pmatrix}\\]\n\nCalcul des valeurs propres : \\[\\det(A - \\lambda I) = \\det\\begin{pmatrix} 3-\\lambda & 1 \\\\ 0 & 2-\\lambda \\end{pmatrix} = (3-\\lambda)(2-\\lambda)\\]\nDonc \\(\\lambda_1 = 3\\) et \\(\\lambda_2 = 2\\)."
  },
  {
    "objectID": "slides/02-revisions-algebre-slides.html#valeurs-et-vecteurs-propres-exemple-1",
    "href": "slides/02-revisions-algebre-slides.html#valeurs-et-vecteurs-propres-exemple-1",
    "title": "Révisions",
    "section": "Valeurs et vecteurs propres : exemple",
    "text": "Valeurs et vecteurs propres : exemple\n\\[A = \\begin{pmatrix} 3 & 1 \\\\ 0 & 2 \\end{pmatrix}\\]\n\nCalcul des vecteurs propres :\n\nPour \\(\\lambda_1 = 3\\) : \\(u_1 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\\).\nPour \\(\\lambda_2 = 2\\) : \\(u_2 = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}\\)."
  },
  {
    "objectID": "slides/02-revisions-algebre-slides.html#propriétés-des-vecteurs-propres",
    "href": "slides/02-revisions-algebre-slides.html#propriétés-des-vecteurs-propres",
    "title": "Révisions",
    "section": "Propriétés des vecteurs propres",
    "text": "Propriétés des vecteurs propres\n\n\n\nPropriétés\n\n\n\nSi \\(u\\) est vecteur propre pour \\(\\lambda\\), alors \\(cu\\) l’est aussi (pour \\(c \\neq 0\\)).\nPour matrices symétriques : Si \\(u_1\\) et \\(u_2\\) sont deux vecteurs propres correspondent à des valeurs propres différentes, alors : \\[u_1^\\top u_2 = 0 \\text{ (orthogonalité)}.\\]"
  },
  {
    "objectID": "slides/02-revisions-algebre-slides.html#preuve-de-lorthogonalité",
    "href": "slides/02-revisions-algebre-slides.html#preuve-de-lorthogonalité",
    "title": "Révisions",
    "section": "Preuve de l’orthogonalité",
    "text": "Preuve de l’orthogonalité\nSoient \\(\\lambda_1 \\neq \\lambda_2\\) et \\(Au_1 = \\lambda_1 u_1\\), \\(Au_2 = \\lambda_2 u_2\\).\n\nOn a \\[\\lambda_1 u_1^\\top u_2 = u_1^\\top A u_2 = \\lambda_2 u_1^\\top u_2.\\]\n\n\nDonc \\[(\\lambda_1 - \\lambda_2)u_1^\\top u_2 = 0\\]\n\n\nComme \\(\\lambda_1 \\neq \\lambda_2\\), on a nécessairement \\(u_1^\\top u_2 = 0\\)\n→ Cette propriété est cruciale pour l’ACP !"
  },
  {
    "objectID": "slides/02-revisions-algebre-slides.html#caractérisation-des-valeurs-propres",
    "href": "slides/02-revisions-algebre-slides.html#caractérisation-des-valeurs-propres",
    "title": "Révisions",
    "section": "Caractérisation des valeurs propres",
    "text": "Caractérisation des valeurs propres\n\n\n\n\nPropriétés\n\n\n\nMatrice symétrique → toutes les valeurs propres sont réelles.\nMatrice définie positive → toutes les valeurs propres sont strictement positives."
  },
  {
    "objectID": "slides/02-revisions-algebre-slides.html#caractérisation-des-valeurs-propres-exemple",
    "href": "slides/02-revisions-algebre-slides.html#caractérisation-des-valeurs-propres-exemple",
    "title": "Révisions",
    "section": "Caractérisation des valeurs propres : exemple",
    "text": "Caractérisation des valeurs propres : exemple\n\nSoit \\(A = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix} \\text{ (symétrique et définie positive)}\\).\n\nValeurs propres : \\(\\lambda_1 = 3, \\lambda_2 = 1\\) (toutes réelles et &gt; 0).\nVecteurs propres : \\(u_1 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}, u_2 = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}\\).\nVérifions l’orthogonalité : \\(u_1^\\top u_2 = 1 \\times 1 + 1 \\times (-1) = 0\\)."
  },
  {
    "objectID": "slides/02-revisions-algebre-slides.html#diagonalisation-de-matrices",
    "href": "slides/02-revisions-algebre-slides.html#diagonalisation-de-matrices",
    "title": "Révisions",
    "section": "Diagonalisation de matrices",
    "text": "Diagonalisation de matrices\n\n\n\n\nDéfinition\n\n\nUne matrice \\(A\\) est diagonalisable s’il existe \\(P\\) inversible et \\(D\\) diagonale telles que : \\[A = P D P^{-1} \\Leftrightarrow P^{-1} A P = D.\\]\n\n\n\n\nInterprétation : On peut “simplifier” \\(A\\) en changeant de base."
  },
  {
    "objectID": "slides/02-revisions-algebre-slides.html#théorème-de-décomposition-spectrale",
    "href": "slides/02-revisions-algebre-slides.html#théorème-de-décomposition-spectrale",
    "title": "Révisions",
    "section": "Théorème de décomposition spectrale",
    "text": "Théorème de décomposition spectrale\n\n\n\n\nThéorème\n\n\nPour toute matrice symétrique \\(A\\), il existe une matrice orthogonale \\(P\\) telle que : \\(A = P \\Lambda P^\\top\\) où \\(\\Lambda = \\text{diag}(\\lambda_1, \\ldots, \\lambda_n)\\).\n\n\n\nLa matrice \\(P\\) est formée des vecteurs propres de \\(A\\) et \\(\\Lambda\\) est la matrice diagonale des valeurs propres associées."
  },
  {
    "objectID": "slides/02-revisions-algebre-slides.html#exemple",
    "href": "slides/02-revisions-algebre-slides.html#exemple",
    "title": "Révisions",
    "section": "Exemple",
    "text": "Exemple\nEn reprenant l’example précédent.\nSoit \\(A = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix}\\). On a \\(P = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 & 1 \\\\ 1 & -1 \\end{pmatrix}\\) et \\(\\Lambda = \\begin{pmatrix} 3 & 0 \\\\ 0 & 1 \\end{pmatrix}.\\)\nOn peut vérifier le théorème de décomposition spectrale :\n\\[P \\Lambda P^\\top = \\frac{1}{2}\\begin{pmatrix} 1 & 1 \\\\ 1 & -1 \\end{pmatrix}\\begin{pmatrix} 3 & 0 \\\\ 0 & 1 \\end{pmatrix}\\begin{pmatrix} 1 & 1 \\\\ 1 & -1 \\end{pmatrix} = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix} = A.\\]"
  },
  {
    "objectID": "slides/02-revisions-algebre-slides.html#lien-avec-le-déterminant-et-la-trace",
    "href": "slides/02-revisions-algebre-slides.html#lien-avec-le-déterminant-et-la-trace",
    "title": "Révisions",
    "section": "Lien avec le déterminant et la trace",
    "text": "Lien avec le déterminant et la trace\n\n\n\n\nPropriétés\n\n\nSi la matrice \\(A\\) a pour valeurs propres \\(\\lambda_1, \\ldots, \\lambda_n\\) :\n\n\\(\\det(A) = \\prod_{i=1}^{n} \\lambda_i\\).\n\\(\\text{tr}(A) = \\sum_{i=1}^{n} \\lambda_i\\)."
  },
  {
    "objectID": "slides/02-revisions-algebre-slides.html#lien-avec-le-déterminant-et-la-trace-exemple",
    "href": "slides/02-revisions-algebre-slides.html#lien-avec-le-déterminant-et-la-trace-exemple",
    "title": "Révisions",
    "section": "Lien avec le déterminant et la trace : exemple",
    "text": "Lien avec le déterminant et la trace : exemple\n\nSoit \\(A = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix}\\). On a vu que ses valeurs propres sont \\(\\lambda_1 = 3\\) et \\(\\lambda_2 = 1\\).\nOn peut donc vérifier les formules pour le déterminant et la trace :\n\\[\\det(A) = 4 - 1 = 3 = 3 \\times 1 = \\lambda_1 \\lambda_2,\\]\n\\[\\text{tr}(A) = 2 + 2 = 4 = 3 + 1 = \\lambda_1 + \\lambda_2.\\]"
  },
  {
    "objectID": "slides/04-dimension-pca-slides-annotated.html#pourquoi-réduire-la-dimension",
    "href": "slides/04-dimension-pca-slides-annotated.html#pourquoi-réduire-la-dimension",
    "title": "Dimension",
    "section": "Pourquoi réduire la dimension ?",
    "text": "Pourquoi réduire la dimension ?\n\nTravailler avec un grand nombre de variables pose plusieurs défis :\n\nVisualisation compliquée : impossible de représenter au-delà de 3 dimensions.\nSéparation des classes difficile : structure cachée dans des combinaisons.\nCoût computationnel élevé : modèles complexes difficiles à ajuster.\nCorrélations fortes : variables redondantes, modèles instables.\n\n\n→ Peut-on réduire la dimension sans perdre trop d’information ?"
  },
  {
    "objectID": "slides/04-dimension-pca-slides-annotated.html#plan",
    "href": "slides/04-dimension-pca-slides-annotated.html#plan",
    "title": "Dimension",
    "section": "Plan",
    "text": "Plan\n\n\nLa théorie de l’analyse en composantes principales\nL’ACP en pratique\nExemple : Le top 10 des artistes Spotify"
  },
  {
    "objectID": "slides/04-dimension-pca-slides-annotated.html#lapproche-pca",
    "href": "slides/04-dimension-pca-slides-annotated.html#lapproche-pca",
    "title": "Dimension",
    "section": "L’approche PCA",
    "text": "L’approche PCA\n\nPrincipe → Ne pas simplement supprimer des variables, mais construire de nouvelles variables.\n\nMéthode → Créer des combinaisons linéaires des variables initiales qui résument l’information essentielle."
  },
  {
    "objectID": "slides/04-dimension-pca-slides-annotated.html#lapproche-pca-1",
    "href": "slides/04-dimension-pca-slides-annotated.html#lapproche-pca-1",
    "title": "Dimension",
    "section": "L’approche PCA",
    "text": "L’approche PCA\n\nAnalyse en Composantes Principales (ACP) → Méthode non-supervisée pour réduire la dimension tout en conservant le maximum d’information.\n\n→ Introduite par Hotelling (1933)."
  },
  {
    "objectID": "slides/04-dimension-pca-slides-annotated.html#applications-courantes",
    "href": "slides/04-dimension-pca-slides-annotated.html#applications-courantes",
    "title": "Dimension",
    "section": "Applications courantes",
    "text": "Applications courantes\n\n\nVisualisation : Représenter des données multidimensionnelles.\nRéduction dimensionnelle : Passer de \\(p\\) à \\(k \\ll p\\) variables.\nCompression : Images ou signaux.\nExploration : Données biologiques, textuelles, environnementales, …"
  },
  {
    "objectID": "slides/04-dimension-pca-slides-annotated.html#exemples-concrets",
    "href": "slides/04-dimension-pca-slides-annotated.html#exemples-concrets",
    "title": "Dimension",
    "section": "Exemples concrets",
    "text": "Exemples concrets\n\n\nSport : Comparer des équipes de hockey sur 6 statistiques.\nCriminologie : Résumer la criminalité entre provinces sur 7 types de crimes.\nImagerie : Compresser des images de \\(1084 \\times 1084\\) pixels.\nBiologie : Identifier des variants de tumeurs à partir de millions de gènes."
  },
  {
    "objectID": "slides/04-dimension-pca-slides-annotated.html#formulation-mathématique",
    "href": "slides/04-dimension-pca-slides-annotated.html#formulation-mathématique",
    "title": "Dimension",
    "section": "Formulation mathématique",
    "text": "Formulation mathématique\n\nSoit \\(X = (X_1, \\ldots, X_p)^{\\top}\\) un vecteur aléatoire centré avec matrice de covariance \\(\\Sigma\\).\nOn cherche une combinaison linéaire des variables \\(X_k\\) : \\[Y_1 = \\alpha_1^{\\top} X = \\sum_{k=1}^p \\alpha_{1k} X_k.\\]\nObjectif → Maximiser \\(\\text{Var}(Y_1)\\) sous contrainte \\(\\|\\alpha_1\\| = 1\\)."
  },
  {
    "objectID": "slides/04-dimension-pca-slides-annotated.html#problème-doptimisation",
    "href": "slides/04-dimension-pca-slides-annotated.html#problème-doptimisation",
    "title": "Dimension",
    "section": "Problème d’optimisation",
    "text": "Problème d’optimisation\n\nLe problème d’optimisation s’écrit : \\[\\max_{\\alpha_1^{\\top} \\alpha_1 = 1} \\text{Var}(Y_1) = \\max_{\\alpha_1^{\\top} \\alpha_1 = 1} \\alpha_1^{\\top} \\Sigma \\alpha_1.\\]"
  },
  {
    "objectID": "slides/04-dimension-pca-slides-annotated.html#problème-doptimisation-1",
    "href": "slides/04-dimension-pca-slides-annotated.html#problème-doptimisation-1",
    "title": "Dimension",
    "section": "Problème d’optimisation",
    "text": "Problème d’optimisation"
  },
  {
    "objectID": "slides/04-dimension-pca-slides-annotated.html#problème-doptimisation-2",
    "href": "slides/04-dimension-pca-slides-annotated.html#problème-doptimisation-2",
    "title": "Dimension",
    "section": "Problème d’optimisation",
    "text": "Problème d’optimisation\n\nSolution (multiplicateurs de Lagrange) : \\[\\Sigma \\alpha_1 = \\lambda_1 \\alpha_1\\]\noù \\(\\lambda_1\\) est la plus grande valeur propre de \\(\\Sigma\\) et \\(\\alpha_1\\) le vecteur propre associé."
  },
  {
    "objectID": "slides/04-dimension-pca-slides-annotated.html#construction-des-composantes",
    "href": "slides/04-dimension-pca-slides-annotated.html#construction-des-composantes",
    "title": "Dimension",
    "section": "Construction des composantes",
    "text": "Construction des composantes\n\nLes composantes suivantes sont construites par orthogonalité :\n\\[\\Sigma \\alpha_k = \\lambda_k \\alpha_k, \\quad \\lambda_1 \\geq \\lambda_2 \\geq \\ldots \\geq \\lambda_p\\]\nComposantes principales : \\[Y_k = \\alpha_k^{\\top} X\\]\noù \\(\\alpha_k\\) est le vecteur propre associé à \\(\\lambda_k\\)."
  },
  {
    "objectID": "slides/04-dimension-pca-slides-annotated.html#représentation-matricielle",
    "href": "slides/04-dimension-pca-slides-annotated.html#représentation-matricielle",
    "title": "Dimension",
    "section": "Représentation matricielle",
    "text": "Représentation matricielle\n\nSoit \\(A = (\\alpha_1, \\ldots, \\alpha_p) \\in \\mathbb{R}^{p \\times p}\\) la matrice des vecteurs propres.\n\\[Y = A^{\\top} X\\]\nPropriétés de \\(A\\) :\n\n\\(A^{\\top} A = AA^{\\top} = I_p\\) (orthogonale)\n\\(A^{\\top} = A^{-1}\\)\n\\(\\Sigma A = A \\Lambda\\) où \\(\\Lambda = \\text{diag}(\\lambda_1, \\ldots, \\lambda_p)\\)"
  },
  {
    "objectID": "slides/04-dimension-pca-slides-annotated.html#variance-des-composantes",
    "href": "slides/04-dimension-pca-slides-annotated.html#variance-des-composantes",
    "title": "Dimension",
    "section": "Variance des composantes",
    "text": "Variance des composantes\n\n\\[\\text{Var}(Y) = A^{\\top} \\Sigma A = \\Lambda\\]\nConséquences :\n\n\\(\\text{Cov}(Y_k, Y_l) = 0\\) si \\(k \\neq l\\) (orthogonalité)\n\\(\\text{Var}(Y_k) = \\lambda_k\\) (variance décroissante)\n\\(\\text{Var}(Y_1) \\geq \\text{Var}(Y_2) \\geq \\ldots \\geq \\text{Var}(Y_p)\\)"
  },
  {
    "objectID": "slides/04-dimension-pca-slides-annotated.html#variation-totale",
    "href": "slides/04-dimension-pca-slides-annotated.html#variation-totale",
    "title": "Dimension",
    "section": "Variation totale",
    "text": "Variation totale\n\nLa trace de Pillai est une mesure globale de la variation présente dans les données :\n\\[\\text{tr}(\\Sigma) = \\sum_{i=1}^p \\lambda_i = \\sum_{k=1}^p \\text{Var}(Y_k).\\]"
  },
  {
    "objectID": "slides/04-dimension-pca-slides-annotated.html#proportion-expliquée-par-y_k",
    "href": "slides/04-dimension-pca-slides-annotated.html#proportion-expliquée-par-y_k",
    "title": "Dimension",
    "section": "Proportion expliquée par \\(Y_k\\)",
    "text": "Proportion expliquée par \\(Y_k\\)\n\nLa proportion de variance expliquée par la composante principale \\(Y_k\\) est\n\\[\\frac{\\lambda_k}{\\lambda_1 + \\cdots + \\lambda_p} = \\frac{\\text{Var}(Y_k)}{\\text{tr}(\\Sigma)}.\\]"
  },
  {
    "objectID": "slides/04-dimension-pca-slides-annotated.html#proportion-expliquée-par-les-m-premières",
    "href": "slides/04-dimension-pca-slides-annotated.html#proportion-expliquée-par-les-m-premières",
    "title": "Dimension",
    "section": "Proportion expliquée par les \\(m\\) premières",
    "text": "Proportion expliquée par les \\(m\\) premières\n\nLa variabilité des données expliquée par les \\(m\\) premières composantes est :\n\\[100\\% \\times \\frac{\\sum_{k=1}^m \\lambda_k}{\\sum_{k=1}^p \\lambda_k}.\\]"
  },
  {
    "objectID": "slides/04-dimension-pca-slides-annotated.html#estimation-pratique",
    "href": "slides/04-dimension-pca-slides-annotated.html#estimation-pratique",
    "title": "Dimension",
    "section": "Estimation pratique",
    "text": "Estimation pratique\n\n\nEn pratique, \\(\\Sigma\\) est inconnue. On l’estime par : \\[\\hat{\\Sigma} = \\frac{1}{n-1} \\sum_{i=1}^n (X_i - \\overline{X})(X_i - \\overline{X})^{\\top}.\\]\nDécomposition spectrale : \\(\\hat{\\Sigma} = \\hat{A} \\hat{\\Lambda} \\hat{A}^{\\top}.\\)\nComposantes estimées : \\(Y_i = \\hat{A}^{\\top} X_i.\\)"
  },
  {
    "objectID": "slides/04-dimension-pca-slides-annotated.html#sensibilité-à-léchelle",
    "href": "slides/04-dimension-pca-slides-annotated.html#sensibilité-à-léchelle",
    "title": "Dimension",
    "section": "Sensibilité à l’échelle",
    "text": "Sensibilité à l’échelle\n\nProblème → Variables avec grandes variances dominent l’ACP.\nExemple → Distance en m vs km multiplie la variance par \\(10^6\\).\nSolution → Standardiser les variables si elles ont des unités différentes.\nÉquivalence → ACP sur matrice de corrélations au lieu de covariances."
  },
  {
    "objectID": "slides/04-dimension-pca-slides-annotated.html#interprétation-géométrique",
    "href": "slides/04-dimension-pca-slides-annotated.html#interprétation-géométrique",
    "title": "Dimension",
    "section": "Interprétation géométrique",
    "text": "Interprétation géométrique\n\n\nRotation des axes → \\(Y = A^{\\top}X\\) représente une rotation orthogonale.\nNouveaux axes → Directions de variation maximale successive.\nScores → \\(Y_{ik} = \\alpha_k^{\\top} X_i\\) (coordonnées dans le nouveau système).\nQualité de représentation \\[Q_{ik} = \\frac{Y_{ik}^2}{\\sum_{j=1}^p Y_{ij}^2}\\]"
  },
  {
    "objectID": "slides/04-dimension-pca-slides-annotated.html#choix-du-nombre-de-composantes",
    "href": "slides/04-dimension-pca-slides-annotated.html#choix-du-nombre-de-composantes",
    "title": "Dimension",
    "section": "Choix du nombre de composantes",
    "text": "Choix du nombre de composantes\n\n\nRègle des 80% : Retenir assez de composantes pour expliquer 80% de la variance.\nRègle de Kaiser : Conserver les composantes avec \\(\\lambda_k &gt; 1\\) (si matrice de corrélations).\nRègle de Joliffe : Variante plus stricte, \\(\\lambda_k &gt; 0.7\\).\nRègle de Cattell (coude) : Chercher un point de rupture dans le graphique des valeurs propres."
  },
  {
    "objectID": "slides/04-dimension-pca-slides-annotated.html#considérations-pratiques",
    "href": "slides/04-dimension-pca-slides-annotated.html#considérations-pratiques",
    "title": "Dimension",
    "section": "Considérations pratiques",
    "text": "Considérations pratiques\n\nUsage dans les modèles : Utiliser les \\(k\\) premières composantes comme prédicteurs.\n\nAvantages :\n\nÉlimine la multicolinéarité\nConserve l’information essentielle\n\n\n\nCompromis :\n\nTrop de composantes : pas de simplification\nTrop peu : perte d’information pertinente"
  },
  {
    "objectID": "slides/04-dimension-pca-slides-annotated.html#exemple-le-top-10-spotify",
    "href": "slides/04-dimension-pca-slides-annotated.html#exemple-le-top-10-spotify",
    "title": "Dimension",
    "section": "Exemple : le top 10 Spotify",
    "text": "Exemple : le top 10 Spotify\nOn considère le jeu de données suivant :\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrack_name\ntrack_artist\ntrack_popularity\ndanceability\nenergy\nloudness\nspeechiness\nacousticness\nliveness\nvalence\ntempo\n\n\n\n\nI Don’t Care (with Justin Bieber) - Loud Luxury Remix\nEd Sheeran\n66\n0.748\n0.9160\n-2.634\n0.0583\n0.102000\n0.0653\n0.5180\n122.036\n\n\nBeautiful People (feat. Khalid) - Jack Wins Remix\nEd Sheeran\n67\n0.675\n0.9190\n-5.385\n0.1270\n0.079900\n0.1430\n0.5850\n124.982\n\n\nCross Me (feat. Chance the Rapper & PnB Rock) - M-22 Remix\nEd Sheeran\n58\n0.679\n0.9230\n-6.500\n0.1810\n0.146000\n0.1240\n0.7520\n121.984\n\n\nSouth of the Border (feat. Camila Cabello & Cardi B) - Andy Jarvis Remix\nEd Sheeran\n66\n0.805\n0.8350\n-4.603\n0.0896\n0.130000\n0.3650\n0.7220\n125.028\n\n\nbad guy (with Justin Bieber)\nBillie Eilish\n83\n0.670\n0.4530\n-11.265\n0.2950\n0.252000\n0.1170\n0.6800\n135.055\n\n\nSouth of the Border (feat. Camila Cabello & Cardi B) - Sam Feldt Remix\nEd Sheeran\n65\n0.789\n0.8580\n-4.388\n0.0404\n0.220000\n0.3350\n0.4910\n110.012\n\n\nFriends (with BloodPop®)\nJustin Bieber\n76\n0.743\n0.7330\n-5.337\n0.0375\n0.004020\n0.3040\n0.6220\n104.977\n\n\nSorry\nJustin Bieber\n81\n0.654\n0.7600\n-3.669\n0.0450\n0.079700\n0.2990\n0.4100\n99.945\n\n\nPoker Face\nLady Gaga\n69\n0.851\n0.8060\n-4.618\n0.0786\n0.120000\n0.1210\n0.7760\n119.001\n\n\nSouth of the Border (feat. Camila Cabello & Cardi B) - Cheat Codes Remix\nEd Sheeran\n69\n0.845\n0.7660\n-5.727\n0.0658\n0.212000\n0.1110\n0.7820\n114.054\n\n\nHappier - Tiësto’s AFTR:HRS Remix\nEd Sheeran\n53\n0.684\n0.8230\n-4.881\n0.0296\n0.005760\n0.2010\n0.3840\n117.993\n\n\nbad guy\nBillie Eilish\n95\n0.701\n0.4250\n-10.965\n0.3750\n0.328000\n0.1000\n0.5620\n135.128\n\n\nA Sky Full of Stars\nColdplay\n78\n0.551\n0.6360\n-7.125\n0.0281\n0.007130\n0.2170\n0.1530\n124.996\n\n\neverything i wanted\nBillie Eilish\n97\n0.704\n0.2250\n-14.454\n0.0994\n0.902000\n0.1060\n0.2430\n120.006\n\n\nYummy\nJustin Bieber\n95\n0.662\n0.5190\n-6.554\n0.1060\n0.404000\n0.1210\n0.4950\n145.841\n\n\nBlinding Lights\nThe Weeknd\n98\n0.513\n0.7960\n-4.075\n0.0629\n0.001470\n0.0938\n0.3450\n171.017\n\n\nHeartless\nThe Weeknd\n93\n0.531\n0.7500\n-5.831\n0.1110\n0.006320\n0.1170\n0.1980\n169.954\n\n\nLover (Remix) [feat. Shawn Mendes]\nTaylor Swift\n85\n0.448\n0.6030\n-7.176\n0.0640\n0.433000\n0.0862\n0.4220\n205.272\n\n\nThe Morning\nThe Weeknd\n70\n0.652\n0.5490\n-7.555\n0.0390\n0.317000\n0.1040\n0.1860\n120.073\n\n\nShape of You\nEd Sheeran\n86\n0.825\n0.6520\n-3.183\n0.0802\n0.581000\n0.0931\n0.9310\n95.977\n\n\nBeautiful People (feat. Khalid)\nEd Sheeran\n85\n0.640\n0.6480\n-8.113\n0.1870\n0.124000\n0.0802\n0.5480\n92.977\n\n\nSouth of the Border (feat. Camila Cabello & Cardi B)\nEd Sheeran\n91\n0.857\n0.6210\n-6.376\n0.0824\n0.148000\n0.0865\n0.6680\n97.986\n\n\nI Don’t Care (with Justin Bieber)\nEd Sheeran\n84\n0.798\n0.6750\n-5.041\n0.0442\n0.091200\n0.0894\n0.8420\n101.956\n\n\nOrphans\nColdplay\n78\n0.503\n0.8080\n-5.051\n0.0450\n0.025400\n0.5570\n0.2830\n107.975\n\n\nApplause\nLady Gaga\n70\n0.669\n0.7800\n-4.287\n0.0530\n0.026500\n0.1430\n0.7380\n139.945\n\n\nJudas\nLady Gaga\n56\n0.661\n0.9370\n-3.830\n0.0744\n0.000967\n0.3550\n0.5350\n131.001\n\n\nBlank Space\nTaylor Swift\n78\n0.760\n0.7030\n-5.412\n0.0540\n0.103000\n0.0913\n0.5700\n95.997\n\n\nThe Cure\nLady Gaga\n70\n0.697\n0.5060\n-4.863\n0.0325\n0.079200\n0.0893\n0.5160\n100.002\n\n\nUmbrella\nRihanna\n80\n0.583\n0.8290\n-4.603\n0.1340\n0.008640\n0.0426\n0.5750\n174.028\n\n\nJust The Way You Are\nBruno Mars\n20\n0.637\n0.8430\n-5.413\n0.0432\n0.015100\n0.0876\n0.4340\n109.012\n\n\nDon’t Stop The Music\nRihanna\n17\n0.835\n0.6690\n-5.582\n0.0643\n0.033600\n0.0535\n0.5420\n122.668\n\n\nPhotograph\nEd Sheeran\n84\n0.614\n0.3790\n-10.480\n0.0476\n0.607000\n0.0986\n0.2010\n107.989\n\n\nYou Need To Calm Down\nTaylor Swift\n86\n0.771\n0.6710\n-5.617\n0.0553\n0.009290\n0.0637\n0.7140\n85.026\n\n\n22\nTaylor Swift\n0\n0.661\n0.7240\n-6.548\n0.0373\n0.002880\n0.0631\n0.6760\n104.011\n\n\nBaby\nJustin Bieber\n76\n0.728\n0.8600\n-5.237\n0.1370\n0.040100\n0.1110\n0.5350\n65.043\n\n\nViva La Vida\nColdplay\n78\n0.485\n0.6190\n-7.115\n0.0289\n0.094100\n0.1090\n0.4160\n138.017\n\n\nBad Romance\nLady Gaga\n78\n0.696\n0.9210\n-3.755\n0.0363\n0.003140\n0.0842\n0.7140\n119.001\n\n\nI Knew You Were Trouble.\nTaylor Swift\n76\n0.622\n0.4690\n-6.798\n0.0363\n0.004540\n0.0335\n0.6790\n77.019\n\n\nOur Song\nTaylor Swift\n61\n0.667\n0.6590\n-4.932\n0.0298\n0.113000\n0.3250\n0.5750\n89.128\n\n\nMean\nTaylor Swift\n62\n0.570\n0.7470\n-3.978\n0.0426\n0.445000\n0.2190\n0.8080\n164.004\n\n\nThe Lazy Song\nBruno Mars\n73\n0.738\n0.8040\n-3.851\n0.0621\n0.332000\n0.2790\n0.9490\n174.943\n\n\nLove The Way You Lie (Part II) - Pt. 2\nRihanna\n65\n0.472\n0.8330\n-4.486\n0.1620\n0.297000\n0.0560\n0.4130\n166.271\n\n\nJust Dance\nLady Gaga\n73\n0.822\n0.7390\n-4.541\n0.0311\n0.026400\n0.1810\n0.7450\n118.992\n\n\nPaparazzi\nLady Gaga\n69\n0.762\n0.6920\n-3.973\n0.0438\n0.113000\n0.0940\n0.3970\n114.906\n\n\nAlejandro\nLady Gaga\n8\n0.626\n0.7990\n-6.612\n0.0460\n0.000358\n0.3600\n0.3690\n98.992\n\n\nHotline Bling\nDrake\n0\n0.905\n0.6170\n-8.039\n0.0596\n0.002870\n0.0484\n0.5720\n134.972\n\n\nAdventure of a Lifetime\nColdplay\n74\n0.638\n0.9240\n-3.887\n0.0359\n0.002050\n0.1490\n0.5290\n111.995\n\n\nHymn for the Weekend\nColdplay\n77\n0.490\n0.7430\n-5.155\n0.0409\n0.255000\n0.3000\n0.3630\n89.974\n\n\nI Feel It Coming\nThe Weeknd\n80\n0.773\n0.8190\n-5.946\n0.1180\n0.426000\n0.0679\n0.5850\n92.990\n\n\nParadise\nColdplay\n77\n0.449\n0.5850\n-6.762\n0.0268\n0.052200\n0.0833\n0.2030\n139.627\n\n\nLook What You Made Me Do\nTaylor Swift\n74\n0.766\n0.7090\n-6.471\n0.1230\n0.204000\n0.1260\n0.5060\n128.070\n\n\nThat’s What I Like\nBruno Mars\n82\n0.853\n0.5600\n-4.961\n0.0406\n0.013000\n0.0944\n0.8600\n134.066\n\n\nGorgeous\nTaylor Swift\n67\n0.800\n0.5350\n-6.684\n0.1350\n0.071300\n0.2130\n0.4510\n92.027\n\n\nStarboy\nThe Weeknd\n84\n0.678\n0.5880\n-7.015\n0.2760\n0.141000\n0.1370\n0.4860\n186.005\n\n\nWe Are Never Ever Getting Back Together\nTaylor Swift\n74\n0.628\n0.6760\n-5.911\n0.0916\n0.009570\n0.1210\n0.7500\n85.984\n\n\nAs Long As You Love Me\nJustin Bieber\n72\n0.571\n0.8730\n-3.382\n0.0997\n0.080700\n0.3610\n0.6130\n139.691\n\n\nMistletoe\nJustin Bieber\n84\n0.658\n0.5600\n-9.049\n0.0467\n0.498000\n0.0794\n0.8540\n161.994\n\n\nJust the Way You Are\nBruno Mars\n80\n0.637\n0.8430\n-5.413\n0.0432\n0.015100\n0.0876\n0.4340\n109.012\n\n\nThinking out Loud\nEd Sheeran\n67\n0.781\n0.4450\n-6.061\n0.0295\n0.474000\n0.1840\n0.5910\n78.998\n\n\nDiamonds\nRihanna\n66\n0.564\n0.7100\n-4.920\n0.0461\n0.001250\n0.1090\n0.3930\n91.972\n\n\nCan’t Feel My Face\nThe Weeknd\n80\n0.705\n0.7690\n-5.526\n0.0426\n0.112000\n0.1050\n0.5900\n107.939\n\n\nOne Dance\nDrake\n20\n0.791\n0.6190\n-5.886\n0.0532\n0.007840\n0.3510\n0.3710\n103.989\n\n\nThe Hills\nThe Weeknd\n6\n0.587\n0.5660\n-7.017\n0.0528\n0.084300\n0.1450\n0.1550\n113.025\n\n\nVersace On The Floor (Bruno Mars vs. David Guetta)\nBruno Mars\n63\n0.747\n0.7010\n-4.902\n0.0429\n0.108000\n0.0712\n0.5890\n100.012\n\n\nToo Good\nDrake\n12\n0.804\n0.6480\n-7.805\n0.1170\n0.057300\n0.1020\n0.3920\n117.983\n\n\nShake It Off\nTaylor Swift\n18\n0.648\n0.7850\n-5.414\n0.1650\n0.056100\n0.1480\n0.9430\n160.020\n\n\nLocked Out of Heaven\nBruno Mars\n78\n0.726\n0.6980\n-4.165\n0.0431\n0.049000\n0.3090\n0.8670\n143.994\n\n\nBest I Ever Had\nDrake\n72\n0.431\n0.8940\n-2.673\n0.3300\n0.095100\n0.1880\n0.6040\n162.193\n\n\nHymn for the Weekend - Seeb Remix\nColdplay\n71\n0.565\n0.8490\n-3.516\n0.0517\n0.008680\n0.1200\n0.4270\n102.034\n\n\nFourFiveSeconds\nRihanna\n21\n0.579\n0.2680\n-5.650\n0.0442\n0.868000\n0.1200\n0.3210\n205.903\n\n\nGalway Girl\nEd Sheeran\n80\n0.624\n0.8760\n-3.374\n0.1000\n0.073500\n0.3270\n0.7810\n99.943\n\n\nOnly Girl (In The World)\nRihanna\n8\n0.768\n0.7380\n-3.017\n0.0406\n0.052400\n0.0561\n0.6850\n125.928\n\n\nWe Found Love\nRihanna\n19\n0.734\n0.7660\n-4.485\n0.0383\n0.025000\n0.1080\n0.6000\n127.986\n\n\nIn My Feelings\nDrake\n83\n0.835\n0.6260\n-5.833\n0.1250\n0.058900\n0.3960\n0.3500\n91.030\n\n\nGod’s Plan\nDrake\n86\n0.754\n0.4490\n-9.211\n0.1090\n0.033200\n0.5520\n0.3570\n77.169\n\n\nPassionfruit\nDrake\n15\n0.809\n0.4630\n-11.377\n0.0396\n0.256000\n0.1090\n0.3640\n111.980\n\n\nYellow\nColdplay\n81\n0.429\n0.6610\n-7.227\n0.0281\n0.002390\n0.2340\n0.2850\n173.365\n\n\nFix You\nColdplay\n81\n0.209\n0.4180\n-8.740\n0.0338\n0.163000\n0.1130\n0.1230\n138.265\n\n\nControlla\nDrake\n73\n0.611\n0.4680\n-11.084\n0.1790\n0.078100\n0.1010\n0.3490\n92.092\n\n\nOcean Eyes - Blackbear Remix\nBillie Eilish\n73\n0.661\n0.4190\n-9.807\n0.0404\n0.114000\n0.0848\n0.1450\n144.980\n\n\nThe Scientist\nColdplay\n83\n0.560\n0.4420\n-7.224\n0.0243\n0.727000\n0.1100\n0.2120\n146.448\n\n\nClocks\nColdplay\n75\n0.577\n0.7490\n-7.215\n0.0279\n0.599000\n0.1830\n0.2610\n130.969\n\n\nIn My Place\nColdplay\n70\n0.425\n0.5870\n-5.453\n0.0278\n0.056500\n0.2980\n0.1940\n144.642\n\n\nA Sky Full of Stars - Robin Schulz Edit\nColdplay\n51\n0.638\n0.8470\n-5.648\n0.0338\n0.004050\n0.4000\n0.3330\n123.042\n\n\nWhat Do You Mean?\nJustin Bieber\n78\n0.845\n0.5670\n-8.118\n0.0956\n0.590000\n0.0811\n0.7930\n125.020\n\n\nTreasure\nBruno Mars\n76\n0.874\n0.6920\n-5.280\n0.0431\n0.041200\n0.3240\n0.9370\n116.017\n\n\nWork\nRihanna\n77\n0.725\n0.5340\n-6.238\n0.0946\n0.075200\n0.0919\n0.5580\n91.974\n\n\nWhat’s My Name?\nRihanna\n68\n0.690\n0.7860\n-2.959\n0.0692\n0.229000\n0.0797\n0.5830\n100.049\n\n\n24K Magic\nBruno Mars\n78\n0.818\n0.8030\n-4.282\n0.0797\n0.034000\n0.1530\n0.6320\n106.970\n\n\nME! (feat. Brendon Urie of Panic! At The Disco)\nTaylor Swift\n29\n0.614\n0.8390\n-4.143\n0.0702\n0.029100\n0.1120\n0.6570\n182.123\n\n\nmy boy\nBillie Eilish\n80\n0.692\n0.3940\n-8.745\n0.2070\n0.472000\n0.1170\n0.3240\n89.936\n\n\nparty favor\nBillie Eilish\n76\n0.826\n0.1880\n-12.181\n0.0883\n0.882000\n0.1030\n0.2050\n130.022\n\n\nShallow - Radio Edit\nLady Gaga\n78\n0.575\n0.3300\n-6.557\n0.0310\n0.416000\n0.0872\n0.2770\n95.801\n\n\nbury a friend\nBillie Eilish\n87\n0.905\n0.3890\n-14.505\n0.3320\n0.740000\n0.1060\n0.1960\n120.046\n\n\nyou should see me in a crown\nBillie Eilish\n84\n0.678\n0.5330\n-10.485\n0.1860\n0.462000\n0.1390\n0.3230\n150.455\n\n\nall the good girls go to hell\nBillie Eilish\n86\n0.726\n0.4440\n-8.922\n0.3720\n0.283000\n0.1770\n0.5690\n185.044\n\n\nBehind Barz - Bonus\nDrake\n77\n0.854\n0.5540\n-4.684\n0.1720\n0.769000\n0.0749\n0.2720\n142.069\n\n\nOver\nDrake\n63\n0.350\n0.8450\n-5.614\n0.2000\n0.010700\n0.1230\n0.4500\n99.643\n\n\nMoney In The Grave (Drake ft. Rick Ross)\nDrake\n88\n0.831\n0.5020\n-4.045\n0.0460\n0.101000\n0.1220\n0.1010\n100.541\n\n\nNice For What\nDrake\n18\n0.567\n0.9130\n-6.471\n0.0736\n0.093400\n0.1140\n0.7920\n93.350\n\n\nBack To Back\nDrake\n69\n0.893\n0.4800\n-3.728\n0.3560\n0.008540\n0.1160\n0.3820\n86.976\n\n\nJumpman\nDrake\n75\n0.852\n0.5530\n-7.286\n0.1870\n0.056300\n0.3320\n0.6560\n142.072\n\n\nForever\nDrake\n67\n0.415\n0.9260\n-1.304\n0.3740\n0.253000\n0.2560\n0.5420\n158.423\n\n\nNonstop\nDrake\n13\n0.909\n0.4050\n-8.133\n0.1400\n0.030600\n0.1050\n0.4550\n154.984\n\n\nBitch Better Have My Money\nRihanna\n75\n0.781\n0.7280\n-4.981\n0.0621\n0.050900\n0.2570\n0.3950\n102.990\n\n\nSwine\nLady Gaga\n1\n0.712\n0.8610\n-3.625\n0.1360\n0.006200\n0.6340\n0.2090\n127.890\n\n\nNeeded Me\nRihanna\n14\n0.671\n0.3140\n-8.091\n0.2440\n0.110000\n0.0825\n0.2960\n110.898\n\n\nRude Boy\nRihanna\n12\n0.563\n0.7500\n-4.496\n0.1270\n0.113000\n0.0788\n0.8120\n173.906\n\n\nPortland\nDrake\n74\n0.925\n0.4430\n-8.216\n0.2640\n0.017200\n0.2760\n0.3500\n136.078\n\n\nGyalchester\nDrake\n70\n0.877\n0.3910\n-8.196\n0.0630\n0.031700\n0.0945\n0.1220\n131.018\n\n\nCan’t Have Everything\nDrake\n60\n0.856\n0.5720\n-8.429\n0.3730\n0.013600\n0.1210\n0.4540\n82.445\n\n\nSacrifices\nDrake\n1\n0.903\n0.4390\n-11.813\n0.3970\n0.040700\n0.1380\n0.5320\n139.112\n\n\nUptown (feat. Bun B & Lil Wayne)\nDrake\n62\n0.416\n0.8240\n-3.075\n0.3430\n0.003910\n0.1040\n0.4170\n85.433\n\n\nStarted From the Bottom\nDrake\n1\n0.790\n0.5400\n-6.959\n0.1830\n0.039100\n0.2660\n0.4590\n86.308\n\n\nThe Motto\nDrake\n10\n0.774\n0.4310\n-8.407\n0.3460\n0.000146\n0.1120\n0.3530\n201.843\n\n\nHold On, We’re Going Home\nDrake\n1\n0.765\n0.4520\n-6.644\n0.0680\n0.003300\n0.0916\n0.3310\n100.002\n\n\nKMT\nDrake\n64\n0.875\n0.6000\n-6.818\n0.0435\n0.018900\n0.1320\n0.0906\n119.987\n\n\nThe Hills - RL Grime Remix\nThe Weeknd\n51\n0.571\n0.8970\n-2.463\n0.0432\n0.113000\n0.1580\n0.4460\n139.996\n\n\nMyBoi - TroyBoi Remix\nBillie Eilish\n71\n0.879\n0.5800\n-4.797\n0.0883\n0.006260\n0.0653\n0.3120\n92.004\n\n\nGrammys\nDrake\n1\n0.711\n0.4400\n-9.509\n0.4710\n0.001750\n0.0798\n0.1210\n145.116\n\n\nDigital Dash\nDrake\n1\n0.889\n0.5430\n-7.666\n0.1280\n0.003110\n0.2340\n0.3140\n140.029\n\n\nAlways in My Head\nColdplay\n65\n0.499\n0.5020\n-9.307\n0.0254\n0.012800\n0.0680\n0.0397\n97.544\n\n\nMagic\nColdplay\n72\n0.738\n0.4830\n-8.915\n0.0403\n0.026200\n0.1060\n0.3550\n93.458\n\n\nInk\nColdplay\n62\n0.688\n0.7060\n-7.310\n0.0324\n0.066100\n0.1180\n0.6890\n112.019\n\n\nTrue Love\nColdplay\n58\n0.482\n0.5740\n-7.963\n0.0280\n0.003820\n0.1230\n0.1840\n119.994\n\n\nMidnight\nColdplay\n64\n0.548\n0.4750\n-12.420\n0.0355\n0.615000\n0.0944\n0.0349\n126.976\n\n\nAnother’s Arms\nColdplay\n57\n0.518\n0.5720\n-9.037\n0.0291\n0.007150\n0.1390\n0.3470\n160.005\n\n\nOceans\nColdplay\n55\n0.194\n0.1320\n-15.783\n0.0349\n0.919000\n0.1590\n0.0400\n74.848\n\n\nO\nColdplay\n63\n0.230\n0.2160\n-14.727\n0.0376\n0.911000\n0.0934\n0.0618\n92.022\n\n\nAll Your Friends\nColdplay\n36\n0.507\n0.3170\n-12.177\n0.0283\n0.183000\n0.1120\n0.2010\n74.980\n\n\nGhost Story\nColdplay\n37\n0.595\n0.6380\n-7.941\n0.0307\n0.104000\n0.0699\n0.5100\n122.005\n\n\nO - Reprise\nColdplay\n32\n0.201\n0.3270\n-11.975\n0.0285\n0.869000\n0.1170\n0.0945\n107.019\n\n\nTalk\nColdplay\n62\n0.413\n0.5570\n-10.841\n0.0335\n0.005480\n0.1550\n0.1580\n120.318\n\n\nTrouble\nColdplay\n71\n0.561\n0.5470\n-7.496\n0.0321\n0.189000\n0.1700\n0.1900\n139.755\n\n\nLost!\nColdplay\n52\n0.359\n0.7790\n-7.520\n0.0412\n0.005700\n0.3790\n0.5820\n86.951\n\n\nBeautiful People (feat. Khalid) - NOTD Remix\nEd Sheeran\n66\n0.702\n0.7940\n-5.913\n0.0562\n0.007150\n0.1100\n0.3700\n103.972\n\n\nStarboy - Kygo Remix\nThe Weeknd\n5\n0.751\n0.6230\n-7.215\n0.0939\n0.209000\n0.1680\n0.3830\n93.010\n\n\nGalway Girl - Martin Jensen Remix\nEd Sheeran\n58\n0.726\n0.7480\n-4.576\n0.0438\n0.024500\n0.1410\n0.7310\n100.055\n\n\nPhotograph - Felix Jaehn Remix\nEd Sheeran\n0\n0.733\n0.8210\n-4.982\n0.0407\n0.081500\n0.1220\n0.3970\n120.011\n\n\nThinking out Loud - Alex Adair Remix\nEd Sheeran\n49\n0.828\n0.7450\n-6.717\n0.1520\n0.210000\n0.0870\n0.5170\n117.008\n\n\nOften - Kygo Remix\nThe Weeknd\n65\n0.523\n0.4910\n-9.876\n0.0491\n0.123000\n0.3310\n0.4080\n80.156\n\n\nMarry You\nBruno Mars\n72\n0.620\n0.8320\n-4.848\n0.0360\n0.332000\n0.1040\n0.4810\n144.926\n\n\nPrincess of China - Acoustic\nColdplay\n52\n0.245\n0.4710\n-7.952\n0.0313\n0.652000\n0.1260\n0.2870\n165.996\n\n\nThe A Team - Acoustic\nEd Sheeran\n50\n0.649\n0.2640\n-9.899\n0.2800\n0.727000\n0.2920\n0.4240\n121.815\n\n\nMagic - Live at the Enmore Theatre, Sydney\nColdplay\n54\n0.500\n0.5020\n-11.775\n0.0424\n0.180000\n0.7290\n0.3620\n93.361\n\n\nA Sky Full of Stars - Live at the Royal Albert Hall, London\nColdplay\n54\n0.467\n0.7650\n-7.846\n0.0311\n0.004160\n0.7080\n0.1860\n125.013\n\n\nNothing On You (feat. Paulo Londra & Dave)\nEd Sheeran\n59\n0.747\n0.6490\n-6.218\n0.2190\n0.333000\n0.1720\n0.8400\n88.011\n\n\nSomebody To Love Remix\nJustin Bieber\n54\n0.704\n0.8360\n-4.619\n0.0283\n0.003880\n0.1600\n0.5530\n129.989\n\n\nEenie Meenie\nJustin Bieber\n4\n0.718\n0.6050\n-4.240\n0.0307\n0.044300\n0.0962\n0.8180\n121.236\n\n\nOne Time\nJustin Bieber\n68\n0.691\n0.8530\n-2.528\n0.0372\n0.063100\n0.0820\n0.7620\n145.999\n\n\nOne Less Lonely Girl\nJustin Bieber\n67\n0.580\n0.7500\n-4.421\n0.1940\n0.159000\n0.2740\n0.7280\n86.938\n\n\nLove Me\nJustin Bieber\n65\n0.729\n0.8680\n-3.319\n0.0609\n0.009390\n0.2830\n0.6320\n124.957\n\n\nStupid In Love\nRihanna\n47\n0.738\n0.5620\n-5.435\n0.0347\n0.633000\n0.1020\n0.4760\n127.452\n\n\nGrenade\nBruno Mars\n75\n0.706\n0.5580\n-7.237\n0.0593\n0.146000\n0.1180\n0.2270\n110.443\n\n\nHard\nRihanna\n56\n0.310\n0.7460\n-3.599\n0.1110\n0.012100\n0.6490\n0.1630\n182.032\n\n\nRussian Roulette\nRihanna\n65\n0.480\n0.4860\n-5.754\n0.0447\n0.046000\n0.1070\n0.2650\n80.051\n\n\nS&M\nRihanna\n72\n0.766\n0.6820\n-5.020\n0.0419\n0.011300\n0.1040\n0.8330\n127.966\n\n\nComplicated\nRihanna\n48\n0.710\n0.8970\n-3.802\n0.0372\n0.175000\n0.0918\n0.3300\n137.972\n\n\nPop Style\nDrake\n51\n0.713\n0.4620\n-10.027\n0.1230\n0.192000\n0.1050\n0.0595\n133.054\n\n\nFake Love\nDrake\n6\n0.924\n0.5430\n-7.474\n0.2980\n0.182000\n0.1030\n0.7360\n133.907\n\n\nWhen I Was Your Man\nBruno Mars\n82\n0.612\n0.2800\n-8.648\n0.0434\n0.932000\n0.0880\n0.3870\n72.795\n\n\nYou Belong With Me\nTaylor Swift\n52\n0.688\n0.7750\n-4.436\n0.0381\n0.151000\n0.1040\n0.4690\n129.969\n\n\nI Knew You Were Trouble\nTaylor Swift\n16\n0.499\n0.4670\n-6.744\n0.0461\n0.005930\n0.0361\n0.6370\n76.760\n\n\nWildest Dreams\nTaylor Swift\n65\n0.553\n0.6640\n-7.417\n0.0741\n0.070900\n0.1060\n0.4670\n140.060\n\n\nPon de Replay - Radio Edit\nRihanna\n34\n0.750\n0.6320\n-8.681\n0.2150\n0.000147\n0.1080\n0.5320\n98.997\n\n\nPerfect\nEd Sheeran\n86\n0.599\n0.4480\n-6.312\n0.0232\n0.163000\n0.1060\n0.1680\n95.050\n\n\nPray For Me (with Kendrick Lamar)\nThe Weeknd\n76\n0.735\n0.6770\n-4.979\n0.0930\n0.076200\n0.1110\n0.1880\n100.584\n\n\nCross Me (feat. Chance the Rapper & PnB Rock)\nEd Sheeran\n73\n0.746\n0.7870\n-6.373\n0.1200\n0.214000\n0.0669\n0.6070\n95.005\n\n\nThe Zone\nThe Weeknd\n61\n0.619\n0.5210\n-8.092\n0.0744\n0.260000\n0.1010\n0.2540\n139.886\n\n\nTill Dawn (Here Comes The Sun)\nThe Weeknd\n53\n0.697\n0.5420\n-8.946\n0.0437\n0.821000\n0.1340\n0.5470\n99.988\n\n\nSame Old Song\nThe Weeknd\n55\n0.771\n0.5600\n-5.636\n0.0692\n0.415000\n0.5360\n0.3890\n109.978\n\n\nEchoes Of Silence\nThe Weeknd\n55\n0.214\n0.1560\n-17.515\n0.0443\n0.951000\n0.1010\n0.1320\n75.654\n\n\nValerie\nThe Weeknd\n62\n0.641\n0.5390\n-6.351\n0.0463\n0.346000\n0.0837\n0.4360\n148.004\n\n\nGone\nThe Weeknd\n54\n0.494\n0.6970\n-7.232\n0.3090\n0.708000\n0.0602\n0.3440\n107.669\n\n\nThe Party & The After Party\nThe Weeknd\n60\n0.438\n0.6340\n-7.456\n0.3370\n0.476000\n0.1130\n0.3970\n203.585\n\n\nLosers\nThe Weeknd\n62\n0.666\n0.4850\n-9.850\n0.0527\n0.035500\n0.5040\n0.2880\n91.009\n\n\nTell Your Friends\nThe Weeknd\n68\n0.565\n0.4540\n-11.091\n0.0461\n0.265000\n0.1810\n0.3120\n74.809\n\n\nOften\nThe Weeknd\n76\n0.569\n0.6290\n-9.837\n0.0485\n0.218000\n0.1350\n0.0726\n134.035\n\n\nDark Times\nThe Weeknd\n69\n0.501\n0.4030\n-9.607\n0.0752\n0.114000\n0.1240\n0.2700\n132.608\n\n\nPrisoner\nThe Weeknd\n64\n0.550\n0.4040\n-12.755\n0.0398\n0.441000\n0.1100\n0.3290\n135.016\n\n\nSecrets\nThe Weeknd\n71\n0.665\n0.7710\n-5.779\n0.0533\n0.015500\n0.4600\n0.7640\n109.945\n\n\nTrue Colors\nThe Weeknd\n69\n0.624\n0.4240\n-8.237\n0.2400\n0.284000\n0.1730\n0.5840\n76.669\n\n\nSidewalks\nThe Weeknd\n70\n0.542\n0.7210\n-5.136\n0.0625\n0.147000\n0.3480\n0.6230\n149.831\n\n\nSix Feet Under\nThe Weeknd\n69\n0.772\n0.5040\n-8.219\n0.0761\n0.129000\n0.1250\n0.2440\n140.035\n\n\nDie For You\nThe Weeknd\n76\n0.586\n0.5250\n-7.163\n0.0615\n0.111000\n0.1340\n0.5080\n133.629\n\n\nCall Out My Name\nThe Weeknd\n24\n0.449\n0.5990\n-4.892\n0.0369\n0.211000\n0.3270\n0.1650\n134.144\n\n\nTry Me\nThe Weeknd\n19\n0.466\n0.6720\n-5.777\n0.0680\n0.622000\n0.1470\n0.1080\n91.673\n\n\nWasted Times\nThe Weeknd\n18\n0.562\n0.6740\n-6.049\n0.0851\n0.622000\n0.1110\n0.2930\n122.699\n\n\nI Was Never There\nThe Weeknd\n17\n0.324\n0.7350\n-4.054\n0.0337\n0.135000\n0.1690\n0.1690\n113.923\n\n\nHurt You\nThe Weeknd\n17\n0.484\n0.7160\n-7.193\n0.1670\n0.300000\n0.1120\n0.3650\n184.131\n\n\nPrivilege\nThe Weeknd\n16\n0.391\n0.5950\n-7.094\n0.0411\n0.913000\n0.1450\n0.3790\n93.780\n\n\nTwenty Eight\nThe Weeknd\n61\n0.639\n0.6330\n-7.338\n0.0352\n0.574000\n0.1110\n0.2430\n129.996\n\n\nCrew Love\nDrake\n51\n0.418\n0.5460\n-9.057\n0.2380\n0.227000\n0.2290\n0.2940\n160.152\n\n\nGet It Together\nDrake\n69\n0.781\n0.7210\n-9.133\n0.0514\n0.094500\n0.1040\n0.8490\n123.011\n\n\nSummers Over Interlude\nDrake\n57\n0.698\n0.2550\n-8.647\n0.0303\n0.405000\n0.0985\n0.2430\n132.036\n\n\nTears In The Rain\nThe Weeknd\n43\n0.655\n0.6360\n-7.840\n0.0453\n0.109000\n0.0842\n0.1850\n160.010\n\n\nAdaptation\nThe Weeknd\n44\n0.642\n0.7900\n-6.800\n0.0681\n0.151000\n0.3050\n0.6130\n114.960\n\n\nJaded\nDrake\n68\n0.766\n0.2710\n-5.697\n0.0412\n0.037000\n0.1750\n0.0371\n116.020\n\n\nDreams Money Can Buy\nDrake\n61\n0.423\n0.5870\n-6.635\n0.5290\n0.765000\n0.1140\n0.3300\n180.331\n\n\nFeels (feat. Young Thug & J Hus)\nEd Sheeran\n37\n0.665\n0.6800\n-4.899\n0.2900\n0.112000\n0.0960\n0.6900\n91.670\n\n\nHow Bout Now\nDrake\n66\n0.789\n0.4040\n-8.364\n0.2300\n0.205000\n0.2190\n0.4720\n122.062\n\n\nJodeci Freestyle (feat. J. Cole)\nDrake\n57\n0.498\n0.8440\n-2.834\n0.2680\n0.011100\n0.1020\n0.2820\n88.435\n\n\nShallow\nLady Gaga\n88\n0.572\n0.3850\n-6.362\n0.0308\n0.371000\n0.2310\n0.3230\n95.799\n\n\nI See Fire\nEd Sheeran\n23\n0.641\n0.1760\n-11.692\n0.0349\n0.638000\n0.2520\n0.2970\n76.031\n\n\n8\nBillie Eilish\n80\n0.735\n0.2350\n-13.239\n0.3150\n0.698000\n0.1070\n0.4620\n62.446\n\n\ni love you\nBillie Eilish\n85\n0.421\n0.1310\n-18.435\n0.0382\n0.952000\n0.1090\n0.1200\n137.446\n\n\ngoodbye\nBillie Eilish\n77\n0.153\n0.1380\n-21.877\n0.0503\n0.837000\n0.2540\n0.0503\n74.318\n\n\nxanny\nBillie Eilish\n83\n0.521\n0.1250\n-17.832\n0.2390\n0.751000\n0.2650\n0.0528\n111.554\n\n\nocean eyes\nBillie Eilish\n85\n0.511\n0.3630\n-7.650\n0.0410\n0.816000\n0.0840\n0.1690\n144.892\n\n\nBest Part of Me (feat. YEBBA)\nEd Sheeran\n74\n0.565\n0.2420\n-8.367\n0.0318\n0.762000\n0.2850\n0.2370\n81.657\n\n\nhostage\nBillie Eilish\n79\n0.427\n0.1230\n-15.228\n0.0497\n0.868000\n0.0732\n0.0394\n129.565\n\n\n&burn (with Vince Staples)\nBillie Eilish\n68\n0.776\n0.3790\n-9.351\n0.3310\n0.213000\n0.0969\n0.3460\n82.531\n\n\nSummer Games\nDrake\n4\n0.776\n0.4820\n-11.268\n0.0356\n0.481000\n0.1100\n0.1280\n136.015\n\n\nJungle\nDrake\n70\n0.690\n0.2350\n-8.651\n0.0541\n0.618000\n0.1070\n0.3780\n99.906\n\n\nTake Care\nDrake\n71\n0.629\n0.5150\n-10.358\n0.2650\n0.026700\n0.0888\n0.2990\n121.845\n\n\nMarvins Room\nDrake\n74\n0.492\n0.2600\n-17.341\n0.0921\n0.646000\n0.0705\n0.3120\n111.519\n\n\nlovely (with Khalid)\nBillie Eilish\n89\n0.351\n0.2960\n-10.109\n0.0333\n0.934000\n0.0950\n0.1200\n115.284\n\n\nMan Down\nRihanna\n59\n0.479\n0.9060\n-4.030\n0.1880\n0.040600\n0.0510\n0.5360\n155.887\n\n\nidontwannabeyouanymore\nBillie Eilish\n85\n0.483\n0.4120\n-8.461\n0.0402\n0.737000\n0.1160\n0.2470\n170.163\n\n\nwhen the party’s over\nBillie Eilish\n26\n0.498\n0.1040\n-14.080\n0.0621\n0.979000\n0.0895\n0.2050\n124.001\n\n\n0 To 100 / The Catch Up\nDrake\n5\n0.559\n0.7160\n-3.964\n0.3900\n0.737000\n0.2520\n0.5040\n176.618\n\n\nThe Language\nDrake\n53\n0.823\n0.6620\n-8.426\n0.3180\n0.011800\n0.0805\n0.5330\n120.117\n\n\n6 Man\nDrake\n1\n0.812\n0.5350\n-9.306\n0.4490\n0.217000\n0.1140\n0.3210\n78.974\n\n\nUsed To\nDrake\n0\n0.926\n0.4270\n-7.584\n0.1070\n0.070300\n0.0818\n0.3050\n96.023\n\n\nNo Tellin’\nDrake\n1\n0.823\n0.4670\n-10.394\n0.3010\n0.543000\n0.1350\n0.6180\n95.024\n\n\nPurpose\nJustin Bieber\n71\n0.481\n0.2850\n-10.848\n0.0424\n0.900000\n0.1230\n0.3160\n130.029\n\n\nBellyache - Marian Hill Remix\nBillie Eilish\n71\n0.832\n0.3680\n-8.627\n0.1330\n0.200000\n0.3530\n0.2000\n100.045\n\n\nmy strange addiction\nBillie Eilish\n82\n0.939\n0.3050\n-10.952\n0.3540\n0.350000\n0.1050\n0.5630\n100.029\n\n\nilomilo\nBillie Eilish\n81\n0.855\n0.4230\n-15.044\n0.0585\n0.724000\n0.0896\n0.5720\n120.020\n\n\nlisten before i go\nBillie Eilish\n81\n0.319\n0.0561\n-23.023\n0.0450\n0.935000\n0.3880\n0.0820\n79.764\n\n\nBored\nBillie Eilish\n76\n0.614\n0.3180\n-12.695\n0.0478\n0.896000\n0.0795\n0.1120\n119.959\n\n\nSex With Me\nRihanna\n69\n0.580\n0.5320\n-5.668\n0.0424\n0.294000\n0.0948\n0.3900\n112.049\n\n\nPour It Up\nRihanna\n62\n0.785\n0.4780\n-8.735\n0.0419\n0.025400\n0.0901\n0.3900\n134.002\n\n\nFinesse - Remix; feat. Cardi B\nBruno Mars\n77\n0.704\n0.8590\n-4.877\n0.0996\n0.018500\n0.0215\n0.9260\n105.115\n\n\nEmotionless\nDrake\n6\n0.413\n0.6760\n-5.124\n0.2130\n0.023900\n0.0811\n0.0827\n173.478\n\n\nI Forgot That You Existed\nTaylor Swift\n79\n0.664\n0.3160\n-10.345\n0.5190\n0.298000\n0.0812\n0.5410\n92.875\n\n\nEnd Game\nTaylor Swift\n68\n0.649\n0.5890\n-6.237\n0.0558\n0.008450\n0.1080\n0.1510\n159.073\n\n\nPerfect Duet (Ed Sheeran & Beyoncé)\nEd Sheeran\n77\n0.587\n0.2990\n-7.365\n0.0263\n0.779000\n0.1230\n0.3560\n94.992\n\n\nSigns\nDrake\n70\n0.794\n0.7350\n-4.204\n0.0540\n0.108000\n0.1120\n0.2230\n107.374\n\n\nLove Yourself\nJustin Bieber\n83\n0.609\n0.3780\n-9.828\n0.4380\n0.835000\n0.2800\n0.5150\n100.418\n\n\nKiss It Better - KAYTRANADA Edition\nRihanna\n45\n0.854\n0.4980\n-7.922\n0.3320\n0.022900\n0.3000\n0.3430\n108.049\n\n\nHigher\nRihanna\n1\n0.292\n0.5670\n-4.191\n0.0416\n0.112000\n0.1190\n0.3920\n153.199\n\n\nJames Joint\nRihanna\n0\n0.399\n0.2030\n-11.649\n0.0298\n0.660000\n0.2700\n0.4050\n134.202\n\n\nCan’t Feel My Face - Martin Garrix Remix\nThe Weeknd\n2\n0.607\n0.7070\n-5.509\n0.1060\n0.008420\n0.3480\n0.1170\n128.014\n\n\nShe Don’t Like The Lights - Acoustic Version\nJustin Bieber\n36\n0.612\n0.4280\n-8.520\n0.0292\n0.750000\n0.0921\n0.6850\n123.822\n\n\nWork - R3hab Remix\nRihanna\n2\n0.715\n0.8000\n-5.488\n0.0555\n0.010400\n0.3190\n0.2640\n100.004\n\n\nBitch Better Have My Money - R3hab Remix\nRihanna\n1\n0.788\n0.9210\n-3.210\n0.0550\n0.006000\n0.1050\n0.5510\n128.007\n\n\nBitch Better Have My Money - GTA Remix\nRihanna\n0\n0.768\n0.8770\n-3.114\n0.1510\n0.000768\n0.0739\n0.4800\n103.036\n\n\nYou Need To Calm Down - Clean Bandit Remix\nTaylor Swift\n54\n0.726\n0.8800\n-4.137\n0.0581\n0.072000\n0.0906\n0.7390\n95.037\n\n\n…Ready For It? - BloodPop® Remix\nTaylor Swift\n50\n0.582\n0.8350\n-4.899\n0.2220\n0.133000\n0.1020\n0.4960\n160.090\n\n\n24K Magic - R3hab Remix\nBruno Mars\n59\n0.763\n0.7080\n-5.692\n0.2090\n0.146000\n0.3300\n0.5660\n106.175\n\n\nPerfect - Mike Perry Remix\nEd Sheeran\n46\n0.732\n0.7560\n-4.309\n0.0324\n0.260000\n0.1300\n0.3430\n113.980\n\n\nKiss It Better - R3hab Remix\nRihanna\n1\n0.728\n0.7380\n-3.939\n0.0455\n0.050500\n0.1040\n0.4580\n102.000\n\n\nNeeded Me - R3hab Remix\nRihanna\n44\n0.659\n0.6830\n-5.485\n0.0488\n0.008350\n0.6250\n0.2780\n99.987\n\n\nParadise - Tiësto Remix\nColdplay\n48\n0.637\n0.7360\n-6.003\n0.0404\n0.012300\n0.0798\n0.6430\n130.015\n\n\nCalifornia King Bed - Bassjackers Radio\nRihanna\n0\n0.589\n0.8210\n-3.385\n0.0413\n0.000318\n0.3900\n0.1590\n127.986\n\n\nShape of You - Galantis Remix\nEd Sheeran\n51\n0.838\n0.7650\n-2.332\n0.0462\n0.051400\n0.0297\n0.9050\n100.021\n\n\nThat’s What I Like - Alan Walker Remix\nBruno Mars\n63\n0.692\n0.8960\n-4.017\n0.0502\n0.012100\n0.1990\n0.7980\n134.056\n\n\nBad Blood\nTaylor Swift\n62\n0.652\n0.8020\n-6.114\n0.1810\n0.087100\n0.1480\n0.2950\n170.157\n\n\nBeauty And A Beat\nJustin Bieber\n72\n0.601\n0.8430\n-4.870\n0.0593\n0.000704\n0.0681\n0.5340\n128.002\n\n\nDelicate\nTaylor Swift\n71\n0.750\n0.4040\n-10.178\n0.0682\n0.216000\n0.0911\n0.0499\n95.045\n\n\nFinesse\nBruno Mars\n68\n0.741\n0.7980\n-5.342\n0.0718\n0.007810\n0.0232\n0.9390\n104.953\n\n\nA Sky Full of Stars - Hardwell Remix\nColdplay\n18\n0.439\n0.6480\n-3.643\n0.0325\n0.019000\n0.3430\n0.0546\n127.954"
  },
  {
    "objectID": "slides/04-dimension-pca-slides-annotated.html#exemple-le-top-10-spotify-1",
    "href": "slides/04-dimension-pca-slides-annotated.html#exemple-le-top-10-spotify-1",
    "title": "Dimension",
    "section": "Exemple : le top 10 Spotify",
    "text": "Exemple : le top 10 Spotify\n\nlibrary(tidyverse)\n\nggplot(df, aes(x = danceability, y = energy, color = track_artist)) +\n  geom_point() +\n  theme_bw()"
  },
  {
    "objectID": "slides/04-dimension-pca-slides-annotated.html#exemple-le-top-10-spotify-2",
    "href": "slides/04-dimension-pca-slides-annotated.html#exemple-le-top-10-spotify-2",
    "title": "Dimension",
    "section": "Exemple : le top 10 Spotify",
    "text": "Exemple : le top 10 Spotify\n\nlibrary(tidyverse)\n\nggplot(df, aes(x = loudness, y = speechiness, color = track_artist)) +\n  geom_point() +\n  theme_bw()"
  },
  {
    "objectID": "slides/04-dimension-pca-slides-annotated.html#exemple-le-top-10-spotify-3",
    "href": "slides/04-dimension-pca-slides-annotated.html#exemple-le-top-10-spotify-3",
    "title": "Dimension",
    "section": "Exemple : le top 10 Spotify",
    "text": "Exemple : le top 10 Spotify\n\nCodePlotValeurs propres\n\n\n\nlibrary(FactoMineR)\nlibrary(factoextra)\npca_not_scaled &lt;- df |&gt; select(\n    danceability, energy, loudness, speechiness,\n    acousticness, liveness, valence, tempo\n  ) |&gt; PCA(scale.unit = FALSE, graph = FALSE)\nfviz_eig(pca_not_scaled)\nget_eigenvalue(pca_scaled)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\neigenvalue\nvariance.percent\ncumulative.variance.percent\n\n\n\n\nDim.1\n810.6332465\n98.6720169\n98.67202\n\n\nDim.2\n10.7552317\n1.3091499\n99.98117\n\n\nDim.3\n0.0567231\n0.0069045\n99.98807\n\n\nDim.4\n0.0441830\n0.0053781\n99.99345\n\n\nDim.5\n0.0210597\n0.0025634\n99.99601\n\n\nDim.6\n0.0134860\n0.0016415\n99.99765\n\n\nDim.7\n0.0100835\n0.0012274\n99.99888\n\n\nDim.8\n0.0091876\n0.0011183\n100.00000"
  },
  {
    "objectID": "slides/04-dimension-pca-slides-annotated.html#exemple-le-top-10-spotify-4",
    "href": "slides/04-dimension-pca-slides-annotated.html#exemple-le-top-10-spotify-4",
    "title": "Dimension",
    "section": "Exemple : le top 10 Spotify",
    "text": "Exemple : le top 10 Spotify\n\nCodePlotValeurs propres\n\n\n\nlibrary(FactoMineR)\nlibrary(factoextra)\npca_scaled &lt;- df |&gt; select(\n    danceability, energy, loudness, speechiness,\n    acousticness, liveness, valence, tempo\n  ) |&gt; PCA(scale.unit = TRUE, graph = FALSE)\nfviz_eig(pca_scaled)\nget_eigenvalue(pca_scaled)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\neigenvalue\nvariance.percent\ncumulative.variance.percent\n\n\n\n\nDim.1\n2.8090242\n35.112803\n35.11280\n\n\nDim.2\n1.3209372\n16.511715\n51.62452\n\n\nDim.3\n1.1505090\n14.381362\n66.00588\n\n\nDim.4\n0.9610515\n12.013144\n78.01902\n\n\nDim.5\n0.6455193\n8.068991\n86.08801\n\n\nDim.6\n0.5897927\n7.372408\n93.46042\n\n\nDim.7\n0.3466595\n4.333244\n97.79367\n\n\nDim.8\n0.1765066\n2.206333\n100.00000"
  },
  {
    "objectID": "slides/04-dimension-pca-slides-annotated.html#exemple-le-top-10-spotify-5",
    "href": "slides/04-dimension-pca-slides-annotated.html#exemple-le-top-10-spotify-5",
    "title": "Dimension",
    "section": "Exemple : le top 10 Spotify",
    "text": "Exemple : le top 10 Spotify\n\nCodePlot\n\n\n\nfviz_pca_var(\n  pca_scaled,\n  col.var = \"cos2\",\n  gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"), \n  repel = TRUE # Avoid text overlapping\n)"
  },
  {
    "objectID": "slides/04-dimension-pca-slides-annotated.html#exemple-le-top-10-spotify-6",
    "href": "slides/04-dimension-pca-slides-annotated.html#exemple-le-top-10-spotify-6",
    "title": "Dimension",
    "section": "Exemple : le top 10 Spotify",
    "text": "Exemple : le top 10 Spotify\n\nCodeContribution à la première composanteContribution à la deuxième composante\n\n\n\nfviz_contrib(pca_scaled, choice = \"var\", axes = 1)\nfviz_contrib(pca_scaled, choice = \"var\", axes = 2)"
  },
  {
    "objectID": "slides/04-dimension-pca-slides-annotated.html#exemple-le-top-10-spotify-7",
    "href": "slides/04-dimension-pca-slides-annotated.html#exemple-le-top-10-spotify-7",
    "title": "Dimension",
    "section": "Exemple : le top 10 Spotify",
    "text": "Exemple : le top 10 Spotify\n\nCodeGraphique des individusAvec ellipse\n\n\n\nfviz_pca_ind(\n  pca_scaled,\n  geom.ind = \"point\", # show points only (nbut not \"text\")\n  col.ind = df$track_artist, # color by groups\n  legend.title = \"Artists\",\n) + scale_shape_manual(values = rep(19, 10))"
  },
  {
    "objectID": "slides/04-dimension-pca-slides-annotated.html#exemple-le-top-10-spotify-8",
    "href": "slides/04-dimension-pca-slides-annotated.html#exemple-le-top-10-spotify-8",
    "title": "Dimension",
    "section": "Exemple : le top 10 Spotify",
    "text": "Exemple : le top 10 Spotify\n\nCodeBiplotClean\n\n\n\nfviz_pca_biplot(\n  pca_scaled,\n  repel = TRUE,\n  col.var = \"#2E9FDF\", # Variables color\n  col.ind = \"#696969\"  # Individuals color\n)"
  },
  {
    "objectID": "slides/04-dimension-pca-slides-annotated.html#conclusion",
    "href": "slides/04-dimension-pca-slides-annotated.html#conclusion",
    "title": "Dimension",
    "section": "Conclusion",
    "text": "Conclusion\n\nL’ACP est un outil puissant pour la réduction de dimension qui :\n\nPréserve l’information maximale.\nÉlimine la redondance d’information.\nFacilite la visualisation et la modélisation.\nNécessite des choix méthodologiques.\n\nProchaine étape → Que faire si les données ne sont pas qualitatives ?"
  },
  {
    "objectID": "contents/remainders/03-programming.html",
    "href": "contents/remainders/03-programming.html",
    "title": "Programmation",
    "section": "",
    "text": "Dans cette partie, on présente quelques références sur les bases de la programmation dans différents langages. Ce cours ne spécifie pas de langage à utiliser, vous êtes donc libre de choisir celui qui vous convient le mieux pour faire les différents exercices, ainsi que votre projet. Dans tous les cas, il y a un ensemble de bonnes pratiques qui permet d’avoir un code lisible, compréhensible et réutilisable. Vous trouvez un (rapide) guide des bonnes pratiques en programmation à ce lien.",
    "crumbs": [
      "Modules",
      "02 - Révisions",
      "Programmation"
    ]
  },
  {
    "objectID": "contents/remainders/03-programming.html#r",
    "href": "contents/remainders/03-programming.html#r",
    "title": "Programmation",
    "section": "R",
    "text": "R\nR est un langage de programmation spécialisé dans l’analyse statistique et la visualisation de données. Il est gratuit, open-source et disponible sur Windows, macOS et Linux. Régulièrement mis à jour, de nouveaux packages sont disponibles chaque jour (cf. CRAN).\nVoici un guide qui présente les bases de R: Apprendre R en Y minutes et un guide de bonnes pratiques en R. Enfin, un point important du langage R est l’opérateur pipe. Celui-ci permet de rendre le code plus clair. Vous pouvez trouver une explication de cette opérateur ici.",
    "crumbs": [
      "Modules",
      "02 - Révisions",
      "Programmation"
    ]
  },
  {
    "objectID": "contents/remainders/03-programming.html#python",
    "href": "contents/remainders/03-programming.html#python",
    "title": "Programmation",
    "section": "Python",
    "text": "Python\nPython est un langage de programmation généraliste. Il est gratuit, open-source et disponible sur Windows, macOS et Linux. Bien que généraliste, il y a une importante communauté autour du l’analyse de données, machine learning en Python. Ainsi, les méthodes usuelles ont déjà été implementées, e.g. dans le package sklearn, mais si besoin, il est possible d’aller voir le code pour le modifier.\nVoici un guide qui présente les bases de Python: Apprendre Python en Y minutes et un guide de bonnes pratiques en Python.",
    "crumbs": [
      "Modules",
      "02 - Révisions",
      "Programmation"
    ]
  },
  {
    "objectID": "contents/remainders/03-programming.html#julia",
    "href": "contents/remainders/03-programming.html#julia",
    "title": "Programmation",
    "section": "Julia",
    "text": "Julia\nJulia est aussi un langage de programmation généraliste. C’est un langage bien plus récent que ses deux compères, R et Python (début des années 2010 pour Julia vs. début des années 1990 pour R et Python). Il peut donc sembler moins mature sur certains points, mais la plupart des méthodes d’analyse de données classiques ont été implémentées. De même que R et Python, ce langage est gratuit, open-source et disponible sur les principaux systèmes d’exploitation. Julia a l’avantage d’être plus rapide que R et Python. Il inclut aussi un support natif d’appel à des librairies en C ou en Fortran, et un support non-natif d’appel à des librairies en R et Python.\nVoici un guide qui présente les bases de Julia: Learn Julia in Y minutes (il n’y a pas de version française pour l’instant) et un guide de bonnes pratiques en Julia (en anglais).",
    "crumbs": [
      "Modules",
      "02 - Révisions",
      "Programmation"
    ]
  },
  {
    "objectID": "contents/remainders/03-programming.html#sas",
    "href": "contents/remainders/03-programming.html#sas",
    "title": "Programmation",
    "section": "SAS",
    "text": "SAS\nSAS est un langage de programmation propriétaire spécialisé dans l’analyse de données. Bien qu’utilisé dans certaines industries, e.g. pharmaceutique, son utilisation tend à diminuer. De plus, la license à renouveler tous les ans, l’absence de communauté en ligne (pour avoir de l’aide, à part la documentation officielle, il n’y a pas grand chose) et l’impossibilité de voir le code des différentes procédures font que je déconseille l’utilisation de SAS dans le cadre de ce cours. Cependant, si vous le voulez, l’université peut fournir des licenses moyennant paiement.",
    "crumbs": [
      "Modules",
      "02 - Révisions",
      "Programmation"
    ]
  },
  {
    "objectID": "contents/05-supervisee.html",
    "href": "contents/05-supervisee.html",
    "title": "Supervisée",
    "section": "",
    "text": "Slides:\n\nGénéralités\nAnalyse discriminante\nArbres\nModèles ensemblistes\n\nTD: link\nTP: link",
    "crumbs": [
      "Modules",
      "05 - Supervisée"
    ]
  },
  {
    "objectID": "contents/05-supervisee.html#sommaire",
    "href": "contents/05-supervisee.html#sommaire",
    "title": "Supervisée",
    "section": "Sommaire",
    "text": "Sommaire\n\nAnalyse discriminante\nArbres\nModèles ensemblistes\n\n\n\n\nExtrapolating (xkcd:605).",
    "crumbs": [
      "Modules",
      "05 - Supervisée"
    ]
  },
  {
    "objectID": "contents/07-further.html",
    "href": "contents/07-further.html",
    "title": "Aspects éthiques",
    "section": "",
    "text": "Slides:\n\nÉthique\nL’analyse de données est aujourd’hui utilisé dans de nombreux domaines : santé, éducation, politique publique, etc. Les données jouent même un rôle central dans la plupart des domaines. Les analyses permettent de prendre des décisions mieux informées et parfois automatisées. Dans certains domaines, ces décisions sont inconséquentes, mais pour d’autres domaines, cela soulève des questions éthiques majeures. Par exemple, est-ce que mes données représentent fidélement ma population ? quelles décisions sont prises à partir de ces analyses ? quelles sont les conséquences pour les individus concernés ?\nLes données ne sont pas neutres, elles sont le reflet d’un contexte social, institutionnel et technique. Ainsi, les modèles qui en sont issus peuvent renforcer les inégalités, reproduire des biais historiques, ou éventuellement porter atteinte à la vie privée des individus. Il est donc essentiel, lorsque l’on fait une analyse de données, d’avoir une certaine vigilance éthique, en réflechissant aux impacts sociaux, aux limites méthodologiques et aux responsabilités associées à ses choix.",
    "crumbs": [
      "Modules",
      "07 - Éthique"
    ]
  },
  {
    "objectID": "contents/07-further.html#confidentialité-des-données",
    "href": "contents/07-further.html#confidentialité-des-données",
    "title": "Aspects éthiques",
    "section": "Confidentialité des données",
    "text": "Confidentialité des données\nLorsqu’on manipule des données individuelles, il est essentiel de protéger la vie privée des personnes concernées. Plusieurs approches existent pour minimiser les risques de ré-identification ou d’exploitation abusive.\nL’anonymisation des données consiste à supprimer ou à transformer les identifiants directs (e.g. les noms et adresses) et indirects (e.g. les dates de naissance et les codes postaux) susceptibles de permettre l’identification d’un individu. Il convient de faire particulièrement attention à ce cas, dans certaines situations, c’est le croisement de plusieurs variables qui permet l’identification d’un individu et non une unique variable. Par exemple, deux chercheurs de l’université du Texas ont été capable d’identifier des utilisateurs de Netflix en utilisant des notes sur IMDb (c.f. lien). On peut aussi réduire la granularité des informations en limitant les détails fournis (e.g. en regroupant les âges en tranches et en n’utilisant pas l’âge exact) pour réduire le risque de ré-identification.\nLorsque certaines modalités d’une variable catégorielle sont associées à très peu d’individus, on peut les regrouper pour éviter qu’une combinaison unique de caractéristiques ne permette de retrouver une personne. On peut aussi ajouter du bruit aux données ou aux résultats. Cela permet de préserver des tendances globales tout en rendant les identification individuelles plus difficiles. Il faut faire attention à ne pas ajouter trop de bruit, sinon on risque de compromettre la validité des analyses. Enfin, on peut appliquer une approche de confidentialité différentielle consistant à garantir qu’un individu ne peut pas être identifié, même en connaissant toutes les autres données du jeu.",
    "crumbs": [
      "Modules",
      "07 - Éthique"
    ]
  },
  {
    "objectID": "contents/07-further.html#implications-sociales",
    "href": "contents/07-further.html#implications-sociales",
    "title": "Aspects éthiques",
    "section": "Implications sociales",
    "text": "Implications sociales\nL’utilisation des résultats d’analyses statistiques dans des contextes sociaux ou décisionnels soulève d’importants enjeux éthiques. En effet, plusieurs sources de discrimination peuvent intervenir dans un processus apparemment rigoureux.\nIl peut y avoir un biais dans l’échantillonnage si le jeu de données ne reflète pas fidèlement la population cible (e.g. la sous-représentation de certaines communautés). Dans ce cas, les modèles entraînés risquent d’être inéquitables. Il peut aussi y avoir un biais dans la variable à expliquer. Par exemple, pour un modèle basé sur des décisions judiciaires passées, il est possible qu’il perpétue les biais historiques des jugements.\nOn peut aussi se trouver avec une validité variable selon les groupes. Un même modèle peut très bien fonctionner pour un sous-groupe de la population, mais être très mauvais pour un autre, menant à des erreurs systématiques. Enfin, si certaines classes sont mal représentées, e.g. des maladies rares ou des groupes démographiques minoritaires, et donc peu présentes dans les données, le modèle peut mal les prédire ou même les ignorer.",
    "crumbs": [
      "Modules",
      "07 - Éthique"
    ]
  },
  {
    "objectID": "contents/07-further.html#déconstruire-quelques-mythes",
    "href": "contents/07-further.html#déconstruire-quelques-mythes",
    "title": "Aspects éthiques",
    "section": "Déconstruire quelques mythes",
    "text": "Déconstruire quelques mythes\n\n“La machine apprend toute seule.”\n\nEn réalité, le choix des données, des variables, de la variable à expliquer et de l’algorithme est faite par des êtres humains. L’“apprentissage” dépend entièrement des décisions humaines en amont.\n\n“C’est objectif, c’est basé sur des données.”\n\nLes données ne sont jamais neutres. Elles sont le produit d’un contexte social, institutionnel et méthodologique. Les modèles entraînés sur ces données héritent de leurs biais.\n\n“Mon modèle ne peut pas être sexiste car je n’ai pas utilisé le genre pour le construire.”\n\nMême sans inclure explicitement une variable comme le genre, un modèle peut apprendreà en inférer à partir d’autres variables corrélées comme le profession ou le parcours scolaire. L’exclusion explicite d’une variable sensible ne garantit pas l’absence de biais.",
    "crumbs": [
      "Modules",
      "07 - Éthique"
    ]
  },
  {
    "objectID": "contents/07-further.html#que-faire-pour-réduire-ces-biais",
    "href": "contents/07-further.html#que-faire-pour-réduire-ces-biais",
    "title": "Aspects éthiques",
    "section": "Que faire pour réduire ces biais ?",
    "text": "Que faire pour réduire ces biais ?\nPlusieurs approches peuvent être mises en oeuvre à différentes étapes du processus :\n\nAgir en amont sur les données : rééquilibrer l’échantillon, sur-représenter certains groupes ou encore corriger les biais connus dans la variable à expliquer.\nModifier les modèles a posteriori : appliquer des corrections sur les résultats produits comme le recalibrage des probabilités ou l’ajustement des seuils de décision pour certaines classes.\nModifier la fonction de perte utilisée pour l’apprentissage : intégrer des pénalités visant à réduire les inégalités de performance entre les groupes. On peut, par exemple, imposer une équité en terme de faux positifs pour les différents groupes dans la population.",
    "crumbs": [
      "Modules",
      "07 - Éthique"
    ]
  },
  {
    "objectID": "contents/supervised/03-ensemble.html",
    "href": "contents/supervised/03-ensemble.html",
    "title": "Méthodes ensemblistes",
    "section": "",
    "text": "Lorsqu’on travaille avec de grands jeux de données, il est souvent plus efficace de combiner les prédictions de plusieurs modèles simples, appelés classificateurs faibles, que d’essayer de construire un unique modèle très complexe. Cette idée est à la base des méthodes d’ensemble, qui visent à produire une prédiction unique à partir des prédictions de plusieurs modèles. En regroupant ainsi plusieurs modèles, on peut généralement obtenir une meilleure performance globale et souvent une meilleure robustesse par rapport au bruit et au sur-ajustement.\nPlusieurs stratégies peuvent être employées pour combiner les résultats de plusieurs modèles. Une première approche consiste à faire la moyenne des probabilités prédites par chaque modèle. Une autre méthode courante est le vote de majorité, où la classe la plus souvent prédite est retenue. On peut également adopter une stratégie inspirée de la sélection de variables utilisée dans le modèle de régression. Au lieu de choisir les “meilleures” variables, on sélectionne les modèles les plus pertinents pour l’ensemble. Enfin, des techniques plus avancées comme le bagging, le boosting et les forêts aléatoires ont été développées pour automatiser et optimiser cette combinaison de modèles.",
    "crumbs": [
      "Modules",
      "05 - Supervisée",
      "Méthodes ensemblistes"
    ]
  },
  {
    "objectID": "contents/supervised/03-ensemble.html#bagging",
    "href": "contents/supervised/03-ensemble.html#bagging",
    "title": "Méthodes ensemblistes",
    "section": "Bagging",
    "text": "Bagging\nLa méthode du bagging (bootstrap aggregating) consiste à construire plusieurs versions d’un même modèle à partir d’échantillons différents du jeu de données d’origine. Plus précisement, on commence par générer \\(B\\) échantillons bootstrap (tirages aléatoires avec remise) à partir des données d’apprentissage. Un arbre de classification est ensuite construit pour chacun de ces \\(B\\) échantillons. On obtient donc \\(B\\) arbres de classification. On prédit la classe d’une nouvelle observation pour chacun des \\(B\\) arbres et on retient la classe prédite le plus souvent, ce qui correspond à un vote de majorité. Il est également possible de calculer la moyenne des probabilités de classe fournies par chaque arbre pour obtenir un score plus nuancé.\nLe principal avantage du bagging est qu’il permet de réduire la variance et d’augmenter la stabilité des prédictions. Cependant, cette méthode rend plus difficile l’interprétation du rôle joué par chaque variable dans la classification. Pour évaluer l’importance d’une variable, on peut additionner les réductions de l’indice de Gini observées à chaque division basée sur cette variable, puis faire la moyenne de ces réductions sur tous les arbres. On exprime enfin ces moyennes sous forme de pourcentage relatif à la réduction maximale observée parmi toutes les variables.",
    "crumbs": [
      "Modules",
      "05 - Supervisée",
      "Méthodes ensemblistes"
    ]
  },
  {
    "objectID": "contents/supervised/03-ensemble.html#forêts-aléatoires",
    "href": "contents/supervised/03-ensemble.html#forêts-aléatoires",
    "title": "Méthodes ensemblistes",
    "section": "Forêts aléatoires",
    "text": "Forêts aléatoires\nLes forêts aléatoires reprennent le principe du bagging, mais introduisent une source supplémentaire d’aléa qui permet de décorréler les arbres construits. En effet, les arbres construits par l’algorithme de bagging tendent à se ressembler car les classes les plus fréquentes dominent. Pour résoudre ce problème, la méthode des forêts aléatoires impose qu’à chaque division dans un arbre, on sélectionne aléatoirement un sous-ensemble de \\(m\\) variables parmi les \\(p\\) disponibles et que seules ces \\(m\\) variables soient considérées pour choisir la meilleure division. Un choix usuel est \\(m \\approx \\sqrt{p}\\). Cette diversification améliore la performance globale de l’algorithme, ainsi que sa robustesse.",
    "crumbs": [
      "Modules",
      "05 - Supervisée",
      "Méthodes ensemblistes"
    ]
  },
  {
    "objectID": "contents/supervised/03-ensemble.html#boosting",
    "href": "contents/supervised/03-ensemble.html#boosting",
    "title": "Méthodes ensemblistes",
    "section": "Boosting",
    "text": "Boosting\nÀ la différence du bagging, le boosting adopte une approche séquentielle. L’idée est de construire une série de modèles simples en accordant progressivement plus d’importance aux observations mal classées par les modèles précédents. Chaque nouveau modèle est entraîné pour corriger les erreurs du précédent, ce qui permet d’atteindre une très bonne précision globale. Pour fonctionner efficacement, le boosting doit utiliser des modèles faibles, laissant au processus le soin de combiner leurs prédictions de manière optimale.\nIl existe plusieurs variantes du boosting. L’une des plus connues est AdaBoost (boosting adaptatif), qui ajuste les poids des observations à chaque itération. Une autre approche très utilisée aujourd’hui est le gradient boosting, qui repose sur la minimisation d’une fonction de perte à l’aide d’une procédure itérative inspirée de la descente du gradient.",
    "crumbs": [
      "Modules",
      "05 - Supervisée",
      "Méthodes ensemblistes"
    ]
  },
  {
    "objectID": "contents/supervised/03-ensemble.html#optimisation-des-hyper-paramètres",
    "href": "contents/supervised/03-ensemble.html#optimisation-des-hyper-paramètres",
    "title": "Méthodes ensemblistes",
    "section": "Optimisation des hyper-paramètres",
    "text": "Optimisation des hyper-paramètres\nComme toute méthode de classification, les modèles d’ensembles nécessitent une optimisation de leur hyper-paramètres pour atteindre un bon compromis entre performance et robustesse. On observe d’ailleurs empiriquement que plus une méthode est puissante, plus l’optimisation des hyper-paramètres est complexe et sensible.\nVoici quelques exemples de paramètres à ajuster selon les méthodes :\n\nBagging : nombre d’échantillons bootstrap \\(B\\);\nForêts aléatoires : nombre d’échantillons bootstrap \\(B\\) et nombre de variables \\(m\\) à considérer à chaque division;\nBoosting: taille des arbres, fraction du jeu de données à échantillonner à chaque étape, nombre d’itérations de l’algorithme, …\n\nPour optimiser ces hyper-paramètres et éviter le sur-ajustement, une validation croisée pour chacun des paramètres est généralement nécessaire.",
    "crumbs": [
      "Modules",
      "05 - Supervisée",
      "Méthodes ensemblistes"
    ]
  },
  {
    "objectID": "contents/misc/pipe.html",
    "href": "contents/misc/pipe.html",
    "title": "Opérateur pipe en R",
    "section": "",
    "text": "Cette page est basé sur un document qu’Aurélien Nicosia (ULaval) a créé en 2023 appelé “Opérateur pipe”. Celui-ci a été mis à jour.\nDepuis la version 4.1.0 de R, sorti en mai 2021, le langage a introduit l’opérateur pipe |&gt; en s’inspirant de ce que faisait le package magrittr. À partir de R 4.3.0, le guide de style du tidyverse recommande l’utilisation de l’opérateur pipe de base, et non celui de magrittr.\n\n\n\n\n\n\nNoteRaccourcis clavier\n\n\n\nDans RStudio, le raccourci clavier pour insérer l’opérateur pipe est:\n\nSous Windows : Ctrl + Shift + M\nSous macOS : ⌘ + ⇧ + M\n\n\n\nCet opérateur introduit une façon d’enchaîner les intructions et de passer des arguments à des fonctions de manière plus lisible que la manière classique.\nPour résumer le fonctionnement de cette opérateur, voici comment il transforme quelques appels de fonctions:\n\nf(x) devient x |&gt; f();\nf(x, y) devient x |&gt; f(y);\nh(g(f(x))) devient x |&gt; f() |&gt; g() |&gt; h().\n\nCette opérateur permet de mettre en avant la séquence d’actions et non l’objet sur lequel la séquence d’actions est faite. Cela rend le code plus lisible (et avoir un code lisible est une bonne pratique). En lisant de gauche à droite l’instruction h(g(f(x))), nous voyons d’abord l’appel à la fonction h, puis l’appel à la fonction g et finalement l’appel à la focntion f. Pourtant, l’évaluation de cette instruction se fait dans le sens inverse. En effet, R va d’abord :\n\névaluer f(x);\npuis, il passera le résultat à la fonction g et retournera le résultat;\nqui sera passé à la fonction h et le résultat final sera retourné.\n\nSi nous voulons écrire un code qui reflète l’ordre des évaluations correctement, nous pourrions écrire :\n\nres1 &lt;- f(x)\nres2 &lt;- g(res1)\nh(res2)\n\nCe code a cependant le défaut de créer des objects que nous souhaitons pas nécessairement conserver. L’opérateur |&gt; n’a pas ce défaut ! En effet, une instruction écrite en utilisant l’opérateur |&gt; permet de suivre l’ordre des évaluations, sans créer d’objets inutilement en mémoire.\nPour encore plus de clarté, il est possible d’étendre sur plusieurs lignes un instruction contenant plusieurs opérateur |&gt; de façon à avoir une fonction par ligne :\n\nx |&gt; \n  f() |&gt; \n  g() |&gt; \n  h()\n\nSi l’argument que nous souhaitons passer avec l’opérateur |&gt; n’est pas celui en première position, il faut utiliser _ comme suit avec un paramètre nommé: f(y, z = x) devient x |&gt; f(y, z = _).\nPrenons un exemple pour illustrer l’utilisation de l’opérateur |&gt;. Supposons que nous avons la chaîne de caractères suivantes :\n\ntext &lt;- \"Ceci est un example\"\n\net que nous souhaitons la corriger—remplacer “example” par “exemple” et ajouter un point à la fin—avec l’intruction suivante :\n\npaste0(gsub(pattern = \"example\", replacement = \"exemple\", x = text), \".\")\n\n[1] \"Ceci est un exemple.\"\n\n\nCette instruction est un peu difficile à lire en raison de l’appel à la fonction gsub imbriqué dans un appel de fontion paste0. Nous pourrions la réécrire comme suit avec l’opérateur |&gt; :\n\ntext |&gt; \n  gsub(pattern = \"example\", replacement = \"exemple\", x = _) |&gt; \n  paste0(\".\")\n\n[1] \"Ceci est un exemple.\"\n\n\nPrenons un autre exemple numérique. On souhaite faire le calcul suivant : \\[\\frac{(2 + 4) \\times 8}{2}.\\]\nPour cela, nous avons besoin de quelques fonctions mathématiques.\n\nadd &lt;- function(x, y) {\n  x + y\n}\n\nmul &lt;- function(x, y) {\n  x * y\n}\n\ndiv &lt;- function(x, y) {\n  x / y\n}\n\nOn peut faire le calcul de trois manières différentes:\n\n# En créant différents objets \nres1 &lt;- add(2, 4)\nres2 &lt;- mul(res1, 8)\nres3 &lt;- div(res2, 2)\nprint(res3)\n\n[1] 24\n\n# En imbriquant les fonctions\nres &lt;- div(mul(add(2, 4), 8), 2)\nprint(res)\n\n[1] 24\n\n# Avec l'opérateur pipe \nres &lt;- 2 |&gt;\n  add(4) |&gt; \n  mul(8) |&gt; \n  div(2)\nprint(res)\n\n[1] 24"
  },
  {
    "objectID": "contents/misc/good-practices.html",
    "href": "contents/misc/good-practices.html",
    "title": "Bonnes pratiques de programmation",
    "section": "",
    "text": "Cette page est basé sur un document qu’Aurélien Nicosia (ULaval) a créé en 2023 appelé “Bonnes pratiques de programmation en R”. Celui-ci a été mis à jour et écrit dans un contexte plus général.\nPeu importe le langage informatique, employer de bonnes pratiques de programmation signifie respecter certaines normes afin de créer du “bon” code. On peut donc se demander ce qu’est un “bon” code. Pour moi, un “bon” code est un code qui remplit trois objectifs:\nPourquoi est-ce souhaitable d’adopter de bonnes pratiques ? Cela permet que le code soit compris et utilisé par n’importe qui (et en particulier, soi-même dans le futur). À long terme, les bonnes pratiques apportent une augmentation de notre productivité en évitant les répétitions inutiles."
  },
  {
    "objectID": "contents/misc/good-practices.html#objectif-1-produire-les-résultats-escomptés",
    "href": "contents/misc/good-practices.html#objectif-1-produire-les-résultats-escomptés",
    "title": "Bonnes pratiques de programmation",
    "section": "Objectif 1 : Produire les résultats escomptés",
    "text": "Objectif 1 : Produire les résultats escomptés\nLa priorité lors du développement de tout code informatique est certainement l’écriture d’un code qui réalise bien ce qu’il doit réaliser. Donc, un “bon”” code doit produire les bons résultats. Pour y arriver, le code doit d’abord être fonctionnel, c’est-à-dire ne pas contenir de bogues. Pour s’assurer d’écrire du code qui fonctionne correctement, il faut simplement le tester. Il vaut mieux tester fréquemment, à chaque petit ajout, plutôt que de produire beaucoup de code avant de le tester. Ainsi, il y a beaucoup moins de débogage à faire. Un courant de pensée en informatique prône même l’écriture des tests avant l’écriture du code (test driven development). Cependant, une meilleure pratique est de formaliser les tests afin de pouvoir facilement les lancer à nouveau lors de modifications futures apportées au code."
  },
  {
    "objectID": "contents/misc/good-practices.html#objectif-2-code-facile-à-maintenir",
    "href": "contents/misc/good-practices.html#objectif-2-code-facile-à-maintenir",
    "title": "Bonnes pratiques de programmation",
    "section": "Objectif 2 : Code facile à maintenir",
    "text": "Objectif 2 : Code facile à maintenir\nMaintenir un code informatique signifie de s’assurer qu’il continue de fonctionner correctement dans le futur, malgré les modifications qui lui sont apportées. Un code utilisé fréquemment est un code appelé à être mis à jour, soit pour y ajouter des fonctionnalités, soit pour corriger des bogues non détectés par les tests, mais découverts par des utilisateurs. Reprendre un code écrit par quelqu’un d’autre, ou écrit par nous-mêmes quelques mois auparavant, n’est pas toujours une tâche facile. Cependant, s’il s’agit d’un code correctement écrit, il ne devrait pas être trop difficile à comprendre et à modifier.\nLa maintenance d’un code est basé sur trois principes: son versionnage, sa compréhensibilité et sa réutilisabilité.\nLe principe du versionnage est d’utiliser un logiciel qui enregistre les différentes modifications faites sur le code. Le plus connu est Git. Il permet de naviguer entre les différentes versions de votre code, de créer plusieurs versions (appelée branches), de collaborer à plusieurs sur un même code. C’est vraiment un indispensable à avoir dans sa panoplie. Le versionnage peut ensuite être enregistré sur Github qui gère la machinerie sous-jacente. À titre d’exemple, ce site utilise Git et Github pour son versionnage et vous pouvez voir les différentes versions ici.\nUn code compréhensible est clair et se lit bien (presque comme du texte). Il comporte souvent des instructions qui parlent d’elles-mêmes. Ces instructions sont typiquement succinctes, car une instruction trop longue effectue souvent plusieurs tâches difficilement discernables. Si la lecture d’une instruction ne permet pas à un programmeur initié dans le langage informatique employé de comprendre ce qu’elle réalise, il est alors recommandé d’insérer un commentaire dans le code pour expliquer à quoi sert l’instruction. En plus de commentaires pour expliquer certains instructions, toutes fonctions devraient être documentées. La documentation d’une fonction devrait contenir: un texte explicatif de ce que fait la fonction, une description des arguments acceptés en entrée, une description des résultats produits et un exemple d’utilisation. Lorsque que l’on programme, il est aussi de bon ton de suivre un guide de style. Un guide de style est un ensemble de règles sur lesquelles les développeurs se sont mis d’accord et qui permet d’avoir une syntaxe similaire à travers les différents projets. En R, vous pouvez utiliser le guide de style du tidyverse. En Python, vous pouvez utiliser le guide de style PEP8 écrit par le créateur de Python. Et en Julia, vous pouvez utiliser le guide de style fournit avec le manuel du langage. Vous pouvez utiliser un linter, un outil d’analyse de code statique pour vous aidez à respecter ces guides de styles. À noter que ces guides de style sont des recommendations et il n’y a rien d’obligatoire à les suivre. Certaines règles se contredisant, je vous recommanderai même de ne pas en suivre certaines.\nLa façon la plus commune d’avoir un code facile à réutiliser est d’en faire des fonctions que l’on peut ensuite partager à travers un package."
  },
  {
    "objectID": "contents/misc/good-practices.html#objectif-3-code-suffisamment-rapide",
    "href": "contents/misc/good-practices.html#objectif-3-code-suffisamment-rapide",
    "title": "Bonnes pratiques de programmation",
    "section": "Objectif 3 : Code suffisamment rapide",
    "text": "Objectif 3 : Code suffisamment rapide\nAprès nous être assurés que notre code fonctionne correctement et qu’il est facilement maintenable, nous pouvons nous préoccuper de son temps d’exécution. Bien qu’il ne s’agisse pas du critère le plus important pour définir ce qu’est du “bon” code, c’est tout de même un critère à ne pas négliger, car un code trop lent risque de ne pas être utilisé. Pour produire du code computationnellement efficace, il faut :\n\nmettre en pratique quelques trucs simples, i.e. utiliser les syntaxes optimisées des différents langages;\ncomparer le temps d’exécution de différentes façons de programmer une tâche;\nparfois faire du calcul en parallèle;\nparfois programmer des bouts de code dans un autre langage plus bas niveau."
  },
  {
    "objectID": "contents/misc/good-practices.html#en-résumé",
    "href": "contents/misc/good-practices.html#en-résumé",
    "title": "Bonnes pratiques de programmation",
    "section": "En résumé",
    "text": "En résumé\nEn résumer, pour adopter de bonnes pratiques de programmation, il faut :\n\nTester son code fréquemment son code.\nUtiliser un logiciel de gestion de versions.\nDocumenter son code.\nRespecter un guide de style.\nFactoriser son code en créant des fonctions et des packages.\nOptimiser le temps d’exécution."
  },
  {
    "objectID": "contents/04-dimension.html",
    "href": "contents/04-dimension.html",
    "title": "Dimension",
    "section": "",
    "text": "Slides:\n\nAnalyse en composantes principales - annoté\nAnalyse factorielle des correspondances - annoté\nAnalyse des correspondances multiples - annoté\n\nTD: link\nTP: link",
    "crumbs": [
      "Modules",
      "04 - Dimension"
    ]
  },
  {
    "objectID": "contents/04-dimension.html#sommaire",
    "href": "contents/04-dimension.html#sommaire",
    "title": "Dimension",
    "section": "Sommaire",
    "text": "Sommaire\n\nAnalyse en composantes principales\nAnalyse factorielle des correspondances\nAnalyse des correspondances multiples\n\n\n\n\nFlatland (xkcd:721).",
    "crumbs": [
      "Modules",
      "04 - Dimension"
    ]
  },
  {
    "objectID": "contents/06-nonsupervisee.html",
    "href": "contents/06-nonsupervisee.html",
    "title": "Non-supervisée",
    "section": "",
    "text": "Slides:\n\n\\(k\\)-moyennes\nMéthodes hiérarchiques\nMélanges de gaussiennes\n\nTD: link\nTP: link",
    "crumbs": [
      "Modules",
      "06 - Non-supervisée"
    ]
  },
  {
    "objectID": "contents/06-nonsupervisee.html#sommaire",
    "href": "contents/06-nonsupervisee.html#sommaire",
    "title": "Non-supervisée",
    "section": "Sommaire",
    "text": "Sommaire\n\n\\(k\\)-moyennes\nMéthodes hiérarchiques\nMélanges de gaussiennes\n\n\n\n\n\\(k\\)-means (xkcd:2731).",
    "crumbs": [
      "Modules",
      "06 - Non-supervisée"
    ]
  },
  {
    "objectID": "contents/generalities/04-bias-variance.html",
    "href": "contents/generalities/04-bias-variance.html",
    "title": "Biais/Variance",
    "section": "",
    "text": "Cette section est basée sur James et al. (2021), chapitre 2.",
    "crumbs": [
      "Modules",
      "03 - Généralités",
      "Biais/Variance"
    ]
  },
  {
    "objectID": "contents/generalities/04-bias-variance.html#quel-est-notre-objectif",
    "href": "contents/generalities/04-bias-variance.html#quel-est-notre-objectif",
    "title": "Biais/Variance",
    "section": "Quel est notre objectif ?",
    "text": "Quel est notre objectif ?\nNous souhaitons modéliser la relation entre une variable réponse \\(Y\\), pouvant être quantitative, qualitative ou de nature différente, et un ensemble de \\(p\\) variables explicatives \\(X = (X_{1}, \\dots, X_{p})\\), elles aussi de (potentiellement) différents types. L’idée centrale est qu’il existe un relation entre \\(Y\\) et les variables explicatives \\(X\\). De manière générales, nous modélisons cette relation par le modèle :\n\\[Y = f(X) + \\varepsilon. \\tag{1}\\]\nIci, \\(f\\) est une fonction déterministe (non-aléatoire) représentant l’information systématique que les variables explicatives \\(X_{1}, \\dots, X_{p}\\) apportent sur \\(Y\\), et \\(\\varepsilon\\) est un terme d’erreur aléatoire, modélisant les variations de \\(Y\\) non expliquées par \\(X\\). Dans le cadre de ce cours, nous ferons les hypothèses suivantes : la variable aléatoire \\(\\varepsilon\\) est indépendente de des variables explicatives \\(X\\), \\(\\mathbb{E}[\\varepsilon] = 0\\) et \\(\\mathrm{Var}(\\varepsilon) = \\sigma^2\\). Le modèle Équation 1 est général. Il sert de cadre pour l’ensemble des méthodes que nous allons étudier, même lorsque la forme explicite de \\(f\\) n’est pas connue.\nLa Figure 1 illustre les différents éléments du modèle: les données observées \\((X_i, Y_i)\\), la fonction \\(f\\) (en bleu) et les écarts aléatoires \\(\\varepsilon_i\\) représentés par des lignes pointillées.\n\n\nCode\nlibrary(tibble)\nlibrary(dplyr)\n\ngenerate_noisy_data &lt;- function(n = 100, noise_levels = c(0, 0.1, 0.3, 0.5)) {\n  # x values (avoid 0 for log)\n  x_vals &lt;- seq(0.01, 0.99, length.out = n)\n  \n  # True function\n  f &lt;- function(x) 4 * x * (1 - x) * log(x) + 2\n  \n  # Generate data for each noise level\n  data &lt;- lapply(noise_levels, function(sigma) {\n    y_true &lt;- f(x_vals)\n    y_noisy &lt;- y_true + rnorm(n, mean = 0, sd = sqrt(sigma))\n    \n    tibble(\n      x = x_vals,\n      y = y_noisy,\n      noise = sigma\n    )\n  }) %&gt;% bind_rows()\n  \n  return(data)\n}\n\n# Example usage\nset.seed(123)\n\nnoise_levels &lt;- seq(0, 0.5, by=0.01)\ndf &lt;- generate_noisy_data(noise_levels = noise_levels)\nwrite.csv(df, './data.csv')\n\n\n\n\nCode\ndata = FileAttachment(\"../../include/data/bias-variance/data.csv\").csv({ typed: true })\n\nviewof noise = Inputs.range(\n  [0.01, 0.5], \n  {value: 0.25, step: 0.01, label: tex`\\sigma^2`}\n)\n\nfiltered = data.filter(function(df) {\n  return df.noise == noise;\n})\n\ntrue_curve = data.filter(function(df) {\n  return df.noise == 0;\n})\n\nerrors = filtered.map(f =&gt; {\n  const location = true_curve.find(loc =&gt; loc.x === f.x);\n  return { ...f, y_end: location?.y };\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nPlot.plot({\n  grid: true,\n  x: {\n    domain: [0, 1],\n    label: \"X\",\n  },\n  y: {\n    domain: [0, 3],\n    label: \"Y\",\n  },\n  marks: [\n    Plot.dot(filtered, {x: \"x\", y: \"y\", fill: \"#444444\", r: 3}),\n    Plot.line(true_curve, {x: \"x\", y: \"y\", stroke: \"#B0E1FA\", strokeWidth: 5}),\n    Plot.link(errors, {\n      x1: \"x\",\n      x2: \"x\",\n      y1: \"y\",\n      y2: \"y_end\",\n      stroke: \"#AAAAAA\",\n      strokeDasharray: \"5,5\",\n    })\n  ]\n})\n\n\n\n\n\n\n\n\n\nFigure 1: Les différents éléments du modèle. Les points représentent les données observées \\((X_i, Y_i)\\). La courbe bleue représente la fonction \\(f\\) et les lignes pointillées représentent l’erreur associée à chaque observation.\n\n\n\n\nDans la suite du cours, nous verrons différentes méthodes permettant d’estimer la fonction \\(f\\) à partir de données. Cependant avant d’étudier comment contruire un estimateur \\(\\widehat{f}\\) de \\(f\\), nous allons nous interroger sur la qualité d’un tel estimateur : que signifie “bien estimer” \\(f\\) ? Et comment évaluer la qualité de l’estimation ?\n\n\n\n\n\n\nNoteExemple : Régression linéaire simple\n\n\n\nDans ce cadre très simple, nous faisons l’hypothèse que la fonction \\(f\\) est de la forme : \\(f(x) = a x + b\\). Dans ce cas, l’estimation de la fonction \\(f\\) se résume à l’estimation des coefficients \\(a\\) et \\(b\\).\n\n\n\n\n\n\n\n\nAstuceRemarque : Compromis entre exactitude et interprétabilité\n\n\n\nDépendant de l’objectif de l’étude, nous devons généralement faire un choix entre l’exactitude de nos prédictions et l’interprétabilité de notre modèle. Un modèle simple, comme la régression linéaire, sera facile à interpréter mais capturera mal des relations complexes. À l’inverse, un modèle plus flexible, comme une forêt aléatoire, aura de meilleur prédiction, mais sera plus difficilement interprétable. Le choix dépend donc de l’objectif de l’analyse : compréhension ou performance prédictive ?\n\n\n\n\n\n\n\n\nAstuceRemarque : No free lunch in statistics\n\n\n\nPourquoi ne pas simplement utiliser le modèle “ultime”, celui qui serait toujours optimal quelque soit le jeu de données ? Parce qu’un tel modèle n’existe pas ! Il n’y a pas de méthode universellement meilleure pour tous les jeux de données et tous les objectifs. Une méthode performante dans un contexte donné peut échouer ailleurs. Il faut donc toujours adapter l’approche au problème (explication, prédiction, classification, …).",
    "crumbs": [
      "Modules",
      "03 - Généralités",
      "Biais/Variance"
    ]
  },
  {
    "objectID": "contents/generalities/04-bias-variance.html#comment-mesurer-la-qualité-dun-estimateur",
    "href": "contents/generalities/04-bias-variance.html#comment-mesurer-la-qualité-dun-estimateur",
    "title": "Biais/Variance",
    "section": "Comment mesurer la qualité d’un estimateur ?",
    "text": "Comment mesurer la qualité d’un estimateur ?\nUne fois que nous disposons d’un estimateur \\(\\widehat{f}\\) de la fonction \\(f\\), obtenu à partir de \\(n\\) observations \\((y_1, x_1), \\dots, (y_n, x_n)\\), nous cherchons à évaluer la précision des prédictions \\(\\widehat{Y} = \\widehat{f}(X)\\). L’idée est de vérifier dans quelle mesure \\(\\widehat{Y}\\) est proche de la vraie valeur de \\(Y\\).\n\n\n\n\n\n\nAvertissementDéfinition : Erreur quadratique moyenne\n\n\n\nLorsque \\(Y\\) est une variable quantitative, une mesure classique de la qualité de \\(\\widehat{f}\\) est l’erreur quadratique moyenne (mean square error, MSE) : \\[MSE(Y, \\widehat{Y}) = \\frac{1}{n} \\sum_{i = 1}^{n} \\left( y_i - \\widehat{y}_i\\right)^2 = \\frac{1}{n} \\sum_{i = 1}^{n} \\left( y_i - \\widehat{f}(x_i) \\right)^2,\\] où \\(\\widehat{y}_i = \\widehat{f}(x_i)\\) est la prédiction que \\(\\widehat{f}\\) donne pour l’observation \\(x_i\\).\n\n\nUne MSE faible indique que les prédictions sont proches des observations. Nous pouvons aussi l’interpréter comme la distance moyenne entre les valeurs observées et les valeurs prédites. Nous cherchons donc à avoir une distance moyenne faible.\nDans le cas où \\(Y\\) est une variable qualitative, e.g. une classe ou un label, on utilise une autre mesure : le taux d’erreur.\n\n\n\n\n\n\nAvertissementDéfinition: Taux d’erreur\n\n\n\nLorsque \\(Y\\) est une variable qualitative, une mesure classique de la qualité de \\(\\widehat{f}\\) est le taux d’erreur (error rate, ER) : \\[ER(Y, \\widehat{Y}) = \\frac{1}{n} \\sum_{i = 1}^{n} \\mathbb{1}(y_i \\neq \\widehat{y}_i) = \\frac{1}{n} \\sum_{i = 1}^{n} \\mathbb{1}(y_i \\neq \\widehat{f}(x_i)).\\] où \\(\\widehat{y}_i = \\widehat{f}(x_i)\\) est la prédiction que \\(\\widehat{f}\\) donne pour l’observation \\(x_i\\).\n\n\nLe taux d’erreur mesure la proportion de mauvaises prédictions. Il s’agit, là encore, d’une mesure de la distance moyenne entre \\(Y\\) et \\(\\widehat{Y}\\), adaptée aux variables qualititatives.",
    "crumbs": [
      "Modules",
      "03 - Généralités",
      "Biais/Variance"
    ]
  },
  {
    "objectID": "contents/generalities/04-bias-variance.html#le-compromis-biaisvariance",
    "href": "contents/generalities/04-bias-variance.html#le-compromis-biaisvariance",
    "title": "Biais/Variance",
    "section": "Le compromis biais/variance",
    "text": "Le compromis biais/variance\nNotre objectif est souvent de minimiser l’erreur de prédiction, non seulement sur les données observées, mais surtout sur de nouvelles données (récupérée après avoir estimer le modèle). Pour cela, nous nous intéressons à l’erreur de prédiction : \\[\\mathbb{E}\\left[ \\left( Y - \\widehat{Y} \\right)^2 \\right] = \\mathbb{E}\\left[ \\left( Y - \\widehat{f}(X) \\right)^2 \\right].\\]\nCette erreur peut se décomposer en trois composantes :\n\nLe biais : l’erreur due à une approximation systématique, e.g. si on impose un modèle linéaire alors que la relation est non linéaire.\nLa variance : la sensibilité de l’estimateur aux fluctuations de l’échantillon d’apprentissage.\nL’erreur irréductible : la variance intrinsèque du bruit \\(\\varepsilon\\), notée \\(\\sigma^2\\).\n\n\n\n\n\n\n\nImportantDécomposition biais/variance\n\n\n\nOn a : \\[\\mathbb{E}\\left[ (Y - \\widehat{Y})^2 \\right] = \\mathbb{E}\\left[ (Y - \\widehat{f}(X))^2 \\right] = \\mathrm{Biais}(\\widehat{f}(X))^2 + \\mathrm{Var}(\\widehat{f}(X)) + \\sigma^2.\\]\n\n\n\n\n\n\n\n\nNotePreuve\n\n\n\n\n\nTout d’abord, montrons que l’espérance de l’erreur de l’estimateur se décompose en une partie réductible et en une partie irréductible.\n\\[\\begin{align*}\n\\mathbb{E}\\left[ \\left( Y - \\widehat{Y} \\right)^2 \\right]\n&= \\mathbb{E}\\left[ \\left( Y - \\widehat{f}(X) \\right)^2 \\right] \\\\\n&= \\mathbb{E}\\left[ \\left( f(X) + \\varepsilon - \\widehat{f}(X) \\right)^2 \\right] \\\\\n&= \\mathbb{E}\\left[ \\left( f(X) - \\widehat{f}(X) \\right)^2 \\right] + 2\\mathbb{E}\\left[ \\left( f(X) - \\widehat{f}(X) \\right)\\varepsilon \\right] + \\mathbb{E}[\\varepsilon^2] \\\\\n&= \\mathbb{E}\\left[ \\left( f(X) - \\widehat{f}(X) \\right)^2 \\right] + 2\\mathbb{E}\\left[ \\left( f(X) - \\widehat{f}(X) \\right) \\right] \\underbrace{\\mathbb{E}\\left[ \\varepsilon \\right]}_{= 0} + \\sigma^2 \\\\\n&= \\underbrace{\\mathbb{E}\\left[ \\left( f(X) - \\widehat{f}(X) \\right)^2 \\right]}_{\\text{réductible}} + \\underbrace{\\sigma^2}_{\\text{irréductible}}.\n\\end{align*}\\]\nOn utilise la linéarité de l’espérance et le fait que \\(X\\) et \\(\\varepsilon\\) soient indépendants. On s’intéresse maintenant à la partie “réductible”. L’astuce est de faire apparaître \\(\\mathbb{E}\\left[ \\widehat{f}(X) \\right]\\).\n\\[\\begin{align*}\n\\mathbb{E}\\left[ \\left( f(X) - \\widehat{f}(X) \\right)^2 \\right]\n  &= \\mathbb{E}\\left[ \\left( f(X) - \\mathbb{E}\\left[ \\widehat{f}(X) \\right] + \\mathbb{E}\\left[ \\widehat{f}(X) \\right] - \\widehat{f}(X) \\right)^2 \\right] \\\\\n  &= \\underbrace{\\mathbb{E}\\left[ \\left( f(X) - \\mathbb{E}\\left[ \\widehat{f}(X) \\right] \\right)^2 \\right]}_{\\text{A}} \\\\\n  &\\quad - 2 \\underbrace{\\mathbb{E}\\left[ \\left( f(X) - \\mathbb{E}\\left[ \\widehat{f}(X) \\right] \\right) \\left( \\widehat{f}(X) - \\mathbb{E}\\left[ \\widehat{f}(X) \\right] \\right) \\right]}_{\\text{B}} \\\\\n  &\\quad + \\underbrace{\\mathbb{E}\\left[ \\left( \\widehat{f}(X) - \\mathbb{E}\\left[ \\widehat{f}(X) \\right] \\right)^2 \\right]}_{\\text{C}}.\n\\end{align*}\\]\nA. La fonction \\(f(X)\\) n’étant pas aléatoire, on a \\(\\mathbb{E}\\left[ f(X) \\right] = f(X)\\) et donc\n\\[\\begin{align*}\n\\mathbb{E}\\left[ \\left( f(X) - \\mathbb{E}\\left[ \\widehat{f}(X) \\right] \\right)^2 \\right]\n  &= \\mathbb{E}\\left[ \\left( \\mathbb{E}\\left[ f(X) - \\widehat{f}(X) \\right] \\right)^2 \\right] \\\\\n  &= \\mathbb{E}\\left[ f(X) - \\widehat{f}(X) \\right]^2 \\\\\n  &= \\text{Biais}(\\widehat{f}(X))^2.\n\\end{align*}\\]\nB. En développant l’expression et en utilisant l’indépendance des variables, on trouve que \\(B = 0\\).\nC. En utilisant la définition de la variance,\n\\[\\mathbb{E}\\left[ \\left( \\widehat{f}(X) - \\mathbb{E}\\left[ \\widehat{f}(X) \\right] \\right)^2 \\right] = \\mathrm{Var}(\\widehat{f}).\\]\nFinalement, on a\n\\[\\mathbb{E}\\left[ \\left( f(X) - \\widehat{f}(X) \\right)^2 \\right] = \\text{Biais}(\\widehat{f}(X))^2 + \\mathrm{Var}(\\widehat{f}(X)).\\]\nD’où le résultat.\n\n\n\nCette décomposition met en avant un compromis fondamental en analyse de données :\n\nSi on choisit un modèle peu flexible, le biais sera élevé, mais la variance sera faible.\nSi on choisit un modèle flexible, le biais sera faible, mais la variance peut être très élevée.\n\nNotre objectif est donc de trouver un juste équilibre entre biais et variance, i.e. un modèle qui prédit correctement, tout en étant généralisable à de nouvelles données. La Figure 2 (a) présente un jeu de données et différents estimateurs \\(\\widehat{f}\\). En faisant varier le paramètre \\(\\lambda\\), on obtient des modèles plus ou moins flexible (lorsque \\(\\lambda = 0.15\\), le modèle est flexible et lorsque \\(\\lambda = 1\\), le modèle est rigide). La Figure 2 (b) montre la valeur du biais, de la variance et de la MSE pour les modèles estimés pour la Figure 2 (a). On remarque que plus \\(\\lambda\\) est petit, plus la variance est grande, mais le biais est petit (le modèle est flexible). Inversement, plus \\(\\lambda\\) est grand, plus le biais est grand et la variance petite (le modèle est rigide). La courbe de MSE en fonction du paramètre est une courbe en U. Comme on cherche à minimiser la MSE, i.e. à faire un compromis entre le biais et la variance, on peut prendre \\(\\lambda = 0.5\\).\n\n\nCode\n# Load packages\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\n\nset.seed(42)\n\n# 1. Simulate a single dataset\nn &lt;- 100\nsigma2 &lt;- 0.1\nx &lt;- sort(runif(n, 0.05, 1))\ny &lt;- 4 * x * (1 - x) * log(x) + 2 + rnorm(n, 0, sqrt(sigma2))\ndf &lt;- data.frame(x = x, y = y, span = 0)\n\n# 2. Define grid and spans to compare\nx_grid &lt;- seq(0.05, 1, length.out = 300)\nspans_to_plot &lt;- seq(0.1, 1, by = 0.1)\n\n# 3. Compute loess fits for each span\nfits &lt;- lapply(spans_to_plot, function(s) {\n  loess_model &lt;- loess(y ~ x, data = df, span = s)\n  y_hat &lt;- predict(loess_model, newdata = data.frame(x = x_grid))\n  data.frame(x = x_grid, y = y_hat, span = s)\n})\n\nfit_df &lt;- bind_rows(fits)\nfit_df &lt;- fit_df |&gt; add_row(df)  # Add data points\n \nwrite.csv(fit_df, './data_fit.csv')\n\n\n# Parameters\nn_sim &lt;- 100          # number of simulated datasets\nspans &lt;- seq(0.1, 1, by = 0.05)  # LOESS smoothing parameters\nx_grid &lt;- seq(0.1, 1, length.out = 200)\nf_true &lt;- 4 * x_grid * (1 - x_grid) * log(x_grid) + 2\n\n# Storage for predictions\nresults &lt;- list()\n\nfor (s in spans) {\n  pred_matrix &lt;- matrix(NA, nrow = length(x_grid), ncol = n_sim)\n  \n  for (sim in 1:n_sim) {\n    x &lt;- sort(runif(n, 0.01, 1.1))\n    y &lt;- 4 * x * (1 - x) * log(x) + 2 + rnorm(n, 0, sqrt(sigma2))\n    df &lt;- data.frame(x = x, y = y)\n    \n    # Fit loess model with span = s\n    model &lt;- loess(y ~ x, data = df, span = s, degree = 2)\n    pred &lt;- predict(model, newdata = data.frame(x = x_grid), )\n    \n    pred_matrix[, sim] &lt;- pred\n  }\n  \n  # For each point in x_grid, compute bias², variance, MSE\n  mean_pred &lt;- rowMeans(pred_matrix, na.rm = TRUE)\n  bias2 &lt;- (mean_pred - f_true)^2\n  var_pred &lt;- apply(pred_matrix, 1, var, na.rm = TRUE)\n  mse &lt;- bias2 + var_pred\n  \n  results[[as.character(s)]] &lt;- data.frame(\n    span = s,\n    Biais2 = mean(bias2),\n    Variance = mean(var_pred),\n    MSE = mean(mse)\n  )\n}\n\n# Combine and reshape results\nresults_df &lt;- bind_rows(results)\nresults_long &lt;- pivot_longer(\n  results_df,\n  cols = c(\"Biais2\", \"Variance\", \"MSE\"),\n  names_to = \"component\", values_to = \"value\"\n)\n\nwrite.csv(results_long, './data_mse.csv')\n\n\n\n\nCode\ndata_fit = FileAttachment(\"../../include/data/bias-variance/data_fit.csv\").csv({ typed: true })\ndata_mse = FileAttachment(\"../../include/data/bias-variance/data_mse.csv\").csv({ typed: true })\n\nviewof span = Inputs.range(\n  [0.1, 1], \n  {value: 0.5, step: 0.1, label: tex`\\lambda`}\n)\n\nfiltered_fit = data_fit.filter(function(df) {\n  return df.span == span;\n})\n\npoints = data_fit.filter(function(df) {\n  return df.span == 0;\n})\n\ncurrent_mse = data_mse.filter(function(df) {\n  return df.span == span;\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nPlot.plot({\n  grid: true,\n  x: {\n    domain: [0, 1],\n    label: \"X\",\n  },\n  y: {\n    domain: [0, 3],\n    label: \"Y\",\n  },\n  marks: [\n    Plot.dot(\n      points,\n      {x: \"x\", y: \"y\", fill: \"#444444\", r: 3}\n    ),\n    Plot.line(\n      filtered_fit, \n      {x: \"x\", y: \"y\", stroke: \"#B0E1FA\", strokeWidth: 5}\n    ),\n\n  ]\n})\n\n\nCode\nPlot.plot({\n  grid: true,\n  color: {\n    legend: true,\n  },\n  x: {\n    domain: [0, 1],\n    label: \"λ\",\n  },\n  y: {\n    domain: [0, 0.04],\n    label: \"Y\",\n  },\n  marks: [\n    Plot.line(\n      data_mse, \n      {x: \"span\", y: \"value\", stroke: \"component\", strokeWidth: 5}\n    ),\n    Plot.dot(\n      current_mse,\n      {x: \"span\", y: \"value\", fill: \"component\", r: 10}\n    ),\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Différents estimateurs \\(\\widehat{f}\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Différentes parties de l’erreur.\n\n\n\n\n\n\nFigure 2: Illustration du compromis biais/variance. Le paramètre \\(\\lambda\\) contrôle la flexibilité du modèle, plus \\(\\lambda\\) est petit, plus le modèle est flexible.\n\n\n\n\nDe manière générale, lorsque la flexibilité augmente, la diminution du biais est plus importante que l’augmentation de la variance, ce qui fait décroître l’erreur de prédiction. Cependant, à partir d’un certain niveau de flexibilité, le biais devient négligeable, et toute baisse supplémentaire est compensée par l’augmentation rapide de la variance. L’erreur de prédiction commence donc à croître. Il en résulte une courbe en U de l’erreur de prédiction en fonction de la flexibilité du modèle : un modèle trop rigide engendre un fort biais, tandis qu’un modèle trop flexible conduit à une trop grande variance.\n\n\n\n\n\n\nAstuceRemarque : Pourquoi un compromis ?\n\n\n\nIl est toujours possible de construire un modèle très flexible avec un biais nul, e.g. un modèle qui passe par tous les points d’observations, mais qui aura une variance énorme. À l’opposé, un modèle trop rigide, e.g. une constante, aura un biais très important mais une variance presque nulle. Le compromis biais/variance consiste à choisir un modèle qui contrôle ces deux quantités.",
    "crumbs": [
      "Modules",
      "03 - Généralités",
      "Biais/Variance"
    ]
  },
  {
    "objectID": "contents/generalities/03-distance.html",
    "href": "contents/generalities/03-distance.html",
    "title": "Distances",
    "section": "",
    "text": "Dans tout projet d’analyse de données, il est nécessaire de pouvoir quantifier la ressemblance (ou la dissemblance) entre deux observations. Pour cela, on utilise la notion de distance (ou similarité) entre les observations. Le choix de cette distance influence directement les résultats des algorithmes d’apprentissage, de regroupement et de visualisations.",
    "crumbs": [
      "Modules",
      "03 - Généralités",
      "Distances"
    ]
  },
  {
    "objectID": "contents/generalities/03-distance.html#notion-de-distance",
    "href": "contents/generalities/03-distance.html#notion-de-distance",
    "title": "Distances",
    "section": "Notion de distance",
    "text": "Notion de distance\nUne distance est une fonction mathématique mesurant à quel point deux objets sont éloigné l’un de l’autre dans un espace donnée. Plus la distance est grande, plus les observations somt éloigné.\n\n\n\n\n\n\nAvertissementDéfinition de mesure de distance\n\n\n\nUne fonction \\(d: \\mathcal{X} \\times \\mathcal{X} \\to \\mathbb{R}\\) est une distance sur un ensemble \\(\\mathcal{X}\\) si, pour tout \\(x, y, z \\in \\mathcal{X}\\), les conditions suivantes sont vérifiées :\n\nnon-négativité: \\(d(x, y) \\geq 0\\);\nséparation: \\(d(x, y) = 0 \\Leftrightarrow x = y\\);\nsymétrie: \\(d(x, y) = d(y, x)\\);\ninégalité triangulaire: \\(d(x, y) \\leq d(x, z) + d(y, z)\\).\n\n\n\n\n\n\n\n\n\nNoteLa distance euclidienne\n\n\n\nLorsque les observations sont représentées par des vecteurs numériques dans \\(\\mathbb{R}^p\\) de même ordre de grandeur, la distance euclidienne est souvent un bon choix.\nSoit \\(x, y \\in \\mathbb{R}^p\\), la distance euclidienne est données par : \\[d(x, y) = \\left\\| x - y \\right\\|_2 = \\left( \\sum_{i = 1}^{p} (x_i - y_i)^2 \\right)^{1/2}.\\]\n\n\n\n\n\n\n\n\nNoteLa distance \\(L_q\\) (ou de Minkowski)\n\n\n\nSoit \\(x, y \\in \\mathbb{R}^p\\), la distance \\(L_q\\) est donnée, pour \\(q &gt; 0\\), par : \\[d(x, y) = \\left\\| x - y \\right\\|_q = \\left( \\sum_{i = 1}^{p} |x_i - y_i|^q \\right)^{1 / q}.\\]\nCas particuliers:\n\nPour \\(q = 1\\), on obtient la distance de Manhattan : \\[d(x, y) = \\left\\| x - y \\right\\|_1 = \\sum_{i = 1}^{p} |x_i - y_i|.\\]\nPour \\(q = 2\\), on obtient la distance euclidienne.\n\n\n\n\n\n\n\n\n\nNoteExemple\n\n\n\nConsidérons le jeu de données suivant :\n\nTaille et poids moyens au Canada (Source: Statistique Canada, Enquête sur la santé dans les collectivités canadiennes (2008)).\n\n\nNom\nTaille\nPoids\n\n\n\n\nAlice\n162.1\n66.8\n\n\nBob\n175.8\n81.6\n\n\n\nLa distance euclidienne entre Alice et Bob est \\[d(\\text{Alice}, \\text{Bob}) = \\sqrt{(162.1 - 175.8)^2 + (66.8 - 81.6)^2} = 20.16.\\]\nLa distance de Manhattan entre Alice et Bob est \\[d(\\text{Alice}, \\text{Bob}) = |162.1 - 175.8| + |66.8 - 81.6| = 28.5.\\]\n\n\nLa distance \\(L_q\\) n’est pas invariante aux changements d’échelle. Par exemple, si on multiplie toutes les composantes d’un vecteur par un facteur \\(\\lambda\\), la distance entre deux vecteurs change du facteur \\(\\lambda\\).\nEn practique, on préfère travailler avec des variables standardisées. Ainsi, en notant, \\(\\mu_i\\), la moyenne, et \\(\\sigma_i\\), l’écart-type de la variable \\(i\\), la distance euclidienne avec des variables standardisées est donnée par : \\[d(x, y) = \\sum_{i = 1}^{p} \\left\\{ \\frac{x_i - \\mu_i}{\\sigma_i} - \\frac{y_i - \\mu_i}{\\sigma_i} \\right\\}^2 = \\sum_{i = 1}^{p} \\left( \\frac{x_i - y_i}{\\sigma_i} \\right)^2.\\]\n\n\n\n\n\n\nImportantPropriété\n\n\n\nLa distance euclidienne avec des variables standardisées est invariante par changement d’échelle.\n\n\n\n\n\n\n\n\nNotePreuve\n\n\n\n\n\nSoit \\(\\lambda \\neq 0\\) et soit \\(X\\) une variable aléatoire. On a \\(\\mathbb{E}(\\lambda X) = \\lambda \\mathbb{E}(X)\\) et \\(\\mathrm{Var}(\\lambda X) = \\lambda^2 \\mathrm{Var}(X)\\). Donc\n\\[d(\\lambda x, \\lambda y) = \\sum_{i = 1}^{p} \\left\\{ \\frac{\\lambda x_i - \\lambda \\mu_i}{\\lambda \\sigma_i} - \\frac{\\lambda y_i - \\lambda \\mu_i}{\\lambda \\sigma_i} \\right\\}^2 = \\sum_{i = 1}^{p} \\left( \\frac{x_i - y_i}{\\sigma_i} \\right)^2 = d(x, y).\\]",
    "crumbs": [
      "Modules",
      "03 - Généralités",
      "Distances"
    ]
  },
  {
    "objectID": "contents/generalities/03-distance.html#notion-de-similarité",
    "href": "contents/generalities/03-distance.html#notion-de-similarité",
    "title": "Distances",
    "section": "Notion de similarité",
    "text": "Notion de similarité\nÀ l’opposé de la notion de distance, une mesure de similarité quantifie à quel point deux observations sont proches dans un espace donné. Ainsi, plus la similarité est grande, plus les observations sont proches.\n\n\n\n\n\n\nAvertissementDéfinition de mesure de similarité\n\n\n\nUne fonction \\(s: \\mathcal{X} \\times \\mathcal{X} \\to \\mathbb{R}\\) est une mesure de similarité sur un ensemble \\(\\mathcal{X}\\) si, pour tout \\(x, y \\in \\mathcal{X}\\), les conditions suivantes sont vérifiées :\n\n\\(s(x, y) \\geq 0\\);\n\\(s(x, y) = s(y, x)\\);\n\\(s(x, x) = 1 \\geq s(x, y)\\).\n\n\n\nUne distance peut se transformer en similarité en posant \\[s(x, y) = \\frac{1}{1 + d(x, y)}.\\]\nCette transformation garantit que plus la distance est grande, plus la similarité est faible. Toutefois, l’inverse n’est pas toujours possible car une mesure de similarité ne respecte pas forcément l’inégalité triangulaire. On peut aussi définir la dissemblance entre deux objets: \\[d^\\star(x, y) = 1 - s(x, y).\\]",
    "crumbs": [
      "Modules",
      "03 - Généralités",
      "Distances"
    ]
  },
  {
    "objectID": "contents/generalities/03-distance.html#cas-de-variables-qualitatives",
    "href": "contents/generalities/03-distance.html#cas-de-variables-qualitatives",
    "title": "Distances",
    "section": "Cas de variables qualitatives",
    "text": "Cas de variables qualitatives\nLorsque l’on travaille avec des variables qualitatives, les distances numériques habituelles (comme les distances \\(L_p\\)) n’ont généralement pas de sens. Par exemple, si une variable prend ses valeurs dans l’ensemble \\[\\mathcal{X} = \\{ \\text{Rouge}, \\text{Vert}, \\text{Bleu} \\},\\] alors il n’y a pas de sens à calculer la différence entre \\(\\text{Bleu}\\) et \\(\\text{Rouge}\\), car ces modalités ne portent aucune structure numérique intrinsèque et n’ont aucune notion d’ordre ou d’écart.\nUne mauvaise pratique consisterait à attribuer arbitrairement des valeurs numériques aux momdalités (e.g. \\(\\text{Rouge} = 1\\), \\(\\text{Vert} = 2\\), \\(\\text{Bleu} = 3\\)) ce qui introduirait un ordre artificiel entre elles. Cela risquerait de biaiser fortement les analyses.\n\nEncodage \\(1\\) parmi \\(K\\)\nLorsque l’on veut utiliser un modèle se basant sur une notion de distance entre les observations (i.e. la plupart des modèles), on doit utiliser un encodage adapté. L’encodage \\(1\\) parmi \\(K\\) (one-hot encoding) consiste à encoder une variable qualitative à \\(K\\) modalités sous la forme d’un vecteur binaire de dimension \\(K\\), dans lequel une seule entrée est à \\(1\\), les autres à \\(0\\). Ainsi, pour l’exemple de \\(\\mathcal{X} = \\{ \\text{Rouge, Vert, Bleu} \\}\\), on obtiendra l’encodage suivant: “Rouge” donne \\((1, 0, 0)\\), “Vert” donne \\((0, 1, 0)\\) et “Bleu” donne \\((0, 0, 1)\\).\nCette méthode d’encodage a l’avantage de ne pas introduire d’ordre artificiel entre les modalités. Cependant, si la variable a beaucoup de modalités, l’espace de représentation sera de grande dimension, ce qui peut nuire à l’efficacité de certaines méthodes d’analyse.\n\n\nDéfinir une distance adaptée\nUne fois les modalités encodées, on peut définir une distance entre deux observations de variables qualitatives.\n\n\n\n\n\n\nNoteLa distance discrète (ou distance de Hamming)\n\n\n\nSoit \\(\\mathcal{X}\\) un ensemble discret et soient \\(x\\) et \\(y\\) deux observations de \\(X\\), la distance discrète est donnée par \\[d(x, y) = \\begin{cases}\n  0, & \\text{si } x = y,\\\\\n  1, & \\text{si } x \\neq y\n\\end{cases}.\\]\n\n\nPour des vecteurs de variables qualitatives (e.g. la comparaison de plusieurs individus décrits par plusieurs caractéristiques), la distance discrète est la somme des désaccords entre les composantes : \\[d(x, y) = \\sum_{i = 1}^{p} \\mathbb{1}(x_i \\neq y_i),\\] où \\(p\\) est le nombre de variables.\n\n\n\n\n\n\nNoteExemple\n\n\n\nPrenons les caractéristiques de trois personnes.\n\n\n\nTable 1: Caractéristiques de trois personnes.\n\n\n\n\n\nNom\nCouleur\nYeux\nCheveux\n\n\n\n\nAlice\nRouge\nBleu\nBlond\n\n\nBob\nVert\nBleu\nRoux\n\n\nChris\nRouge\nVert\nBlond\n\n\n\n\n\n\nOn calcule la distance entre deux personnes comme le nombre de caractéristiques différentes. Ainsi,\n\\[d(\\text{Alice}, \\text{Bob}) = 1 + 0 + 1 = 2,\\]\n\\[d(\\text{Alice}, \\text{Chris}) = 0 + 1 + 0 = 1,\\]\n\\[d(\\text{Bob}, \\text{Chris}) = 1 + 1 + 1 = 3.\\]\n\n\nPlutôt que de compter les différences, on peut aussi compter les accords et les normaliser :\n\\[s(x, y) = \\frac{1}{p}\\sum_{i = 1}^{p} \\mathbb{1}(x_i = y_i),\\]\nce qui donne une mesure de similarité comprise entre \\(0\\) (aucun accord) et \\(1\\) (identique).\n\n\n\n\n\n\nNoteExemple\n\n\n\nEn reprenant l’exemple précédent (cf. Table 1), on trouve les similarités suivantes :\n\\[s(\\text{Alice}, \\text{Bob}) = \\frac{0 + 1 + 0}{3} = \\frac{1}{3},\\]\n\\[s(\\text{Alice}, \\text{Chris}) = \\frac{1 + 0 + 1}{3} = \\frac{2}{3},\\]\n\\[s(\\text{Bob}, \\text{Chris}) = \\frac{0 + 0 + 0}{3} = 0.\\]\n\n\n\n\nDistance de Jaccard\nLorsque le nombre de variables binaires est grand (e.g. dans le cas où un encodage \\(1\\) parmi \\(K\\) a été fait sur \\(p\\) variables qualitatives), la distance discrète n’est pas forcément très adaptée car le nombre d’accords risque d’être petit par rapport au nombre total de variables (\\(K \\times p\\) dans l’exemple précédent), ce qui va donner de petites distances dans tous les cas. Une des solutions est de se concentrer uniquement sur les attributs qui valent \\(1\\), car généralement, une variable binaire à \\(0\\) n’apporte pas spécialement d’information (en tout cas, moins qu’une variable binaire à \\(1\\)). L’indice de Jaccard (Intersection over Union, IoU) a été introduit pour prendre cela en compte.\n\n\n\n\n\n\nAvertissementDéfinition : Indice de Jaccard\n\n\n\nConsidérons deux observations \\(x\\) et \\(y\\) de \\(K\\) variables binaires. Toutes les variables peuvent prendre les valeurs \\(0\\) et \\(1\\).\nDéfinissons les quantités suivantes :\n\n\\(M_{11}\\), le nombre de variables à \\(1\\) pour \\(x\\) et \\(y\\);\n\\(M_{10}\\), le nombre de variables à \\(1\\) pour \\(x\\) et \\(0\\) pour \\(y\\);\n\\(M_{01}\\), le nombre de variables à \\(0\\) pour \\(x\\) et \\(1\\) pour \\(y\\);\n\\(M_{00}\\), le nombre de variables à \\(0\\) pour \\(x\\) et \\(y\\).\n\nChaque variable binaire étant forcément comptée, soit dans \\(M_{11}\\), soit dans \\(M_{10}\\), soit dans \\(M_{01}\\), soit dans \\(M_{00}\\), leur somme est donc égale à \\(K\\).\nL’indice de Jaccard est défini comme \\[J(x, y) = \\frac{M_{11}}{M_{10} + M_{01} + M_{11}} = \\frac{M_{11}}{K - M_{00}}.\\]\n\n\nEn faisant attention au cas où les deux observations ne sont constituées que de \\(0\\) (on prend \\(J(x, y) = 1\\) dans ce cas), l’indice de Jaccard est une mesure de similarité.\n\n\n\n\n\n\nImportantPropriété : Distance de Jaccard\n\n\n\nPour deux observations \\(x\\) et \\(y\\) de \\(K\\) variables binaires, la distance de Jaccard est donnée par \\[d(x, y) = 1 - J(x, y).\\]\n\n\n\n\n\n\n\n\nNotePreuve\n\n\n\n\n\nPour montrer que la distance de Jaccard est bien une distance, on doit montrer les quatres propriétés des distances. Notons d’abord que la distance de Jaccard peut se réécrire comme \\[d(x, y) = \\frac{M_{10} + M_{01}}{M_{01} + M_{10} + M_{11}}.\\]\n\nTous les termes au numérateur et au dénominateur sont positifs, donc \\(d(x, y) \\geq 0\\).\nMontrons que \\(d(x, y) = 0 \\Leftrightarrow x = y\\).\n\nSupposons que \\(d(x, y) = 0\\). Alors \\(M_{01} + M_{10} = 0\\). Donc, il n’y a pas de variables qui valent \\(0\\) pour \\(x\\) et \\(1\\) pour \\(y\\) et inversement. Comme \\(M_{01} + M_{10} + M_{11} &gt; 0\\), on a \\(x = y\\).\nMaintenant, supposons que \\(x = y\\). Alors \\(M_{01} = M_{10} = 0\\). Donc \\(d(x, y) = 0\\).\n\nOn a que \\(d(x, y) = d(y, x)\\) car l’indice de Jaccard est symétrique.\nLa preuve de l’inégalité triangulaire sera faite en exercice (cf. TD)).\n\n\n\n\n\n\n\n\n\n\nNoteExemple\n\n\n\nPrenons un questionnaire de \\(5\\) questions fermées. Supposons que la réponse “Oui” soit encodée par \\(1\\) et la réponse “Non” soit encodée par \\(0\\).\n\n\n\nTable 2: Caractéristiques de deux personnes.\n\n\n\n\n\nNom\nQ1\nQ2\nQ3\nQ4\nQ5\n\n\n\n\nAlice\n1\n0\n1\n0\n0\n\n\nBob\n1\n0\n0\n1\n0\n\n\n\n\n\n\nPour la distance entre Alice et Bob, on a \\(M_{11} = 1\\), \\(M_{10} = 1\\), \\(M_{01} = 1\\) et \\(M_{00} = 2\\). Donc, la similarité de Jaccard est donnée par \\(J(\\text{Alice}, \\text{Bob}) = \\frac{1}{3}\\). Ainsi, la distance de Jaccard est \\(d(\\text{Alice}, \\text{Bob}) = 1 - J(x, y) = \\frac{2}{3}\\).",
    "crumbs": [
      "Modules",
      "03 - Généralités",
      "Distances"
    ]
  },
  {
    "objectID": "contents/generalities/05-model-evaluation.html",
    "href": "contents/generalities/05-model-evaluation.html",
    "title": "Évaluation de modèles",
    "section": "",
    "text": "Cette section est basée sur James et al. (2021), chapitre 5.",
    "crumbs": [
      "Modules",
      "03 - Généralités",
      "Évaluation de modèles"
    ]
  },
  {
    "objectID": "contents/generalities/05-model-evaluation.html#évaluer-la-performance-dun-modèle-prédictif",
    "href": "contents/generalities/05-model-evaluation.html#évaluer-la-performance-dun-modèle-prédictif",
    "title": "Évaluation de modèles",
    "section": "Évaluer la performance d’un modèle prédictif",
    "text": "Évaluer la performance d’un modèle prédictif\nDans la section précédente, nous avons introduit des outils pour mesurer la qualité d’un estimateur : l’erreur quadratique moyenne (MSE) pour les variables quantitatives et le taux d’erreur (ER) pour les variables qualitatives. Ces mesures comparent les valeurs prédites \\(\\widehat{Y} = \\widehat{f}(X)\\) aux valeurs observées \\(Y\\). Cependant, si l’on calcule ces erreurs uniquement à partir des données qui ont servi à entraîner le modèle, on risque de sous-estimer la véritable erreur de prédiction. Pourquoi ? Parce que l’estimateur \\(\\widehat{f}\\) a été ajusté pour minimiser l’erreur sur ces mêmes données. Il s’y adapte donc bien, et généralement, trop bien ! Cela peut conduire à l’illlusion que notre modèle est performant. En effet, un modèle très flexible peut avoir une erreur faible sur les données d’entraînement simplement parce qu’il capture le bruit plutôt que le signal. Mais si le modèle s’adapte trop aux données d’entraînement, il risque de mal généraliser à de nouvelles données, i.e. des données qu’il n’a jamais vues. Ce phénomène s’appelle le sur-ajustement (overfitting).\n\n\n\n\n\n\nAstuceRemarque: Sur-ajustement et sous-ajustement\n\n\n\nUn modèle trop flexible peut s’adapter parfaitement aux données d’entraînement, y compris au bruit aléatoire. Il aura une erreur faible sur ces données mais une erreur élevée sur de nouvelles observations. On dira qu’il y a sur-ajustement (overfitting) du modèle. À l’inverse, un modèle trop rigide (par exemple, une droite constante) ne pourra pas capturer la structure des données, même sur l’ensemble d’entraînement. On dira qu’il y a sous-ajustement (underfitting) du modèle.\nL’objectif est de trouver le bon compromis entre flexibilité et capacité de généralisation.\n\n\nPour évaluer objectivement un modèle, l’idéal serait de le tester sur des données complètement indépendantes de celles utilisé pour l’apprentissage. On distingue donc deux ensembles : un jeu d’entraînement, utilisé pour ajuster le modèle et un jeu de test, utilisé pour évaluer la performance prédictive du modèle. En practique, nous n’avons généralement pas accès à un jeu de test pour faire cette évaluation. Dans cette section, nous allons deux approches permettant de contourner ce problème.",
    "crumbs": [
      "Modules",
      "03 - Généralités",
      "Évaluation de modèles"
    ]
  },
  {
    "objectID": "contents/generalities/05-model-evaluation.html#jeu-de-données-de-validation",
    "href": "contents/generalities/05-model-evaluation.html#jeu-de-données-de-validation",
    "title": "Évaluation de modèles",
    "section": "Jeu de données de validation",
    "text": "Jeu de données de validation\nQuand on ne dispose que d’un seul jeu de données, une solution simple consiste à le diviser aléatoirement en deux sous-ensembles : un jeu d’entraînement pour ajuster le modèle et un jeu de validation pour estimer l’erreur de prédiction. On parle alors d’approche par jeu de validation. La Figure 1 présente un schéma de cette approche et la Figure 2 présente plusieurs combinaisons de jeux de données d’entraînement et de validation, ainsi que les MSE associées.\n\n\n\n\n\n\nFigure 1: Schéma de l’approche par jeu de validation.\n\n\n\n\n\nCode\n# Load packages\nlibrary(tidyverse)\nlibrary(dplyr)\n\n# Set seed for reproducibility\nset.seed(42)\n\n# Function definition\nf &lt;- function(x) {\n  4 * x * (1 - x) * log(x) + 2\n}\n\ngenerate_splits &lt;- function(data, train_prop = 0.7, K = 10) {\n  rows &lt;- list()\n  \n  for (k in 1:K) {\n    idx_train &lt;- sample(nrow(data), size = floor(train_prop * nrow(data)))\n    train &lt;- data[idx_train, ]\n    validation &lt;- data[-idx_train, ]\n    \n    train_tbl &lt;- tibble(\n      split = k,\n      set = \"train\",\n      x = train$x,\n      y = train$y\n    )\n    \n    validation_tbl &lt;- tibble(\n      split = k,\n      set = \"validation\",\n      x = validation$x,\n      y = validation$y\n    )\n    \n    rows[[k]] &lt;- bind_rows(train_tbl, validation_tbl)\n  }\n  \n  bind_rows(rows)\n}\n\n# Generate N data points in the interval (0, 1]\nN &lt;- 200\nspans &lt;- seq(0.1, 1, by = 0.05)  # LOESS smoothing parameters\nx_all &lt;- runif(N, min = 0.01, max = 1)  # avoid 0 due to log(x)\ny_all &lt;- f(x_all) + rnorm(N, mean = 0, sd = 0.1)  # add Gaussian noise\n\n# Put everything in a data.frame\ndata &lt;- data.frame(x = x_all, y = y_all)\n\n# Generate 10 training/validation splits\nsplits &lt;- generate_splits(data, train_prop = 0.7, K = 10)\nwrite.csv(splits, './splits.csv')\n\n\n# Fit a model with different smoothing parameters on each split\nsplits_number &lt;- unique(splits$split)\n\nresults &lt;- list()\nfor (n_split in splits_number) {\n  \n  df_split &lt;- splits |&gt; dplyr::filter(split == n_split)\n  train &lt;- df_split |&gt; \n    dplyr::filter(set == 'train') |&gt; \n    dplyr::select(x, y)\n  validation &lt;- df_split |&gt; \n    dplyr::filter(set == 'validation') |&gt; \n    dplyr::select(x, y)\n  \n  results_splits &lt;- list()\n  for (s in spans) {\n\n    # Fit loess model with span = s on train and predict on validation\n    model &lt;- loess(y ~ x, data = train, span = s, degree = 2)\n    pred &lt;- predict(model, newdata = validation$x)\n      \n    # For each point in x_grid, compute bias², variance, MSE\n    bias2 &lt;- mean((pred - validation$y)^2, na.rm = TRUE)\n    var_pred &lt;- var(pred, na.rm = TRUE)\n    mse &lt;- bias2 + var_pred\n    \n    results_splits[[as.character(s)]] &lt;- data.frame(\n      split = n_split,\n      span = s,\n      MSE = mse\n    )\n  }\n  results_splits_df &lt;- bind_rows(results_splits)\n  results[[as.character(n_split)]] &lt;- results_splits_df\n}\n\nresults &lt;- results |&gt; bind_rows()\nwrite.csv(results, './splits_mse.csv')\n\n\n\n\nCode\ndata = FileAttachment(\"../../include/data/model-evaluation/splits.csv\").csv({ typed: true })\ndata_mse = FileAttachment(\"../../include/data/model-evaluation/splits_mse.csv\").csv({ typed: true })\n\nviewof split = Inputs.range(\n  [1, 10], \n  {value: 1, step: 1, label: \"Autre jeu de validation\"}\n)\n\nfiltered = data.filter(function(df) {\n  return df.split == split;\n})\n\ncurrent_mse = data_mse.filter(function(df) {\n  return df.split == split;\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nPlot.plot({\n  grid: true,\n  x: {\n    domain: [0, 1],\n    label: \"X\",\n  },\n  y: {\n    domain: [0, 3],\n    label: \"Y\",\n  },\n  color: {\n    domain: [\"train\", \"validation\"],\n    range: [\"#f7a941\", \"#e30513\"],\n    legend: true\n  },\n  marks: [\n    Plot.dot(\n      filtered,\n      {x: \"x\", y: \"y\", fill: \"set\", r: 5}\n    ),\n  ]\n})\n\n\nCode\nPlot.plot({\n  grid: true,\n  color: {\n    legend: true,\n  },\n  x: {\n    domain: [0, 1],\n    label: \"λ\",\n  },\n  y: {\n    domain: [0.1, 0.2],\n    label: \"Y\",\n  },\n  marks: [\n    Plot.line(\n      data_mse, \n      {\n        x: \"span\", y: \"MSE\", z: \"split\",\n        stroke: \"#AAAAAA\", strokeWidth: 2\n      }\n    ),\n    Plot.line(\n      current_mse,\n      {x: \"span\", y: \"MSE\", stroke: \"#e30513\", strokeWidth: 5}\n    ),\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Différents jeux d’entraînement et validation.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) MSE pour différents jeux de validation.\n\n\n\n\n\n\nFigure 2: Illustration de l’approche par jeu de données de validation.\n\n\n\n\n\n\n\n\n\n\nAstuceComment choisir la taille des sous-ensembles ?\n\n\n\nEn général :\n\nSi l’on dispose d’un grand nombre d’observations (disons plusieurs milliers), on peut faire une division \\(50-50\\).\nSi l’on dispose de moins d’observations, on préférera garder plus d’observations pour l’entraînement. On peut, par exemple, faire une division \\(70-30\\) ou \\(80-20\\).\n\nCependant, il n’existe pas de règle universelle. Le bon choix dépend du contexte, de la complexité du modèle et de la quantité de données disponibles.\n\n\nLa méthode a cependant deux inconvénients. Le premier est que l’estimation de l’erreur est instable. En effet, la valeur de l’erreur de prédiction dépend des observations qui sont dans le jeu de validation. Un autre jeu de validation peut donner un résultat différent. Le deuxième est qu’il y a moins de données pour ajuster le modèle. Comme une partie des données est réservée à la validation, le modèle est appris sur un ensemble plus petit, et cela peut donc surestimer son erreur réelle par rapport à s’il avait été appris sur l’ensemble de données complet.",
    "crumbs": [
      "Modules",
      "03 - Généralités",
      "Évaluation de modèles"
    ]
  },
  {
    "objectID": "contents/generalities/05-model-evaluation.html#validation-croisée",
    "href": "contents/generalities/05-model-evaluation.html#validation-croisée",
    "title": "Évaluation de modèles",
    "section": "Validation croisée",
    "text": "Validation croisée\nPour contourner les limites de l’approche précédente, on utilise souvent la validation croisée (cross-validation). Cette méthode est plus robuste et plus stable. Le principe est de répéter l’approche par jeu de validation plusieurs fois sur différents sous-ensembles du jeu de données.\nL’approche consiste à découper aléatoirement l’ensemble des observations en \\(K\\) sous-ensembles de taille équivalentes (appelés folds). Le premier fold est utilisé comme jeu de données de validation et le modèle est ajusté sur les \\(K - 1\\) folds restant. L’erreur de prédiction est calculé sur le premier fold. Cette procédure est répèté \\(K\\) fois; à chaque fois, un différent fold est utilisé comme jeu de données de validation. À la fin, on a donc \\(K\\) valeurs pour l’erreur de prédiction. On calcule enfin la moyenne des \\(K\\) valeurs de prédiction. La Figure 3 présente un schéma de cette approche.\n\n\n\n\n\n\nFigure 3: Schéma de l’approche par validation croisée\n\n\n\n\n\n\n\n\n\nAstuceComment choisir le nombre de sous-ensembles \\(K\\) ?\n\n\n\nLe choix du nombre de sous-ensembles \\(K\\) a un impact sur la qualité de l’estimation de l’erreur de prédiction, ainsi que sur le coût computationnel de la procédure. En pratique, on utilise souvent \\(K = 5\\) ou \\(K = 10\\). Ce choix repose sur un compromis entre la précision de l’estimation de l’erreur et le temps de calcul nécessaire. En effet, pour chaque valeur de \\(K\\), le modèle est ajusté \\(K\\) fois. Par conséquent, plus \\(K\\) est grand, plus le coût computationnel augmente.\nDans le cas limite où \\(K = n\\), i.e. \\(K\\) est égal au nombre d’observations dans le jeu de données, on parle de validation croisée leave-one-out (LOOCV). Dans ce cas, chaque observation sert une fois de validation et le modèle est entraîner sur les \\(n - 1\\) autres observations.\nCe choix de \\(K = n\\) minimise le biais dans l’estimation de l’erreur de prédiction, car à chaque itération, le modèle est ajusté sur presque toutes les observations du jeu de données. Cependant, cela se fait au prix d’une forte variance. En effet, comme les ensembles d’entraînement sont presque identiques, les erreurs de prédiction sont très corrélées entre elles, ce qui rend l’estimation globale de l’erreur instable.\nInversement, des valeurs plus faibles de \\(K\\) introduisent un léger biais dans l’estimation de l’erreur (car les modèles sont ajustés sur des ensembles contenant moins d’observations), mais réduisent la variance de cette estimation. Ce compromis biais/variance, couplé à une réduction significative du temps de calcul, explique pourquo \\(K = 5\\) ou \\(K = 10\\) sont des choix standards en pratique.\n\n\nPour finir, la validation croisée est une méthode générale qui peut être appliquée avec la plupart des modèles.",
    "crumbs": [
      "Modules",
      "03 - Généralités",
      "Évaluation de modèles"
    ]
  },
  {
    "objectID": "contents/dimension/02-ca.html",
    "href": "contents/dimension/02-ca.html",
    "title": "Analyse factorielle des correspondances",
    "section": "",
    "text": "L’analyse factorielle des correspondances (AFC) est une méthode d’analyse exploratoire qui vise à représenter graphiquement les relations entre les modalités de deux variables qualitatives. Elle permet de représenter simultanément les profils-lignes (dans \\(\\mathbb{R}^p\\)) et les profils-colonnes (dans \\(\\mathbb{R}^n\\)) d’un tableau de contingences, dans un espace de faible dimension, tout en préservant au mieux la distance du \\(\\chi^2\\). L’objectif de l’AFC est de trouver une représentation bidimensionnelle (voir tridimensionnelle) dans laquelle les proximités géométriques entre points reflètent au mieux les similarités entre les modalités.",
    "crumbs": [
      "Modules",
      "04 - Dimension",
      "Analyse factorielle des correspondances"
    ]
  },
  {
    "objectID": "contents/dimension/02-ca.html#notation",
    "href": "contents/dimension/02-ca.html#notation",
    "title": "Analyse factorielle des correspondances",
    "section": "Notation",
    "text": "Notation\nOn considère un tableau de contingence \\(K = (k_{ij})\\), où \\(k_{ij}\\) est le nombre d’individus appartenant à la classe \\(i \\in \\{ 1, \\dots, n \\}\\) et à la catégorie \\(j \\in \\{ 1, \\dots, p \\}\\). On travaille ensuite avec le tableau des fréquences relatives en normalisant ce tableau. Comme les fréquences sont proportionnelles à la taille d’échantillon \\(n\\), le tableau des fréquences relatives contient plus d’information. Notons \\(F = (f_{ij})\\), dans lequel \\[f_{ij} = \\frac{k_{ij}}{k_{\\bullet\\bullet}} = \\frac{k_{ij}}{\\sum_{l = 1}^{n} \\sum_{m = 1}^{p} k_{lm}}.\\]\nLes marges lignes (resp. colonnes) du tableau correspondent à la somme des colonnes pour chaque ligne (resp. à la somme des lignes pour chaque colonne): \\[\\begin{align}\nf_{i \\bullet} &= \\sum_{j = 1}^{p} f_{ij} = \\frac{k_{i \\bullet}}{k_{\\bullet\\bullet}}, \\quad 1 \\leq i \\leq n; \\\\\nf_{\\bullet j} &= \\sum_{i = 1}^{n} f_{ij} = \\frac{k_{\\bullet j}}{k_{\\bullet\\bullet}}, \\quad 1 \\leq j \\leq p.\n\\end{align}\\]\nOn a \\(f_{\\bullet \\bullet} = \\sum_{i = 1}^{n} f_{i \\bullet} = \\sum_{j = 1}^{p} f_{\\bullet j} = 1\\).\n\n\n\n\n\n\nNoteExemple\n\n\n\nComme exemple, on va considérer la majeure et le type d’admission des étudiants inscrit au cours STT-2200 à l’automne 2025.\n\nTableau de contingence des étudiants inscrit pour le cours STT-2200 (Automne 2025) croisant leur majeure et leur type d’admission.\n\n\n\nCollège\nUniversité Laval\nAutre université\nHors Québec\n\n\n\n\nActuariat\n2\n0\n0\n1\n\n\nStatistique\n2\n4\n1\n0\n\n\nBio-info\n4\n2\n0\n2\n\n\nFinance\n2\n0\n0\n0\n\n\nMaths\n1\n0\n0\n0\n\n\nInfo\n2\n1\n0\n1\n\n\n\nIci, on trouve \\(k_{\\bullet \\bullet} = 25\\). C’est tout simplement le nombre d’étudiants inscrit au cours. On trouve donc le tableau de fréquences suivant :\n\nTableau de fréquences associé au tableau de contingence précédent.\n\n\n\n\n\n\n\n\n\n\n\nCollège\nUniversité Laval\nAutre université\nHors Québec\n\\(f_{i \\bullet}\\)\n\n\n\n\nActuariat\n0.08\n0\n0\n0.04\n0.12\n\n\nStatistique\n0.08\n0.16\n0.04\n0\n0.28\n\n\nBio-info\n0.16\n0.08\n0\n0.08\n0.32\n\n\nFinance\n0.08\n0\n0\n0\n0.08\n\n\nMaths\n0.04\n0\n0\n0\n0.04\n\n\nInfo\n0.08\n0.04\n0\n0.04\n0.16\n\n\n\\(f_{\\bullet j}\\)\n0.52\n0.28\n0.04\n0.16\n1",
    "crumbs": [
      "Modules",
      "04 - Dimension",
      "Analyse factorielle des correspondances"
    ]
  },
  {
    "objectID": "contents/dimension/02-ca.html#indépendance-statistique",
    "href": "contents/dimension/02-ca.html#indépendance-statistique",
    "title": "Analyse factorielle des correspondances",
    "section": "Indépendance statistique",
    "text": "Indépendance statistique\nLe tableau des fréquences relatives \\(F = (f_{ij})\\) peut être interprété comme une estimation des probabilités conjointes des modalités des deux variables qualitatives. Si les deux variables sont statistiquement indépendantes, on s’attend à ce que la probabilité conjointe s’approche du produit des probabilités marginales : \\[f_{ij} \\approx f_{i \\bullet} f_{\\bullet j}, \\quad i \\in \\{ 1, \\dots, n \\},~ j \\in \\{ 1, \\dots, p \\}.\\]\nPour tester si les écarts observés entre \\(f_{ij}\\) et \\(f_{i \\bullet} f_{\\bullet j}\\) sont significatifs, on utilise le test du \\(\\chi^2\\) d’indépendance : \\[T = \\sum_{i = 1}^{n} \\sum_{j = 1}^{p} \\frac{\\left( k_{ij} - \\mathbb{E}(k_{ij}) \\right)^2}{\\mathbb{E}(k_{ij})} = \\sum_{i = 1}^{n} \\sum_{j = 1}^{p} \\frac{\\left( k_{ij} - \\frac{k_{i \\bullet}k_{\\bullet j}}{k_{\\bullet\\bullet}} \\right)^2}{\\left( \\frac{k_{i \\bullet} k_{\\bullet j}}{k_{\\bullet\\bullet}} \\right)}.\\] Sous l’hypothèse d’indépendance, cette statistique suit approximativement une loi du \\(\\chi^2\\). Si les variables sont indépendantes, la statistique \\(T\\) doit être proche de \\(0\\).",
    "crumbs": [
      "Modules",
      "04 - Dimension",
      "Analyse factorielle des correspondances"
    ]
  },
  {
    "objectID": "contents/dimension/02-ca.html#profils-lignes-et-profils-colonnes",
    "href": "contents/dimension/02-ca.html#profils-lignes-et-profils-colonnes",
    "title": "Analyse factorielle des correspondances",
    "section": "Profils-lignes et profils-colonnes",
    "text": "Profils-lignes et profils-colonnes\nPour analyser les structures dans le tableau de contingence, on introduit la notion de profil. Chaque ligne du tableau peut être vue comme un profil-ligne \\[L_i = \\left( \\frac{k_{i 1}}{k_{i \\bullet}}, \\dots, \\frac{k_{i p}}{k_{i \\bullet}} \\right) = \\left( \\frac{f_{i 1}}{f_{i \\bullet}}, \\dots, \\frac{f_{i p}}{f_{i \\bullet}} \\right).\\] Le profil-ligne représente la répartition des modalités \\(i\\) de la première variable parmi les modalités de la seconde.\nDe même, chaque colonne du tableau peut être vue comme un profil-colonne \\[C_j = \\left( \\frac{k_{1 j}}{k_{\\bullet j}}, \\dots, \\frac{k_{n j}}{k_{\\bullet j}} \\right) = \\left( \\frac{f_{1 j}}{f_{\\bullet j}}, \\dots, \\frac{f_{n j}}{f_{\\bullet j}} \\right)\n.\\] Le profil-colonne représente la répartition des modalités \\(j\\) de la deuxième variable parmi les modalités de la première.\nOn peut ensuite s’intéresser au profil-ligne moyen (resp. profil-colonne moyen) obtenus comme la moyenne pondérée des profils-lignes (resp. profils-colonnes). Autrement dit, ils correspond aux fréquences marginales colonnes (resp. fréquences marginales lignes). Le profil-ligne moyen est donné par \\[\\left( \\sum_{i = 1}^{n} f_{i \\bullet} \\frac{f_{i 1}}{f_{i \\bullet}}, \\dots, \\sum_{i = 1}^{n} f_{i \\bullet} \\frac{f_{i p}}{f_{i \\bullet}} \\right) = \\left( f_{\\bullet 1}, \\dots, f_{\\bullet p} \\right),\\] et le profil-colonne moyen est donné par \\[\\left( \\sum_{j = 1}^{p} f_{{\\bullet j}} \\frac{f_{1 j}}{f_{\\bullet j}}, \\dots, \\sum_{j = 1}^{p} f_{\\bullet j} \\frac{f_{n j}}{f_{\\bullet j}} \\right) = \\left( f_{{1 \\bullet}}, \\dots, f_{n \\bullet} \\right).\\]\nSi les variables sont indépendantes, tous les profiles sont égaux à leur profils moyens respectifs. Autrement dit, pour tout \\(i \\in \\{ 1, \\dots, n \\}\\) et \\(j \\in \\{ 1, \\dots, p \\}\\), \\[\\left( \\frac{f_{i 1}}{f_{i \\bullet}}, \\dots, \\frac{f_{i p}}{f_{i \\bullet}} \\right) = \\left( f_{\\bullet 1}, \\dots, f_{\\bullet p} \\right) \\quad\\text{et}\\quad\n\\left( \\frac{f_{1 j}}{f_{\\bullet j}}, \\dots, \\frac{f_{n j}}{f_{\\bullet j}} \\right) = \\left( f_{{1 \\bullet}}, \\dots, f_{n \\bullet} \\right).\\] Ainsi, plus les profils s’éloignent de leurs moyennes, plus les variables montrent une dépendance.\nPour mesurer la différence entre deux profils-lignes, on utilise la distance du \\(\\chi^2\\) pondérée par les fréquences marginales : \\[d^2(L_i, L_{i^\\prime}) = \\sum_{j = 1}^{p} \\frac{1}{f_{\\bullet j}} \\left( \\frac{f_{ij}}{f_{i \\bullet}} - \\frac{f_{i^\\prime j}}{f_{i^\\prime \\bullet}} \\right)^2.\\] On peut faire de même pour la différence entre deux profils-colonnes : \\[d^2(C_j, C_{j^\\prime}) = \\sum_{i = 1}^{n} \\frac{1}{f_{i \\bullet}} \\left( \\frac{f_{ij}}{f_{\\bullet j}} - \\frac{f_{i j^\\prime}}{f_{\\bullet j^\\prime}} \\right)^2.\\]\nOn peut écrire cela sous forme matricielle. Notons \\(D_n = \\text{diag}(f_{i \\bullet})\\) la matrice diagonale des poids des lignes et \\(D_p = \\text{diag}(f_{\\bullet j})\\) la matrice diagonale des poids des colonnes. La matrice \\(D_n^{-1}F\\) a pour lignes les profils-lignes et la matrice \\(D_p^{-1}F^{\\top}\\) a pour lignes les profils-colonnes. la distance du \\(\\chi^2\\) entre deux profils-lignes \\(L_i\\) et \\(L_{i^\\prime}\\) s’écrit alors \\[d^2(L_i, L_{i^\\prime}) = (L_i - L_{i^\\prime})^\\top D_p^{-1} (L_i - L_{i^\\prime}),\\] et de manière analogue pour deux profils-colonnes \\(C_j\\) et \\(C_{j^\\prime}\\) \\[d^2(C_j, C_{j^\\prime}) = (C_j - C_{j^\\prime})^\\top D_n^{-1} (C_j - C_{j^\\prime}).\\]\nCes distances sont à la base de la représentation géométrique dans l’analyse des correspondances, où l’on cherche une projection des profils dans un espace de faible dimension qui conserve au mieux ces distances.",
    "crumbs": [
      "Modules",
      "04 - Dimension",
      "Analyse factorielle des correspondances"
    ]
  },
  {
    "objectID": "contents/dimension/02-ca.html#estimation-des-éléments-propres",
    "href": "contents/dimension/02-ca.html#estimation-des-éléments-propres",
    "title": "Analyse factorielle des correspondances",
    "section": "Estimation des éléments propres",
    "text": "Estimation des éléments propres\nL’analyse des profils-lignes s’appelle l’analyse directe. On considère les profils-lignes contenus dans la matrice \\(D_n^{-1}F \\in \\mathbb{R}^{n \\times p}\\). On projette les profils-lignes dans un espace muni de la métrique du \\(\\chi^2\\) sur les colonnes, définie par \\[\\left\\langle x, y \\right\\rangle = x^{\\top} D_p^{-1} y.\\]\nL’analyse des profils-colonnes s’appelle l’analyse duale. On considère les profils-colonnes contenus dans la matrice \\(D_p^{-1} F^{\\top} \\in \\mathbb{R}^{p \\times n}\\). On projette les profils-colonnes dans un espace muni de la métrique du \\(\\chi^2\\) sur les lignes, définie par \\[\\left\\langle x, y \\right\\rangle = x^{\\top} D_n^{-1} y.\\]\nPour l’analyse directe, on cherche le premier axe factoriel, i.e. la direction \\(u \\in R^p\\) qui maximise la variance projetée des profils-lignes, sous contrainte que \\(u\\) soit normé. On cherche donc \\[\\max_{u} u^{\\top} D_p^{-1} F^{\\top} D_n^{-1} F D_p^{-1} u, \\quad\\text{s.c.}\\quad u^{\\top} D_p^{-1} u = 1.\\] Ce problème d’optimisation revient à chercher le premier vecteur propre de la matrice \\[S = F^{\\top} D_n^{-1} F D_p^{-1}.\\] La matrice \\(S\\) joue un rôle analogue à la matrice de covariance dans l’ACP. Le premier vecteur propre \\(u_1\\) vérifie donc la relation \\[S u_{1} = F^{\\top} D_n^{-1} F D_p^{-1} u_1 = \\lambda_{1} u_1,\\] avec \\(\\lambda_1\\) la valeur propre associée à \\(u_1\\). Les vecteurs propres de la matrice \\(S\\) donnent les axes factoriels dans l’espace des colonnes. Les coordonnées des profils-lignes sur le premier axe factoriel sont obtenues par la relation \\[\\Phi_1 = D_n^{-1} F D_p^{-1} u_1.\\] On obtient les autres couples de valeurs propres et vecteurs propres, ainsi que les coordonnées des profils-lignes sur les axes factoriels associés de manière similaire.\nL’analyse duale se fait de façon similaire. On cherche le premier vecteur propre de la matrice \\[T = F D_p^{-1} F^{\\top} D_n^{-1}.\\] Le premier vecteur propre \\(v_1\\) vérifie donc la relation \\[T v_{1} = F D_p^{-1} F^{\\top} D_n^{-1} v_1 = \\mu_1 v_1,\\] avec \\(\\mu_{1}\\) la valeur propre associée à \\(v_{1}\\). Les vecteurs propres de la matrice \\(T\\) donnent les axes factoriels dans l’espace des lignes. Les coordonnées des profils-colonnes sur le premier axe factoriel sont obtenues par la relation \\[\\Psi_1 = D_p^{-1} F^{\\top} D_n^{-1} v_1 .\\] On obtient les autres couples de valeurs propres et vecteurs propres, ainsi que les coordonnées des profils-colonnes sur les axes factoriels associés de manière similaire.\n\n\n\n\n\n\nImportantPropriété\n\n\n\nLes matrices \\(S\\) et \\(T\\) ont les mêmes \\(r = \\min(n - 1, p - 1)\\) premières valeurs propres positives. Cela garantit une représentation cohérente des lignes et des colonnes dans le mêmes espace réduit. Pour \\(k = 1, \\dots, r\\), les relations entre les vecteurs propres \\(u_k\\) et \\(v_k\\) sont \\[u_k = \\frac{1}{\\sqrt{\\lambda_k}} F^{\\top} D_n^{-1} v_k \\quad \\text{et} \\quad v_k = \\frac{1}{\\sqrt{\\lambda_k}} F D_p^{-1} u_k.\\]\n\n\n\n\n\n\n\n\nAstucePreuve\n\n\n\n\n\nEn partant de l’équation \\[T v_{1} = F D_p^{-1} F^{\\top} D_n^{-1} v_1 = \\mu_1 v_1,\\] en multipliant à gauche par \\(F^{\\top} D_n^{-1}\\), on obtient : \\[F^{\\top} D_n^{-1} F D_p^{-1} F^{\\top} D_n^{-1} v_1 = \\mu_1 F^{\\top} D_n^{-1} v_1.\\] Ainsi le vecteur \\(F^{\\top} D_n^{-1} v_1\\) est un vecteur propre de la matrice \\(F^{\\top} D_n^{-1} F D_p^{-1}\\) associée à la valeur propre \\(\\mu_1\\). Comme \\(\\lambda_1\\) est la plus grande valeur propre de \\(F^{\\top} D_n^{-1} F D_p^{-1}\\), on en déduit que \\(\\mu_1 \\leq \\lambda_{1}\\). En procédant de la même manière, en partant de \\(S u_1 = \\lambda_{1} u_{1}\\), on déduit que \\(\\lambda_{1} \\leq \\mu_{1}\\). Donc \\(\\lambda_{1} = \\mu_{1}\\). On peut ensuite faire de même pour les \\(r\\) premières valeurs propres. On en déduit aussi les relations entre les valeurs propres.\n\n\n\n\n\n\n\n\n\nAstuceRemarque\n\n\n\nEn centrant les profils, on peut projeter les profils-lignes et les profils-colonnes dans un même repère, facilitant ainsi l’interprétation géométrique conjointe.",
    "crumbs": [
      "Modules",
      "04 - Dimension",
      "Analyse factorielle des correspondances"
    ]
  },
  {
    "objectID": "contents/dimension/02-ca.html#centre-de-gravité-et-inertie",
    "href": "contents/dimension/02-ca.html#centre-de-gravité-et-inertie",
    "title": "Analyse factorielle des correspondances",
    "section": "Centre de gravité et inertie",
    "text": "Centre de gravité et inertie\nDans les sorties des logiciels de statistique, le nuage des points issus d’une AFC est généralement centré en \\((0, 0)\\). Cette convention reflète une analyse relative aux centres de gravité des profils-lignes et des profils-colonnes. Ce centrage est à la fois pratique et interprétable. En effet, il fait apparaître les distances entre les modalités par rapport à leur moyenne poindérée, i.e. par rapport au comportement moyen dans la population.\nChaque modalité (ligne ou colonne) est associée à un poids, correspondant à sa fréquence marginale : le poids de la \\(i\\)e ligne est \\(f_{i \\bullet}\\) et le poids de la \\(j\\)e colonne est \\(f_{\\bullet j}\\). Le centre de gravité des lignes est la moyenne pondérée des profils-lignes :\n\\[G_L = \\left( g_{1}, \\dots, g_{p} \\right)^{\\top}, \\quad \\text{où}\\quad g_j = \\sum_{i = 1}^{n} f_{i \\bullet} \\frac{f_{i j}}{f_{i \\bullet}} = \\sum_{i = 1}^{n} f_{i j} = f_{\\bullet j}, j \\in \\{ 1, \\dots, p \\}.\\]\nDe même, le centre de gravité des colonnes est \\[G_C = \\left( f_{1 \\bullet}, \\dots, f_{n \\bullet} \\right)^{\\top}.\\]\nPour recentrer les profils autour du centre de gravité, on soustrait leur valeur moyenne : \\[\\frac{f_{i j}}{f_{i \\bullet}} - g_{j} = \\frac{f_{i j}}{f_{i \\bullet}} - f_{\\bullet j} = \\frac{f_{i j} - f_{i \\bullet} f_{\\bullet j}}{f_{i \\bullet}}.\\] Ce centrage garantit que chaque profil-ligne \\(i \\in \\{ 1, \\dots, n \\}\\) est moyenné à zéro : \\[\\sum_{j = 1}^{p} \\frac{f_{i j} - f_{i \\bullet}f_{\\bullet j}}{f_{i \\bullet}} = 0.\\]\nL’AFC ne se fait donc plus sur la matrice \\(S\\) mais plutôt sur une matrice centrée \\(S^\\star = (s_{j j^\\prime}^\\star),\\) où \\[s_{j j^\\prime}^\\star = \\sum_{i = 1}^{n} \\frac{\\left( f_{i j} - f_{i \\bullet} f_{\\bullet j} \\right) \\left( f_{i j^\\prime} - f_{i \\bullet} f_{\\bullet j^\\prime} \\right)}{f_{i \\bullet} f_{\\bullet j^\\prime}}.\\]\nPar définition, la trace de la matrice \\(S^\\star\\) donne l’inertie totale : \\[\\text{tr}(S^\\star) = \\sum_{j = 1}^{p} \\sum_{i = 1}^{n} \\frac{\\left( f_{i j} - f_{i \\bullet}f_{\\bullet j} \\right)^2}{f_{i \\bullet} f_{\\bullet j}}.\\] Celle-ci correspond à la statistique du \\(\\chi^2\\) normalisée que l’on utilise pour tester l’indépendance entre les variables.\n\n\n\n\n\n\nImportantPropriété\n\n\n\nOn a que, pour tout \\(j, j^\\prime \\in \\{ 1, \\dots, p \\}\\), \\(s_{j j^\\prime}^\\star = s_{j j^\\prime} - f_{\\bullet j},\\) où \\[s_{j j^\\prime} = \\sum_{i = 1}^{n} \\frac{f_{i j}f_{i j^\\prime}}{f_{i \\bullet} f_{\\bullet j^\\prime}}.\\]\n\n\n\n\n\n\n\n\nAstucePreuve\n\n\n\n\n\n\n\n\n\nLa propriété précédente entraine que les matrices \\(S\\) et \\(S^\\star\\) one les mêmes vecteurs propres pour les \\(p\\) premières dimensions, ce qui permet d’effectuer l’analyse factorielle sur la version centrée.",
    "crumbs": [
      "Modules",
      "04 - Dimension",
      "Analyse factorielle des correspondances"
    ]
  },
  {
    "objectID": "contents/dimension/02-ca.html#coordonnées-factorielles",
    "href": "contents/dimension/02-ca.html#coordonnées-factorielles",
    "title": "Analyse factorielle des correspondances",
    "section": "Coordonnées factorielles",
    "text": "Coordonnées factorielles\nOn a que, pour tout \\(k = 1, \\dots, r\\), \\[\\Phi_k = D_n^{-1} F D_p^{-1} u_k \\quad \\text{et} \\quad \\Psi_k = D_p^{-1} F^{\\top} D_n^{-1} v_k.\\] Or, on a aussi vu les relations entre les vecteurs propres \\(u_k\\) et \\(v_k\\), \\[u_k = \\frac{1}{\\sqrt{\\lambda_k}} F^{\\top} D_n^{-1} v_k \\quad \\text{et} \\quad v_k = \\frac{1}{\\sqrt{\\lambda_k}} F D_p^{-1} u_k.\\] On en déduit donc les relations entre les coordonnées factorielles des profils-lignes et celles des profils-colonnes : \\[\\Phi_k = \\frac{1}{\\sqrt{\\lambda_k}} D_n^{-1} F \\Psi_k \\quad \\text{et} \\quad \\Psi_k = \\frac{1}{\\sqrt{\\lambda_k}} D_p^{-1} F^{\\top} \\Phi_k.\\]\nOn peut maintenant examiner ces relations sur chacune des composantes : \\[\\left[ \\Phi_k \\right]_i = \\frac{1}{\\sqrt{\\lambda_k}} \\sum_{j = 1}^{p} \\frac{f_{ij}}{f_{i \\bullet}} \\left[ \\Psi_k \\right]_j \\quad \\text{et} \\quad \\left[ \\Psi_k \\right]_j = \\frac{1}{\\sqrt{\\lambda_k}} \\sum_{i = 1}^{n} \\frac{f_{ij}}{f_{\\bullet j}} \\left[ \\Phi_k \\right]_i,\\] où \\(\\left[ \\Phi_k \\right]_i\\) désigne la coordonnée du profil-ligne \\(L_i\\) sur le \\(k\\)e axe factoriel et \\(\\left[ \\Psi_k \\right]_j\\) désigne la coordonnée du profil-colonne \\(C_j\\) sur le même axe factoriel. Ces relations expriment, à un facteur \\(1 / \\sqrt{\\lambda_k}\\) près, que chaque profil-ligne est au barycentre des projections des profils-colonnes affectés du poids de la colonne \\(j\\) dans la ligne \\(i\\) et que chaque profil-colonne est au barycentre des projections des profils-lignes affectés du poids de la ligne \\(i\\) dans la colonne \\(j\\).\n\n\n\n\n\n\nAstuceRemarque\n\n\n\nAinsi, en AFC, nous avons une double représentation barycentrique. Sur les axes factoriels, chaque point d’un nuage est au barycentre des points de l’autre nuage.",
    "crumbs": [
      "Modules",
      "04 - Dimension",
      "Analyse factorielle des correspondances"
    ]
  },
  {
    "objectID": "contents/unsupervised/02-hierarchy.html",
    "href": "contents/unsupervised/02-hierarchy.html",
    "title": "Méthodes hiérarchiques",
    "section": "",
    "text": "L’algorithme \\(k\\)-means présente plusieurs limitations. Par exemple, celles-ci peuvent être problématique lorsque l’on ne dispose que d’une matrice de similiarité ou de distance entre les observations et que l’on n’a pas accès aux données originales. Dans un tel contexte, les méthodes de classification hiérarchique sont pertinentes.\nLa classification hiérarchique permet d’obtenir une série de partitions imbriquées, allant de la partition la plus fine (chaque observation dans son propre groupe) à la plus grossière (toutes les observations dans un seul groupe). Cette approche ne fournit donc pas une seule parition, mais une hiérarchie de partitions. Cette hiérarchie peut être représenté à l’aide d’un dendogramme, un arbre résumant comment ces partitions sont imbriquées. Il existe deux types d’algorithmes pour effectuer une classification hiérarchique:\nDans les deux cas, on obtient \\(n\\) partitions hiérarchiques constituées de \\(1\\) à \\(n\\) groupes.",
    "crumbs": [
      "Modules",
      "06 - Non-supervisée",
      "Méthodes hiérarchiques"
    ]
  },
  {
    "objectID": "contents/unsupervised/02-hierarchy.html#algorithmes",
    "href": "contents/unsupervised/02-hierarchy.html#algorithmes",
    "title": "Méthodes hiérarchiques",
    "section": "Algorithmes",
    "text": "Algorithmes\n\nAlgorithmes descendants\nLes algorithmes descendants commencent avec l’ensemble des \\(n\\) observations réunis dans un seul groupe. À chaque étape, le groupe jugé le moins homogéne est divisé en deux sous-groupes, en cherchant à maximiser la dissimilarité entre les deux sous-groupes. On continue ainsi jusqu’à ce que chaque observation soit isolé dans son propre groupe.\nCe type d’algorithme est coûteux en temps de calcul, car il faut évaluer, à chaque étape, toutes les manières possibles de diviser un groupe en deux. On l’utilise donc rarement en pratique.\n\n\nAlgorithme ascendant\nÀ l’inverse, les algorithmes ascendants débutent avec \\(n\\) groupes distincts, chacun contenant une seule observation. À chaque étape, on fusionne les deux groupes les plus similaires, i.e. ceux dont la dissimilarité est la plus faible selon un critère choisi. L’algorithme continue jusqu’à ce qu’il ne reste plus qu’un seul groupe contenant toutes les observations.",
    "crumbs": [
      "Modules",
      "06 - Non-supervisée",
      "Méthodes hiérarchiques"
    ]
  },
  {
    "objectID": "contents/unsupervised/02-hierarchy.html#distance-entre-groupes",
    "href": "contents/unsupervised/02-hierarchy.html#distance-entre-groupes",
    "title": "Méthodes hiérarchiques",
    "section": "Distance entre groupes",
    "text": "Distance entre groupes\nPour mettre en oeuvre les algorithmes précédent, on doit définir la distance entre deux groupes d’observations \\(A\\) et \\(B\\), notée \\(d(A, B)\\). Si l’on sait généralement mesurer distance entre deux individus, on doit définir une distance entre deux groupes contenant un nombre différents d’éléments. Il existe plusieurs façon de calculer une telle distance entre deux groupes.\n\nMéthode du plus proche voisin (single linkage)\nDans cette approche, la distance entre deux groupes est définie comme la plus petite distance entre un individu de \\(A\\) et un individu de \\(B\\) :\n\\[d(A, B) = \\min \\{ d_{i j}: i \\in A, j \\in B \\}.\\]\nDit autrement, deux groupes \\(A\\) et \\(B\\) sont considérés comme proche si un élément de \\(A\\) est proche d’un élément de \\(B\\). Cette méthode présente plusieurs avantages. Elle donne de bons résultats lorsque les variables sont de nature différente (e.g., quantitatives et qualitatives) et permet de construire des groupes aux formes irrégulière. De plus, elle est relativement robuste aux données aberrantes. Enfin, ses propriétés mathématiques théoriques sont intéressantes.\nCependant, cette méthode tend à créer des groupes déséquilibrés : un grand groupe central entouré de plusieurs petits groupes satellites. Elle est moins performante lorsque les groupes naturels sont de forme régulière. Bien qu’elle est de bonnes propriétés mathématiques, celles-ci ne se vérifie pas toujours empiriquement.\n\n\nMéthode du voisin le plus distant (complete linkage)\nÀ l’inverse de la méthode précédente, la distance entre deux groupes est définie comme la plus grande distance entre un individu de \\(A\\) et un individu de \\(B\\) :\n\\[d(A, B) = \\max \\{ d_{i j}: i \\in A, j \\in B \\}.\\]\nDit autrement, deux groupes sont considérés proches si tous les éléments de \\(A\\) sont proches de tous les éléments de \\(B\\). Cette méthode a tendence à produire des groupes réguliers de taille homogène. Comme la méthode du plus proche voisin, elle est bien adaptée aux variables de différents types. Cependant, elle est extrêmement sensible aux données aberrantes. En effet, un seul individu peut augmenter artificiellement la distance entre deux groupes. De plus, elle a tendance à forcer la formation de groupes de même taille, ce qui n’est pas toujours justifié en pratique.\n\n\nMéthode de la moyenne (average linkage)\nIci, la distance entre deux groupes est définie comme la moyenne des distances entre toutes les paires d’individus, issu de \\(A\\) et de \\(B\\) :\n\\[d(A, B) = \\frac{1}{n_{A}n_{B}} \\sum_{i \\in A} \\sum_{j \\in B} d(X_{i}, X_{j}).\\] où \\(n_{A}\\) est le nombre d’observations dans le groupe \\(A\\) et \\(n_{B}\\) est le nombre d’observations dans le groupe \\(B\\).\nCette méthode consiste à considérer toutes les interactions possibles entre les éléments des deux groupes, puis à en faire la moyenne. Elle tend à produire des groupes dont la variance interne est faible, i.e. relativement homogène. Toutefois, cette méthode privilégie la formation de groupes de variance similaire, ce qui n’est pas toujours justifié en pratique.\n\n\nMéthode du centroïde (centroid method)\nPour cette méthode, la distance entre deux groupes est définie comme la distance entre leurs centroïdes, i.e. les moyennes des observations de chaque groupe :\n\\[d(A, B) = d(\\overline{X}_{A}, \\overline{X}_{B}).\\] où \\[\\overline{X}_{A} = \\frac{1}{n_{A}} \\sum_{i \\in A} X_i, \\quad\\text{et}\\quad \\overline{X}_B = \\frac{1}{n_{B}} \\sum_{j \\in B} X_j\\]\nAprès la fusion de \\(A\\) et de \\(B\\), le nouveau centroïde \\(\\overline{X}_{AB}\\) est donné par la moyenne pondérée : \\[\\overline{x}_{AB} = \\frac{n_{A} \\overline{x}_{A} + n_{B} \\overline{x}_{B}}{n_{A} + n_{B}}.\\]\nCette approche est assez robuste aux données aberrantes, mais elle est généralement peu performante lorsqu’il n’y en a pas.\n\n\nMéthode de la médiane (median method)\nLa méthode de la médiane repose sur une mise à jour des distances de façon récursive. Lorsqu’on fusionne deux groupes \\(A\\) et \\(B\\), on définit la distance entre le nouveau groupe \\(AB\\) et autre groupe \\(C\\) par la formule : \\[d(AB, C) = \\frac{d(A, C) + d(B, C)}{2} - \\frac{d(A, B)}{4}.\\]\nCette méthode est particulièrement robuste aux données aberrantes (davantage que la méthode du centroïde). Elle est cependant très peu efficace lorsque de telles valeurs extrêmes sont absentes.\n\n\nMéthode de Ward (Ward’s method)\nLa méthode de Ward est une variante de la méthode du centroïde. Elle est optimale dans le cas où les observations suivent des lois normales multivariées, de même matrice de variance-covariance mais de moyennes différentes. Elle est basée sur une mesure de l’inertie intra-groupe. Pour chaque groupe \\(A\\), groupe \\(B\\) et groupe \\(A \\cup B\\), noté \\(AB\\), on définit\n\\[SC_A = \\sum_{i \\in A} (X_i - \\overline{X}_A)^\\top (X_i - \\overline{X}_A),\\]\n\\[SC_B = \\sum_{j \\in B} (X_j - \\overline{X}_B)^\\top (X_j - \\overline{X}_B),\\]\n\\[SC_AB = \\sum_{k \\in A \\cup B} (X_k - \\overline{X}_{AB})^\\top (X_k - \\overline{X}_{AB}).\\]\noù \\(\\overline{X}_A\\), \\(\\overline{X}_B\\) et \\(\\overline{X}_{AB}\\) sont calculées comme dans la méthode du centroïde. On regroupe les groupes \\(A\\) et \\(B\\) qui minimisent l’augmentation de l’inertie : \\[I_{AB} = SC_{AB} - SC_A - SC_B = \\frac{d^2(\\overline{X}_A, \\overline{X}_B)}{\\frac{1}{n_{A}} + \\frac{1}{n_{B}}}.\\]\nCette méthode est très efficace lorsque les groupes sont homogènes, de taille comparable et que les hypothèses gaussiennes sont raisonnablement satisfaites. En revanche, elle est sensible aux données aberrantes et tend à former des regroupements de même taille.\n\n\nMéthode flexible (flexible clustering)\nLa méthode flexible repose sur une formule générale permettant de représenter plusieurs méthodes de mise à jour des distances. Si l’on fusionne deux groupes \\(A\\) et \\(B\\) pour former \\(AB\\), et que l’on souhaite calculer la distance entre \\(AB\\) et autre groupe \\(C\\), on peut utiliser la relation \\[d(C, AB) = \\alpha_A d(C, A) + \\alpha_B d(C, B) + \\beta d(A, B) + \\gamma \\left| d(C, A) - d(C, B) \\right|.\\]\nSelon les valeurs choisies pour les coefficients \\(\\alpha_A\\), \\(\\alpha_B\\), \\(\\beta\\) et \\(\\gamma\\), on peut retrouver les formules de mise à jour correspondant aux différentes méthodes précédentes. Le Table 1 présente les valeurs des différents coefficients à choisir pour retrouver les différentes méthodes.\n\n\n\nTable 1: Coefficients pour retrouver les méthodes précédentes.\n\n\n\n\n\n\n\n\n\n\n\n\nMéthode\n\\(\\alpha_A\\)\n\\(\\alpha_B\\)\n\\(\\beta\\)\n\\(\\gamma\\)\n\n\n\n\nPlus proche\n\\(1/2\\)\n\\(1/2\\)\n\\(0\\)\n\\(-1/2\\)\n\n\nPlus distant\n\\(1/2\\)\n\\(1/2\\)\n\\(0\\)\n\\(1/2\\)\n\n\nMédiane\n\\(1/2\\)\n\\(1/2\\)\n\\(-1/4\\)\n\\(0\\)\n\n\nMoyenne\n\\(\\frac{n_A}{n_A+n_B}\\)\n\\(\\frac{n_B}{n_A+n_B}\\)\n\\(0\\)\n\\(0\\)\n\n\nCentroïde\n\\(\\frac{n_A}{n_A+n_B}\\)\n\\(\\frac{n_B}{n_A+n_B}\\)\n\\(-\\frac{n_An_B}{n_A + n_B}\\)\n\\(0\\)\n\n\nWard\n\\(\\frac{n_A+n_C}{n_A+n_B+n_C}\\)\n\\(\\frac{n_B+n_C}{n_A+n_B+n_C}\\)\n\\(-\\frac{n_C}{n_A+n_B+n_C}\\)\n\\(0\\)\n\n\n\n\n\n\nPour la méthode flexible, on impose arbitrairement les contraintes suivantes: \\[\\alpha_A + \\alpha_B + \\beta = 1, \\quad \\alpha_A = \\alpha_B, \\quad \\gamma = 0.\\]\nAinsi, on a \\(\\alpha_A = \\alpha_B = \\frac{1 - \\beta}{2}\\). Il ne reste qu’un seul paramètre à fixer. Généralement, on choisit \\(\\beta = -0.25\\). Si l’on soupçonne la présence de données aberrantes, on peut opter pour \\(\\beta = -0.5\\) afin d’accroître la robustesse de l’algortihme.",
    "crumbs": [
      "Modules",
      "06 - Non-supervisée",
      "Méthodes hiérarchiques"
    ]
  },
  {
    "objectID": "contents/unsupervised/02-hierarchy.html#quelle-partition-choisir",
    "href": "contents/unsupervised/02-hierarchy.html#quelle-partition-choisir",
    "title": "Méthodes hiérarchiques",
    "section": "Quelle partition choisir ?",
    "text": "Quelle partition choisir ?\nUn algorithme de classification hierarchique, ascendant ou descendant, produit une séquence de \\(n\\) partitions imbriquées, allant de \\(n\\) groupes (où chaque observation est isolée) à un seul groupe contenant toutes les observations. En pratique, on se pose donc la question suivante : quelle partition faut-il considérer ?\nPlusieurs pistes peuvent guider ce choix :\n\nUne des partitions est-elle particulièrement interprétable d’un point de vue scientifique ou métier ?\nUne des partitions est-elle pertinente sur le plan opérationnel, par exemple pour orienter des décisions ?\nCherche-t-on explicitement à segmenter la population en un nombre de groupes \\(K\\), déterminé à l’avance ?\n\nLorsque ces considérations pratiques ne suffisent pas à trancher, on peut s’appuyer sur des critères statistiques pour évaluer la qualité des partitions et suggérer un nombre “optimal” de groupes. Ces critères sont surtout adaptés lorsque les variables sont continues.\nLa librairie R NbClust en propose une trentaine et la librairie Python sklearn une dizaine. On présente ensuite les critères les plus fréquemment utilisés.\n\nCritères basés sur l’inertie\nL’inertie mesure la dispersion des observations appartenant à un groupe par rapport à son centroïde. L’inertie totale \\(I_{\\text{tot}}\\) de l’ensemble des \\(n\\) observations est \\[I_{\\text{tot}} = \\frac{1}{n}\\sum_{i = 1}^{n} d(X_i, G),\\] où la fonction \\(d\\) est une distance et \\(G\\) le centre de gravité de l’ensemble des observations. L’inertie totale \\(I_{tot}\\) se décompose en une inertie intra-groupe et une inertie inter-groupe de la façon suivante : \\[I_{\\text{tot}} = I_{\\text{intra-groupe}} + I_{\\text{inter-groupe}}.\\] L’inertie inter-groupe \\(I_{\\text{inter-groupe}}\\) est l’inertie des centres de gravité des groupes pondérés par le nombre d’observations dans le groupe. Elle mesure la séparation entre les groupes. Elle est donné par \\[I_{\\text{inter-groupe}} = \\frac{1}{n}\\sum_{k = 1}^{K} n_k d(G_k, G),\\] où \\(G_k\\) est le centre de gravité du groupe \\(k\\) et \\(n_k\\) le nombre d’observations dans ce groupe. L’inertie intra-groupe \\(I_{\\text{intra-groupe}}\\) est la somme des inerties des groupes. Elle mesure donc l’hétérogénéité des groupes. Elle est donnée par \\[I_{\\text{intra-groupe}} = \\frac{1}{n}\\sum_{k = 1}^{K} \\sum_{i \\in C_k} d(X_i, G_k),\\] où l’ensemble \\(C_k\\) contient les observations du groupe \\(k\\).\nPlus les groupes sont compacts, plus l’inertie intra-groupe est faible, ce qui signifie que l’inertie inter-groupe est élevé. Une partition de bonne qualité maximise donc l’inertie inter-groupe. Ces critères supposent généralement que les variables sont continues et ont été standardisées pour éviter qu’une variable à grande échelle ne domine les autres. Parmi les critères se basant sur l’inertie, on peut citer : le pseudo-\\(R^2\\) et la statistique de Calinski-Harabasz (CH).\nLe pseudo-\\(R^2\\) mesure la proportion d’inertie expliquée par la partition :\n\\[\\text{pseudo-}R^2 =\\frac{I_{\\text{inter-groupe}}}{I_{\\text{tot}}}.\\]\nUn pseudo-\\(R^2\\) élevé indique que la parition capture une grande part de la structure des données. La statistique CH est une variation du pseudo-\\(R^2\\) normalisé par le nombre de groupes : \\[\\text{CH} = \\frac{I_{\\text{inter-groupe}} / (K - 1)}{I_{\\text{intra-groupe}} / (n - K)}.\\] Un score élevé de la statistique CH suggère un bon équilibre entre groupes compactes et groupes séparés.\n\n\nCritères basés sur la distance\nOn peut aussi utiliser des critères se basant sur la distance entre les observations pour mesurer la qualité d’une partition. Parmi les critères se basant sur la distance, on peut citer : l’indice de Dunn et l’indice de silhouette.\nL’indice de Dunn cherche à maximiser la distance minimale entre deux groupes, tout en minimisant la distance maximale entre les observations à l’intérieur d’un groupe : \\[D= \\frac{\\text{Distance minimale entre 2 groupes}}{\\text{Distance maximale dans un groupe}}.\\] Ce critère favorise des groupes denses et bien séparés. En ce qui concerne le critère de silhouette, il mesure la qualité d’affection d’une observation \\(X_i\\) à son groupe : \\[S(X_i) = \\frac{b_i - a_i}{max(b_i, a_i)}\\] où \\(a_i\\) est la distance moyenne entre l’observation \\(X_i\\) et les autres observations de son groupe et \\(b_i\\) est la distance moyenne entre l’observation \\(X_i\\) et les observations du groupe le plus proche de \\(X_i\\). On souhaite maximiser la silhouette moyenne des observations. La silhouette moyenne sur toutes les observations est donc un bon indicateur de la cohérence globale de la partition.\nEn résumé, plusieurs critères peuvent nous guider dans le choix du nombre de groupes, mais aucun n’est parfait. Il est souvent recommendé de croiser connaissance métier, visualisation des partitions et indicateurs statistiques pour prendre une décision éclairée.",
    "crumbs": [
      "Modules",
      "06 - Non-supervisée",
      "Méthodes hiérarchiques"
    ]
  }
]