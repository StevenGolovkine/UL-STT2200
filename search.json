[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STT-2200: Analyse de données",
    "section": "",
    "text": "Bienvenue à tous au cours STT-2200 de l’automne 2025.\nVous trouverez sur ce site toute les informations nécessaire pour les cours, les différents travaux et les examens. Les contenus seront mis à jour au cours de la session avec les corrigés de certains travaux. Je vous invite à regarder les différents modules avant les cours.\nCe site a été créé en s’aidant des notes de cours de Aurélien Nicosia et de Thierry Duchesne.\nCe site a été créé avec Quarto.\n\n\n\nData trap (xkcd:2582)."
  },
  {
    "objectID": "td/02-revision-td.html",
    "href": "td/02-revision-td.html",
    "title": "TD: Révision",
    "section": "",
    "text": "Dans ce problème, on cherche à modéliser la répartition des vélos en libre service dans les différentes stations de l’université Laval. Le campus comporte cinq stations pouvant acceuillir un total de \\(136\\) vélos (lien): Abitibi-Prince (ABP), Alphone-Desjardins (ADJ), Charles-De Koninck (DKN), Ferdinand-Vandry (VND) et PEPS.\nSupposons que tous les vélos sont retournés à une des cinq stations à la fin de la journée, i.e. pour chaque jour, il y a un moment dans la journée (par exemple, à minuit), où tous les vélos sont à une certaine station. Nous pouvons donc nous intéresser à ces stations à ce moment de la journée pour chaque jour. Nous cherchons à modéliser les movements des vélos de minuit pour un jour donné jusqu’à minuit du jour suivant. Nous trouvons que :\n\npour les vélos empruntés à ABP, \\(50\\%\\) y retourne, \\(10\\%\\) vont à ADJ, \\(20\\%\\) vont à DKN et \\(20\\%\\) vont à VND.\npour les vélos empruntés à ADJ, \\(30\\%\\) y retourne, \\(10\\%\\) vont à ABP, \\(10\\%\\) vont à DKN, \\(10\\%\\) vont à VND et \\(40\\%\\) vont au PEPS.\npour les vélos empruntés à DKN, \\(80\\%\\) y retourne, \\(5\\%\\) vont à ADJ, et \\(15\\%\\) vont au PEPS.\npour les vélos empruntés à VND, \\(50\\%\\) y retourne, \\(20\\%\\) vont à ABP, \\(5\\%\\) vont à ADJ, \\(15\\%\\) vont à DKN et \\(10\\%\\) vont au PEPS.\npour les vélos empruntés au PEPS, \\(0\\%\\) y retourne, \\(25\\%\\) vont à ABP, \\(25\\%\\) vont à ADJ, \\(25\\%\\) vont à DKN et \\(25\\%\\) vont à VND.\n\n\nFaire un graphe de la situation.\nÉcrire la matrice \\(T\\) tel que chaque entrée \\(t_{ij}\\) corresponde à la probabilité de passer de la station \\(i\\) à la station \\(j\\), \\(i, j \\in \\{ \\text{ABP}, \\text{ADJ}, \\text{DKN}, \\text{VND}, \\text{PEPS} \\}\\). On appelle cette matrice, la matrice de transition.\nQuelle est la probabilité qu’un vélo soit à la station PEPS au jour 2 sachant qu’il était à la station ADJ au début ?\nSupposons qu’il y ait \\(20\\) vélos à ABP, \\(35\\) vélos à ADJ, \\(26\\) vélos à DKN, \\(45\\) à VND et \\(10\\) au PEPS. En utilisant une diagonalisation de la matrice \\(T\\), donner la répartition des vélos après \\(10\\) jours."
  },
  {
    "objectID": "td/02-revision-td.html#exercice-1-un-problème-de-vélo",
    "href": "td/02-revision-td.html#exercice-1-un-problème-de-vélo",
    "title": "TD: Révision",
    "section": "",
    "text": "Dans ce problème, on cherche à modéliser la répartition des vélos en libre service dans les différentes stations de l’université Laval. Le campus comporte cinq stations pouvant acceuillir un total de \\(136\\) vélos (lien): Abitibi-Prince (ABP), Alphone-Desjardins (ADJ), Charles-De Koninck (DKN), Ferdinand-Vandry (VND) et PEPS.\nSupposons que tous les vélos sont retournés à une des cinq stations à la fin de la journée, i.e. pour chaque jour, il y a un moment dans la journée (par exemple, à minuit), où tous les vélos sont à une certaine station. Nous pouvons donc nous intéresser à ces stations à ce moment de la journée pour chaque jour. Nous cherchons à modéliser les movements des vélos de minuit pour un jour donné jusqu’à minuit du jour suivant. Nous trouvons que :\n\npour les vélos empruntés à ABP, \\(50\\%\\) y retourne, \\(10\\%\\) vont à ADJ, \\(20\\%\\) vont à DKN et \\(20\\%\\) vont à VND.\npour les vélos empruntés à ADJ, \\(30\\%\\) y retourne, \\(10\\%\\) vont à ABP, \\(10\\%\\) vont à DKN, \\(10\\%\\) vont à VND et \\(40\\%\\) vont au PEPS.\npour les vélos empruntés à DKN, \\(80\\%\\) y retourne, \\(5\\%\\) vont à ADJ, et \\(15\\%\\) vont au PEPS.\npour les vélos empruntés à VND, \\(50\\%\\) y retourne, \\(20\\%\\) vont à ABP, \\(5\\%\\) vont à ADJ, \\(15\\%\\) vont à DKN et \\(10\\%\\) vont au PEPS.\npour les vélos empruntés au PEPS, \\(0\\%\\) y retourne, \\(25\\%\\) vont à ABP, \\(25\\%\\) vont à ADJ, \\(25\\%\\) vont à DKN et \\(25\\%\\) vont à VND.\n\n\nFaire un graphe de la situation.\nÉcrire la matrice \\(T\\) tel que chaque entrée \\(t_{ij}\\) corresponde à la probabilité de passer de la station \\(i\\) à la station \\(j\\), \\(i, j \\in \\{ \\text{ABP}, \\text{ADJ}, \\text{DKN}, \\text{VND}, \\text{PEPS} \\}\\). On appelle cette matrice, la matrice de transition.\nQuelle est la probabilité qu’un vélo soit à la station PEPS au jour 2 sachant qu’il était à la station ADJ au début ?\nSupposons qu’il y ait \\(20\\) vélos à ABP, \\(35\\) vélos à ADJ, \\(26\\) vélos à DKN, \\(45\\) à VND et \\(10\\) au PEPS. En utilisant une diagonalisation de la matrice \\(T\\), donner la répartition des vélos après \\(10\\) jours."
  },
  {
    "objectID": "td/02-revision-td.html#exercice-2-paradoxe-des-deux-enfants",
    "href": "td/02-revision-td.html#exercice-2-paradoxe-des-deux-enfants",
    "title": "TD: Révision",
    "section": "Exercice 2: Paradoxe des deux enfants",
    "text": "Exercice 2: Paradoxe des deux enfants\nOn cherche à modéliser les probabilités d’avoir des enfants d’un certain sexe. Dans toutes les questions, les enfants sont soit de sexe masculin, soit de sexe féminin, de façon équiprobable.\n\nM. Gagnon a deux enfants. L’enfant aîné est une fille. Quelle est la probabilité que son deuxième enfant soit aussi une fille ?\nMme Tremblay a deux enfants. On lui pose la question suivante : “Avez-vous au moins un garçon ?” et elle répond : “Oui”. Quelle est la probabilité que les deux enfants soient des garçons ?\nEst-ce que la probabilité de Mme Tremblay d’avoir deux garçons change si on lui pose la question suivante : “Indiquez moi le sexe de l’un de vos enfants.” et qu’elle répond : “J’ai (au moins) un garçon.” ?\nOn croise M. Gagnon dans la rue en train de se balader avec sa fille. On lui demande quel jour de la semaine elle est née. Elle nous répond: “Vendredi”. Quelle est la probabilité que son deuxième enfant soit aussi une fille ?"
  },
  {
    "objectID": "td/02-revision-td.html#exercice-3-la-loi-dohm",
    "href": "td/02-revision-td.html#exercice-3-la-loi-dohm",
    "title": "TD: Révision",
    "section": "Exercice 3: La loi d’Ohm",
    "text": "Exercice 3: La loi d’Ohm\nOn cherche à calculer la valeur d’un résistance. Pour cela, on lui envoie un courant électrique (intensité en ampères, A) et on mesure la différence de potentiel entre les bornes de la résistance (tension en volt, V). On trouve les valeurs suivantes :\n\n\n\nIntensité (A)\nVoltage (V)\n\n\n\n\n0.2\n4.0\n\n\n0.5\n10.4\n\n\n0.9\n18.7\n\n\n1.0\n21.1\n\n\n1.2\n25.1\n\n\n1.3\n27.4\n\n\n1.8\n37.8\n\n\n\n\nCalculer l’intensité moyenne et le voltage moyen.\nCalculer les variances de l’intensité et du voltage.\nCalculer la covariance entre l’intensité et le voltage.\nLa loi d’Ohm nous dit que l’intensité et le voltage sont proportionnelle suivant la relation \\(V = R A\\) où \\(R\\) est la valeur de la résistance en Ohm. Cette relation est équivalente à estimer un modèle linéaire entre l’intensité et le voltage. La résistance \\(R\\) est donc donnée par le ratio entre la covariance entre l’intensité et le voltage et la variance de l’intensité (cf. cours de régression pour une preuve). Calculer la valeur de \\(R\\).\nEn reprenant les valeurs du tableau, vérifier la valeur de \\(R\\). Pourquoi est-ce que l’on ne retrouve pas exactement les valeurs données ?"
  },
  {
    "objectID": "tp/03-generalities-tp.html",
    "href": "tp/03-generalities-tp.html",
    "title": "TP: Généralités",
    "section": "",
    "text": "Vous pouvez faire les exercices dans le langage de votre choix."
  },
  {
    "objectID": "informations/evaluations.html",
    "href": "informations/evaluations.html",
    "title": "Évaluations",
    "section": "",
    "text": "Voici les différents examens prévus pour ce module.\n\n\n\nTitre\nDate\nMode de travail\nPondération\n\n\n\n\nExamen 1\n\nIndividuel\n25%\n\n\nExamen 2\n\nIndividuel\n50%\n\n\nProjet\n\nEn équipe\n25%\n\n\n\n\nIdentification. Lors d’un examen, une carte d’identité avec photo admissible doit être déposée sur le coin de votre table. Les cartes admissibles sont la carte de l’Université Laval en plastique, un permis de conduire canadien, une carte d’assurance-maladie avec photo émise par une province canadienne ou un passeport canadien ou étranger.",
    "crumbs": [
      "Informations",
      "Évaluations"
    ]
  },
  {
    "objectID": "informations/evaluations.html#modalités-dévaluations",
    "href": "informations/evaluations.html#modalités-dévaluations",
    "title": "Évaluations",
    "section": "",
    "text": "Voici les différents examens prévus pour ce module.\n\n\n\nTitre\nDate\nMode de travail\nPondération\n\n\n\n\nExamen 1\n\nIndividuel\n25%\n\n\nExamen 2\n\nIndividuel\n50%\n\n\nProjet\n\nEn équipe\n25%\n\n\n\n\nIdentification. Lors d’un examen, une carte d’identité avec photo admissible doit être déposée sur le coin de votre table. Les cartes admissibles sont la carte de l’Université Laval en plastique, un permis de conduire canadien, une carte d’assurance-maladie avec photo émise par une province canadienne ou un passeport canadien ou étranger.",
    "crumbs": [
      "Informations",
      "Évaluations"
    ]
  },
  {
    "objectID": "informations/evaluations.html#informations-détaillées-sur-les-évaluations",
    "href": "informations/evaluations.html#informations-détaillées-sur-les-évaluations",
    "title": "Évaluations",
    "section": "Informations détaillées sur les évaluations",
    "text": "Informations détaillées sur les évaluations\nExamen 1\nDate et lieu:\nMode de travail: Individuel\nPondération: 25%\nRemise de l’évaluation:\nDirectives de l’évaluation: Tout le contenu du cours est susceptible d’être a l’examen.\nMatériel autorisé: pas de restriction.\nInformation supplementaire: lien.\nExamen 2\nDate et lieu:\nMode de travail: Individuel\nPondération: 50%\nRemise de l’évaluation:\nDirectives de l’évaluation: Tout le contenu du cours est susceptible d’être à l’examen.\nMatériel autorisé: pas de restriction.\nProjet\nDate et lieu:\nMode de travail: En équipe\nPondération: 25%\nRemise de l’évaluation:\nDirectives de l’évaluation:\nMatériel autorisé: pas de restriction.",
    "crumbs": [
      "Informations",
      "Évaluations"
    ]
  },
  {
    "objectID": "informations/evaluations.html#échelle-des-cotes",
    "href": "informations/evaluations.html#échelle-des-cotes",
    "title": "Évaluations",
    "section": "Échelle des cotes",
    "text": "Échelle des cotes\n\n\n\nCote\n% minimum\n% maximum\n\n\n\n\nA+\n92\n100\n\n\nA\n88\n91.99\n\n\nA-\n84\n87.99\n\n\nB+\n80\n83.99\n\n\nB\n75\n79.99\n\n\nB-\n70\n74.99\n\n\nC+\n65\n69.99\n\n\nC\n60\n64.99\n\n\nC-\n55\n59.99\n\n\nD+\n52\n54.99\n\n\nD\n50\n51.99\n\n\nE\n0\n49.99",
    "crumbs": [
      "Informations",
      "Évaluations"
    ]
  },
  {
    "objectID": "informations/evaluations.html#détails-sur-les-modalités-dévaluation",
    "href": "informations/evaluations.html#détails-sur-les-modalités-dévaluation",
    "title": "Évaluations",
    "section": "Détails sur les modalités d’évaluation",
    "text": "Détails sur les modalités d’évaluation\nConformément à la politique du Département de mathématiques et de statistique en matière d’amélioration et de consolidation de la connaissance du français, la qualité de l’écrit sera sanctionnée dans tous les travaux et examens. Un maximum de 10% des points pourra être enlevé pour la qualité de la langue et de la rédaction.\nAucun retard n’est accepté pour la remise des travaux.\nToute reprise d’évaluation accordée par la politique de reprise d’évaluation du Département aura lieu lors des dates de reprises officielles de la faculté. Pour toute demande de révision de note, vous devez suivre la procédure du chapitre 4 du Réglement des études. Vous trouverez un formulaire à remplir à l’adresse suivante: lien.",
    "crumbs": [
      "Informations",
      "Évaluations"
    ]
  },
  {
    "objectID": "informations/evaluations.html#absence-à-une-activité-obligatoire",
    "href": "informations/evaluations.html#absence-à-une-activité-obligatoire",
    "title": "Évaluations",
    "section": "Absence à une activité obligatoire",
    "text": "Absence à une activité obligatoire\nCes modalités s’appliquent en vertu des articles 4.41 et 4.42 du Réglement des études de l’Université Laval:\n\nArticle 4.41: Tout défaut de se soumettre à une activité d’évaluation entraîne la note de zéro pour cette activité d’évaluation, à moins que l’étudiante ou l’étudiant ne démontre que cette omission est attribuable à des motifs sérieux.\nArticle 4.42: La reprise d’une évaluation est possible pour des motifs sérieux. Elle se fait selon les modalités prévues par l’unité responsable de l’activité de formation.\n\nLa reprise d’une évaluation peut donc execptionnellement être autorisée pour des motifs jugés sérieux, dans la mesure où la procédure décrite ci-dessous est respectée.\nMotifs d’absence jugés sérieux\nLes motifs suivants sont jugés sérieux et donc acceptables pour demander une reprise d’évaluation:\n\nmaladie ou accident empêchant de se déplacer;\nhospitalisation;\nmaladie grave ou décès d’un proche;\nparticipation à une activité sportive de haut niveau;\nconvocation en cour de justice.\n\nProcédure à suivre\nDès que possible et au plus tard cinq (5) jours ouvrables après la date de l’évaluation (ou dans certains cas, avant la date de l’évaluation, dès que le motif sera connu), l’étudiante ou l’étudiant qui veut faire une demande de reprise d’évaluation doit remplir et soumettre le formulaire électronique “Demande de reprise d’une évaluation” en prenant soin d’y joindre les pièces justificatives requises.\nPour avoir plus de détail sur les procédures à suivre et les motifs sérieux pouvant donner droit à une reprise d’évaluation, consulter le document “Modalités et procédure de reprise d’une évaluation sommative à la Faculté des sciences et de génie” disponible sur le site web de la FSG.\nDans certains cas, la ou le responsable du cours pourrait adopter une procédure simplifiée de gestion des demandes de reprises d’évaluation, tout en respectant les critères décrits dans cette politique. Dans ces cas, des explications particulières seront données à cet effet dans le plan de cours et présentées lors de la première séance.",
    "crumbs": [
      "Informations",
      "Évaluations"
    ]
  },
  {
    "objectID": "informations/evaluations.html#politique-sur-lutilisation-dappareils-électroniques",
    "href": "informations/evaluations.html#politique-sur-lutilisation-dappareils-électroniques",
    "title": "Évaluations",
    "section": "Politique sur l’utilisation d’appareils électroniques",
    "text": "Politique sur l’utilisation d’appareils électroniques\nLa politique sur l’utilisation d’appareils électroniques de la Faculté des Sciences et de Génie peut être consultée à l’adresse: lien.",
    "crumbs": [
      "Informations",
      "Évaluations"
    ]
  },
  {
    "objectID": "informations/evaluations.html#politique-sur-le-plagiat-et-la-fraude-académique",
    "href": "informations/evaluations.html#politique-sur-le-plagiat-et-la-fraude-académique",
    "title": "Évaluations",
    "section": "Politique sur le plagiat et la fraude académique",
    "text": "Politique sur le plagiat et la fraude académique\nRègles disciplinaires\nTout étudiant qui commet une infraction au Règlement disciplinaire à l’intention des étudiants de l’Université Laval dans le cadre du présent cours, notamment en matière de plagiat, est passible des sanctions qui sont prévues dans ce règlement. Il est très important pour tout étudiant de prendre connaissance des articles 23 à 46 du Règlement disciplinaire. Celui-ci peut être consulté à l’adresse suivante: lien.\nPlagiat\nTout étudiant est tenu de respecter les règles relatives au plagiat. Constitue notamment du plagiat le fait de:\n\ncopier textuellement un ou plusieurs passages provenant d’un ouvrage sous format papier ou électronique sans mettre ces passages entre guillemets et sans en mentionner la source;\nrésumer l’idée originale d’un auteur en l’exprimant dans ses propres mots (paraphraser) sans en mentionner la source;\ntraduire partiellement ou totalement un texte sans en mentionner la provenance;\nremettre un travail copié d’un autre étudiant (avec ou sans l’accord de cet autre étudiant);\nremettre un travail téléchargé d’un site d’achat ou d’échange de travaux scolaires.\n\nL’Université Laval étant abonnée à un service de détection de plagiat, il est possible que l’enseignant soumette vos travaux pour analyse.",
    "crumbs": [
      "Informations",
      "Évaluations"
    ]
  },
  {
    "objectID": "informations/evaluations.html#étudiants-ayant-une-situation-de-handicap-liée-à-une-limitation-fonctionnelle",
    "href": "informations/evaluations.html#étudiants-ayant-une-situation-de-handicap-liée-à-une-limitation-fonctionnelle",
    "title": "Évaluations",
    "section": "Étudiants ayant une situation de handicap liée à une limitation fonctionnelle",
    "text": "Étudiants ayant une situation de handicap liée à une limitation fonctionnelle\nAfin de bénéficier de mesures d’accommodement pour les cours ou les examens, un rendez-vous avec une conseillère ou un conseiller du Centre d’aide aux étudiants travaillant en Accueil et soutien aux étudiants en situation de handicap (ACSESH) est nécessaire. Pour ce faire, les étudiants présentant une situation de handicap liée à une limitation fonctionnelle permanente doivent visiter le site “Accommodement” et prendre un rendez-vous, le plus tôt possible.\nAu cours de la semaine qui suit l’autorisation des mesures, leur activation doit être effectuée dans “Accommodement” pour assurer leur mise en place.\nLes étudiants ayant déjà obtenu des mesures d’accommodements scolaires doivent procéder à l’activation de leurs mesures pour les cours et/ou les examens dans “Accommodement” afin que celles-ci puissent être mises en place. Il est à noter que l’activation doit s’effectuer au cours de deux premières semaines de cours.\nLes étudiants concernés recevront par la suite des facultés et départements responsables de leurs cours les informations détaillées sur les modalités permettant d’appliquer les mesures d’accommodement identifiées.",
    "crumbs": [
      "Informations",
      "Évaluations"
    ]
  },
  {
    "objectID": "informations/materials.html",
    "href": "informations/materials.html",
    "title": "Matériel",
    "section": "",
    "text": "Cette page regroupe différentes ressources utiles pour le cours.\nThe Elements of Statistical Learning: Data Mining, Inference and Prediction, 2nd Edition\nAuteurs: Trevor Hastie, Robert Tibshirani et Jerome Friedman\nÉditeur: Springer (New York, 2009)\nISBN: 0-387-84857-0\nLien: site web des authors\nAn Introduction to Statistical Learning: with application in R, 2nd Edition\nAuteurs: Gareth James, Daniela Witten, Trevor Hastie et Robert Tibshirani\nÉditeur: Springer (New York, 2021)\nISBN: 1-0716-1417-4\nLien: site web des authors\nAn Introduction to Statistical Learning: with application in Python\nAuteurs: Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani et Jonathan Taylor\nÉditeur: Springer (Suisse, 2023)\nISBN: 3-031-38746-3\nLien: site web des authors",
    "crumbs": [
      "Informations",
      "Matériel"
    ]
  },
  {
    "objectID": "informations/schedule.html",
    "href": "informations/schedule.html",
    "title": "Plan",
    "section": "",
    "text": "Le plan est donné à titre indicatif.\n\n\n\nSemaine\nDate\nDurée\nPlan\nÉvaluations",
    "crumbs": [
      "Informations",
      "Plan"
    ]
  },
  {
    "objectID": "slides/01-introduction-slides.html#test",
    "href": "slides/01-introduction-slides.html#test",
    "title": "Introduction",
    "section": "Test",
    "text": "Test\nCoucou"
  },
  {
    "objectID": "contents/02-revisions.html",
    "href": "contents/02-revisions.html",
    "title": "Révisions",
    "section": "",
    "text": "Slides: link\nTD: link\nTP: link",
    "crumbs": [
      "Modules",
      "02 - Révisions"
    ]
  },
  {
    "objectID": "contents/02-revisions.html#sommaire",
    "href": "contents/02-revisions.html#sommaire",
    "title": "Révisions",
    "section": "Sommaire",
    "text": "Sommaire\n\nAlgèbre linéaire\nProbabilités et statistiques\nAlgorithmique / Programmation\n\n\n\n\nPrediction (xkcd:2370).",
    "crumbs": [
      "Modules",
      "02 - Révisions"
    ]
  },
  {
    "objectID": "contents/generalities/02-spaces.html",
    "href": "contents/generalities/02-spaces.html",
    "title": "Espace",
    "section": "",
    "text": "Variable numérique: Variable dont la valeur numérique mesure quelque chose de quantifiable et dont la différence entre les valeurs reflète la différence entre les objets. Exemples: revenu en dollars, masse, âge, …\nVariable ordinale: Variable qui ne donne pas une quantification précise d’un phénomène, mais dont les modalités peuvent être naturellement ordonnées. Examples: revenu faible, moyen ou élevé, ou niveau d’accord entre “tout-à-fait en désaccord”, “en désaccord”, “pas d’avis”, “d’accord”, “tout-à-fait d’accord”.\nNominales symétriques: Variable qualitative dont toutes les modalités sont aussi informatives l’une que l’autre. Examples: Sexe, sections d’un cours.\nNominales asymétriques: Variable qualitative dont les modalités ne contiennent pas toutes le même niveau d’information, habituellement lorsque l’une des modalités est très fréquente, un peu la modalité par defaut, mais que les autres ne le sont pas. Exemples: variable indiquant si un individu est daltonien ou non (deux individus daltoniens ont quelque chose en commun, mais deux individus non-daltoniens n’ont pas forcément quelque chose en commun), variable indiquant si une transaction est frauduleuse.\nIl y a plein d’autre type de variables: fonctions, textes, images, graphs, …",
    "crumbs": [
      "Modules",
      "03 - Généralités",
      "Espace"
    ]
  },
  {
    "objectID": "contents/generalities/02-spaces.html#types-de-variables",
    "href": "contents/generalities/02-spaces.html#types-de-variables",
    "title": "Espace",
    "section": "",
    "text": "Variable numérique: Variable dont la valeur numérique mesure quelque chose de quantifiable et dont la différence entre les valeurs reflète la différence entre les objets. Exemples: revenu en dollars, masse, âge, …\nVariable ordinale: Variable qui ne donne pas une quantification précise d’un phénomène, mais dont les modalités peuvent être naturellement ordonnées. Examples: revenu faible, moyen ou élevé, ou niveau d’accord entre “tout-à-fait en désaccord”, “en désaccord”, “pas d’avis”, “d’accord”, “tout-à-fait d’accord”.\nNominales symétriques: Variable qualitative dont toutes les modalités sont aussi informatives l’une que l’autre. Examples: Sexe, sections d’un cours.\nNominales asymétriques: Variable qualitative dont les modalités ne contiennent pas toutes le même niveau d’information, habituellement lorsque l’une des modalités est très fréquente, un peu la modalité par defaut, mais que les autres ne le sont pas. Exemples: variable indiquant si un individu est daltonien ou non (deux individus daltoniens ont quelque chose en commun, mais deux individus non-daltoniens n’ont pas forcément quelque chose en commun), variable indiquant si une transaction est frauduleuse.\nIl y a plein d’autre type de variables: fonctions, textes, images, graphs, …",
    "crumbs": [
      "Modules",
      "03 - Généralités",
      "Espace"
    ]
  },
  {
    "objectID": "contents/generalities/02-spaces.html#espaces-associés",
    "href": "contents/generalities/02-spaces.html#espaces-associés",
    "title": "Espace",
    "section": "Espaces associés",
    "text": "Espaces associés",
    "crumbs": [
      "Modules",
      "03 - Généralités",
      "Espace"
    ]
  },
  {
    "objectID": "contents/generalities/04-bias-variance.html",
    "href": "contents/generalities/04-bias-variance.html",
    "title": "Biais/Variance",
    "section": "",
    "text": "On veut prédire \\(Y\\) à partir de \\(X\\). Pour cela, on cherche à estimer \\(f(X)\\) la fonction qui relie \\(X\\) à \\(Y\\) par \\[Y = f(X) + \\epsilon\\]\nOn peut mesurer deux fonctions qui vont nous aider:\n\nFonction de perte (\\(L\\)): c’est la mesure de l’écart par rapport à ce qu’on souhaite mesurer, par example: \\[L(Y, f(X)) = (Y - f(X))^2.\\]\nFonction de risque : c’est la quantité que l’on cherche à minimiser. Il s’agit de l’espérance de la fonction de perte.\n\nComment trouver \\(f\\):\nObjectif: Trouver une fonction \\(\\widehat{f}\\) qui minimise le risque.\nComment: Supposer une certaine forme pour \\(f(X)\\) et minimiser la fonction de perte de faon analytique ou numérique.\n\nParamétrique: On donne une forme explication à \\(f(X)\\) qui dépend de paramètres. On cherche une méthode d’estimation des paramètres.\nNon-paramétrique: aucune forme forme particulière de \\(f\\), on estime une courbe ou fonction.\n\n\n\n\n\n\n\nExample: la régression linéaire simple\n\n\n\nOn suppose que \\(f(X) = \\beta_{0} + \\beta_{1}X\\), on cherche donc les valeurs \\(\\widehat{\\beta}_0\\) et \\(\\widehat{\\beta}_1\\) qui minimisent : \\[\\mathbb{E}(Y - (\\beta_{0} + \\beta_{1}X))^2.\\]\n\n\nQue faire dans le cas discret ?\nOn cherche à prédire \\(G\\) (un groupe ou facteur) à partir de \\(X\\). Supposons que nous avons \\(\\widehat{G}\\) qui prédise le classe des observations sachant \\(X\\), alors on peut définir la fonction de perte (0-1) comme le nombre d’erreur que l’on a effectué: \\[L(G, \\widehat{G}) = \\mathbb{1}_{G \\neq \\widehat{G}}.\\]\nAvec cette fonction de perte 0-1, \\(\\widehat{G}(x)\\) est la classe \\(g\\) qui maximise \\(\\mathbb{P}(g | X = x)\\).\nOn peut décomposer l’erreur quadratique moyenne (EQM): \\[\\mathbb{E}\\left( (Y - \\widehat{f}(x_0))^2 \\right) = \\text{Biais}(\\widehat{f}(x_{0}))^2 + \\mathrm{Var}(\\widehat{f}(x_{0}) + \\sigma^2_{\\epsilon}.\\]\nDémontrer la décomposition de l’EQM en un compromis biais variance.",
    "crumbs": [
      "Modules",
      "03 - Généralités",
      "Biais/Variance"
    ]
  },
  {
    "objectID": "contents/generalities/03-distance.html",
    "href": "contents/generalities/03-distance.html",
    "title": "Distances",
    "section": "",
    "text": "On a besoin d’une notion de distance et de similarité. On a besoin d’une définition de ce que sont des observations similaires ou des observations différentes. On veut quantifier la similarité ou la distance entre deux observations. Le choix de la métrique est une étape essentielle dans le processus d’analyse de données.",
    "crumbs": [
      "Modules",
      "03 - Généralités",
      "Distances"
    ]
  },
  {
    "objectID": "contents/generalities/03-distance.html#introduction",
    "href": "contents/generalities/03-distance.html#introduction",
    "title": "Distances",
    "section": "",
    "text": "On a besoin d’une notion de distance et de similarité. On a besoin d’une définition de ce que sont des observations similaires ou des observations différentes. On veut quantifier la similarité ou la distance entre deux observations. Le choix de la métrique est une étape essentielle dans le processus d’analyse de données.",
    "crumbs": [
      "Modules",
      "03 - Généralités",
      "Distances"
    ]
  },
  {
    "objectID": "contents/generalities/03-distance.html#notion-de-distance",
    "href": "contents/generalities/03-distance.html#notion-de-distance",
    "title": "Distances",
    "section": "Notion de distance",
    "text": "Notion de distance\n\n\n\n\n\n\nDéfinition de distance\n\n\n\nUne mesure de distance \\(d\\) doit satisfaire les propriétés suivantes pour tout \\(x, y, z \\in A\\), \\(A\\) étant un ensemble quelconque:\n\n\\(d(x, y) \\geq 0\\);\n\\(d(x, x) = 0\\);\n\\(d(x, y) = d(y, x)\\);\n\\(d(x, y) \\leq d(x, z) + d(y, z)\\).\n\n\n\nLa distance euclidienne:\nSi les observations sont constituées de \\(p\\) nombres réels de même ordre de grandeur, alors la distance euclidienne entre deux éléments de \\(\\mathbb{R}^p\\) est une mesure raisonnable.\nSoit \\(x, y \\in \\mathbb{R}^n\\), la distance euclidienne est données par : \\[d(x, y) = \\left\\| x - y \\right\\|_2 = \\left( \\sum_{i = 1}^{n} (x_i - y_i)^2 \\right)^{1/2}.\\]\nQue fait-on dans d’autres cas ? Plusieurs mesures ont été développées pour leur application particulière. Voici les plus classiques.\nUne distance est peu plus générale que la distance euclidienne est la distance \\(L_p\\). Soit \\(x, y \\in \\mathbb{R}^n\\), la distance \\(L_p\\) est donnée, pour \\(p &gt; 0\\), par : \\[d(x, y) = \\left\\| x - y \\right\\|_p = \\left( \\sum_{i = 1}^{n} |x_i - y_i|^p \\right)^{1 /p} .\\]\nLa distance euclidienne correspond à la distance \\(L_p\\) avec \\(p = 2\\). Cas particulier avec \\(p = 1\\), distance de Manhattan:\n\\[d(x, y) = \\left\\| x - y \\right\\|_1 = \\sum_{i = 1}^{n} | x_i - y_i |.\\]\n\n\n\n\n\n\nProperties\n\n\n\nLa distance \\(L_p\\) n’est pas invariante à un changement d’échelle, i.e., \\(L_p(x, y) \\neq L_p(\\lambda x, \\lambda y)\\).\n\n\nStandardisation",
    "crumbs": [
      "Modules",
      "03 - Généralités",
      "Distances"
    ]
  },
  {
    "objectID": "contents/generalities/03-distance.html#notion-de-similarité",
    "href": "contents/generalities/03-distance.html#notion-de-similarité",
    "title": "Distances",
    "section": "Notion de similarité",
    "text": "Notion de similarité\n\n\n\n\n\n\nDéfinition d’un indice de similarité\n\n\n\nUn indice de similarité \\(s\\) doit satisfaire les propriétés suivantes pour tout \\(x, y \\in A\\), \\(A\\) étant un ensemble quelconque:\n\n\\(s(x, y) \\geq 0\\);\n\\(s(x, y) = s(y, x)\\);\n\\(s(x, x) = 1 \\geq s(x, y)\\).\n\n\n\nUne distance peut se transformer en similarité en posant \\(s(x, y) = \\frac{1}{1 + d(x, y)}\\). L’inverse n’est pas vrai, dû à l’inégalité triangulaire. On peut aussi définir la dissemblance entre deux objects: \\(d^\\star(x, y) = 1 - s(x, y)\\).",
    "crumbs": [
      "Modules",
      "03 - Généralités",
      "Distances"
    ]
  },
  {
    "objectID": "contents/remainders/02-probabilities.html",
    "href": "contents/remainders/02-probabilities.html",
    "title": "Probabilités et Statistiques",
    "section": "",
    "text": "Dans cette partie, on présente quelques résultats en probabilités et statistiques dans le cadre de ce cours. Pour plus d’information, vous pouvez vous référer au cours STT-1000, à Wasserman (2010) (en anglais) et à Delmas (2013) (en français).",
    "crumbs": [
      "Modules",
      "02 - Révisions",
      "Probabilités et Statistiques"
    ]
  },
  {
    "objectID": "contents/remainders/02-probabilities.html#modéliser-le-hasard",
    "href": "contents/remainders/02-probabilities.html#modéliser-le-hasard",
    "title": "Probabilités et Statistiques",
    "section": "Modéliser le hasard",
    "text": "Modéliser le hasard\nBeaucoup de phénomènes réels ne sont pas prévisibles et généralement, leurs résultats contiennent une certaine variabilité. Cette variabilité est prise en compte grâce à une mesure de l’incertitude que l’on appelle mesure de probabilités.\n\n\n\n\n\n\nDéfinition\n\n\n\nL’espace d’évènements \\(S\\) est l’ensemble de tous les résultats possibles d’un phénomène. Un évènement est un sous-ensemble de l’espace d’évènements \\(S\\).\n\n\n\n\n\n\n\n\nExemples\n\n\n\n\nSi l’expérience consiste à lancer un pièce, \\(S = \\{0, 1\\}\\). Le résultat de cette expérience ne peut pas être connu à l’avance. Par exemple, \\(E = \\{1\\}\\) est un évènement de \\(S\\).\nSi on s’intéresse à la durée de vie d’un téléphone, \\(S = \\mathbb{R}_{+}\\). On peut aussi choisir \\(S = [0, M]\\), car cette durée de vie n’est probablement pas infini ! L’évènement \\(E = [10, \\infty)\\) représente l’évènement “une durée de vie de plus de 10 unités de temps”.\nPour le nombre de jours sans neige à Québec dans l’année, on peut choisir \\(S = \\mathbb{N}\\). L’évènement \\(E = (0, 5]\\) représente l’évènement “moins de 5 jours sans neige à Québec dans l’année”.\n\n\n\n\n\n\n\n\n\nDéfinition\n\n\n\nUne mesure de probabilités \\(\\mathbb{P}\\) sur \\(S\\) est une application (fonction) définie sur l’espace d’évènements et satisfaisant les propriétés suivantes :\n\nPour chaque évènement \\(E\\), \\(\\mathbb{P}(E) \\in [0, 1]\\).\n\\(\\mathbb{P}(S) = 1\\).\nSoient \\(E_{1}, E_{2}, \\dots\\), une séquence d’évènements (finie ou infinie) mutuellement exclusive, i.e. \\(\\forall i \\neq j, E_{i} \\cap E_{j} = \\varnothing\\). On a \\[\\mathbb{P}(\\bigcup_{n = 1}^{\\infty} E_n) = \\sum_{n = 1}^{\\infty} \\mathbb{P}(E_n).\\]\n\nOn appelle \\(\\mathbb{P}(E)\\), la probabilité de l’évènement \\(E\\).\n\n\nLa définition de mesures de probabilités peut être subjective et lié à l’expérience du statisticien. En reprenant l’exemple 3 sur le nombre de jours sans neige à Québec dans l’année. Une personne venant d’arriver au Canada peut vouloir donner la même probabilité à chacun des jours, alors qu’un Québécois aura plus d’information et pourras faire varier les probabilités en fonction de cette connaissance.\n\n\n\n\n\n\nDéfinition\n\n\n\nDeux évènements \\(E\\) et \\(F\\) sont dits indépendants si \\(\\mathbb{P}(E \\cap F) = \\mathbb{P}(E) \\times \\mathbb{P}(F)\\).\n\n\n\n\n\n\n\n\nDéfinition\n\n\n\nSoient \\(E\\) et \\(F\\), deux évènements, la probabilité conditionelle que \\(E\\) se réalise sachant que \\(F\\) s’est réalisé est définie par : \\[\\mathbb{P}(E \\mid F) = \\frac{\\mathbb{P}(E \\cap F)}{\\mathbb{P}(F)}.\\]\n\n\nDe façon intuitive, deux évènements sont indépendants si la connaissance de l’un ne donne aucune information sur la réalisation de l’autre. On a aussi \\(\\mathbb{P}(E \\mid F) = \\mathbb{P}(E)\\).",
    "crumbs": [
      "Modules",
      "02 - Révisions",
      "Probabilités et Statistiques"
    ]
  },
  {
    "objectID": "contents/remainders/02-probabilities.html#variables-aléatoires",
    "href": "contents/remainders/02-probabilities.html#variables-aléatoires",
    "title": "Probabilités et Statistiques",
    "section": "Variables aléatoires",
    "text": "Variables aléatoires\nEn probabilité, la convention est d’exprimer le résultat d’expériences comme la valeur d’une fonction appelé variable aléatoire. Cette caractérisation est toujours possible.\n\n\n\n\n\n\nDéfinition\n\n\n\nSoit une variable aléatoire \\(X\\). La distribution de cette variable aléatoire est définie par l’application \\(A \\mapsto \\mathbb{P}(X \\in A)\\).\n\n\n\n\n\n\n\n\nDéfinition\n\n\n\nSoit une variable aléatoire \\(X\\). Cette variable aléatoire est discrète si elle prend, au plus, un nombre dénombrable de valeurs. Dans ce cas, la distribution de \\(X\\) est donnée par les probabilités \\(\\mathbb{P}(X = x)\\) pour tout résultat \\(x\\).\n\n\n\n\n\n\n\n\nDéfinition\n\n\n\nSoit une variable aléatoire \\(X\\). Cette variable aléatoire est continue si les probabilités \\(\\mathbb{P}(X \\in A)\\) sont données par des intégrales de la forme \\(\\int_{A} f(x) dx\\) où \\(f: \\mathbb{R}^d \\to \\mathbb{R}_+\\) est une fonction intégrable tel que \\(\\int_{\\mathbb{R}^d} f(x) dx = 1\\). Notons que, pour un résultat \\(x\\) fixé, \\(\\mathbb{P}(X = x) = 0\\).\n\n\n\n\n\n\n\n\nDéfinition\n\n\n\nSoit une variable aléatoire \\(X\\). L’espérance mathématique \\(\\mathbb{E}(X)\\) de \\(X\\) est la valeur moyenne du résultat de \\(X\\) par rapport à sa distribution de probabilité. L’espérance est généralement noté \\(\\mu\\).\n\n\nSoit \\(F\\) un ensemble dénombrable. Une variable aléatoire discrète \\(X\\) a pour espérance \\(\\mathbb{E}(X) = \\sum_{x \\in F} x \\mathbb{P}(X = x)\\). Soit une variable aléatoire continue \\(X\\) ayant pour densité \\(f\\), son espérance est donnée par \\(\\mathbb{E}(X) = \\int_{\\mathbb{R}^d} x f(x) dx\\).\n\n\n\n\n\n\nThéorème de transfert\n\n\n\nSoit une variable aléatoire \\(X\\). Soit \\(g: \\mathbb{R}^d \\mapsto \\mathbb{R}\\) une fonction telle que \\(\\mathbb{E}\\left[ g(X) \\right]\\) existe. On a :\n\nSi \\(X\\) est une variable aléatoire discrète, \\(\\mathbb{E}\\left[ g(X) \\right] = \\sum_{x \\in F} g(x) \\mathbb{P}(X = x)\\);\nSi \\(X\\) est une variable aléatoire continue de densité \\(f\\), \\(\\mathbb{E}\\left[ g(X) \\right] = \\int_{\\mathbb{R}^d} g(x)f(x) dx\\).\n\n\n\n\n\n\n\n\n\nPropriétés: Linéarité de l’espérance\n\n\n\nSoient \\(X\\) et \\(Y\\), deux variables aléatoires, dont les espérances sont définies et soit \\(\\lambda \\in R\\). On a :\n\n\\(\\mathbb{E}(X + Y) = \\mathbb{E}(X) + \\mathbb{E}(Y)\\);\n\\(\\mathbb{E}(\\lambda X) = \\lambda \\mathbb{E}(X)\\).\n\n\n\n\n\n\n\n\n\nPreuve\n\n\n\n\n\nLa preuve se déduit du théorème de transfert et de la linéarité de l’addition et de l’intégration.\n\n\n\n\n\n\n\n\n\nDéfinition\n\n\n\nSoit \\(X\\) une variable aléatoire telle que l’espérance de son carré existe. La variance de \\(X\\) est définie par \\[\\mathrm{Var}(X) = \\mathbb{E}\\left[ \\left( X - \\mathbb{E}(X) \\right)^2 \\right] = \\mathbb{E}\\left[ X^2 \\right] - \\mathbb{E}\\left[ X \\right]^2.\\]\n\n\nLa variance mesure la dispersion d’une variable aléatoire autour de sa moyenne. On peut aussi s’intéresser à l’écart-type, défini comme la racine carrée de la variance : \\(\\sigma(X) = \\sqrt{\\mathrm{Var}(X)}\\).\n\n\n\n\n\n\nDéfinition\n\n\n\nSoient \\(X\\) et \\(Y\\), deux variables aléatoires et \\(A\\) et \\(B\\), deux évenements. Si les évenements \\(\\left\\{ X \\in A \\right\\}\\) et \\(\\left\\{ Y \\in B \\right\\}\\) sont indépendants, alors on dit que les variables aléatoires \\(X\\) et \\(Y\\) sont indépendantes.\n\n\nDe cette définition, on en déduit que :\n\npour des fonctions \\(f\\) et \\(g\\), les variables aléatoires \\(f(X)\\) et \\(g(Y)\\) sont indépendantes;\nsi les variables aléatoires \\(X\\) et \\(Y\\) sont à valeurs réelles et que leur espérance existe, alors l’espérance du produit \\(XY\\) existe et \\(\\mathbb{E}(XY) = \\mathbb{E}(X) \\times \\mathbb{E}(Y)\\).\n\n\n\n\n\n\n\nDéfinition\n\n\n\nSoit \\(X\\) une variable aléatoire. La fonction de répartition \\(F: \\mathbb{R} \\mapsto [0, 1]\\) de \\(X\\) est définie par \\[F(t) = \\mathbb{P}(X \\leq t), \\quad t \\in \\mathbb{R}.\\]",
    "crumbs": [
      "Modules",
      "02 - Révisions",
      "Probabilités et Statistiques"
    ]
  },
  {
    "objectID": "contents/remainders/02-probabilities.html#vecteurs-aléatoires",
    "href": "contents/remainders/02-probabilities.html#vecteurs-aléatoires",
    "title": "Probabilités et Statistiques",
    "section": "Vecteurs aléatoires",
    "text": "Vecteurs aléatoires\nSupposons que \\(X = (X_{1}, X_{2})\\) est une variable aléatoire de dimension \\(2\\) de densité \\(f_{X}\\). On appelle généralement les variables aléatoires de dimension supérieure à \\(1\\), des vecteurs aléatoires. Les densités de \\(X_{1}\\) et \\(X_{2}\\) sont appelées les densités marginales. Lorsque \\(X_{1}\\) et \\(X_{2}\\) sont indépendantes, on a : \\[f_X(x, y) = f_{X_{1}}(x) \\cdot f_{X_{2}}(y), \\quad (x, y) \\in \\mathbb{R}^2.\\]\n\n\n\n\n\n\nExemple de la loi normale multidimensionnelle\n\n\n\nOn dit qu’un vecteur aléatoire \\(X\\) de dimension \\(p\\) suit une loi normale multidimensionnelle de moyenne \\(\\mu\\) et de variance \\(\\Sigma\\), si sa densité est donnée par \\[f_X(x) = \\frac{1}{(2 \\pi)^{p /2}} \\cdot \\frac{1}{(\\text{det} \\Sigma)^{1/2}} \\cdot \\exp\\left\\{ -\\frac{1}{2}\\left( x - \\mu \\right)^\\top \\Sigma^{-1} \\left( x - \\mu \\right) \\right\\}, \\quad x \\in \\mathbb{R}^p.\\]\nOn note \\(X \\sim \\mathcal{N}_{p}(\\mu, \\Sigma)\\).\n\n\nEn statistiques, une quantité importante à mesurer est la dépendance linéaire entre \\(X_{1}\\) et \\(X_{2}\\). Pour cela, on peut utiliser la covariance ou la correlation.\n\n\n\n\n\n\nDéfinition\n\n\n\nSoit \\(X = (X_{1}, X_{2})\\) un vecteur aléatoire tel que l’espérance du carré de \\(X_{1}\\) et de \\(X_{2}\\) existe. La covariance entre \\(X_{1}\\) et \\(X_{2}\\) est donnée par \\[\\mathrm{Cov}(X_{1}, X_{2}) = \\mathbb{E}\\left[ (X_{1} - \\mathbb{E}(X_{1})) (X_{2} - \\mathbb{E}(X_{2}))\\right].\\]\nLa corrélation entre \\(X_{1}\\) et \\(X_{2}\\) est une version de la covariance normalisée par l’écart-type des variables aléatoires. Elle est donnée par \\[\\mathrm{Corr}(X_{1}, X_{2}) = \\frac{\\mathrm{Cov}(X_{1}, X_{2})}{\\sigma(X_{1}) \\sigma(X_{2})}.\\]\n\n\nOn peut interpréter le signe de la covariance et de la corrélation. Si elles sont strictement positives, \\(X_{1}\\) et \\(X_{2}\\) ont tendance à aller dans la même direction. Si \\(X_{1}\\) augmente, alors \\(X_{2}\\) aussi, et inversement. Si elles sont strictement négatives, \\(X_{1}\\) et \\(X_{2}\\) ont tendance à aller dans des directions opposées. Si \\(X_{1}\\) augmente, alors \\(X_{2}\\) diminue, et inversement. Si la covariance est égales à \\(0\\), il n’y a pas de règles et \\(X_{1}\\) et \\(X_{2}\\) sont dites orthogonales.\n\n\n\n\n\n\nPropriétés\n\n\n\nSoit \\(X = (X_{1}, X_{2})\\) un vecteur aléatoire. On a\n\n\\(\\mathrm{Cov}(X_{1}, X_{2}) = \\mathbb{E}(X_{1}X_{2}) - \\mathbb{E}(X_{1})\\mathbb{E}(X_{2})\\);\n\\(\\mathrm{Cov}(X_{1}, X_{2}) = \\mathrm{Cov}(X_{2}, X_{1})\\);\n\\(\\mathrm{Cov}(X_{1} + \\lambda Y_{1}, X_{2}) = \\mathrm{Cov}(X_{1}, X_{2}) + \\lambda \\mathrm{Cov}(Y_{1}, X_{2})\\).\n\n\n\n\n\n\n\n\n\nPreuve\n\n\n\n\nOn trouve le résultat en développement le produit dans la définition de la covariance.\nEn utilisant le point 1. et la commutativité de la multiplication.\nEn utilisant le point 1. et la linéarité de l’espérance.",
    "crumbs": [
      "Modules",
      "02 - Révisions",
      "Probabilités et Statistiques"
    ]
  },
  {
    "objectID": "contents/remainders/02-probabilities.html#estimation",
    "href": "contents/remainders/02-probabilities.html#estimation",
    "title": "Probabilités et Statistiques",
    "section": "Estimation",
    "text": "Estimation\nEn practique, nous n’avons pas une connaissance parfaite de nos vecteurs aléatoires, mais seulement des réalisations de ceux-ci (que l’on appelle échantillon). Notons \\(x_{1}, \\dots, x_{n}\\), \\(n\\) réalisations indépendantes d’un vecteur aléatoire \\(X\\) de moyenne \\(\\mu\\) et de variance \\(\\Sigma\\).\nL’estimateur de la moyenne \\(\\mu\\) est donné par \\[\\widehat{\\mu} = \\overline{X} \\coloneqq \\frac{1}{n} \\sum_{i = 1}^{n} x_i.\\]\nL’estimateur de la variance \\(\\Sigma\\) est donné par \\[\\widehat{\\Sigma} \\coloneqq \\frac{1}{n - 1}\\sum_{i = 1}^{n} (x_i - \\widehat{\\mu})(x_i - \\widehat{\\mu})^\\top.\\]\nPourquoi divise t-on cette somme par \\(n -1\\) et non par \\(n\\) pour estimer la variance ? Si l’on divise par \\(n\\), \\(\\widehat{\\Sigma}\\) est un estimateur biaisé de la variance. En effet, il faut prendre en compte que l’on utilise un estimateur biaisé de la moyenne dans l’estimateur de la variance et donc corriger pour cette estimation.\nNotons \\(D = \\{\\text{diag}(\\widehat{\\Sigma})\\}^{1/2}\\), la matrice des écarts-types calculés sur l’échantillon. On peut estimer la matrice des corrélations sur l’échantillon par \\[\\widehat{R} = D^{-1} \\widehat{\\Sigma} D^{-1}.\\]",
    "crumbs": [
      "Modules",
      "02 - Révisions",
      "Probabilités et Statistiques"
    ]
  },
  {
    "objectID": "contents/08-conclusion.html",
    "href": "contents/08-conclusion.html",
    "title": "Conclusion",
    "section": "",
    "text": "Y a-t-il des algorithmes/stratégies/critères pour identifier les bonnes valeurs des hyper-paramètres (en ACP, en regroupement hiérarchique, arbres, méthodes d’ensemble) ?\nComment définir/construire les variables \\(X_1,\\ldots,X_p\\) (feature engineering) ?\nComment identifier les données aberrantes et décider si on les enlève ou les garde ?\nComment scinder les données en entrainement vs validation ?\nLes données sont-elles représentatives de la “population cible” ?\nOn fait quoi si pour certaines observations, des valeurs de certaines variables sont manquantes ?",
    "crumbs": [
      "Modules",
      "08 - Conclusion"
    ]
  },
  {
    "objectID": "contents/dimension/03-kernel.html",
    "href": "contents/dimension/03-kernel.html",
    "title": "Noyau",
    "section": "",
    "text": "Dans certains cas, réduire la dimension des données n’est pas la chose à faire. Dans certains cas, il est même préférable d’augmenter la dimension des données ! Pourquoi ?\nRegardons l’exemple suivant: ajouter exemple en 2d, type cercle.\nDans ce cas, on s’intéresse à la méthode des noyaux (kernel trick).",
    "crumbs": [
      "Modules",
      "04 - Dimension",
      "Noyau"
    ]
  },
  {
    "objectID": "contents/dimension/03-kernel.html#la-méthode-du-noyau",
    "href": "contents/dimension/03-kernel.html#la-méthode-du-noyau",
    "title": "Noyau",
    "section": "",
    "text": "Dans certains cas, réduire la dimension des données n’est pas la chose à faire. Dans certains cas, il est même préférable d’augmenter la dimension des données ! Pourquoi ?\nRegardons l’exemple suivant: ajouter exemple en 2d, type cercle.\nDans ce cas, on s’intéresse à la méthode des noyaux (kernel trick).",
    "crumbs": [
      "Modules",
      "04 - Dimension",
      "Noyau"
    ]
  },
  {
    "objectID": "contents/dimension/04-others.html",
    "href": "contents/dimension/04-others.html",
    "title": "Divers",
    "section": "",
    "text": "L’idée est de présenter rapidement d’autres méthodes de changement de dimension.",
    "crumbs": [
      "Modules",
      "04 - Dimension",
      "Divers"
    ]
  },
  {
    "objectID": "contents/dimension/04-others.html#t-sne",
    "href": "contents/dimension/04-others.html#t-sne",
    "title": "Divers",
    "section": "\\(t\\)-SNE",
    "text": "\\(t\\)-SNE",
    "crumbs": [
      "Modules",
      "04 - Dimension",
      "Divers"
    ]
  },
  {
    "objectID": "contents/dimension/04-others.html#umap",
    "href": "contents/dimension/04-others.html#umap",
    "title": "Divers",
    "section": "UMAP",
    "text": "UMAP",
    "crumbs": [
      "Modules",
      "04 - Dimension",
      "Divers"
    ]
  },
  {
    "objectID": "contents/unsupervised/02-hierarchy.html",
    "href": "contents/unsupervised/02-hierarchy.html",
    "title": "Hierarchique",
    "section": "",
    "text": "Certains problèmes de l’algorithme \\(k\\)-means peuvent être résolu avec des algorithmes de classification hiérarchique. Par exemple, lorque l’on a à dispostion qu’une matrice de similiarité/distance entre les observations.\nLa classification hiérarchique permet d’obtenir des partitions toutes imbriquées les unes dans les autres. Il existe deux types d’algortihmes pour effectuer de la classification hiérarchique:\nDans les deux cas, on obtient \\(n\\) partitions hiérarchiques constituées de \\(1\\) à \\(n\\) groupes.",
    "crumbs": [
      "Modules",
      "06 - Non-supervisée",
      "Hierarchique"
    ]
  },
  {
    "objectID": "contents/unsupervised/02-hierarchy.html#algorithme-descendant",
    "href": "contents/unsupervised/02-hierarchy.html#algorithme-descendant",
    "title": "Hierarchique",
    "section": "Algorithme descendant",
    "text": "Algorithme descendant\nUn algorithme descendant fonctionne ainsi:\n\nAu départ, toutes les observations sont dans un seul et même groupe de \\(n\\) observations;\nÀ chaque étape, on divise le groupe le moins homogéne en deux groupes.\nÀ la fin, après \\(n\\) étapes, chaque observation à son propre groupe, c’est-à-dire qu’on obtient \\(n\\) groupes contenant une seule observation.\n\nL’exécution de cet algorithme ne donne pas une seule parition, mais \\(n\\) partitions, que l’on peut résumer à l’aide d’un graphique en forme d’arbre appelé dendogramme. Certains critères peuvent aider à choisir l’une parmi les \\(n\\) partitions proposées par l’algorithme. Ils demandent beaucoup de temps de calcul.",
    "crumbs": [
      "Modules",
      "06 - Non-supervisée",
      "Hierarchique"
    ]
  },
  {
    "objectID": "contents/unsupervised/02-hierarchy.html#algorithme-ascendant",
    "href": "contents/unsupervised/02-hierarchy.html#algorithme-ascendant",
    "title": "Hierarchique",
    "section": "Algorithme ascendant",
    "text": "Algorithme ascendant\nUn algorithme ascendant fonctionne ainsi:\n\nAu départ, chaque observation est son propre groupe, c’est-à-dire uq’on démarre avec \\(n\\) groupes content chacun une seule observation.\nÀ chaque étape, on fusionne les deux groupes les plus similaires.\nÀ la fin, après \\(n\\) étapes, on obtient un seul groupe contenant toutes les \\(n\\) observations.\n\nDifférences entre les algorithmes:\n\ncaractère ascendant ou descendant;\nleur faon de mesurer les distances / similiarité entre deux observations;\nleur faon de mesurer les distances / similiarité entre deux groupes.",
    "crumbs": [
      "Modules",
      "06 - Non-supervisée",
      "Hierarchique"
    ]
  },
  {
    "objectID": "contents/unsupervised/02-hierarchy.html#distance-entre-groupes",
    "href": "contents/unsupervised/02-hierarchy.html#distance-entre-groupes",
    "title": "Hierarchique",
    "section": "Distance entre groupes",
    "text": "Distance entre groupes\nPour mettre en oeuvre les algorithmes précédent, on doit définir la distance entre deux groupes d’observations \\(A\\) et \\(B\\), \\(d(A, B)\\).\nExample: On sait comment mesurer la distance entre les trois paires possibles de \\(\\{ 1 \\}\\), \\(\\{  2 \\}\\) et \\(\\{ 3 \\}\\), mais comment mesurer la distance entre \\(\\{ 1, 2 \\}\\) et \\(\\{ 3 \\}\\) ?\nIl existe plusieurs faons de calculer une telle distance entre deux groupes.\n\nMéthode du plus proche voisin (single linkage)\n\n\\[d(A, B) = \\min \\{ d_{i j}: i \\in A, j \\in B \\}.\\]\n\\[s(A, B) = \\max \\{ s_{i j}: i \\in A, j \\in B \\}.\\]\nLa distance/similiarité entre deux groupes d’observations est tout simplement la distance/similiarité entre les points de chaque groupe qui sont les plus rapprochés/similaires\nAvantages: * Donne de bons résultats lorsque les variables sont de types différents * Possède d’excellentes propriétés théoriques * Permet de créer des groupes dont la forme est très irrégulière * Est robuste aux données aberrantes\nDésavantages: * Tend à former un grand groupe avec plusieurs petits groupes satellites * Perd de l’efficacité si les vrais groupes sont de forme réegulière * Possède des propriétés théoriques ne semblant pas se réaliser en pratique dans certaines études\n\nMéthode du voisin le plus distant (complete linkage)\n\n\\[d(A, B) = \\max \\{ d_{i j}: i \\in A, j \\in B \\}.\\]\n\\[s(A, B) = \\min \\{ s_{i j}: i \\in A, j \\in B \\}.\\]\nLa distance/similiarité entre deux groupes d’observations est tout simplement la distance/similiarité entre les points de chaque groupe qui sont les plus éloignés/dissimilaires.\nAvantages: * Donne de bons résultats lorsque les variables sont de types différents * Tend à former des groupes de taille égale\nDésavantages: * Est extrêmement sensible aux données aberrantes * Tend à former des groupes de taille égale * Est très peu utilisée en pratique\n\nMéthode de la moyenne (average linkage)\n\n\\[d(A, B) = \\frac{1}{n_{A}n_{B}} \\sum_{i \\in A} \\sum_{j \\in B} d(x_{i}, x_{j}).\\] où \\(n_{A}\\) est le nombre d’observations dans le groupe \\(A\\) et \\(n_{B}\\) est le nonmbre d’observations dans le group \\(B\\).\nOn doit calculer les \\(n_{A} \\times n_{B}\\) distances/similiarités possibles entre les points des deux groupes, ensuite on prend la moyenne de ces distances/similiarités comme étant celle qui sépare les groupes.\nAvantages: * Tend à former des groupes de faible variance\nDésavantages: * Tend à former des groupes de même variance\n\nMéthode du centroïde (centroid method)\n\n\\[d(A, B) = d(\\overline{x}_{A}, \\overline{x}_{B}).\\] où \\[\\overline{x}_{A} = \\frac{1}{n_{A}} \\sum_{i \\in A} x_i, \\quad\\text{et}\\quad \\overline{x}_B = \\frac{1}{n_{B}} \\sum_{j \\in B} x_j\\]\nLa moyenne \\(\\overline{x}_{AB}\\) du nouveau groupe résultant de la fusion des groupes \\(A\\) et \\(B\\) se calcule comme suit: \\[\\overline{x}_{AB} = \\frac{n_{A} \\overline{x}_{A} + n_{B} \\overline{x}_{B}}{n_{A} + n_{B}}.\\]\nAvantages: * Est très robuste aux données aberrantes\nDésavantages: * Est peu efficace en l’absence de données aberrantes\n\nMéthode de la médiane (median method)\n\nÀ une étape donnée, nous avons toujours à notre disposition la distance entre les groupes déjà formés. On fusionne les deux groupes les moins distants/les plus similaires, disons \\(A\\) et \\(B\\) pour obtenir un groupe \\(AB\\). On met à jour la matrice des distances: la distance entre le nouveau groupe \\(AB\\) et tout autre groupe \\(C\\) est donnée par \\[d(AB, C) = \\frac{d(A, C) + d(B, C)}{2} - \\frac{d(A, B)}{4}.\\]\nAvantages: * Est encore plus robuste en présence de données aberrantes\nDésavantages: * Est très peu efficace en l’absence de données aberrante\n\nMéthode de Ward (Ward’s method)\n\nVariante de la méthode du centroïde pour tenir compte de la taille des groupes. Elle a été conue pour être optimale si les \\(n\\) vecteurs \\(x_{1}, \\dots, x_{n}\\) suivent des lois normales multivariées de \\(K\\) moyennes différentes mais toutes de même matrice de variance-covariance.\nBasée sur les sommes des carrées \\[SC_A = \\sum_{i \\in A} (x_i - \\overline{x}_A)^\\top (x_i - \\overline{x}_A).\\]\n\\[SC_B = \\sum_{j \\in B} (x_j - \\overline{x}_B)^\\top (x_j - \\overline{x}_B).\\]\n\\[SC_AB = \\sum_{k \\in A \\cup B} (x_k - \\overline{x}_{AB})^\\top (x_k - \\overline{x}_{AB}).\\]\noù \\(\\overline{x}_A\\), \\(\\overline{x}_B\\) et \\(\\overline{x}_{AB}\\) sont calculées comme dans la méthode du centroïde. On regroupe les classes \\(A\\) et \\(B\\) pour lesquelles \\[I_{AB} = SC_{AB} - SC_A - SC_B = \\frac{d^2(\\overline{x}_A, \\overline{x}_B)}{\\frac{1}{n_{A} + \\frac{1}{n_{B}}}}\\] est minimale.\nAvantages: * Est optimale si les observations sont approximativement distribuées selon une loi normale multidimensionnelle de même matrice de variances-covariances\nDésavantages: * Est sensible aux données aberrantes * Tend à former des groupes de petite taille * Tend à former des groupes de même taille\n\nMéthode flexible\n\nLes auteurs de cette méthode ont remarqué que pour plusieurs méthodes connues, on a les relations suivantes: \\[d(C, AB) = \\alpha_A d(C, A) + \\beta d(C, B) + \\beta d(A, B) + \\gamma \\left| d(C, A) - d(C, B) \\right|.\\]\n\n\n\n\n\n\n\n\n\n\nMéthode\n\\(\\alpha_A\\)\n\\(\\alpha_B\\)\n\\(\\beta\\)\n\\(\\gamma\\)\n\n\n\n\nPlus proche\n\\(1/2\\)\n\\(1/2\\)\n\\(0\\)\n\\(-1/2\\)\n\n\nPlus distant\n\\(1/2\\)\n\\(1/2\\)\n\\(0\\)\n\\(1/2\\)\n\n\nMédiane\n\\(1/2\\)\n\\(1/2\\)\n\\(-1/4\\)\n\\(0\\)\n\n\nMoyenne\n\\(\\frac{n_A}{n_A+n_B}\\)\n\\(\\frac{n_B}{n_A+n_B}\\)\n\\(0\\)\n\\(0\\)\n\n\nCentroïde\n\\(\\frac{n_A}{n_A+n_B}\\)\n\\(\\frac{n_B}{n_A+n_B}\\)\n\\(-\\frac{n_An_B}{n_A + n_B}\\)\n\\(0\\)\n\n\nWard\n\\(\\frac{n_A+n_C}{n_A+n_B+n_C}\\)\n\\(\\frac{n_B+n_C}{n_A+n_B+n_C}\\)\n\\(-\\frac{n_C}{n_A+n_B+n_C}\\)\n\\(0\\)\n\n\n\nAvec la méthode flexible, on impose arbitrairement les contraintes suivantes: \\[\\alpha_A + \\alpha_B + \\beta = 1, \\quad \\alpha_A = \\alpha_B, \\quad \\gamma = 0.\\]\nAinsi, \\[\\alpha_A = \\alpha_B = \\frac{1 - \\beta}{2}.\\] Et il ne reste qu’à choisir \\(\\beta\\). Les autheurs suggèrent de poser \\(\\beta = -0.25\\) sauf quand on souponne la présence de données aberrantes où l’on suggère \\(\\beta = -0.5\\).",
    "crumbs": [
      "Modules",
      "06 - Non-supervisée",
      "Hierarchique"
    ]
  },
  {
    "objectID": "contents/unsupervised/02-hierarchy.html#pratique",
    "href": "contents/unsupervised/02-hierarchy.html#pratique",
    "title": "Hierarchique",
    "section": "Pratique",
    "text": "Pratique\nL’exécution d’un algorithme nous donne une séquence de \\(n\\) partitions ayant de \\(n\\) à \\(1\\) groupes.\nQuelle partition de cette séquence devrions-nous choisir?\nL’une des \\(n\\) partitions est-elle particulièrement interprétable? L’une des \\(n\\) partitions a-t-elle a un sens pratique? Visions-nous séparer la population en un nombre \\(K\\) de groupes?\nS’il n’y a pas de réponse claire à ces questions, des critères peuvent nous guider …\nIl y a plusieurs indications pour nous aider dans le choix du nombre de classe (surtout si les variables sont continues). La librairie NbClust en contient une trentaine: https://www.rdocumentation.org/packages/NbClust/versions/3.0.1/topics/NbClust\n\nLes indicateurs basées sur l’inertie\n\n\\[I_{tot} = I_{intra-groupe}+ I_{inter-groupe}\\] Ces indicateurs sont plus pertinents avec des variables continues. Prendre garde au poids des variables et à la standardisation.\n\nPseudo- \\(R^2\\)\n\n\\[Pseudo-R^2 =\\frac{I_{inter-groupe}}{I_{tot}}\\]\n\nStatistique de Caliliski-Harabasz (CH):\n\n\\[CH = \\frac{I_{inter-groupe}/(k-1)}{I_{intra-groupe}/(n-k)}\\]\n\nIndice de Dunn: On maxime l’indice suivant:\n\n\\[D= \\frac{\\text{Distance minimale entre 2 groupes}}{\\text{Distance maximale dans un groupe}}\\]\nL’indice de Dunn cherche donc à créer des groupes denses et bien séparés.\n\nIndice de silhouette\n\nLa silhouette de l’observation \\(i\\) mesure la confiance dans le choix du groupe pour l’observation \\(i\\): \\[S(i) = \\frac{b_i-a_i}{max(b_i,a_i)}\\] où \\(a_i\\) est la distance moyenne entre l’observation \\(i\\) et les autres observations de son groupe et \\(b_i\\) est la distance moyenne entre l’observation \\(i\\) et les observations du groupe le plus proche de \\(i\\). On souhaite maximiser la silhouette moyenne des observations.\n\nCritère de classification cubique (CCC)\n\nOn fait un graphique avec le CCC en ordonnée et le nombre de groupes en abscisse. Pour le nombre de groupes, on ne considère que les partitions de \\(K=1\\) à \\(K=n/10\\). Si \\(CCC&gt;2\\), on est en présence d’une classification de bonne qualité. Si \\(0&lt;CCC&lt;2\\), on est en présence d’une classification de qualité moyenne. Si \\(CCC&lt;0\\), on est en présence d’une classification de mauvaise qualité. Pour choisir le nombre de classes à retenir, on peut considérer les nombres de classes associés aux fortes hausses du critère CCC entre deux nombres de classes subséquents. On considère les pics, atteignant des valeurs du critère supérieures à 2 ou à 3 comme étant de fortes hausses de ce critère.\nIl ne faut pas utiliser le critère CCC avec la méthode du plus proche voisin, ou lorsque l’on suspecte que les groupes sont de forme très allongée ou irrégulière. Le critères ne fonctionne pas bien quand le nombre d’observations dans certains groupes est inférieur à 10.\n\nStatistique pseudo-\\(F\\)\n\nStatistique presque distribuée selon une loi \\(F\\) lorsque la loi des données pas trop loin de la normale multivariée avec variances égales dans toutes les classes. Même si on est loin de la normalité, en pratique cette statistique peut quand-même être informative. On cherche des nombres de classes pour lesquels la statistique du pseudo-\\(F\\) se démarque par une grande valeur. Sur un graphique de la statistique du pseudo-\\(F\\) en fonction du nombre de classes, ceci se traduit par la recherche de pics. Il ne faut pas utiliser la statistique du pseudo-\\(F\\) avec la méthode du plus proche voisin.\n\nStatistique du pseudo-\\(t^2\\)\n\nStatistique presque distribuée selon une loi \\(t\\) lorsque loi des données pas trop loin de la normale multivariée avec des variances égales dans toutes les classes. En pratique, on regarde le graphique de la statistique du pseudo-\\(t^2\\) en fonction du nombre de classes de droite à gauche, on essaie de trouver des valeurs de la statistique qui sont beaucoup plus élevées que la valeur précédente. Supposons que la forte hausse se produit entre \\(k\\) et \\(k-1\\) classes. On choisit \\(k\\) classes dans le partitionnement de nos observations. Il ne faut pas utiliser la statistique du pseudo-\\(t^2\\) avec la méthode du plus proche voisin.",
    "crumbs": [
      "Modules",
      "06 - Non-supervisée",
      "Hierarchique"
    ]
  },
  {
    "objectID": "contents/unsupervised/01-kmeans.html",
    "href": "contents/unsupervised/01-kmeans.html",
    "title": "\\(k\\)-means",
    "section": "",
    "text": "Algorithme des \\(k\\)-moyennes:\n\\[\\mu_{k} = \\frac{1}{N_k} \\sum_{i: C(i) = k} x_i, \\quad k = 1, \\dots, K,\\]\noù \\(N_{k}\\) est le nombre d’observations dans le group \\(k\\).\nExample\nInconvénients des \\(k\\)-means:\nExpliciter chaque nconvénients et de potentielles solutions.",
    "crumbs": [
      "Modules",
      "06 - Non-supervisée",
      "$k$-means"
    ]
  },
  {
    "objectID": "contents/unsupervised/01-kmeans.html#k-médoides",
    "href": "contents/unsupervised/01-kmeans.html#k-médoides",
    "title": "\\(k\\)-means",
    "section": "\\(k\\)-médoides",
    "text": "\\(k\\)-médoides\nOn n’utilise plus le centre, mais l’observation qui minimise les distances dans chaque groupe. La médiane plutôt que la moyenne.\nAvantages:\n\nPermet d’intégrer des variables ordinales\nRobuste\nPermet de bien spécifier la matrix de distance\n\nInconvénients:\n\nIl faut connaître le nombre de groupes.",
    "crumbs": [
      "Modules",
      "06 - Non-supervisée",
      "$k$-means"
    ]
  },
  {
    "objectID": "contents/misc/good-practices.html",
    "href": "contents/misc/good-practices.html",
    "title": "Bonnes pratiques de programmation",
    "section": "",
    "text": "Cette page est basé sur un document qu’Aurélien Nicosia (ULaval) a créé en 2023 appelé “Bonnes pratiques de programmation en R”. Celui-ci a été mis à jour et écrit dans un contexte plus général.\nPeu importe le langage informatique, employer de bonnes pratiques de programmation signifie respecter certaines normes afin de créer du “bon” code. On peut donc se demander ce qu’est un “bon” code. Pour moi, un “bon” code est un code qui remplit trois objectifs:\nPourquoi est-ce souhaitable d’adopter de bonnes pratiques ? Cela permet que le code soit compris et utilisé par n’importe qui (et en particulier, soi-même dans le futur). À long terme, les bonnes pratiques apportent une augmentation de notre productivité en évitant les répétitions inutiles."
  },
  {
    "objectID": "contents/misc/good-practices.html#objectif-1-produire-les-résultats-escomptés",
    "href": "contents/misc/good-practices.html#objectif-1-produire-les-résultats-escomptés",
    "title": "Bonnes pratiques de programmation",
    "section": "Objectif 1 : Produire les résultats escomptés",
    "text": "Objectif 1 : Produire les résultats escomptés\nLa priorité lors du développement de tout code informatique est certainement l’écriture d’un code qui réalise bien ce qu’il doit réaliser. Donc, un “bon”” code doit produire les bons résultats. Pour y arriver, le code doit d’abord être fonctionnel, c’est-à-dire ne pas contenir de bogues. Pour s’assurer d’écrire du code qui fonctionne correctement, il faut simplement le tester. Il vaut mieux tester fréquemment, à chaque petit ajout, plutôt que de produire beaucoup de code avant de le tester. Ainsi, il y a beaucoup moins de débogage à faire. Un courant de pensée en informatique prône même l’écriture des tests avant l’écriture du code (test driven development). Cependant, une meilleure pratique est de formaliser les tests afin de pouvoir facilement les lancer à nouveau lors de modifications futures apportées au code."
  },
  {
    "objectID": "contents/misc/good-practices.html#objectif-2-code-facile-à-maintenir",
    "href": "contents/misc/good-practices.html#objectif-2-code-facile-à-maintenir",
    "title": "Bonnes pratiques de programmation",
    "section": "Objectif 2 : Code facile à maintenir",
    "text": "Objectif 2 : Code facile à maintenir\nMaintenir un code informatique signifie de s’assurer qu’il continue de fonctionner correctement dans le futur, malgré les modifications qui lui sont apportées. Un code utilisé fréquemment est un code appelé à être mis à jour, soit pour y ajouter des fonctionnalités, soit pour corriger des bogues non détectés par les tests, mais découverts par des utilisateurs. Reprendre un code écrit par quelqu’un d’autre, ou écrit par nous-mêmes quelques mois auparavant, n’est pas toujours une tâche facile. Cependant, s’il s’agit d’un code correctement écrit, il ne devrait pas être trop difficile à comprendre et à modifier.\nLa maintenance d’un code est basé sur trois principes: son versionnage, sa compréhensibilité et sa réutilisabilité.\nLe principe du versionnage est d’utiliser un logiciel qui enregistre les différentes modifications faites sur le code. Le plus connu est Git. Il permet de naviguer entre les différentes versions de votre code, de créer plusieurs versions (appelée branches), de collaborer à plusieurs sur un même code. C’est vraiment un indispensable à avoir dans sa panoplie. Le versionnage peut ensuite être enregistré sur Github qui gère la machinerie sous-jacente. À titre d’exemple, ce site utilise Git et Github pour son versionnage et vous pouvez voir les différentes versions ici.\nUn code compréhensible est clair et se lit bien (presque comme du texte). Il comporte souvent des instructions qui parlent d’elles-mêmes. Ces instructions sont typiquement succinctes, car une instruction trop longue effectue souvent plusieurs tâches difficilement discernables. Si la lecture d’une instruction ne permet pas à un programmeur initié dans le langage informatique employé de comprendre ce qu’elle réalise, il est alors recommandé d’insérer un commentaire dans le code pour expliquer à quoi sert l’instruction. En plus de commentaires pour expliquer certains instructions, toutes fonctions devraient être documentées. La documentation d’une fonction devrait contenir: un texte explicatif de ce que fait la fonction, une description des arguments acceptés en entrée, une description des résultats produits et un exemple d’utilisation. Lorsque que l’on programme, il est aussi de bon ton de suivre un guide de style. Un guide de style est un ensemble de règles sur lesquelles les développeurs se sont mis d’accord et qui permet d’avoir une syntaxe similaire à travers les différents projets. En R, vous pouvez utiliser le guide de style du tidyverse. En Python, vous pouvez utiliser le guide de style PEP8 écrit par le créateur de Python. Et en Julia, vous pouvez utiliser le guide de style fournit avec le manuel du langage. Vous pouvez utiliser un linter, un outil d’analyse de code statique pour vous aidez à respecter ces guides de styles. À noter que ces guides de style sont des recommendations et il n’y a rien d’obligatoire à les suivre. Certaines règles se contredisant, je vous recommanderai même de ne pas en suivre certaines.\nLa façon la plus commune d’avoir un code facile à réutiliser est d’en faire des fonctions que l’on peut ensuite partager à travers un package."
  },
  {
    "objectID": "contents/misc/good-practices.html#objectif-3-code-suffisamment-rapide",
    "href": "contents/misc/good-practices.html#objectif-3-code-suffisamment-rapide",
    "title": "Bonnes pratiques de programmation",
    "section": "Objectif 3 : Code suffisamment rapide",
    "text": "Objectif 3 : Code suffisamment rapide\nAprès nous être assurés que notre code fonctionne correctement et qu’il est facilement maintenable, nous pouvons nous préoccuper de son temps d’exécution. Bien qu’il ne s’agisse pas du critère le plus important pour définir ce qu’est du “bon” code, c’est tout de même un critère à ne pas négliger, car un code trop lent risque de ne pas être utilisé. Pour produire du code computationnellement efficace, il faut :\n\nmettre en pratique quelques trucs simples, i.e. utiliser les syntaxes optimisées des différents langages;\ncomparer le temps d’exécution de différentes façons de programmer une tâche;\nparfois faire du calcul en parallèle;\nparfois programmer des bouts de code dans un autre langage plus bas niveau."
  },
  {
    "objectID": "contents/misc/good-practices.html#en-résumé",
    "href": "contents/misc/good-practices.html#en-résumé",
    "title": "Bonnes pratiques de programmation",
    "section": "En résumé",
    "text": "En résumé\nEn résumer, pour adopter de bonnes pratiques de programmation, il faut :\n\nTester son code fréquemment son code.\nUtiliser un logiciel de gestion de versions.\nDocumenter son code.\nRespecter un guide de style.\nFactoriser son code en créant des fonctions et des packages.\nOptimiser le temps d’exécution."
  },
  {
    "objectID": "contents/misc/pipe.html",
    "href": "contents/misc/pipe.html",
    "title": "Opérateur pipe en R",
    "section": "",
    "text": "Cette page est basé sur un document qu’Aurélien Nicosia (ULaval) a créé en 2023 appelé “Opérateur pipe”. Celui-ci a été mis à jour.\nDepuis la version 4.1.0 de R, sorti en mai 2021, le langage a introduit l’opérateur pipe |&gt; en s’inspirant de ce que faisait le package magrittr. À partir de R 4.3.0, le guide de style du tidyverse recommande l’utilisation de l’opérateur pipe de base, et non celui de magrittr.\n\n\n\n\n\n\nRaccourcis clavier\n\n\n\nDans RStudio, le raccourci clavier pour insérer l’opérateur pipe est:\n\nSous Windows : Ctrl + Shift + M\nSous macOS : ⌘ + ⇧ + M\n\n\n\nCet opérateur introduit une façon d’enchaîner les intructions et de passer des arguments à des fonctions de manière plus lisible que la manière classique.\nPour résumer le fonctionnement de cette opérateur, voici comment il transforme quelques appels de fonctions:\n\nf(x) devient x |&gt; f();\nf(x, y) devient x |&gt; f(y);\nh(g(f(x))) devient x |&gt; f() |&gt; g() |&gt; h().\n\nCette opérateur permet de mettre en avant la séquence d’actions et non l’objet sur lequel la séquence d’actions est faite. Cela rend le code plus lisible (et avoir un code lisible est une bonne pratique). En lisant de gauche à droite l’instruction h(g(f(x))), nous voyons d’abord l’appel à la fonction h, puis l’appel à la fonction g et finalement l’appel à la focntion f. Pourtant, l’évaluation de cette instruction se fait dans le sens inverse. En effet, R va d’abord :\n\névaluer f(x);\npuis, il passera le résultat à la fonction g et retournera le résultat;\nqui sera passé à la fonction h et le résultat final sera retourné.\n\nSi nous voulons écrire un code qui reflète l’ordre des évaluations correctement, nous pourrions écrire :\n\nres1 &lt;- f(x)\nres2 &lt;- g(res1)\nh(res2)\n\nCe code a cependant le défaut de créer des objects que nous souhaitons pas nécessairement conserver. L’opérateur |&gt; n’a pas ce défaut ! En effet, une instruction écrite en utilisant l’opérateur |&gt; permet de suivre l’ordre des évaluations, sans créer d’objets inutilement en mémoire.\nPour encore plus de clarté, il est possible d’étendre sur plusieurs lignes un instruction contenant plusieurs opérateur |&gt; de façon à avoir une fonction par ligne :\n\nx |&gt; \n  f() |&gt; \n  g() |&gt; \n  h()\n\nSi l’argument que nous souhaitons passer avec l’opérateur |&gt; n’est pas celui en première position, il faut utiliser _ comme suit avec un paramètre nommé: f(y, z = x) devient x |&gt; f(y, z = _).\nPrenons un exemple pour illustrer l’utilisation de l’opérateur |&gt;. Supposons que nous avons la chaîne de caractères suivantes :\n\ntext &lt;- \"Ceci est un example\"\n\net que nous souhaitons la corriger—remplacer “example” par “exemple” et ajouter un point à la fin—avec l’intruction suivante :\n\npaste0(gsub(pattern = \"example\", replacement = \"exemple\", x = text), \".\")\n\n[1] \"Ceci est un exemple.\"\n\n\nCette instruction est un peu difficile à lire en raison de l’appel à la fonction gsub imbriqué dans un appel de fontion paste0. Nous pourrions la réécrire comme suit avec l’opérateur |&gt; :\n\ntext |&gt; \n  gsub(pattern = \"example\", replacement = \"exemple\", x = _) |&gt; \n  paste0(\".\")\n\n[1] \"Ceci est un exemple.\"\n\n\nPrenons un autre exemple numérique. On souhaite faire le calcul suivant : \\[\\frac{(2 + 4) \\times 8}{2}.\\]\nPour cela, nous avons besoin de quelques fonctions mathématiques.\n\nadd &lt;- function(x, y) {\n  x + y\n}\n\nmul &lt;- function(x, y) {\n  x * y\n}\n\ndiv &lt;- function(x, y) {\n  x / y\n}\n\nOn peut faire le calcul de trois manières différentes:\n\n# En créant différents objets \nres1 &lt;- add(2, 4)\nres2 &lt;- mul(res1, 8)\nres3 &lt;- div(res2, 2)\nprint(res3)\n\n[1] 24\n\n# En imbriquant les fonctions\nres &lt;- div(mul(add(2, 4), 8), 2)\nprint(res)\n\n[1] 24\n\n# Avec l'opérateur pipe \nres &lt;- 2 |&gt;\n  add(4) |&gt; \n  mul(8) |&gt; \n  div(2)\nprint(res)\n\n[1] 24"
  },
  {
    "objectID": "contents/05-supervisee.html",
    "href": "contents/05-supervisee.html",
    "title": "Supervisée",
    "section": "",
    "text": "On considère une population comportant \\(q\\) groupes. On observe \\(p\\) variables \\(X_{1}, \\dots, X_{p}\\) pour chaque individu/objet de la population. On cherche à obtenir un modèle/algortihme pour classer de nouveaux individus/objets dans les bons groupes, c’est-à-dire de prédire \\(Y\\) à partir de \\(X_{1}, \\dots, X_{p}\\).\n\n\n\n\n\n\nExample\n\n\n\n\nRevenue Québce désire identifier les déclarations fiscales méritant d’être examinées de faon plus approfondies (détection de fraude).\nReconnaissance automatique des chiffres et des lettres des codes postaux écrits à la main.\nIdentification de nouveaux clients potentiels.\nFiltrage de courriels indésirables.\nReconnaissance d’images.\n\n\n\nApproche générale\n\nSélectionner un certain nombre d’individus dont on connaît le groupe d’appartenance.\nMesurer \\(p\\) caractéristiques \\(X_{1}, \\dots, X_{p}\\) sur ces individus.\nDiviser ce jeu de données en deux:\n\n\nUn jeu de données pour la modélisation (entrainement, “train”)\nUn jeu de données pour la vérification (valisation, “test”)\n\n\nDévelopper un modèle/algorithme pour classer le mieux possible les individus du jeu de données d’entrainement.\nÉvaluer notre modèle/algorithme sur le jeu de données de valisation.\n(Répéter étapes 3-4-5 avec d’autres modèles/algorithmes et choisir le meilleur).\n\nQuelques méthodes:\n\nAnalyse discriminante\nArbre de classification\nRégression\nClassificateur naïf de Bayes\nMéthode des \\(k\\) plus proches voisins\nSupport vector machine\nRéseaux de neurones.\n\nIl n’y a aucun algorithme qui garantit le meilleur classificateur pour toute situation donnée. Chaque problème est nouveau et on doit tenter de trouver la meilleure façon de procéder par essai et erreur. Ceci étant dit, certains principes s’appliquent plus généralement * Commencer par une exploration des données (p.ex. statistiques descriptives sur toutes les variables prises individuellement, ACP, ACB/ACM, classification non-supervisée) * Tirer avantage de la connaissance du sujet des experts qui nous entourent * Voir si certaines méthodes n’ont pas déjà eu du succès dans des analyses similaires\nLa principale difficulté vient habituellement de la dimension du problème : le nombre de modèles/méthodes possibles pour un problème donné est énorme et croît rapidement avec le nombre de variables disponibles. Parfois, réduire la dimension du problème (ACP, ACB/ACM, classification non-supervisée) peut aider : on applique ces techniques à un sous-ensemble des variables, et on utilise ensuite les scores produits comme prédicteurs dans les algos de classifications Il n’y a pas de recette générale pour savoir quel sous-ensemble choisir … Allez voir sur des sites de concours d’analyse de données (p.ex. Kaggle, KD Nuggets, etc.) et regardez les approches utilisées par les gagnants des concours pour lesquels le problème à résoudre s’apparente un peu au vôtre.",
    "crumbs": [
      "Modules",
      "05 - Supervisée"
    ]
  },
  {
    "objectID": "contents/supervised/02-discriminant.html",
    "href": "contents/supervised/02-discriminant.html",
    "title": "Discriminant",
    "section": "",
    "text": "La méthode a été introduite en 1936 par R. A. Fisher. Il s’intéressait à la taxonimie végétale, c’est-à-dire déterminer l’espèce de fleurs à partir de diverses mesures.\nNotation:\nSoit \\(X = (X_{i j})\\), qui est une matrice de dimension \\(n \\times p\\), où \\(n\\) est le nombre d’individus dans l’échantillon, \\(p\\) est le nombre de variables et \\(X_{i j}\\) est la valeur de la \\(j\\)e variable pour le \\(i\\)e individus.\nIdentification des groupes:\nScore de l’analyse discriminante: on a des observations dans \\(R^p\\). Pour faire de la classification à partir de \\(X_{1}, \\dots, X_p\\), on doit partionner \\(R^p\\) en \\(q\\) sous-ensembles de sorte que chacun des \\(q\\) sous-ensembles est associé à un des \\(q\\) groupes.\nOn va chercher à passer de la dimension \\(p\\) à la dimension \\(1\\) en calculant un score \\(f(x_{1}, \\dots, x_p) \\in \\mathbb{R}\\) pour chaque observation et ensuite utiliser ce score pour déterminer le groupe d’appartenance (et donc partionner \\(R\\)). Le score proposé par Fisher est une combinaison linéaire des variables, c’est-à-dire \\[f(X_{1}, \\dots, X_p) = a^{\\top} X + b = a_{1} X_{1} + \\cdots + a_p X_p + b.\\]\nOn en déduira \\(q\\) intervalles de décision \\(S_{1}, \\dots, S_q\\) associés aux groupes.\nOn voudrait choisir le vecteur \\(a\\) de sorte que les scores soient, à la fois, très différents entre les groupes et très similaires à l’intérieur d’un groupe. On s’intéresse donc à la variabilité des scores à l’intérieur des groupes et entre les groupes.\nÉtant donné \\(a \\in R^p\\), on a: \\[\\mathrm{Var}(f(X_{1}, \\dots, X_p)) = \\mathrm{Var}(a^{\\top} X) = a^{\\top} \\mathrm{Var}(X) a,\\] que nous estimons à partir des \\(n\\) observations par \\[\\widehat{\\mathrm{Var}}(f(X_{1}, \\dots, X_p)) = \\frac{1}{n} a^{\\top} S a.\\]\nLa base de l’analyse discriminante repose sur le fait que \\[S = W + B,\\] où \\(W\\) est la matrice de variance intragroupe et \\(B\\) est la matrice de variance intergroupe.\nOn peut prouver ce résultat en considérant la définition des matrices \\(S, W\\) et \\(B\\). La moyenne de la variable \\(j\\) pour tous les individus de l’échantillon est \\[\\overline{X}_j = \\frac{1}{n}\\sum_{i = 1}^{n} X_{i j}.\\]\nLa moyenne de la variable \\(j\\) pour les individus du groupe \\(k\\) est \\[\\overline{X}_{k j} = \\frac{1}{n_{k}} \\sum_{i \\in I_k} X_{i j}.\\]\nLa somme des carrés totale est \\[s_{j j^\\prime} \\sum_{i = 1}^{n} (X_{i j} - \\overline{X}_j)(X_{i j^\\prime} - \\overline{X}_{j^\\prime}).\\]\nOn tirerait de la matrice \\(S\\) une estimation de \\(\\mathrm{Cov}(X_j, X_{j^\\prime})\\) si toutes les observations provenaient d’un même groupe. On définit \\(s_{j j^\\prime}\\) comme étant \\[s_{j j^\\prime} = w_{j j^\\prime} + b_{j j^\\prime},\\] où \\[w_{j j^\\prime} = \\sum_{k = 1}^{q} \\sum_{i \\in I_k} (X_{i j} - \\overline{X}_{k j})(X_{i j^\\prime} - \\overline{X}_{k j^\\prime}),\\]\n\\[b_{j j^\\prime} = \\sum_{k = 1}^{q} n_k (\\overline{X}_{k j} - \\overline{X}_j)(\\overline{X}_{k j^\\prime} - \\overline{X}_{j^\\prime}).\\]\nPreuve:\nOn obtient \\[\\widehat{\\mathrm{Var}}(a^{\\top} X) = \\frac{1}{n} a^{\\top} S a = \\frac{1}{n} \\left( a^{\\top} W a + a^{\\top} B a \\right).\\]\nOn se rappelle que l’on veut choisir le vecteur \\(a\\) pour que les scores puissent facilement séparer les groupes. En d’autres mots, on veut des scores les plus similaires possible à l’intérieur d’un groupe et des scores les plus différents possible entre les groupes.\nOn propose de choisir le vecteur \\(a in \\mathbb{R}^p\\) pour maximiser \\[\\frac{a^{\\top} B a}{a^{\\top} W a} \\quad\\text{où}\\quad \\frac{a^{\\top} B a}{a^{\\top} S a}.\\] ce vecteur est unique à une constante près.\nCe problème peut être reformuler des faons suivantes:\nEn écrivant la troisième formulation \\[c^{\\top} \\left( S^{-1/2} B S^{-1/2} \\right) c \\quad\\text{s.c.} c^{\\top} c = 1,\\] on peut prendre \\(a = S^{-1/2} c\\), où \\(c\\) est un vecteur propre normé associé à \\(\\lambda_{1}\\) la première valeur propre de \\(S^{-1/2} B S^{-1/2}\\). De faon équivalente, de la deuxième formulation, on peut prendre \\(a\\), un vecteur propre normé associé à \\(\\lambda_{1}\\) la première valeur propre de \\(S^{-1} B\\). Notons que comme \\[S^{-1/2} B S^{-1/2} c = \\lambda c \\quad\\text{et}\\quad a = S^{-1/2} c,\\] alors \\[S^{-1/2} B a = \\lambda S^{1/2} a \\Rightarrow S^{-1} B a = \\lambda a.\\] Les valeurs propres de \\(S^{-1} B\\) et de \\(S^{-1/2} B S^{-1/2}\\) sont donc les mêmes.\nLa fonction discriminante de Fisher est donc \\[f(x) = a^{\\top} (x - \\overline{X}),\\] où \\(a\\) est le vecteur propre normé associé à la plus grande valeur propre de \\(S^{-1} B\\). Les scores \\(U_i = a^{\\top} (X_{i} - \\overline{X})\\) sont les scores linéaires en \\(X_i\\) qui ont le rapport (variance inter) / (variance intra) le plus élevé. On peut aussi prendre \\(U_i = a^{\\top} X_i\\), car ajouter la même constante à toutes les observations \\(i = 1, \\dots, n\\) ne change rien.\nPuisque la matrice \\(S^{-1/2} B S^{-1/2}\\) est symétrique et définie positive, ses valeurs propres sont toutes réelles et positives. De plus, on a que \\(S^{-1} B a - \\lambda_{1} a\\). Ainsi, \\[B a = \\lambda_{1} S a \\Rightarrow a^{\\top} B a = \\lambda_{1} a^{\\top} S a \\Rightarrow \\lambda_{1} = \\frac{a^{\\top} B a}{a^{\\top} S a}.\\]\nOn a donc \\(0 \\leq \\lambda_{1} \\leq 1\\). La valeur propre \\(\\lambda_{1}\\) peut donc être vue comme le pouvoir discriminant de \\(f\\):",
    "crumbs": [
      "Modules",
      "05 - Supervisée",
      "Discriminant"
    ]
  },
  {
    "objectID": "contents/supervised/02-discriminant.html#example",
    "href": "contents/supervised/02-discriminant.html#example",
    "title": "Discriminant",
    "section": "Example",
    "text": "Example",
    "crumbs": [
      "Modules",
      "05 - Supervisée",
      "Discriminant"
    ]
  },
  {
    "objectID": "contents/supervised/05-ensemble.html",
    "href": "contents/supervised/05-ensemble.html",
    "title": "Ensemble",
    "section": "",
    "text": "On se rend compte qu’en présence de grands jeux de données, il est plus facile d’avoir une classification performante en combinant les prévisions de plusieurs classificateurs faibles qu’en construisant un seul classificateur très complexe. On appelle méthode d’ensemble une méthode qui consiste à déduire une prévision unique à partir des prévisions de plusieurs modèles.\nIl existe plusieurs stratégies pour combiner les prévisions de modèles multiples : * Moyenne des probabilités prescrites par chaque modèle. * Vote de majorité * Approche de type “sélection de variables” utilisée en régression, mais on prend un modèles au lieu d’une variable. * Bagging, boosting, forêts aléatoires",
    "crumbs": [
      "Modules",
      "05 - Supervisée",
      "Ensemble"
    ]
  },
  {
    "objectID": "contents/supervised/05-ensemble.html#bagging",
    "href": "contents/supervised/05-ensemble.html#bagging",
    "title": "Ensemble",
    "section": "Bagging",
    "text": "Bagging\nLa méthode consiste à\n\nSélectionner \\(B\\) échantillons “bootstrap” (tirage avec remise).\nConstruire un arbre de classification avec chacun des échantillons “bootstrap”\nPrédire la classe d’une nouvelle observation avec chacun des \\(B\\) arbres\nAttribuer à la nouvelle observation la classe dans laquelle elle est prédite le plus souvent. On peut calculer un score qui est la moyenne des probabilités prédites par chaque arbre.\n\nLa méthode du bagging augmente la stabilité des prédictions. Cependant, il est plus difficile d’interpréter l’importance de chaque variable dans le processus de classification. On peut procéder ainsi: * additionner la réduction de l’indice de Gini pour toutes les divisions basées sur une certaine variable; * calculer la moyenne de ces réductions sur tous les arbres * exprimer la moyenne de réduction de chaque variable en pourcentage de la moyenne de réduction maximale observée sur l’ensemble des variables.",
    "crumbs": [
      "Modules",
      "05 - Supervisée",
      "Ensemble"
    ]
  },
  {
    "objectID": "contents/supervised/05-ensemble.html#forêts-aléatoires",
    "href": "contents/supervised/05-ensemble.html#forêts-aléatoires",
    "title": "Ensemble",
    "section": "Forêts aléatoires",
    "text": "Forêts aléatoires\nTrès similaire à la méthode du bagging; la méthode des forêts aléatoires considère elle aussi \\(B\\) arbres obtenus à partir de \\(B\\) échantillons bootstrap.\nL’idée est que lors de la construction des \\(B\\) arbres, dans la méthode des forêts aléatoires avant chaque division on choisit aléatoirement \\(m&lt;p\\) des variables \\(X_1,\\ldots,X_p\\) et on ne considère que ces \\(m\\) variables pour la division optimale. Un choix commun est \\(m\\approx\\sqrt{p}\\).\nL’avantage de procéder ainsi est de décorréler les arbres obtenus, puisque le gros point faible de la méthode du bagging est que les \\(B\\) arbres obtenus se ressemblent souvent beaucoup.",
    "crumbs": [
      "Modules",
      "05 - Supervisée",
      "Ensemble"
    ]
  },
  {
    "objectID": "contents/supervised/05-ensemble.html#boosting",
    "href": "contents/supervised/05-ensemble.html#boosting",
    "title": "Ensemble",
    "section": "Boosting",
    "text": "Boosting\nL’idée derrière cette approche est de créer des arbres de faon successive en donnant un poids plus élevé aux observations mal classées par les arbres précédents. Il est important d’utiliser des arbres peu performants (très, très simples)} et de laisser le boosting déterminer les bons poids à fournir à ces arbres.\nIl existe plusieurs variantes à ce type d’approche * boosting adaptatif (AdaBoost); * boosting par descente du gradient (gradient boosting).",
    "crumbs": [
      "Modules",
      "05 - Supervisée",
      "Ensemble"
    ]
  },
  {
    "objectID": "contents/supervised/05-ensemble.html#descente-du-gradient-stochastique",
    "href": "contents/supervised/05-ensemble.html#descente-du-gradient-stochastique",
    "title": "Ensemble",
    "section": "Descente du gradient stochastique",
    "text": "Descente du gradient stochastique\nOn doit choisir un nombre \\(M\\) d’itérations, la taille \\(J\\) de l’arbre de classification à construire à chaque itération (on suggère quelque chose de petit, comme \\(4\\le J\\le 8\\)), la fraction \\(f\\) des données d’entrainement à échantillonner “au hasard” à chaque itération (on suggère \\(0.5\\le f\\le 0.8\\)).\nOn construit un premier arbre de taille \\(J\\) au jeu de données, disons \\(T_1\\). Pour \\(m=2,\\ldots,M\\): * On calcule les erreurs de prévisions (pseudo-résidus) obtenus avec l’arbre \\(T_{m-1}\\) pour chaque observation. * On échantillonne une fraction \\(f\\) du jeu de données d’entrainement et on construit un nouvel arbre de taille \\(J\\), disons \\(t_m\\), pour ces données, mais ce coup-ci en utilisant les pseudo-résidus comme variable à prédire. * On calcule un multiplicateur \\(\\gamma_m\\) qui minimise une certaine fonction de perte. * On pose \\(T_m=T_{m-1}+\\gamma_mt_m\\).\n\nLe mode d’échantillonnage aléatoire à l’étape 3: donner une probabilité d’être sélectionnée plus élevée aux observations pour lesquelles le pseudo-résidu est plus loin de 0\nLa fonction de perte calculée à l’étape 4, la façon dont elle est optimisée, le poids qu’elle donne à chaque observation\nInclusion de pénalités de régularisation pour prévenir le sur-ajustement\nImplémentation informatique (XGBoost vs LightGBM, utilisation de GPU, etc.)",
    "crumbs": [
      "Modules",
      "05 - Supervisée",
      "Ensemble"
    ]
  },
  {
    "objectID": "contents/supervised/05-ensemble.html#tuning",
    "href": "contents/supervised/05-ensemble.html#tuning",
    "title": "Ensemble",
    "section": "Tuning",
    "text": "Tuning\nToute m'ethode de classification requiert qu’on règle (tune) la valeur des hyper-paramètres.\nMême si ce n’est pas un théorème, on semble très souvent observer le phénomène que plus la méthode est puissante, plus son réglage est difficile (p.ex. plusieurs hyper-paramètres, sensibilité à la valeur des hyper-paramètres, etc.)\n\nAnalyse discriminante: linéaire ou quadratique? ; probabilités a priori\nArbres de classification: indice d’impureté ; nombre de feuilles maximal ; profondeur maximale ; nombre minimal d’observations dans un noeud avant de tenter un scission ; nombre minimal d’observations dans une feuille\nBagging: Arbres de classification + nombre d’échantillons bootstrap (\\(B\\))\nForêts aléatoires: Bagging + nombre de variables à considérer pour chaque division (\\(m\\))\nBoosting: Arbres de classification + fraction du jeu de données à échantillonner (\\(f\\)) + quelques autres hyper-paramètres selon la variante …",
    "crumbs": [
      "Modules",
      "05 - Supervisée",
      "Ensemble"
    ]
  },
  {
    "objectID": "contents/01-introduction.html",
    "href": "contents/01-introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "Slides: link",
    "crumbs": [
      "Modules",
      "01 - Introduction"
    ]
  },
  {
    "objectID": "contents/01-introduction.html#quest-ce-que-lanalyse-de-données",
    "href": "contents/01-introduction.html#quest-ce-que-lanalyse-de-données",
    "title": "Introduction",
    "section": "Qu’est-ce que l’analyse de données ?",
    "text": "Qu’est-ce que l’analyse de données ?\nL’analyse de données est un ensemble de méthodes permettant de retirer de l’information d’un jeu de données. On parle aussi d’apprentissage statistique (statistical learning). L’idée est d’utiliser des modèles statistiques pour comprendre comment les données sont structurées et comment elles intéragissent l’une avec l’autre.\n\n\n\n\n\n\nExemple\n\n\n\nImaginons que vous êtes employé par l’Organisation des Nations Unies (ONU). Votre mission est d’analyser l’espérance de vie à travers le monde. Pour cela, vous disposez d’une mesure de l’espérance de vie dans chaque pays membre de l’ONU, bien sûr, mais aussi le PIB par habitant, les montants des dépenses liés à la santé, le taux de fertilité, le taux d’urbanisation, le niveau d’éducation du pays, etc. Le but de l’analyse de données est de trouver des liens entre ses différentes variables et la variable d’intérêt, l’espérance de vie, de visualiser ces données, et éventuellement de prédire l’espérance de vie à partir des autres variables.",
    "crumbs": [
      "Modules",
      "01 - Introduction"
    ]
  },
  {
    "objectID": "contents/01-introduction.html#objectifs-du-cours",
    "href": "contents/01-introduction.html#objectifs-du-cours",
    "title": "Introduction",
    "section": "Objectifs du cours",
    "text": "Objectifs du cours\nDans ce cours, on cherche à introduire des méthodes qui permettent une étude d’un jeu de données de “haute dimension” (dans le sens où l’on ne peut pas faire un simple graphique de l’ensemble des variables pour chaque observation) sans avoir recours à un modèle probabiliste. Les différentes techniques que l’on va voir peuvent servir à:\n\nvisualiser les données;\nréduire la dimension des données;\nidentifier certains liens entre les variables;\ndiviser le jeu de données en groupes/classes.\n\nCe cours n’a pas vocation à être exhaustif, dans le sens de présenter toutes les méthodes possibles. Ce cours n’a pas non plus vocation à être à l’état de l’art, dans le sens où on ne s’intéressera pas aux derniers développements en apprentissage machine. Ce cours n’est pas non plus un cours de programmation.\nPour finir cette indroduction, voici un passage de Statistical Rethinking de Richard McElreath (McElreath 2020) trouvant résonnance dans ce cours.\n\nStatistics courses […] tend to resemble horosscopes. There are two senses to this resemblance. First, in order to remain plausibly correct, they must remain tremendously vague. This is because the targets of the advice, for both horoscopes and statistical advice, are diverse. But only the most general advice applies to all cases. A horoscope uses only the basic facts of birth to forecast life events, and a […] statistical guide uses only the basic facts of measurement and design to dictate a model. It is easy to do better, once more detail is available. In the case of statistical analysis, it is tipically only the scientist whho can provide that detail, not the statistician. Second, there are strong incentives for both astrologers and statisticians to exaggerate the power and importance of their advice. No one likes an astrologer who forecasts doom, and few want a statistician who admits the answers as desired are not in the data as collected. Scientists desire results, and they will buy and attend to statisticians and statistical procedures that promise them. What we end up with is too often horoscopic: vague and optimistic, but still claiming critical importance.\n\n\n\n\nMachine learning (xkcd:1838).",
    "crumbs": [
      "Modules",
      "01 - Introduction"
    ]
  },
  {
    "objectID": "contents/04-dimension.html",
    "href": "contents/04-dimension.html",
    "title": "Dimension",
    "section": "",
    "text": "Slides: link",
    "crumbs": [
      "Modules",
      "04 - Dimension"
    ]
  },
  {
    "objectID": "contents/supervised/03-tree.html",
    "href": "contents/supervised/03-tree.html",
    "title": "Arbres",
    "section": "",
    "text": "Les arbres de régression et de classification (Classification And Regression Trres, CART) font partie des méthodes d’apprentissage supervisée. On dispose d’observations qui appartiennent à \\(\\mathbb{R}^p\\), soit \\(X_{1}, \\dots, X_p\\). Un arbre de classification cherche à assigner une observation à l’un des \\(K\\) groupes sur la base de \\(X_{1}, \\dots, X_p\\). Un arbre de régression cherche à prédire la valeur d’une valeur numérique continue \\(Y\\) à partir de \\(X_{1}, \\dots, X_p\\).\nLa construction d’arbres, on partitionne l’ensemble \\(R^p\\) en hyper-rectangles en effectuant des divisions binaires successives en fonction de la valeur d’une des \\(p\\) variables (algorithme de dédoublements binaires successifs).\n\nOn effectue des divisions binaires successives de l’ensemble \\(R^p\\) en fonction de la valeur d’une variable. Le choix de la variable et le choix de la limite pour la division binaire sont déterminés par un certain critère.\nOn cesse d’effectuer des divisions lorsqu’un critère d’arrêt est rencontré (nombre minimal d’observations par feuille atteint, nombre de feuilles désirées atteint, etc.)\nChaque sous-division finale de l’ensemble \\(\\mathbb{R}^p\\)est appelée “feuille” ou “noeud terminal”. On prédit une catégorie à chaque observation selon la feuille à laquelle elle appartient.\n\nToutes les observations dans une même “feuille” reoivent la même prédiction, soit la classe la plus fréquente parmi les données du jeu d’entrainement qui aboutissent dans cette feuille.\n\n\n\n\n\n\nRemarque\n\n\n\nL’algorithme de partitions binaires successives est un algortihme glouton qui ne nous garantit pas l’arbre optimal…\nSupposons que nous désirions partionner \\(\\mathbb{R}^p\\) en J hyper-rectangles (donc un arbre à \\(J\\) feuilles) et soit \\(c_j\\) un critère de “coût” pour les observations qui sont mal classées dans la région \\(j\\). On aimerait pouvoir trouver les hyper-rectangles \\(R_1, \\dots, R_J\\) tels que \\(\\sum_{j = 1}^{J} c_j\\) soit minimal, mais il n’est pas computationnellement possible de le faire !\n\n\n\nAlgortihme\nPour chaque noeud de l’arbre, on doit décider si on le découpe en deux et si oui, en utilisant quelle valeur de quelle variable :\n\nPour chacune des \\(p\\) variables et pour chaque point de coupure possible pour cette variable, calculer l’amélioration du critère de qualité et choisir la variable et le point de coupure résultant en la meilleur amélioration du critère.\nCréer une division binaire de l’arbre à l’aide de la variable et du point de coupure choisis en 1.\nRépéter 1 et 2 jusqu’à ce que l’arbre ait le nombre de feuilles désiré ou que le nombre minimal d’observations soit atteint dans chaque feuille.\n\n\n\nCritères\nSoit \\(\\widehat{p}_{jk}\\), la proporiton d’observations de la \\(j\\)e région de l’ensemble \\(\\mathbb{R}^p\\) qui appartiennent à la \\(k\\)e classe.\nTrois critères sont habituellement utilisés pour déterminer les divisions optimales à chaque étape de la construction de l’arbre:\n\nTaux d’erreurs de classification \\[E_j = 1 - \\max_k \\widehat{p}_{jk}.\\] Pas assez sensible pour construire l’arbre.\nIndice de Gini \\[G_j = \\sum_{k = 1}^{K} \\widehat{p}_{jk} (1 - \\widehat{p}_{i j}).\\] Plus c’est faible, mieux c’est.\nEntropie croisée \\[D_j = - \\sum_{k = 1}^{K}  \\widehat{p}_{jk} \\log(\\widehat{p}_{m k}).\\] Plus c’est faible, mieux c’est.\n\nLe gain d’information est ce que l’on cherche à maximiser lorque l’on choisit comment scinder en deux un noeud donné. Par example, avec l’indice de Gini, le gain d’information est donné par \\[\\text{indice de Gini avant la division} - \\sum_{j = 1}^{2} \\frac{n_j}{n_{1} + n_{2}} G_j,\\] où \\(j = 1, 2\\) dénote les deux morceaux créés par la division, \\(n_j\\) est le nombre d’observations qui tombent dans le morceau \\(j\\) et \\(G_j\\) est l’indice de Gini pour le morceau \\(j\\).\n\n\nChoix du nombre de divisions/feuilles\nL’algorithme va faire croître l’arbre tant qu’on n’aura pas atteint un nombre de feuilles fixé ou tant que le nombre de d’observations dans chaque feuille ne sera pas trop petit. Mais, pas assez de feuilles veut dire manque d’ajustement car classification trop simpliste et inadéquate. Et trop de feuilles n’est pas mieux, car on fait du sur-ajustement et l’arbre n’aura pas une bonne performance sur un jeu de validation.\nÉlage des arbres de classification (pruning): la stratégie commune en pratique est de laisser croître l’arbre et ensuite de l’élaguer de faon à atteindre un compromis entre ajustement aux données d’entrainement et taille de l’arbre (élagage coût-complexité). Si \\(| T |\\) est la taille (nombre de feuilles) de l’arbre \\(T\\), on veut \\[\\min \\sum_{j = 1}^{|T|} c_j + \\alpha | T |,\\] où \\(\\alpha \\geq 0\\) est un hyper-paramètre qui contrôle le compromis et que l’on chosit par validation croisée.\n\n\nAvantages\n\nFonctionnent aussi bien avec des variables continues que catégorielles.\nAucune hypothèse a priori n’est faite sur la distribution des données.\nRobuste aux données extrêmes.\nFaciles à interpréter.\nTiennent implicitement compte des interactions possibles entre les variables.\nSélectionnent implicitement les variables importantes.\nPermettent d’obtenir un modèle non linéaire.\n\n\n\nPoints faibles\n\nNécessitent un grand nombre de données.\nPeuvent être instables dans les résultats qu’elle produit.\nNe fonctionnent pas très bien lorsque certaines des \\(q\\) classes sont rares ou que certaines des modalités de \\(X_j\\) catégorielles sont rares.\n\n\n\nExample",
    "crumbs": [
      "Modules",
      "05 - Supervisée",
      "Arbres"
    ]
  },
  {
    "objectID": "contents/misc/good-practices-R.html",
    "href": "contents/misc/good-practices-R.html",
    "title": "Bonnes pratiques en R",
    "section": "",
    "text": "Cette page est basée sur un document qu’Aurélien Nicosia (ULaval) a créé en 2023 appelé “Bonnes pratiques de programmation en R”. Celui-ci a été mis à jour.\nR étant “juste” un langage de programmation, il est techniquement possible d’utiliser un simple éditeur de texte pour écrire du R et un terminal pour lancer le code. Il est cependant bien plus commode—en particulier, pour les débutants—d’utiliser un éditeur de code comme RStudio pour lancer son code. En effet, celui-ci permet de lancer son code de façon intéractive, de pouvoir voir son environnement de travail, de tester son code, …\nDe plus, il vaut mieux s’assurer de travailler avec la dernière version de R et des packages dont nous avons besoin. Ainsi, nous risquons moins de rencontrer des bogues et nous pouvons profiter des dernières fonctionnalités. Un point d’attention, cependant, je vous déconseillerais de changer la version de R ou des packages une fois que vous avez commencé à travailler sur quelquechose. En effet, ce changement pourrais introduire des bogues au milieu de l’analyse (il faut donc toujours reporter les versions des packages utilisés pour la reproducibilité). Il est possible d’avoir un résumé de l’environnement de travail avec les fonctions systemInfo() et packageVersion(pkg).\nConcernant la documentation, le package roxygen permet de convertir les commentaires de vos fonctions et packages en une documentation propre.\nUne syntaxe uniforme rend un code beaucoup plus facile à lire et à comprendre. Un guide de style énonce des normes pour avoir une syntaxe uniforme. Le guide de style du tidyverse est recommandé en R. Avant de présenter quelques conventions de style, notons qu’il est possible de modifier rapidement la mise en forme d’un bout de code R dans RStudio dans le menu “Code -&gt; Reformat Selection”.\nRetour à la ligne et indentation\nUne façon simple de rendre som code plus lisible est d’y insérer des retours à la ligne et des indentations appropriés. Par exemple, supposons que nous avons la chaîne de caractère suivante :\n\ntext &lt;- \"Ceci est un example\"\n\nNous souhaitons corriger deux fautes dans cette phrase : le mot “example” écrit en anglais plutôt qu’en français et le point manquant à la fin de la phrase. Ceci peut se faire avec l’instruction :\n\npaste0(gsub(pattern = \"example\", replacement = \"exemple\", x = text), \".\")\n\nCette instruction comporte un appel de fonction imbriqué dans un autre. Elle est bien plus facile à lire comme suit :\n\npaste0(\n  gsub(\n    pattern = \"example\",\n    replacement = \"exemple\",\n    x = text),\n  \".\"\n)\n\nOpérateur d’assignation\nEn R, on utilise &lt;- pour assigner une valeur à une variable et on utilise = pour passer des valeurs à des arguments dans un appel de fonctions.\nConventions de noms\nLe guide de style du tydiverse préconise l’utilisation de lettres minuscules, de nombres et de l’underscore _ pour nommer variables et fonctions. Les underscores sont utilisés pour séparer les mots dans un nom. Bien que l’on puisse trouver d’autres conventions, celles-ci sont à éviter. Dans tous les cas, il est important de choisir une convention et de la respecter. De plus, il est préférable d’éviter les accents dans les noms de variables.\nOrganisation du code\nLorsque le code commence à devenir long, il devient avantageux de le séparer en plusieurs fichiers. Par exemple, on peut avoir un fichier par partie de l’analyse (un pour le nettoyage des données, un pour l’analyse, un pour la visualisation, …). De plus, une analyse de données n’est généralement pas constitué uniquement de code R, e.g. fichiers de code C++, fichiers de données, fichiers du configuration, etc. Il est donc recommandé des créer des sous-dosssiers regroupant les fichiers du même type. Les projets RStudio sont parfaits pour rassembler au même endroit tous les fichiers relatifs au projet. De plus, ils permettent de faciliter le travail sur plusieurs projets simultanément en gérant le passage d’un répertoire de travail à un autre.\nQuelques trucs à faire\n\nRédiger son code dans un script et l’enregistrer fréquemment. Cela permet d’éviter de perdre la trace de certaines instructions importantes parce qu’elles ont été écrites directement dans la console.\nIl est préférable de débuter toute session de travail en R avec un environnement de travail vide. Pour ce faire, il faut désactiver la restauration automatique d’une image de session dans les paramètres. Cela permet d’être conscient de la présence des différents objets dans l’environnement de travail.\nNe pas utiliser la fonction load lorsque l’environnement de travail n’est pas vide. Cela permet de ne pas modifier un objet de l’environnement de travail en l’écrasant.\nNe pas utiliser la fonction attach. Cela permet de ne pas modifier le chemin de recherche des fichiers.\nSauvegarder les options et paramètres graphiques avant de les modifier.\nNe pas utiliser T et F à la place de TRUE et FALSE."
  },
  {
    "objectID": "contents/misc/good-practices-python.html",
    "href": "contents/misc/good-practices-python.html",
    "title": "Bonnes pratiques en Python",
    "section": "",
    "text": "Cette page est inspirée sur ce document.\nDe même que R, Python est “juste” un langage de programmation, il est donc possible d’écrire du code Python dans un éditeur de texte et de lancer ce code via un terminal. Bien que ce soit faisable, il est plus simple d’utilser un éditeur de code. Bien qu’il soit techniquement possible d’utiliser RStudio comme éditeur pour Python, je vous recommanderais dans utiliser spécialiser comme PyCharm. Le format notebook est aussi très populaire en Python. Dans ce cas, je vous conseillerais la librarie marimo qui permet de transformer ses fichiers .py en notebook.\nDe plus, il vaut mieux s’assurer de travailler avec la dernière version stable de Python comptatible avec les packages les utilisés (e.g. numpy, matplotlib, sklearn). En effet, très souvent, il y a un délai pour que les dernières versions des packages soient comptatibles avec la dernière version de Python. Il est donc important de partager la version des packages utilisés avec l’analyse.\nConcernant la documentation, le standard en Python est de suivre les recommendations du PEP 257. La libraries Sphinx permet de générer une documentation à partir des docstring.\nUne syntaxe uniforme rend un code beaucoup plus facile à lire et à comprendre. Un guide de style énonce des normes pour avoir une syntaxe uniforme. Le guide de style PEP 8 est recommandé en Python.\nRetour à la ligne et indentation\nL’indentation fait partie de Python, i.e. les blocks if...else et for sont définis grâce aux indentations. Il est possible d’utiliser un tab ou des espaces. Le choix vous appartient mais il est important d’être consistant dans votre code.\nConvention de noms\nL’utilisation des noms de variables avec un seul caractère est à éviter. Sinon, les variables, fonctions, méthodes, packages et modules sont nommés avec des lettres minuscules, des nombres et l’underscore _. Les classes et exceptions doivent être nommés avec des majuscules pour séparer les mots (UneClasse). Les constants sont en majuscules.\nOrganisation du code\nDe même que pour R, lorsque le code commence à devenir long, il devient avantageux de le séparer en plusieurs fichiers. Par exemple, on peut avoir un fichier par partie de l’analyse (un pour le nettoyage des données, un pour l’analyse, un pour la visualisation, …). On peut aussi créer plusieurs sous-dosssiers.\nQuelques trucs à faire\n\nNe pas faire de comparaison à True, False or None.\n\n\nif attr:\n    print(\"True!\")\n\nif not attr:\n    print(\"False!\")\n\nif attr is None:\n    print(\"None!\")\n\n\nUtiliser la compréhension de liste lorsque cela est possible.\n\n\na = [3, 4, 5]\nb = [i for i in a if i &gt; 4]\n\n\nCharger un fichier avec with. Cela permet d’être sur que la connexion avec le fichier est fermé une fois qu’il a été lu.\n\n\nwith open(\"file.txt\") as f:\n    read(f)\n\n\nUtiliser un maximum de 80 caractères par ligne.\nUtiliser des parenthèses pour aller à la ligne dans les longues chaînes de caractères."
  },
  {
    "objectID": "contents/dimension/01-pca.html",
    "href": "contents/dimension/01-pca.html",
    "title": "ACP",
    "section": "",
    "text": "Il y a plusieurs raisons de vouloir changer le dimension des données. Il est possible que la dimension des données soit trop importante pour avoir une visualisation intéressante, que la dimension actuelle ne permette pas une bonne séparation des classes dans les données, etc.",
    "crumbs": [
      "Modules",
      "04 - Dimension",
      "ACP"
    ]
  },
  {
    "objectID": "contents/dimension/01-pca.html#changer-de-dimension---pourquoi-faire",
    "href": "contents/dimension/01-pca.html#changer-de-dimension---pourquoi-faire",
    "title": "ACP",
    "section": "",
    "text": "Il y a plusieurs raisons de vouloir changer le dimension des données. Il est possible que la dimension des données soit trop importante pour avoir une visualisation intéressante, que la dimension actuelle ne permette pas une bonne séparation des classes dans les données, etc.",
    "crumbs": [
      "Modules",
      "04 - Dimension",
      "ACP"
    ]
  },
  {
    "objectID": "contents/dimension/01-pca.html#analyse-en-composantes-principales",
    "href": "contents/dimension/01-pca.html#analyse-en-composantes-principales",
    "title": "ACP",
    "section": "Analyse en composantes principales",
    "text": "Analyse en composantes principales\nL’analyse en composantes principales (ACP) est une méthode permettant de réduire la dimension d’un jeu de données tout en conservant le plus d’information possible. Cette méthode est utilisée lorsque l’on a \\(n\\) observations de \\(p\\) variables continues avec \\(p\\) trop “grand” pour nos besoins.\nPourquoi l’ACP est-elle utilisée ?\n\nVisualisation d’un jeu de données;\nRéduction du nombre de variables de \\(p\\) à \\(p^{\\prime} \\ll p\\) pour faciliter la construction de modèle. Exemples: analyse de texte, analyse de données génétique;\nEffectuer une rotation d’axes pour simplifier la structure de corrélation;\nCompression de données.\n\nLa méthode a été introduite par H. Hotelling dans (Hotelling 1933).\nVisualiser, comprendre, modéliser, classifier des données sont toutes des tâches beacoup simples à accomplir si le nomnre de variables dans un jeu de données est faible. Si un jeu de données comprend un grand nombre de variables, une première question que l’on peut se poser est: “Est-il possible de réduire la dimension du problème sans trop perdre d’information ?”. En omettant tout simplement des variables, on risque de perdre beaucoup d’information utile. Une meilleure solution consiste à trouver des combinaisons linéaires des variables en vue de conserver le maximum d’information sur le jeu de données.\n\n\n\n\n\n\nExample\n\n\n\n\nComparer des équipes de hockey sur la base de six statistiques de fin de saison.\nComparer la criminalité entre états sur la base des taux de sept types de crimes différents.\nCompresser des images formées de \\(1084 \\times 1084\\) pixels.\nIdentifier le nombre de variantes d’un type de tumeur à partir du degré d’expression de millions de gènes.\n\n\n\n\nLes maths\nSoit un vecteur aléatoire composé de \\(p\\) variables \\(X = \\left( X_{1}, \\dots, X_p \\right)\\) ayant comme matrice de variance \\(\\Sigma\\). On aimerait définir une première composante principale, \\[Y_{1} = \\alpha_{1}^{\\top} X = \\sum_{i = 1}^{p} \\alpha_{1i}X_i,\\] de sorte que la variance de \\(Y_{1}\\) soit maximale. L’idée est simple: on désire combiner \\(p\\) variables en une seule, mais en “capturant” la plus grande partie possible de la variabilité.\nIl faut d’abord ajouter une contrainte sur \\(\\alpha_{1}\\), puisque sinon on n’aurait qu’à prendre \\(\\alpha_{1i} = \\pm \\infty\\) et on aurait \\(\\mathrm{Var}(Y_{1}) = +\\infty\\) ce qui est définitivement maximal ! On verra qu’il est pratique de contraindre \\(\\alpha_{1}\\) de sorte qu’il ait une norme égale à \\(1\\).\n\nCalcul de la première composante:\n\n\\[\\mathrm{Var}(Y_1) = \\alpha_1^\\top \\Sigma \\alpha_{1} \\]\nLe problème est donc de maximiser \\[F(\\alpha_{1} = \\alpha_{1}^\\top \\Sigma \\alpha_1, \\quad\\text{s.c.}\\quad \\alpha_1^\\top \\alpha_1 = 1.\\]\nOn peut récrire ce problème à l’aide des multiplicateurs de Lagrange, soit maximiser \\[F(\\alpha_1, \\lambda) = \\alpha_1^\\top \\Sigma \\alpha_1 - \\lambda (\\alpha_1^\\top \\alpha_1 -1 ),\\] où \\(\\lambda\\) est un multiplicateur de Lagrange.\nPour solutionner ce problème, on dérive \\(F\\) par rapport à \\(\\alpha_{1}\\) et à \\(\\lambda\\).\n\\[\\begin{cases}\n\\frac{\\partial F(\\alpha_{1}, \\lambda)}{\\partial \\alpha_{1}} = 2 \\Sigma \\alpha_{1} - 2 \\lambda \\alpha_{1} \\\\\n\\frac{\\partial F(\\alpha_{1}, \\lambda)}{\\partial \\lambda} = 1 - \\alpha_{1}^\\top \\alpha_{1}\n\\end{cases}\n.\\]\nEnsuite, on égalise à \\(0\\), ce qui donne: \\[\\begin{cases}\n\\Sigma \\alpha_{1} = \\lambda \\alpha_{1} \\\\\n\\alpha_{1}^\\top \\alpha_{1} = 1\n\\end{cases}\n.\\] La seconde équation est bien entendue notre contrainte. La première équation est celle qui nous intéresse. En utilisant cette équation et la définition des éléments propres, on déduit que\n\n\\(\\alpha_{1}\\) est un vecteur propre (normé) de \\(\\Sigma\\);\n\\(\\lambda\\) est la valeur propre correspondante.\n\nOn a donc que \\[\\mathrm{Var}(Y_{1}) = \\alpha_{1}^\\top \\Sigma \\alpha_{1} = \\lambda \\alpha_{1}^\\top \\alpha_1 = \\lambda.\\] Puisque l’on veut maximiser cette quantité, on conclut que:\n\n\\(\\lambda = \\lambda_{1}\\), la plus grande valeur propre de \\(\\Sigma\\);\n\\(\\alpha_{1}\\), le vecteur propre normé correspondant.\n\n\nCalcul de la deuxième composante:\n\nOn poursuit simultanément deux objectifs:\n\nConserver le maximum de variation présente dans \\(X\\);\nSimplifier la structure de dépendancce pour faciliter l’interprétation et assurer la stabilité numérique d’éventuelles méthodes qui utiliseront les composantes principales obtenues.\n\nÉtant donné \\(Y_{1}\\), la deuxième composante principale \\(Y_{2} = \\alpha_{2}^\\top X\\) est définie telle que\n\n\\(\\mathrm{Var}(Y_{2}) = \\alpha_{2}^\\top \\Sigma \\alpha_{2}\\) est maximale;\n\\(\\alpha_{2}^\\top \\alpha_{2} = 1\\);\n\\(\\mathrm{Cov}(Y_{1}, Y_{2}) = 0\\).\n\nOn a que \\[\\mathrm{Cov}(Y_{1}, Y_{2}) = \\mathrm{Cov}(\\alpha_{1}^\\top X, \\alpha_{2}^\\top X) = \\alpha_{1}^\\top \\Sigma \\alpha_{2} = \\alpha_{2}^\\top \\Sigma \\alpha_{1} = \\lambda_{1} \\alpha_{2}^\\top \\alpha_{1}.\\]\nOn cherche donc le vecteur \\(\\alpha_{2}\\) qui maximise: \\[F(\\alpha_{2}, \\lambda, \\kappa) = \\alpha_{2}^\\top \\Sigma \\alpha_{2} - \\lambda (\\alpha_{2}^\\top \\alpha_{2} - 1) - \\kappa (\\alpha_{2}^\\top \\alpha_{1} - 0).\\]\nDe même que pour la première composante, on dérive \\(F\\) par rapport à \\(\\alpha_{2}\\), \\(\\lambda\\) et \\(\\kappa\\).\n\\[\\begin{cases}\n\\frac{\\partial F(\\alpha_{2}, \\lambda, \\kappa)}{\\partial \\alpha_{2}} = 2 \\Sigma \\alpha_{2} - 2 \\lambda \\alpha_{2} - \\kappa \\alpha_1 \\\\\n\\frac{\\partial F(\\alpha_{2}, \\lambda, \\kappa)}{\\partial \\lambda} = 1 - \\alpha_{2}^\\top \\alpha_{2} \\\\\n\\frac{\\partial F(\\alpha_{2}, \\lambda, \\kappa)}{\\partial \\kappa} = - \\alpha_2^\\top \\alpha_1\n\\end{cases}\n\\]\nEn égalisant les équations à \\(0\\), on retrouve les deux équations des contraintes, ainsi que\n\\[2 \\Sigma \\alpha_{2} - 2 \\lambda \\alpha_{2} - \\kappa \\alpha_{1} = 0.\\]\nEn multipliant cette équation à gauche et à droite par \\(\\alpha_{1}^\\top\\), on trouve\n\\[2 \\alpha_{1}^\\top \\Sigma \\alpha_{2} - 2 \\alpha_{1}^\\top \\lambda \\alpha_{2} - \\kappa \\alpha_{1}^\\top \\alpha_{1} = 0.\\]\nOr \\(\\alpha_{1}^\\top \\Sigma = \\lambda_{1} \\alpha_{1}^\\top\\), et \\(\\lambda_{1}^\\top \\alpha_{1} = 1\\), donc \\[2 \\alpha_{1}^\\top \\lambda \\alpha_{2} - 2 \\alpha_{1}^\\top \\lambda \\alpha_{2} - \\kappa \\alpha_{1}^\\top \\alpha_{1} = 0 \\implies - \\kappa = 0.\\]\nEn substituant ce résulat, on obtient \\[\\Sigma \\alpha_{2} = \\lambda \\alpha_{2}.\\]\net donc \\(\\lambda\\) est une autre valeur propre de \\(\\Sigma\\). Puisque \\[\\mathrm{Var}(Y_{2} = \\alpha_{2}^\\top \\Sigma \\alpha_{2} = \\alpha_{2}^\\top \\lambda \\alpha_{2} = \\lambda,\\] on a que cette variance est maximale si \\(\\lambda = \\lambda_{2}\\), la deuxième plus grande valeur propre de \\(\\Sigma\\), et conséquemment \\(\\alpha_{2}\\) est le vecteur propre normé correspondant.\nOn peut généraliser ce résultat en utilisant des maximisations successives. On en conclut que \\[Y_k = \\alpha_k^\\top X,\\] où \\(\\alpha_k\\) est le vecteur propre normé associé à \\(\\lambda_k\\), la \\(k\\)e plus grande valeur propre de \\(\\Sigma\\).\n\nNotation matricielle:\n\nPour définir simultanément et de facon plus compacte les composantes principales, on pose \\[Y = AX,\\] où \\[A = \\left( \\alpha_{1}, \\dots, \\alpha_{p} \\right) = \\begin{pmatrix}\n  \\alpha_{1, 1} & \\alpha_{2, 1} & \\cdots & \\alpha_{p, 1} \\\\\n  \\alpha_{1, 1} & \\alpha_{2, 2} & \\cdots & \\alpha_{p, 1} \\\\\n  \\vdots & \\vdots & \\ddots & \\vdots \\\\\n  \\alpha_{1, p} & \\alpha_{2, p} & \\cdots & \\alpha_{p, p}\n\\end{pmatrix}.\\]\n\n\n\n\n\n\nPropriétés de \\(A\\)\n\n\n\n\nLes colonnes de la matrice \\(A\\) sont les vecteurs propres de \\(\\Sigma\\);\n\\(A^{\\top} A = A A^{\\top} = I_p\\);\n\\(A^{\\top} = A^{-1}\\);\n\\(\\Sigma A = A \\Lambda\\), où \\(\\Lambda = \\text{diag}(\\lambda_{1}, \\dots, \\lambda_p)\\);\n\\(\\mathrm{Var}(Y) = A^{\\top} \\Sigma A = \\Lambda \\implies \\mathrm{Cov}(Y_i, Y_j) = 0\\) si \\(i \\neq j\\) et \\(\\mathrm{Var}(Y_i) = \\lambda_i \\geq \\mathrm{Var}(Y_j) = \\lambda_j\\) si et seulement si \\(i \\leq j\\).\n\n\n\nUne mesure globale de la variation presente dans les données est donnée par la trace de la matrice \\(\\Sigma\\): \\[\\text{tr}(\\Sigma) = \\text{tr}(\\Lambda) = \\sum_{i = 1}^{p} \\lambda_i.\\]\nLa proportion de variation expliquée par la composante principale \\(Y_i\\) est \\[\\frac{\\lambda_i}{\\lambda_{1} + \\cdots + \\lambda_p}.\\]\nSimilairement, les \\(m\\) première composantes expliquent \\[\\frac{\\sum_{i = 1}^{m} \\lambda_i}{\\sum_{i = 1}^{p} \\lambda_i} \\times 100\\%.\\] de la variabilité dans les variables.",
    "crumbs": [
      "Modules",
      "04 - Dimension",
      "ACP"
    ]
  },
  {
    "objectID": "contents/dimension/02-mca.html",
    "href": "contents/dimension/02-mca.html",
    "title": "ACM",
    "section": "",
    "text": "L’analyse en composantes principales concerne des données continues. Que faire lorsque les variables sont rapportées dans des tableaux de contingences où alors comme réponses à des questionnaires à choix multiples ? Que faire lorsque les variables sont catégorielles ? Il est possible de faire la même chose que pour l’ACP pour des variables catégorielles, cette méthode s’appelle l’analyse des correspondances (AC). Cette méthode permet de représenter graphiquement des tableaux de fréquence. Pour des tableaux de fréquences croisant deux variables, on parlera d’analyse des correspondances bianires (ACB) ou alors d’analyse factorielle des correspondances (AFC). Pour des tableaux de fréquences croisant troix variables ou plus, on parlera d’analyse des correspondances multiples (ACM).\n\n\n\n\n\n\nExamples\n\n\n\nLa boussole électorale de Radio-Canada. Étude sur les clients d’une compagnie de télécom. Relation entre la couleur des yeux et des cheveux.",
    "crumbs": [
      "Modules",
      "04 - Dimension",
      "ACM"
    ]
  },
  {
    "objectID": "contents/dimension/02-mca.html#introduction",
    "href": "contents/dimension/02-mca.html#introduction",
    "title": "ACM",
    "section": "",
    "text": "L’analyse en composantes principales concerne des données continues. Que faire lorsque les variables sont rapportées dans des tableaux de contingences où alors comme réponses à des questionnaires à choix multiples ? Que faire lorsque les variables sont catégorielles ? Il est possible de faire la même chose que pour l’ACP pour des variables catégorielles, cette méthode s’appelle l’analyse des correspondances (AC). Cette méthode permet de représenter graphiquement des tableaux de fréquence. Pour des tableaux de fréquences croisant deux variables, on parlera d’analyse des correspondances bianires (ACB) ou alors d’analyse factorielle des correspondances (AFC). Pour des tableaux de fréquences croisant troix variables ou plus, on parlera d’analyse des correspondances multiples (ACM).\n\n\n\n\n\n\nExamples\n\n\n\nLa boussole électorale de Radio-Canada. Étude sur les clients d’une compagnie de télécom. Relation entre la couleur des yeux et des cheveux.",
    "crumbs": [
      "Modules",
      "04 - Dimension",
      "ACM"
    ]
  },
  {
    "objectID": "contents/dimension/02-mca.html#notation",
    "href": "contents/dimension/02-mca.html#notation",
    "title": "ACM",
    "section": "Notation",
    "text": "Notation\nNotation: Soit \\(K = (k_{ij})\\), où \\(k_{ij}\\) est le nombre d’individus appartenant à la classe \\(i \\in \\{ 1, \\dots, n \\}\\) et à la catégorie \\(j \\in \\{ 1, \\dots, p \\}\\).\nTableau des fréquences relatives: Comme les fréquences sont proportionnelles à la taille d’échantillon \\(n\\), le tableau des fréquences relatives contient plus d’information : \\(F = (f_{ij})\\), dans lequel \\[f_{ij} = \\frac{k_{ij}}{k_{\\bullet\\bullet}} = \\frac{k_{ij}}{\\sum_{l = 1}^{n} \\sum_{m = 1}^{p} k_{lm}}.\\]\nLes marges du tableau correspondent à la somme des colonnes pour chaque ligne et de la somme des lignes pour chaque colonne: \\[\\begin{align}\nf_{i \\bullet} &= \\sum_{j = 1}^{p} f_{ij} = \\frac{k_{i \\bullet}}{k_{\\bullet\\bullet}}, \\quad 1 \\leq i \\leq n; \\\\\nf_{\\bullet j} &= \\sum_{i = 1}^{n} f_{ij} = \\frac{k_{\\bullet j}}{k_{\\bullet\\bullet}}, \\quad 1 \\leq j \\leq p.\n\\end{align}\\]\nLes fréquences relatives estiment des probabilités. Dans le cas d’un tableau de fréquences croisant deux variables, sous l’hypothèse d’indépendance, les fréquences relatives devraient être telles qu’on ne s’éloigne pas trop de la relation \\[f_{ij} = f_{i \\bullet} f_{\\bullet j}, \\quad i \\in \\{ 1, \\dots, n \\}, j \\in \\{ 1, \\dots, p \\}.\\]\nStatistique du \\(\\chi^{2}\\): on peut tester si les différences entre \\(f_{ij}\\) et \\(f_{i \\bullet} f_{\\bullet j}\\) sont significatives à l’aide du test du \\(\\chi^2\\): \\[T = \\sum_{i = 1}^{n} \\sum_{j = 1}^{p} \\frac{\\left( k_{ij} - \\mathbb{E}(k_{ij}) \\right)^2}{\\mathbb{E}(k_{ij})} = \\sum_{i = 1}^{n} \\sum_{j = 1}^{p} \\frac{\\left( k_{ij} - \\frac{k_{i \\bullet}k_{\\bullet j}}{k_{\\bullet\\bullet}} \\right)^2}{\\left( \\frac{k_{i \\bullet} k_{\\bullet j}}{k_{\\bullet\\bullet}} \\right)}.\\]\nSi les variables sont indépendantes, la statistique \\(T\\) dit être proche de \\(0\\).\nOn peut aussi s’intéresser à la contribution de chaque cellule à la valeur globale de la statistique \\(T\\). Cela permet d’obtenir le profil de chaque ligne \\(i\\) et de chaque colonne \\(j\\). \\[\\begin{align}\nL_i &= \\left( \\frac{k_{i 1}}{k_{i \\bullet}}, \\dots, \\frac{k_{i p}}{k_{i \\bullet}} \\right) &= \\left( \\frac{f_{i 1}}{f_{i \\bullet}}, \\dots, \\frac{f_{i p}}{f_{i \\bullet}} \\right) \\\\\nC_j &= \\left( \\frac{k_{1 j}}{k_{\\bullet j}}, \\dots, \\frac{k_{n j}}{k_{\\bullet j}} \\right) &= \\left( \\frac{f_{1 j}}{f_{\\bullet j}}, \\dots, \\frac{f_{n j}}{f_{\\bullet j}} \\right)\n.\n\\end{align}\\]\nLe profil-ligne moyen est donné par \\[\\left( \\sum_{i = 1}^{n} f_{i \\bullet} \\frac{f_{i 1}}{f_{i \\bullet}}, \\dots, \\sum_{i = 1}^{n} f_{i \\bullet} \\frac{f_{i p}}{f_{i \\bullet}} \\right) = \\left( f_{\\bullet 1}, \\dots, f_{\\bullet p} \\right).\\] Le profil-colonne moyen est donné par \\[\\left( \\sum_{j = 1}^{p} f_{{\\bullet j}} \\frac{f_{1 j}}{f_{\\bullet j}}, \\dots, \\sum_{j = 1}^{p} f_{\\bullet j} \\frac{f_{n j}}{f_{\\bullet j}} \\right) = \\left( f_{{1 \\bullet}}, \\dots, f_{n \\bullet} \\right).\\]\nEn cas d’indépendance, on a, pour tout \\(i \\in \\{ 1, \\dots, n \\}\\) et \\(j \\in \\{ 1, \\dots, p \\}\\), \\[\\left( \\frac{f_{i 1}}{f_{i \\bullet}}, \\dots, \\frac{f_{i p}}{f_{i \\bullet}} \\right) = \\left( f_{\\bullet 1}, \\dots, f_{\\bullet p} \\right) \\quad\\text{et}\\quad\n\\left( \\frac{f_{1 j}}{f_{\\bullet j}}, \\dots, \\frac{f_{n j}}{f_{\\bullet j}} \\right) = \\left( f_{{1 \\bullet}}, \\dots, f_{n \\bullet} \\right).\\]\nLa dépendance entre les variables est fonction de la ressemblance entre les profils des lignes et les profils des colonnes. On peut mesurer et visualiser cette de diverses manières.\nOn peut mesurer la distance entre deux profils-lignes avec la distance du \\(\\chi^2\\), qui tient compte de l’importance de chaque colonne: \\[d^2(i, i^\\prime) = \\sum_{j = 1}^{p} \\frac{1}{f_{\\bullet j}} \\left( \\frac{f_{ij}}{f_{i \\bullet}} - \\frac{f_{i^\\prime j}}{f_{i^\\prime j}} \\right)^2.\\]\nOn peut écrire cela sous forme matricielle. Notons \\(D_n = \\text{diag}(f_{i \\bullet}\\) et \\(D_p = \\text{diag}(f_{\\bullet j}\\). On a que la matrice \\(D_n^{-1}F\\) a pour lignes les profils-lignes et la matrice \\(D_p^{-1}F^{\\top}\\) a pour lignes les profils-colonnes.\nEn utilisant ces matrices, la distance du \\(\\chi^2\\) au carré est de la forme \\(x^{\\top}D_p^{-1}x\\) pour un point-ligne \\(x \\in \\mathbb{R}^p\\) et de la forme \\(x^{\\top}D_n^{-1}x\\) pour un point-colonne \\(x \\in \\mathbb{R}^n\\).",
    "crumbs": [
      "Modules",
      "04 - Dimension",
      "ACM"
    ]
  },
  {
    "objectID": "contents/dimension/02-mca.html#analyse-factorielle-des-correspondances",
    "href": "contents/dimension/02-mca.html#analyse-factorielle-des-correspondances",
    "title": "ACM",
    "section": "Analyse factorielle des correspondances",
    "text": "Analyse factorielle des correspondances\nL’analyse factorielle des correspondances est une approche graphique permettant de représenter simultanément les profils-lignes appartenant à \\(\\mathbb{R}^p\\) et les profils-colonnes appartenant à \\(\\mathbb{R}^n\\) d’un tableau de fréquences. On cherche donc un espace en deux dimensions où projeter les profils lignes et colonnes de sorte que les points dans cet espace soient le plus près possible des points originaux au sens de la distance du \\(\\chi^2\\). L’analyse des correspondances est très similaire à une double ACP.\nAnalyse directe (sur les lignes): les lignes de la matrice \\(D_n^{-1}F \\in \\mathbb{R}^p\\). On cherche à les représenter dans cet espace muni de la distance \\(x^{\\top} D_p^{-1} x\\).\nAnalyse duale (sur les colonnes): les colonnes de la matrice \\(D_p^{-1}F^{\\top} \\in R^n\\). On cherche à les représenter dans cet espace muni de la distance \\(x^{\\top} D_n^{-1} x\\).\nPremier axe factoriel de l’analyse directe: on cherche le vecteur \\(u \\in \\mathbb{R}^p\\) tel que \\[\\left( u^{\\top} D_p^{-1} F^{\\top} D_n^{-1} \\right) D_n \\left( D_n^{-1} F D_p^{-1} u \\right).\\] soit maximal, sachant que \\(u^{\\top} D_p^{-1} u = 1\\). La solution est donnée par le vecteur propre principal de \\[D_p \\left( D_p^{-1} F^{\\top} D_n^{-1} F D_p^{-1} \\right) = F^{\\top} D_n^{-1} F D_p^{-1} \\equiv S.\\]\nLes formules des coordonnées dans le système d’axes sont plutôt complexes et pas très parlantes… mais à remarquer:\n\n\\(S\\) et \\(T\\) ont les mêmes \\(p\\) premières valeurs propres.\nEn centrant les profils lignes et colonnes, on peut illustrer le résultat des deux graphiques sur les mêmes axes.",
    "crumbs": [
      "Modules",
      "04 - Dimension",
      "ACM"
    ]
  },
  {
    "objectID": "contents/dimension/02-mca.html#centre-de-gravité-et-inertie",
    "href": "contents/dimension/02-mca.html#centre-de-gravité-et-inertie",
    "title": "ACM",
    "section": "Centre de gravité et inertie",
    "text": "Centre de gravité et inertie\nLes logiciels produisent généralement un graphique centré en \\((0, 0)\\). Il s’agit d’une analyse relative aux centres de gravité des lignes et des colonnes. Cette pratique est à la fois commune et commode. En fait, la masse de la \\(i\\)e ligne est \\(f_{i \\bullet}\\), soit la proportion des observations qui tombent sur cette ligne. De faon similaire, la masse de la \\(j\\)e colonne est \\(f_{\\bullet j}\\). Le centre de gravité des lignes est la moyenne des profils-lignes, mais pondérée par la masse de chaque ligne, et similairement pour les profils-colonnes.\nLe centre de gravité des lignes est défini par \\(G_L = \\left( g_{1}, \\dots, g_{p} \\right)^top\\), où \\[g_j = \\sum_{i = 1}^{n} f_{i \\bullet} \\frac{f_{i j}}{f_{i \\bullet}} = \\sum_{i = 1}^{n} f_{i j} = f_{\\bullet j}, \\quad 1 \\leq j \\leq p.\\]\nDe même, le centre de gravité des colonnes est défini par \\[G_C = \\left( f_{1 \\bullet}, \\dots, f_{n \\bullet} \\right)^top.\\]\nCentrage des lignes: \\[\\frac{f_{i j}}{f_{i \\bullet}} - g_{j} = \\frac{f_{i j}}{f_{i \\bullet}} - f_{\\bullet j} = \\frac{f_{i j} - f_{i \\bullet} f_{\\bullet j}}{f_{i \\bullet}}.\\] de sorte que, pour tout \\(i \\in \\{ 1, \\dots, n \\}\\), \\[\\sum_{j = 1}^{p} \\frac{f_{i j} - f_{i \\bullet}f_{\\bullet j}}{f_{i \\bullet}} = 0.\\]\nL’analyse ne se fait plus sur \\[S = F^{\\top} D_n^{-1} F D_p^{-1},\\] mais plutôt sur \\(S^\\star = (s_{j j^\\prime}^\\star),\\) où \\[s_{j j^\\prime}^\\star = \\sum_{i = 1}^{n} \\frac{\\left( f_{i j} - f_{i \\bullet} f_{\\bullet j} \\right) \\left( f_{i j^\\prime} - f_{i \\bullet} f_{\\bullet j^\\prime} \\right)}{f_{i \\bullet} f_{\\bullet j^\\prime}}.\\]\nPar définition, \\[\\text{tr}(S^\\star) = \\sum_{j = 1}^{p} \\sum_{i = 1}^{n} \\frac{\\left( f_{i j} - f_{i \\bullet}f_{\\bullet j} \\right)^2}{f_{i \\bullet} f_{\\bullet j}}.\\]\nOn retrouve l’expression de la statistique du \\(\\chi^2\\) servant à tester l’indépendance entre deux variables.\nOn peut prouver que \\(s_{j j^\\prime}^\\star = s_{j j^\\prime} - f_{\\bullet j},\\) où \\[s_{j j^\\prime} = \\sum_{i = 1}^{n} \\frac{f_{i j}f_{i j^\\prime}}{f_{i \\bullet} f_{\\bullet j^\\prime}}.\\] Ceci entraîne que ces deux matrices ont les mêmes \\(p\\) premiers vecteurs propres normalisés.\nCoordonnées des points-lignes. La projection du \\(i\\)e point-ligne sur l’axe \\(j\\) est donnée par \\[\\left( D_n^{-1} F \\phi_j \\right)_i = \\frac{1}{f_{i \\bullet}} \\sum_{j^\\prime = 1}^{p} f_{i j^\\prime} \\phi_{j j^\\prime} = \\sqrt{\\lambda_j} \\Psi_{j i} \\equiv \\widehat{\\Psi}_{j i}.\\] De plus, \\[\\sum_{i = 1}^{n} f_{i \\bullet} \\widehat{\\Psi}^2_{j i} = \\sum_{i = 1}^{n} f_{i \\bullet} \\left( \\sqrt{\\lambda_j} \\Psi_{j i} \\right)^2 = \\lambda_j.\\]\nCoordonnées des point-colonne. La projection du \\(k\\)e point-ligne sur l’axe \\(j\\) est donnée par \\[\\left( D_p^{-1} F^{\\top} \\psi_j \\right)_k = \\frac{1}{f_{\\bullet k}} \\sum_{i = 1}^{n} f_{i k} \\psi_{j i} = \\sqrt{\\lambda_j} \\Phi_{j k} \\equiv \\widehat{\\Phi}_{j k}.\\] De plus, \\[\\sum_{k = 1}^{p} f_{\\bullet k} \\widehat{\\Phi}^2_{j k} = \\sum_{k = 1}^{p} f_{\\bullet k} \\left( \\sqrt{\\lambda_j} \\Phi_{j k} \\right)^2 = \\lambda_j.\\]\nL’inertie absolue du \\(i\\)e point-ligne sur l’axe \\(j\\) est \\(f_{i \\bullet}\\widehat{\\Psi}_{j i}^2\\). L’inertie relative du \\(i\\)e point-ligne sur l’axe \\(j\\) est \\[\\frac{f_{i \\bullet} \\widehat{\\Psi}_{j i}^2}{\\lambda_j}.\\]\nL’inertie absolue du \\(k\\)e point-colonne sur l’axe \\(j\\) est \\(f_{\\bullet k}\\widehat{\\Phi}_{j k}^2\\). L’inertie relative du \\(k\\)e point-colonne sur l’axe \\(j\\) est \\[\\frac{f_{\\bullet k} \\widehat{\\Phi}_{j k}^2}{\\lambda_j}.\\]\nL’inertie totale est \\(I = T / k_{\\bullet\\bullet}\\).\nla qualité de la représentation du \\(k\\)e point-colonne dans l’axe \\(j\\) est donnée par \\[\\frac{d_j^2(k, G_C)}{d^2(k, G_c)} = \\cos^2(\\theta_{k j}),\\] où \\(\\theta_{k j}\\) est l’angle entre le point \\(k\\) et sa projection sur l’axe \\(j\\).\nInterprétation:\n\nPlus les \\(\\cos^2(\\theta_{k j})\\) sont élevés, mieux les points sont représentés sur l’axe \\(j\\).\nCeci ne signifie pas pour autant que les points sont près du centre du graphique.\nLes points éloignés du centre de gravité se distinguent du centre de gravité.\nUne interprétation semblable existe pour les points-lignes.",
    "crumbs": [
      "Modules",
      "04 - Dimension",
      "ACM"
    ]
  },
  {
    "objectID": "contents/dimension/02-mca.html#example",
    "href": "contents/dimension/02-mca.html#example",
    "title": "ACM",
    "section": "Example",
    "text": "Example",
    "crumbs": [
      "Modules",
      "04 - Dimension",
      "ACM"
    ]
  },
  {
    "objectID": "contents/dimension/02-mca.html#analyse-des-correspondances-multiples",
    "href": "contents/dimension/02-mca.html#analyse-des-correspondances-multiples",
    "title": "ACM",
    "section": "Analyse des correspondances multiples",
    "text": "Analyse des correspondances multiples\nL’analyse des correspondances multiples est une généralisation de l’analyse des correspondances binaires. Elle permet la representation graphique de tableaux de fréquences contnant plus de deux variables. Un exemple classique d’un tableau de fréquences avec plus de deux variables est le tableau présentant les réponses d’individus à un questionnaire contenant \\(Q\\) questions à choix multiples.\nTrès utile pour visualiser les résultats d’une étude par questionnaire.\nL’ACM peut aussi être vue comme une version de l’ACP quand les variables sont catégorielles:\n\nl’analyse duale permet de voir les individus ayant des profils de réponses similaires\non peut obtenir des scores continus pour les individus qui capturent une grande partie de l’information\ndonc aussi utile pour scorer les résultats d’une étude par questionnaire dans un but éventuel de partitionnement, par exemple\n\nEn général, pour un questionnaire contenant \\(Q\\) questions, on a un tableau de la forme suivante: \\[Z = \\left[ Z_1 \\mid \\cdots \\mid Z_{Q} \\right].\\]\nNotation:\n\n\\(Q\\): nombre de questions\n\\(n\\): nombre d’individus répondant au questionnaire\n\\(p_q\\): nombre de modalités (choix de réponses) de la question \\(q\\).\n\\(p = p_{1} + p_{Q}\\)\n\nPotentiel problème: plus le nombre de questions est grand, plus il y aura de cellules vides. C’est aussi le cas si le nombre de réponses aux questions est important. La proportion de cellules non vides est \\[\\frac{nQ}{np} = \\frac{Q}{p}.\\].\nSi toutes les questions ont le même nombre de choix de réponses, alors \\(p_{1} = \\dots = p_{Q} = \\frac{p}{Q}\\), de sorte que \\[\\frac{Q}{p} = \\frac{1}{p_{1}} \\longrightarrow 0, \\quad\\text{quand}~ p_{1} \\rightarrow \\infty.\\]\nLe tableau résumé est un tableau de taille \\(n \\times Q\\). Il contient le numéro de la modalité associée à la réponse de chaque individu pour chacune des questions.\nLa tableau de Burt est une autre faon de présenter un tableau de fréquences contenant plus de deux variables. Étant donné un tableau logique \\(Z = \\left[ Z_{1} \\mid \\cdots \\mid Z_{Q} \\right]\\), le tableau de Burt associé est la matrice carrée \\(p \\times p\\) définie comme étant \\[ B = \\begin{pmatrix}\n  Z_1^\\top Z_1 & \\cdots & Z_1^\\top Z_Q \\\\\n  \\vdots & \\ddots & \\vdots \\\\\n  Z_Q^\\top Z_1 & \\cdots & Z_Q^\\top Z_Q\n\\end{pmatrix}.\\]\n\n\n\n\n\n\nPropriétés de \\(Z_q^\\top Z_q\\)\n\n\n\n\n\\(Z_{q}^\\top Z_q\\) est une matrice diagonale \\(p_q \\times p_q\\) présentant les réponses à la \\(q\\)e question.\nL’élément \\((j, j)\\) de la matrice \\(Z_q^\\top Z_q\\) est égal au nombre d’individus \\(d_{jj}\\) qui appartiennent à la \\(j\\)e catégorie de la \\(q\\)e question.\n\\(Z_{q}^\\top Z_{r}\\) est le tableau de fréquences présentant les répones au x \\(q\\)e et \\(r\\)e questions.\nL’élément \\((j, k)\\) de la matrice \\(Z_q^\\top Z_r\\) est égal au nombre d’individus \\(d_{jk}\\) qui appartiennent à la \\(j\\)e catégorie de la \\(q\\)e question et à la \\(k\\)e catégorie de la \\(r\\)e question.\n\n\n\nD’un point de vue mathématique, l’ACM est une AFC effectuée sur la matrice logique \\(Z\\) ou sur le tableau de Burt \\(B\\). On peut démontrer que l’on obtient les mêmes facteurs, et ce, peu importe la matrice utilisé pour l’analyse.\n\n\n\n\n\n\nNote\n\n\n\nOn peut créé un graphique comme l’AFC. Cependant, en ACM, la distance entre les points de même couleur et la géométrie globale du graphique ne peuvent pas s’interpréter comme en AFC. En fait, on s’intéresse aux points qui sont dans une même direction par rapport à l’origine.",
    "crumbs": [
      "Modules",
      "04 - Dimension",
      "ACM"
    ]
  },
  {
    "objectID": "contents/dimension/02-mca.html#example-1",
    "href": "contents/dimension/02-mca.html#example-1",
    "title": "ACM",
    "section": "Example",
    "text": "Example",
    "crumbs": [
      "Modules",
      "04 - Dimension",
      "ACM"
    ]
  },
  {
    "objectID": "contents/03-generalities.html",
    "href": "contents/03-generalities.html",
    "title": "Généralités",
    "section": "",
    "text": "Slides: link\nTD: link\nTP: link",
    "crumbs": [
      "Modules",
      "03 - Généralités"
    ]
  },
  {
    "objectID": "contents/03-generalities.html#sommaire",
    "href": "contents/03-generalities.html#sommaire",
    "title": "Généralités",
    "section": "Sommaire",
    "text": "Sommaire\n\nProjet d’analyse de données\nEspaces\nDistances\nBiais / Variance\nÉvaluation de modèles\n\n\n\n\nCorrelation (xkcd:552).",
    "crumbs": [
      "Modules",
      "03 - Généralités"
    ]
  },
  {
    "objectID": "contents/remainders/03-programming.html",
    "href": "contents/remainders/03-programming.html",
    "title": "Programmation",
    "section": "",
    "text": "Dans cette partie, on présente quelques références sur les bases de la programmation dans différents langages. Ce cours ne spécifie pas de langage à utiliser, vous êtes donc libre de choisir celui qui vous convient le mieux pour faire les différents exercices, ainsi que votre projet. Dans tous les cas, il y a un ensemble de bonnes pratiques qui permet d’avoir un code lisible, compréhensible et réutilisable. Vous trouvez un (rapide) guide des bonnes pratiques en programmation à ce lien.",
    "crumbs": [
      "Modules",
      "02 - Révisions",
      "Programmation"
    ]
  },
  {
    "objectID": "contents/remainders/03-programming.html#r",
    "href": "contents/remainders/03-programming.html#r",
    "title": "Programmation",
    "section": "R",
    "text": "R\nR est un langage de programmation spécialisé dans l’analyse statistique et la visualisation de données. Il est gratuit, open-source et disponible sur Windows, macOS et Linux. Régulièrement mis à jour, de nouveaux packages sont disponibles chaque jour (cf. CRAN).\nVoici un guide qui présente les bases de R: Apprendre R en Y minutes et un guide de bonnes pratiques en R. Enfin, un point important du langage R est l’opérateur pipe. Celui-ci permet de rendre le code plus clair. Vous pouvez trouver une explication de cette opérateur ici.",
    "crumbs": [
      "Modules",
      "02 - Révisions",
      "Programmation"
    ]
  },
  {
    "objectID": "contents/remainders/03-programming.html#python",
    "href": "contents/remainders/03-programming.html#python",
    "title": "Programmation",
    "section": "Python",
    "text": "Python\nPython est un langage de programmation généraliste. Il est gratuit, open-source et disponible sur Windows, macOS et Linux. Bien que généraliste, il y a une importante communauté autour du l’analyse de données, machine learning en Python. Ainsi, les méthodes usuelles ont déjà été implementées, e.g. dans le package sklearn, mais si besoin, il est possible d’aller voir le code pour le modifier.\nVoici un guide qui présente les bases de Python: Apprendre Python en Y minutes et un guide de bonnes pratiques en Python.",
    "crumbs": [
      "Modules",
      "02 - Révisions",
      "Programmation"
    ]
  },
  {
    "objectID": "contents/remainders/03-programming.html#julia",
    "href": "contents/remainders/03-programming.html#julia",
    "title": "Programmation",
    "section": "Julia",
    "text": "Julia\nJulia est aussi un langage de programmation généraliste. C’est un langage bien plus récent que ses deux compères, R et Python (début des années 2010 pour Julia vs. début des années 1990 pour R et Python). Il peut donc sembler moins mature sur certains points, mais la plupart des méthodes d’analyse de données classiques ont été implémentées. De même que R et Python, ce langage est gratuit, open-source et disponible sur les principaux systèmes d’exploitation. Julia a l’avantage d’être plus rapide que R et Python. Il inclut aussi un support natif d’appel à des librairies en C ou en Fortran, et un support non-natif d’appel à des librairies en R et Python.\nVoici un guide qui présente les bases de Julia: Learn Julia in Y minutes (il n’y a pas de version française pour l’instant) et un guide de bonnes pratiques en Julia (en anglais).",
    "crumbs": [
      "Modules",
      "02 - Révisions",
      "Programmation"
    ]
  },
  {
    "objectID": "contents/remainders/03-programming.html#sas",
    "href": "contents/remainders/03-programming.html#sas",
    "title": "Programmation",
    "section": "SAS",
    "text": "SAS\nSAS est un langage de programmation propriétaire spécialisé dans l’analyse de données. Bien qu’utilisé dans certaines industries, e.g. pharmaceutique, son utilisation tend à diminuer. De plus, la license à renouveler tous les ans, l’absence de communauté en ligne (pour avoir de l’aide, à part la documentation officielle, il n’y a pas grand chose) et l’impossibilité de voir le code des différentes procédures font que je déconseille l’utilisation de SAS dans le cadre de ce cours. Cependant, si vous le voulez, l’université peut fournir des licenses moyennant paiement.",
    "crumbs": [
      "Modules",
      "02 - Révisions",
      "Programmation"
    ]
  },
  {
    "objectID": "contents/remainders/01-linear-algebra.html",
    "href": "contents/remainders/01-linear-algebra.html",
    "title": "Algèbre linéaire",
    "section": "",
    "text": "Dans cette partie, on présente quelques résultats d’algèbre linéaire utiles dans le cadre de ce cours. Pour plus d’information, vous pouvez vous référer au cours MAT-1200, à Deisenroth, Faisal, et Ong (2020) (en anglais) et à Grifone (2024) (en français).",
    "crumbs": [
      "Modules",
      "02 - Révisions",
      "Algèbre linéaire"
    ]
  },
  {
    "objectID": "contents/remainders/01-linear-algebra.html#quelques-propriétés-matricielles",
    "href": "contents/remainders/01-linear-algebra.html#quelques-propriétés-matricielles",
    "title": "Algèbre linéaire",
    "section": "Quelques propriétés matricielles",
    "text": "Quelques propriétés matricielles\nNotons \\(M_{n, m}(\\mathbb{R})\\), l’ensemble des matrices à \\(n\\) lignes et \\(m\\) colonnes dont les entrées appartiennent à \\(\\mathbb{R}\\). Notons \\(M_{n}(\\mathbb{R})\\), l’ensemble des matrices carrées de taille \\(n\\), i.e. à \\(n\\) lignes et \\(n\\) colonnes dont les entrées appartiennent à \\(\\mathbb{R}\\). Soient \\(M\\), \\(N\\) et \\(P\\) des matrices appartenant à \\(M_{n, m}(\\mathbb{R})\\). Soient \\(A\\) et \\(B\\) des matrices appartenant à \\(M_{n}(\\mathbb{R})\\). Notons \\(I_n\\) la matrice identité de taille \\(n\\), i.e. qui contient des \\(1\\) sur le diagonale et des \\(0\\) sur les éléments hors de la diagonale. Soient \\(u\\) et \\(v\\) appartenant à \\(\\mathbb{R}^n\\), i.e. des vecteurs colonnes de taille \\(n\\).\n\n\n\n\n\n\nPropriétés de l’inverse de matrices\n\n\n\nSupposons que les matrices \\(A\\) et \\(B\\) soient inversibles. Alors le produit matriciel \\(AB\\) est inversible et est donné par:\n\\[(AB)^{-1} = B^{-1} A^{-1}.\\]\n\n\n\n\n\n\n\n\nPreuve\n\n\n\n\n\nPosons \\(C = AB\\) et \\(D = B^{-1} A^{-1}\\). Alors\n\\[\\begin{align*}\n  CD &= A B B^{-1} A^{-1} \\\\\n     &= A A^{-1} \\\\\n     &= I_n\n\\end{align*}\\]\nDe la même façon, on trouve que \\(DC = I_n\\). Ainsi, \\(AB\\) est inversible et son inverse est donné par \\(B^{-1} A^{-1}\\).\n\n\n\n\n\n\n\n\n\nPropriétés du déterminant de matrices\n\n\n\nConsidérant les matrices définies en début de section, on a :\n\n\\(\\text{det}(A^\\top) = \\text{det}(A)\\),\n\\(\\text{det}(AB) = \\text{det}(A)\\text{det}(B)\\),\n\\(\\text{det}(A^{-1}) = 1 / \\text{det}(A)\\).\n\n\n\n\n\n\n\n\n\nPreuve\n\n\n\n\n\nLes preuves des propriétés \\(1\\) et \\(2\\) sont techniques et sont omises, mais peuvent être trouvées, par exemple, ici. Pour ce qui est de la troisième propriété, par définition, on a \\(A A^{-1} = I_n\\). Le déterminant de \\(I_n\\) est égale à \\(1\\) (produit des éléments sur la diagonale). Donc \\(\\text{det}(A A^{-1}) = 1\\). Or, d’après la deuxième propriété, \\(\\text{det}(A A^{-1}) = \\text{det}(A)\\text{det}(A^{-1})\\). On a donc bien \\(\\text{det}(A^{-1}) = 1 / \\text{det}(A)\\).\n\n\n\n\n\n\n\n\n\nPropriétés de la trace de matrices\n\n\n\nConsidérant les matrices définies en début de section, on a :\n\n\\(\\text{tr}(A) = \\text{tr}(A^{\\top})\\),\n\\(\\text{tr}(A + B) = \\text{tr}(A) + \\text{tr}(B)\\),\n\\(\\text{tr}(MN^{\\top}) = \\text{tr}(N^{\\top}M)\\).\n\n\n\n\n\n\n\n\n\nPreuve\n\n\n\n\n\nPour une matrice carré \\(A\\), notons \\(a_{ij}\\), l’élément de la matrice \\(A\\) à la ligne \\(i\\) et à la colonne \\(j\\). La trace de \\(A\\) est donnée par la somme des éléments diagonaux, i.e. \\(\\text{tr}(A) = \\sum_{i = 1}^{n} a_{ii}\\).\n\nLa transposition ne changeant pas les éléments diagonaux, le résultat est direct.\nNotons \\(C = A + B\\). Comme \\(A\\) et \\(B\\) sont des matrices carrées, \\(C\\) est une matrice carrée. On a \\(c_{ij} = a_{ij} + b_{ij}\\) pour tout \\(i, j = 1, \\dots, n\\). Donc \\[\\text{tr}(A + B) = \\text{tr}(C) = \\sum_{i = 1}^{n} c_{ii} = \\sum_{i = 1}^{n} a_{ii} + b_{ii} = \\sum_{i = 1}^{n} a_{ii} + \\sum_{i = 1}^{n} b_{ii} = \\text{tr}(A) + \\text{tr}(B).\\]\nLes matrices \\(M N^{\\top}\\) et \\(N^{\\top} M\\) sont carrées, de dimension respectives \\(n \\times n\\) et \\(m \\times m\\), on peut donc bien calculer leur trace. Notons \\(C = M N^{\\top}\\) et \\(D = N^{\\top} M\\). \\[\\text{tr}(M N^{\\top}) = \\text{tr}(C) = \\sum_{i = 1}^{n} c_{ii} = \\sum_{i = 1}^{n} \\sum_{j = 1}^{m} m_{ij} n_{ji} = \\sum_{j = 1}^{m} \\sum_{i = 1}^{n} n_{ji} m_{ij} = \\sum_{j = 1}^{m} d_{jj} = \\text{tr}(D)  = \\text{tr}(N^{\\top} M).\\]\n\n\n\n\n\n\n\n\n\n\nDéfinition\n\n\n\n\nSoit \\(A\\) une matrice symétrique appartenant à \\(M_n(\\mathbb{R})\\). \\(A\\) est définie positive si \\(u^\\top A u &gt; 0\\) pour tout \\(u \\in \\mathbb{R}^n\\) tel que \\(u \\neq 0\\).\nSoit \\(A\\) appartenant à \\(M_n(\\mathbb{R})\\). \\(A\\) est orthogonal si \\(A^\\top A = A A^\\top = I_n\\).",
    "crumbs": [
      "Modules",
      "02 - Révisions",
      "Algèbre linéaire"
    ]
  },
  {
    "objectID": "contents/remainders/01-linear-algebra.html#valeurs-et-vecteurs-propres",
    "href": "contents/remainders/01-linear-algebra.html#valeurs-et-vecteurs-propres",
    "title": "Algèbre linéaire",
    "section": "Valeurs et vecteurs propres",
    "text": "Valeurs et vecteurs propres\n\n\n\n\n\n\nDéfinition\n\n\n\nSoit \\(A\\) appartenant à \\(M_n(\\mathbb{R})\\). On dit que \\(\\lambda \\in \\mathbb{R}\\) est une valeur propre de \\(A\\) s’il existe un vecteur \\(u \\in \\mathbb{R}^n\\) non nul tel que \\[Au = \\lambda u. \\tag{1}\\] Le vecteur \\(u\\) est appelé vecteur propre de \\(A\\) correspondant à la valeur propre \\(\\lambda\\).\nL’ensemble des nombres réels \\(\\lambda\\) satisfaisant Équation 1 est appelé spectre de la matrice \\(A\\) et noté \\(\\text{sp}(A)\\).\n\n\n\n\n\n\n\n\nPropriété des vecteurs propres\n\n\n\n\nSi \\(u\\) est un vecteur propre de \\(A\\) correspondant à une valeur propre \\(\\lambda\\), alors le vecteur \\(cu\\), \\(c \\in \\mathbb{R}^\\star\\) est également un vecteur propre de \\(A\\) correspondant à \\(\\lambda\\).\nSi \\(A\\) est symétrique et \\(u_{1}\\) et \\(u_{2}\\) sont des vecteurs propres correspondant à des valeurs propres différentes de \\(A\\), alors \\(u_{1}\\) et \\(u_{2}\\) sont orthogonaux, i.e. \\(u_{1}^\\top u_{2} = 0\\).\n\n\n\n\n\n\n\n\n\nPreuve\n\n\n\n\n\n\nSoit \\(c \\in \\mathbb{R}^\\star\\) et \\(u\\) un vecteur propre de \\(A\\) associé à la valeur propre \\(\\lambda\\). On a : \\[A(cu) = cAu = c \\lambda u = \\lambda (cu).\\] Donc, le vecteur \\(cu\\) est aussi vecteur propre de \\(A\\) associé à la valeur propre \\(\\lambda\\).\nSoient \\(\\lambda_{1}\\) et \\(\\lambda_{2}\\), les valeurs propres associées à \\(u_{1}\\) et \\(u_{2}\\), tel que \\(\\lambda_{1} \\neq \\lambda_{2}\\). On a \\(A u_{1} = \\lambda_{1} u_{1}\\) et \\(A u_{2} = \\lambda_{2} u_{2}\\). Ensuite \\[\\lambda_{1} u_{1}^{\\top} u_{2} = u_{1}^\\top A u_{2} = \\lambda_{2} u_{1}^\\top u_{2}.\\] Cela implique que \\((\\lambda_{1} - \\lambda_{2})u_{1}^\\top u_{2} = 0\\). Or, \\(\\lambda_{1} \\neq \\lambda_{2}\\). Donc, nécessairement, \\(u_{1}^\\top u_{2} = 0\\).\n\n\n\n\nCette deuxième propriété nous sera utile lorque l’on s’intéressera à la réduction de dimension et, en particulier, à l’analyse en composantes principales.\n\n\n\n\n\n\nCaractérisation de matrices avec ses éléments propres\n\n\n\n\nSi \\(A\\) est symétrique, alors toutes ses valeurs propres sont réelles.\nSi \\(A\\) est définie positive, alors toutes ses valeurs propres sont strictement positives.\n\n\n\n\n\n\n\n\n\nPreuve\n\n\n\n\n\n\nConsidérons le cas plus général où \\(A\\) est une matrice hermitenne. La matrice \\(A\\) est égale la transposé de son conjugué, noté \\(A^*\\). Notons \\(\\lambda\\) une valeur propre associée à un vecteur propre \\(u\\), éventuellement complexe. On a : \\[\\begin{align}\n\\overline{u}^{\\top} A u &= \\overline{u}^\\top \\lambda u = \\lambda \\overline{u}^{\\top} u, \\\\\n\\overline{u}^\\top A u &= \\overline{u}^\\top A^* u = \\overline{Au}^\\top u = \\overline{\\lambda} \\overline{u}^\\top u.\n\\end{align}\\] Cela implique que \\((\\lambda - \\overline{\\lambda}) \\overline{u}^{\\top} u = 0\\). Comme \\(u \\neq 0\\), on a \\(\\lambda = \\overline{\\lambda}\\). Donc \\(\\lambda \\in \\mathbb{R}\\).\nConsidérons \\(u\\), vecteur propre de \\(A\\) associé à la valeur propre \\(\\lambda\\). On a que \\(u^{\\top} A u = \\lambda u^{\\top} u\\). Or, comme \\(u \\neq 0\\), \\(u^{\\top}u \\neq 0\\). Donc \\[\\lambda = \\frac{u^{\\top} A u}{u^{\\top} u}.\\] Comme \\(A\\) est définie postive, \\(u^{\\top} A u &gt; 0\\) pour tout vecteur \\(u\\) non nul. On en déduit que \\(\\lambda &gt; 0\\).",
    "crumbs": [
      "Modules",
      "02 - Révisions",
      "Algèbre linéaire"
    ]
  },
  {
    "objectID": "contents/remainders/01-linear-algebra.html#diagonalisation-de-matrices",
    "href": "contents/remainders/01-linear-algebra.html#diagonalisation-de-matrices",
    "title": "Algèbre linéaire",
    "section": "Diagonalisation de matrices",
    "text": "Diagonalisation de matrices\n\n\n\n\n\n\nDéfinition\n\n\n\nSoit \\(A\\) appartenant à \\(M_n(\\mathbb{R})\\). On dit que \\(A\\) est diagonalisable s’il existe une matrice \\(P\\) appartenant à \\(M_n(\\mathbb{R})\\) non-singulière et une matrice diagonale \\(D\\) appartenant à \\(M_n(\\mathbb{R})\\) telles que \\[P^{-1} A P = D \\Longleftrightarrow A = P D P^{-1}.\\]\n\n\n\n\n\n\n\n\nThéorème de décomposition spectrale\n\n\n\nSoit \\(A\\) une matrice symmétrique appartenant à \\(M_n(\\mathbb{R})\\) et \\(\\lambda_{1}, \\dots, \\lambda_n\\), ses \\(n\\) valeurs propres. Alors, il existe une matrice orthogonal \\(P\\) appartenant à \\(M_n(\\mathbb{R})\\) telle que \\[A = P \\Lambda P^\\top, \\quad\\text{où}\\quad \\Lambda = \\text{diag}(\\lambda_1, \\dots, \\lambda_n).\\]\n\n\nSi \\(A\\) admet \\(n\\) valeurs propres positives distinctes, alors on peut prendre \\(P\\) comme étant la matrice dont la \\(k\\)-ième colonne est le vecteur propre normé correspondant à la \\(k\\)-ième valeur propre \\(\\lambda_k\\) de \\(A\\).\nSoit deux matrices symétriques, \\(A\\) et \\(B\\), comment déterminer le vecteur \\(u\\) tel que \\(u^{\\top} A u\\) soit maximal, sachant que \\(u^{\\top} B u = 1\\) ? Il suffit de prendre \\(u\\) comme le vecteur propre de \\(B^{-1}A\\) associé à \\(\\lambda\\) la valeur propre maximale de \\(B^{-1}A\\). On obtient ainsi \\[u^{\\top} A u = u^{\\top}\\lambda M u = \\lambda U^{\\top} M u = \\lambda.\\]\n\n\n\n\n\n\nCaractérisation du déterminant et de la trace de matrices avec ses éléments propres\n\n\n\nSi \\(A\\) a comme valeurs propres (réelles, mais pas forcément distinctes) \\(\\lambda_{1}, \\dots, \\lambda_{n}\\), alors\n\n\\(\\text{det}(A) = \\prod_{i = 1}^{n} \\lambda_i\\)\n\\(\\text{tr}(A) = \\sum_{i = 1}^{n} \\lambda_i.\\)\n\n\n\n\n\n\n\n\n\nPreuve\n\n\n\n\n\nEn utilisant le théorème de décomposition spectrale, il existe une matrice \\(P\\) inversible tel que \\(A = P \\Lambda P^{-1}\\), où \\(\\Lambda\\) est une matrice diagonale contenant les valeurs propres. On a donc, pour le déterminant,\n\\[\\text{det}(A) = \\text{det}(P \\Lambda P^{-1}) = \\text{det}(P)\\text{det}(\\Lambda)\\text{det}(P^{-1}) = \\text{det}(P)\\text{det}(\\Lambda)\\text{det}(P)^{-1} = \\text{det}(\\lambda) = \\prod_{i = 1}^{n} \\lambda_i, \\]\net, pour la trace,\n\\[\\text{tr}(A) = \\text{tr}(P \\Lambda P^{-1}) = \\text{tr}(P^{-1} P \\Lambda) = \\text{tr}(\\Lambda) = \\sum_{i = 1}^{n} \\lambda_i.\\]",
    "crumbs": [
      "Modules",
      "02 - Révisions",
      "Algèbre linéaire"
    ]
  },
  {
    "objectID": "contents/generalities/05-model-evaluation.html",
    "href": "contents/generalities/05-model-evaluation.html",
    "title": "Evaluation de modeles",
    "section": "",
    "text": "Pourquoi mesurer l’erreur d’un modèle ?",
    "crumbs": [
      "Modules",
      "03 - Généralités",
      "Evaluation de modeles"
    ]
  },
  {
    "objectID": "contents/generalities/05-model-evaluation.html#validation-croisée",
    "href": "contents/generalities/05-model-evaluation.html#validation-croisée",
    "title": "Evaluation de modeles",
    "section": "Validation croisée",
    "text": "Validation croisée\n\nOn sépare l’échantillon en \\(K\\) plus de faon aléatoire.\nPour \\(k\\) allant de \\(1\\) à \\(K\\):\n\n\nOn estime \\(\\widehat{f}(k)\\) en utilisant uniquement les observations qui ne sont pas dans \\(k\\).\nOn prédit \\(\\widehat{Y}(k) = \\widehat{f}(k)(X(k))\\).\nOn calcule \\(CV(k) = \\frac{1}{N_k} \\sum_{i = 1}^{N_k} L(Y(k), \\widehat{Y}(k))\\).\n\n\nOn calcule la moyenne \\(CV = \\frac{1}{K} \\sum_{k = 1}^{K} CV(k)\\).\nOn répète 1 à 3 pour tous les modèles et on choisit le modèle ont la valeur de CV est la plus basse.",
    "crumbs": [
      "Modules",
      "03 - Généralités",
      "Evaluation de modeles"
    ]
  },
  {
    "objectID": "contents/generalities/05-model-evaluation.html#cas-dun-réponse-binaire",
    "href": "contents/generalities/05-model-evaluation.html#cas-dun-réponse-binaire",
    "title": "Evaluation de modeles",
    "section": "Cas d’un réponse binaire",
    "text": "Cas d’un réponse binaire\nDams le cas où il n’existe que deux groupes dans la population, la matrice de confusion est de la forme suivante:\n\n\n\n\nPositif\nNégatif\n\n\n\n\nPositif\nVrais positifs (VP)\nFaux négatif (FN)\n\n\nNégatif\nFaux positifs (FP)\nVrais négatif (VN)\n\n\n\n\nSensibilité (sensitivity, rappel, recall, hit rate)\n\nProbailité qu’un individu qui appartient au groupe positif soit classé dans le groupe positifs: \\[\\frac{VP}{VP + FN}.\\]\n\nSpécifité (specificity)\n\nProbailité qu’un individu qui appartient au groupe négatif soit classé dans le groupe négatif: \\[\\frac{VN}{VN + FP}.\\]\n\nPrécision \\[\\frac{VP}{VP + FP}.\\]\nAccuracy \\[\\frac{VP + VN}{VP + VN + FP + FN}.\\]\nScore \\(F_{1}\\): moyenne harmonique de précision et de sensibilité \\[F_{1} = 2 \\times \\frac{précision \\times sensibilité}{précision + sensibilité} = \\frac{2 VP}{VP + FP + FN}.\\]\n\nEn practique, la situation typique va comme suit:\n\nOn a partagé notre jeu de données en eju d’entrainment et en jeu de validation.\nOn a appliqué \\(M\\) méthodes de classification au jeu d’entrainement.\nOn aimerait utiliser les données de validation pour choisir la meilleure des \\(M\\) méthodes.\n\nOn cherche donc à se doter d’outils qui permettent de mesurer la performance de méthodes de classification.\n\nMesure triviale\nLa première étape consiste à comparer le taux de bonnes classification (accuracy) des données de validation obtenu avec chacune des méthodes. On peut faire la même chose avec le score \\(F_{1}\\).\n\n\nSensibilité et spécifité\nSupposons une classe que nous voulons plus particulièrement identifier, que nous appelons des obbservations positives et on traite toues les autres observations comme si elles étaient dans la même classe, soit les observations négatives.\nLa sensibilité est le ratio entre le nombre de vrais positifs classé positifs et le nombre de vrais positifs. La spécifité est le ratio entre le nombre de vrais négatifs classé négatifs et le nombre de vrais négatifs.\nNotons que si on est prêt à vivre avec une spécifité de \\(0\\%\\), on peut toujours avoir une sensibilité de \\(100\\%\\) et si on est prêt à vivre avec une sensibilité de \\(0\\%\\), on peut toujours avoir une spécifité de \\(100\\%\\). Si la proportion de négatifs est très élevé, par exemple \\(99.9\\%\\), alors une méthode qui classifie \\(100\\%\\) des observations dans la classe négative aura un taux de bonnes classifications de \\(99.9\\%\\). Mais clairement, cette méthode ne nous sera pas très utile si notre but est d’identifier les cas positifs.\nOn peut exploiter l’estimation d’une probabilité. Les méthodes de classification donnent souvent la probabilité qu’une observation appartienne à une classe, pas uniquement une prévision de la classe elle-même. Une meilleure mesure de la performance de la méthode serait de voir si ces probabilité sont réalistes.\n\n\nLa courbe ROC\nLa courbe ROC (Receiver Operating Characteristic) peut être vue comme une variation de la courbe de gain.\n\nSoit \\(p^{(2)}_i\\), la probabilité que l’observation \\(i\\) appartienne à la classe \\(2\\) selon notre modèle.\nOn choisit une valeur de \\(u \\in [0, 1]\\) et on prédit que l’observation \\(i\\) appartient à la classe \\(2\\) si \\(p^{(2)}_i \\geq u\\) et à la classe \\(1\\) sinon.\nOn répète l’étape \\(2\\) pour \\(i = 1, \\dots, n\\) et on calcule la sensibilité \\(SEN(u)\\) et la spécifité \\(SPE(U)\\) ainsi obtenu.\nOn répète les étapes \\(2\\) et \\(3\\) pour tous les \\(u \\in [0, 1]\\), on place les paires \\((1 - SPE(u), SEN(u))\\) sur un graphique et on lie les points part des segments.\n\nInterprétation:\n\nElle part du point \\((0, 0)\\) et finit au point \\((1, 1)\\).\nSi on classifie les points au hasard, on obtiendra une ligne droite à \\(45\\) degrés.\nUne mesure unique du pouvoir prédictif est l’aire sous la courbe ROC (AUC, Area Under the Curve). Une AUC de 0.5 est ce que l’on espère en classifiant au hasard, et plus on s’approche de \\(1\\), meilleure est la méthode.\n\n\n\nExample",
    "crumbs": [
      "Modules",
      "03 - Généralités",
      "Evaluation de modeles"
    ]
  },
  {
    "objectID": "contents/generalities/01-stat.html",
    "href": "contents/generalities/01-stat.html",
    "title": "Projet d’analyse de données",
    "section": "",
    "text": "On présente ici les différentes étapes d’un projet d’analyse de données.",
    "crumbs": [
      "Modules",
      "03 - Généralités",
      "Projet d'analyse de données"
    ]
  },
  {
    "objectID": "contents/generalities/01-stat.html#projet-danalyse-données",
    "href": "contents/generalities/01-stat.html#projet-danalyse-données",
    "title": "Projet d’analyse de données",
    "section": "Projet d’analyse données",
    "text": "Projet d’analyse données\nUn projet d’analyse de données peut se découper en cinq grandes étapes:\n\nDéfinition des objectifs\nDonnées\nÉlaboration et validation des modèles\nMise en oeuvre\nSuivi de la performance et amélioration\n\nLors de la planification d’un projet, il faut prendre en compte que chaque étape à une importance différente, mais aussi que chacune ne prend pas le même temps d’exécution. Pyle (1999) donne une estimation du temps de chaque étape, ainsi que de leur importance dans la réussite du projet (donné en pourcentage du total).\n\n\n\nÉtape\nTemps\nImportance\n\n\n\n\nComprendre le problème\n\\(10\\%\\)\n\\(15\\%\\)\n\n\nExplorer la solution\n\\(9\\%\\)\n\\(14\\%\\)\n\n\nImplementer la solution\n\\(1\\%\\)\n\\(51\\%\\)\n\n\nPréparer les données\n\\(60\\%\\)\n\\(15\\%\\)\n\n\nAnalyser les données\n\\(15\\%\\)\n\\(3\\%\\)\n\n\nModéliser les données\n\\(5\\%\\)\n\\(2\\%\\)\n\n\n\nOn remarque deux faits importants: ce n’est pas parce qu’une étape est très importante qu’elle va prendre beaucoup de temps. L’implémentation de la solution est très importante (sinon il n’y a pas de résultat), mais ne sera généralement pas très longue à faire (possiblement en quelques lignes de code). À l’inverse, la préparation des données est un étape d’importance moyenne (encore que c’est discutable), mais elle prend la majeur partie du temps du projet. En effet, il faut, par exemple, gérer les données manquantes, les données aberrantes, les éventuels accents pour des données en français, etc.",
    "crumbs": [
      "Modules",
      "03 - Généralités",
      "Projet d'analyse de données"
    ]
  },
  {
    "objectID": "contents/generalities/01-stat.html#définition-des-objectifs",
    "href": "contents/generalities/01-stat.html#définition-des-objectifs",
    "title": "Projet d’analyse de données",
    "section": "Définition des objectifs",
    "text": "Définition des objectifs\nEst-ce que l’on veut: visualiser les données ? explorer et émettre des hypothèses ? tester ? regrouper ? comprendre ? prédire ?\nComment fait-on en pratique ? On pose des questions ! Tout d’abord, il faut clarifier les termes. Qui va utiliser le modèle et comment ? Quelle est la population cible ?\nPourquoi est-ce important d’avoir des objectifs clairs lors d’un projet d’analyse de données ? Cela permet de guider la collecte des données et leur mise en forme. Cela permet de définir un modèle adéquat (e.g. classification vs prédiction). Cela permet d’analyser les résultats à la lumière de l’objectif et donc de permettre à d’autres personnes de juger de la pertinence de ceux-ci. Il est important de définir les objectifs avant de s’intéresser aux données pour ne pas être biaisé par celles-ci.\n\n\n\n\n\n\nExemple\n\n\n\nLa Banque National du Canada voudrait lancer un nouveau produit d’épargne et vous donne accès à sa base de données clients.\nMauvais objectif: Analysez les données de la base clients.\nMeilleur objectif: Pouvez-vous prédire quels clients vont acheter le nouveau produit d’épargne ?\n\n\n\n\n\n\n\n\nExemple\n\n\n\nL’équipe du hockey des Canadiens de Montréal souhaite mieux connaître ses adversaires pour développer des nouvelles tactiques de jeu.\nMauvais objectif: Analysez les données des adversaires.\nMeilleur objectif: Pouvez-vous caractériser le style de jeu des adversaires dans l’optique d’y détecter des points faibles ?\n\n\n\n\n\n\n\n\nExemple\n\n\n\nPharmascience souhaite savoir si son nouveau médicament est efficace.\nMauvais objectif: Analysez les données du médicament.\nMeilleur objectif: Pouvez-vous déterminer un protocole de tests (statistiques) permettant de déterminer si le médicament est efficace ?",
    "crumbs": [
      "Modules",
      "03 - Généralités",
      "Projet d'analyse de données"
    ]
  },
  {
    "objectID": "contents/generalities/01-stat.html#données",
    "href": "contents/generalities/01-stat.html#données",
    "title": "Projet d’analyse de données",
    "section": "Données",
    "text": "Données\nLes données sont le coeur du sujet. Pour être utile, les données doivent être disponibles et de bonnes qualités. Une fois les objectifs définis, on effectue une traitement préliminaire et une exploration basique des données pour ensuite aller vers des modèles plus développés.\n\nInventaire et qualité\nQu’est-ce que l’on veut dire par qualité des données ?\n\nEst-ce que les données sont représentatives de la population cible ?\nEst-ce que les données permetternt de tirer des conclusions de causalité ?\nEst-ce que les données sont fiables ?\n\n\n\nConstitution de la base de données\n\n\nExploration et traitement préliminaire\nSource de données:\nQuelques liens pour récupérer des données.\nNettoyage de données: cf R (importation, nettoyage, tidyverse, types de variables, retirer les doublons, uniformiser les modalités, vérifier le format des valeurs spéciales, pivot, opérateur pipe, jointure).\nExploration des données: modalités rares, modalités trop nombreuses, asymétrie, débalancement des classes, valeurs extrêmes ou aberrantes, variables fortement corrélées, valeurs manquantes.\nStatistiques descriptives",
    "crumbs": [
      "Modules",
      "03 - Généralités",
      "Projet d'analyse de données"
    ]
  },
  {
    "objectID": "contents/generalities/01-stat.html#élaboration-et-validation-des-modèles",
    "href": "contents/generalities/01-stat.html#élaboration-et-validation-des-modèles",
    "title": "Projet d’analyse de données",
    "section": "Élaboration et validation des modèles",
    "text": "Élaboration et validation des modèles",
    "crumbs": [
      "Modules",
      "03 - Généralités",
      "Projet d'analyse de données"
    ]
  },
  {
    "objectID": "contents/generalities/01-stat.html#mise-en-oeuvre",
    "href": "contents/generalities/01-stat.html#mise-en-oeuvre",
    "title": "Projet d’analyse de données",
    "section": "Mise en oeuvre",
    "text": "Mise en oeuvre",
    "crumbs": [
      "Modules",
      "03 - Généralités",
      "Projet d'analyse de données"
    ]
  },
  {
    "objectID": "contents/06-nonsupervisee.html",
    "href": "contents/06-nonsupervisee.html",
    "title": "Non-supervisée",
    "section": "",
    "text": "On veut partitionner \\(n\\) observations en \\(K\\) groupes avec comme objectifs:\n\nque les observations dans une même classe soient le plus similaire possible;\nque les observations dans des classes différentes soient les moins similaires possibles.\n\nOn veut donc définir une fonction, que l’on appelle classifieur, qui prend un numéro d’observation \\(i\\) en entrée et qui donne son numéro de groupe en sortie qui va remplir ces deux objectifs.\n\\[\\begin{align}\nC: \\{ 1, \\dots, n \\} &\\rightarrow \\{ 1, \\dots, K\\} \\\\\ni &\\mapsto C(i)\n\\end{align}\\]\nLa fonction objectif est \\[W(C) = \\sum_{k = 1}^{K} \\sum_{i: C(i) = k} \\sum_{j: C(j) = k} d(x_i, x_j),\\]\nou \\(d(x_i, x_j)\\)\nProblème de grande taille!\nComme on ne peut pas explorer l’espace de toutes les possibilités, nous utiliserons des algorithmes gloutons (greedy algorithm), c’est-à-dire qu’ils vont nous donner une règle \\(C\\) qui minimise \\(W(C)\\) sur un espace restreint et qui ne garantissent pas nous ayons trouvé un minimum global.\nDans notre contexte:\n\n\\(p\\) variables sont numériques/ordinales (et habituellement standardisées).\nLa valeur de \\(K\\) est fixé.",
    "crumbs": [
      "Modules",
      "06 - Non-supervisée"
    ]
  },
  {
    "objectID": "informations/description.html",
    "href": "informations/description.html",
    "title": "Description du cours",
    "section": "",
    "text": "Dans ce cours, nous visons à introduire des méthodes qui permettront aux étudiantes et étudiants d’étudier un jeu de données de “haute dimension” (ici, “haute” est pris dans le sens où l’on ne peut pas faire un simple graphique de l’ensemble des variables pour toutes les observations) sans avoir recours à un modèle probabiliste. Les techniques que l’on y enseigne servent à réduire la dimension des données, identifier certains liens entre les variables, visualiser les données ou à diviser le jeu de données en groupes/classes.\nSans négliger la théorie, l’accent sera mis sur l’aspect pratique de l’analyse des données et l’utilisation d’un langage de programmation, que ce soit R, Python ou autre.",
    "crumbs": [
      "Informations",
      "Description"
    ]
  },
  {
    "objectID": "informations/description.html#objectifs",
    "href": "informations/description.html#objectifs",
    "title": "Description du cours",
    "section": "",
    "text": "Dans ce cours, nous visons à introduire des méthodes qui permettront aux étudiantes et étudiants d’étudier un jeu de données de “haute dimension” (ici, “haute” est pris dans le sens où l’on ne peut pas faire un simple graphique de l’ensemble des variables pour toutes les observations) sans avoir recours à un modèle probabiliste. Les techniques que l’on y enseigne servent à réduire la dimension des données, identifier certains liens entre les variables, visualiser les données ou à diviser le jeu de données en groupes/classes.\nSans négliger la théorie, l’accent sera mis sur l’aspect pratique de l’analyse des données et l’utilisation d’un langage de programmation, que ce soit R, Python ou autre.",
    "crumbs": [
      "Informations",
      "Description"
    ]
  },
  {
    "objectID": "informations/description.html#place-du-cours-dans-le-programme",
    "href": "informations/description.html#place-du-cours-dans-le-programme",
    "title": "Description du cours",
    "section": "Place du cours dans le programme",
    "text": "Place du cours dans le programme\nCe cours est généralement suivi par des étudiants du baccalauréat en statistique lors de leur deuxième année de formation. C’est aussi un cours à option en actuariat, en mathématique ainsi que dans certains programmes de génie et d’administration.\nTous les étudiants doivent s’assurer d’avoir suivi au minimum un cours d’algèbre (ex. MAT-1200) et un cours de statistique de base (ex. STT-1000), car la majorité des méthodes d’analyse de données s’appuient sur ces notions.\nLes étudiants devraient aussi avoir une certaine familiarité avec les concepts algorithmiques de base, ainsi qu’avec (au moins) un langage de programmation.",
    "crumbs": [
      "Informations",
      "Description"
    ]
  },
  {
    "objectID": "informations/description.html#objectifs-spécifiques",
    "href": "informations/description.html#objectifs-spécifiques",
    "title": "Description du cours",
    "section": "Objectifs spécifiques",
    "text": "Objectifs spécifiques\nÀ la fin du cours, l’étudiant ou l’étudiante devrait être capable :\n\nde comprendre et décrire succinctement les fondements théoriques des méthodes d’analyse de données étudiées;\nd’identifier correctement les situations où l’emploi de ces méthodes est indiqué;\nd’utiliser efficacement un langage de programmation pour mettre en oeuvre ces méthodes;\nd’analyser et d’interpréter judicieusement les résultats découlant de l’analyse;\nde formuler par écrit les conclusions de l’analyse, dans le respect des limites de la méthodologie.",
    "crumbs": [
      "Informations",
      "Description"
    ]
  },
  {
    "objectID": "informations/description.html#équipement-informatique",
    "href": "informations/description.html#équipement-informatique",
    "title": "Description du cours",
    "section": "Équipement informatique",
    "text": "Équipement informatique\nVous pourriez avoir besoin d’un ordinateur, de haut-parleurs ou d’un casque d’écoute, d’un microphone, d’une webcam et d’une connexion Internet avec fil à large bande ou sans fil. Pour vérifier les paramètres de configuration minimaux selon le système d’exploitation, nous vous invitons à visiter cette page.\nDe plus, ce cours peut nécessiter des besoins logiciels particuliers qui seront alors décrits dans d’autres sections du plan de cours, le cas échéant.",
    "crumbs": [
      "Informations",
      "Description"
    ]
  },
  {
    "objectID": "informations/description.html#fonctionnement",
    "href": "informations/description.html#fonctionnement",
    "title": "Description du cours",
    "section": "Fonctionnement",
    "text": "Fonctionnement\nLes cours et examens auront lieu en présentiel les mardi et vendredi matin, mais le matériel sera disponible en ligne. De façon générale, la séance du vendredi sera une séance magistrale et la séance du mardi sera dédiée à des exercices pratiques réalisés de façon autonome par les étudiants avec le soutien de l’enseignant.",
    "crumbs": [
      "Informations",
      "Description"
    ]
  },
  {
    "objectID": "informations/description.html#approches-pédagogiques",
    "href": "informations/description.html#approches-pédagogiques",
    "title": "Description du cours",
    "section": "Approches pédagogiques",
    "text": "Approches pédagogiques\nL’approche pédagogique privilégiée est l’exposé interactif (vendredi) en alternance avec des périodes de laboratoires (mardi). L’approche pédagogique est très axée sur l’apprentissage actif et exige un engagement soutenu des étudiants tout au long de la session.",
    "crumbs": [
      "Informations",
      "Description"
    ]
  },
  {
    "objectID": "informations/mcq.html",
    "href": "informations/mcq.html",
    "title": "QCM",
    "section": "",
    "text": "Le premier examen est un questionaire à choix multiples avec quantification de l’incertitude.\nL’ utilisation de degrés de certitude permet à l’étudiant et à l’étudiante d’évaluer le niveau de connaissance de la réponse donnée. La barème des notes, basé sur la théorie des décision (Leclercq et al. 1993), peut sembler bizarre mais il a été fait de tel manière que:\n\ndire la vérité soit la stratégie qui rapporte le plus de points;\nceux qui s’auto-évaluent bien gagnent plus de points que si l’on appliquait un barème correctif tenant compte des probabilités d’avoir la réponse correcte aléatoirement.\n\n\n\n\nSi vous condidérez que votre réponse a une probabilité d’être correcte comprise entre…\n\n\nChoisissez le degré de certitude…\n\n\nVous obtiendrez les points suivants en case de réponse…\n\n\n\n\n\n\n\n\ncorrecte\n\n\nincorrecte\n\n\n\n\n0% et 25%\n\n\n0\n\n\n+13\n\n\n+4\n\n\n\n\n25% et 50%\n\n\n1\n\n\n+16\n\n\n+3\n\n\n\n\n50% et 70%\n\n\n2\n\n\n+17\n\n\n+2\n\n\n\n\n70% et 85%\n\n\n3\n\n\n+18\n\n\n0\n\n\n\n\n85% et 95%\n\n\n4\n\n\n+19\n\n\n-6\n\n\n\n\n95% et 100%\n\n\n5\n\n\n+20\n\n\n-10\n\n\n\nGénéralement, les étudiants s’auto-estiment avec réalisme, et sont avantagés par les degrés de certitude. Leur score obtenu est meilleur que s’il avait été calculé uniquement sur la base du nombre de réponses justes.",
    "crumbs": [
      "Informations",
      "Évaluations",
      "QCM"
    ]
  },
  {
    "objectID": "informations/mcq.html#information-sur-le-premier-examen",
    "href": "informations/mcq.html#information-sur-le-premier-examen",
    "title": "QCM",
    "section": "",
    "text": "Le premier examen est un questionaire à choix multiples avec quantification de l’incertitude.\nL’ utilisation de degrés de certitude permet à l’étudiant et à l’étudiante d’évaluer le niveau de connaissance de la réponse donnée. La barème des notes, basé sur la théorie des décision (Leclercq et al. 1993), peut sembler bizarre mais il a été fait de tel manière que:\n\ndire la vérité soit la stratégie qui rapporte le plus de points;\nceux qui s’auto-évaluent bien gagnent plus de points que si l’on appliquait un barème correctif tenant compte des probabilités d’avoir la réponse correcte aléatoirement.\n\n\n\n\nSi vous condidérez que votre réponse a une probabilité d’être correcte comprise entre…\n\n\nChoisissez le degré de certitude…\n\n\nVous obtiendrez les points suivants en case de réponse…\n\n\n\n\n\n\n\n\ncorrecte\n\n\nincorrecte\n\n\n\n\n0% et 25%\n\n\n0\n\n\n+13\n\n\n+4\n\n\n\n\n25% et 50%\n\n\n1\n\n\n+16\n\n\n+3\n\n\n\n\n50% et 70%\n\n\n2\n\n\n+17\n\n\n+2\n\n\n\n\n70% et 85%\n\n\n3\n\n\n+18\n\n\n0\n\n\n\n\n85% et 95%\n\n\n4\n\n\n+19\n\n\n-6\n\n\n\n\n95% et 100%\n\n\n5\n\n\n+20\n\n\n-10\n\n\n\nGénéralement, les étudiants s’auto-estiment avec réalisme, et sont avantagés par les degrés de certitude. Leur score obtenu est meilleur que s’il avait été calculé uniquement sur la base du nombre de réponses justes.",
    "crumbs": [
      "Informations",
      "Évaluations",
      "QCM"
    ]
  },
  {
    "objectID": "informations/general.html",
    "href": "informations/general.html",
    "title": "Informations générales",
    "section": "",
    "text": "Faculté des sciences et de génie\nDépartement de mathématiques et de statistique",
    "crumbs": [
      "Informations",
      "Générales"
    ]
  },
  {
    "objectID": "informations/general.html#stt-2200-analyse-de-données",
    "href": "informations/general.html#stt-2200-analyse-de-données",
    "title": "Informations générales",
    "section": "STT-2200: Analyse de données",
    "text": "STT-2200: Analyse de données\nFormule d’enseignement: Présentiel\nTemps consacré:\n\nCours: 2h\nLaboratoire: 1h\nTravail personnel: 6h\nTotal: 9h\n\nCrédits: 3\nPréalables:\n\nACT-2000 ou STT-1000 ou STT-1300 ou STT-1900\nMAT-1200 ou ACT-2002\nSTT-1100 ou IFT-4902 ou ECN-2090 ou GLO-1901 ou IFT-1004\n\nPlage horaire:\n\n\n\nType\nJour\nHoraire\nDates\n\n\n\n\nLabo\nMardi\n\n\n\n\nClasse\nVendredi",
    "crumbs": [
      "Informations",
      "Générales"
    ]
  },
  {
    "objectID": "informations/general.html#coordonnées-et-disponibilités",
    "href": "informations/general.html#coordonnées-et-disponibilités",
    "title": "Informations générales",
    "section": "Coordonnées et disponibilités",
    "text": "Coordonnées et disponibilités\nEnseignant: Steven Golovkine\nLocal: VCH-2209\nE-mail: steven.golovkine@mat.ulaval.ca\nDisponibilités: Je suis disponible pour vous rencontrer sur rendez-vous par couriel ou sans rendez-vous du lundi au jeudi entre 8h et 10h.\n\nL’Université reconnaît le droit à la déconnexion des professeures et professeurs, des personnes chargées de cours et des autres membres du personnel enseignant. Cela signifie que ces personnes ne sont pas tenues de consulter les messages qui leur sont envoyés (courriel, boîte vocale, message dans un forum, etc.) pendant les soirs, fins de semaine, jours fériés et vacances. La personne qui aura envoyé un message durant ces périodes devra donc s’attendre à recevoir une réponse dans un délai raisonnable, calculé à partir de la reprise des heures normales de travail.",
    "crumbs": [
      "Informations",
      "Générales"
    ]
  },
  {
    "objectID": "tp/02-revision-tp.html",
    "href": "tp/02-revision-tp.html",
    "title": "TP: Révision",
    "section": "",
    "text": "Vous pouvez faire les exercices dans le langage de votre choix."
  },
  {
    "objectID": "tp/02-revision-tp.html#exercice-1-estimer-pi",
    "href": "tp/02-revision-tp.html#exercice-1-estimer-pi",
    "title": "TP: Révision",
    "section": "Exercice 1: Estimer \\(\\pi\\)",
    "text": "Exercice 1: Estimer \\(\\pi\\)\nDans cet exercice, on se propose d’estimer \\(\\pi\\) grâce à la méthode de Monte-Carlo. Le méthode de Monte-Carlo est une méthode algorithmique permettant d’estimer des quantités en utilisant des tirages aléatoires. Pour estimer \\(\\pi\\), l’idée est de générer des points dans un carré de façon uniforme et ensuite de compter la proportion de ces points qui sont dans le cercle unité.\n\nGénérer un nombre \\(n\\) de points \\((x, y)\\) dans un carré de longueur \\(2\\) centré à l’origine, i.e. \\(x \\in [-1, 1]\\) et \\(y \\in [-1, 1]\\).\nPour chaque point \\(n\\), déterminer si le point appartient au cercle unité.\nCalculer la proportion du nombre de points dans le cercle unité.\nÀ partir du résultat précédent, estimer \\(\\pi\\).\nNotons \\(\\widehat{\\pi}(n)\\), l’estimateur de \\(\\pi\\) utilisant \\(n\\) points générés. Tracer l’erreur d’estimation \\(\\left| \\pi - \\widehat{\\pi}(n) \\right|\\) en fonction de \\(n\\)."
  },
  {
    "objectID": "tp/02-revision-tp.html#exercice-2-estimation-dintégrale",
    "href": "tp/02-revision-tp.html#exercice-2-estimation-dintégrale",
    "title": "TP: Révision",
    "section": "Exercice 2: Estimation d’intégrale",
    "text": "Exercice 2: Estimation d’intégrale\nDans cette exercice, on se propose d’estimer \\[I = \\int_{-1}^{1} \\sqrt{1 - x^2} dx .\\]\n\nCalculer \\(I\\) en utilisant une primitive.\nIl est possible d’estimer \\(I = \\int_{-1}^{1} f(x) dx\\) à l’aide des sommes de Riemann \\(\\widehat{I}(n) = \\frac{1}{n}\\sum_{i = 1}^{n} f(\\frac{i}{n}), i = 1, \\dots, n\\). Estimer \\(I\\) en utilisant les sommes de Riemann.\nL’intégrale \\(I\\) peut être vu comme l’espérance d’un variable aléatoire. \\(I = \\mathbb{E}[f(U)]\\), où \\(U \\sim \\mathcal{U}(-1, 1)\\). On peut estimer \\(I\\) en utilisant \\(\\widetilde{I}(n) = \\frac{1}{n} \\sum_{i = 1}^{n} f(u_i)\\), où les \\(u_i\\) sont des réalisations de la variable aléatoire \\(U\\). Estimer \\(I\\) en utilisant cette méthode.\nComparer la qualité de ces deux estimateurs de \\(I\\) en fonction de \\(n\\)."
  },
  {
    "objectID": "tp/02-revision-tp.html#exercice-3-la-loi-des-gaz-parfaits",
    "href": "tp/02-revision-tp.html#exercice-3-la-loi-des-gaz-parfaits",
    "title": "TP: Révision",
    "section": "Exercice 3: La loi des gaz parfaits",
    "text": "Exercice 3: La loi des gaz parfaits\nDans cet exercice, on se propose de vérifier la loi des gaz parfaits à partir des données. On rappelle que la loi des gaz parfaits est donnée par \\(PV = nRT\\) où :\n\n\\(P\\) est la pression à l’intérieur du volume considéré en Pascal (Pa);\n\\(V\\) est le volume du gaz en m\\(^3\\);\n\\(n\\) est la quantité de matière en mole (mol);\n\\(R = 8.314\\) est la constante universelle des gaz parfaits en J.mol\\(^{-1}\\).K\\(^{-1}\\);\n\\(T\\) est la température à l’intérieur du volume considéré en Kelvin (K).\n\nOn conduit une expérience consistant à chauffer une quantité fixe de gaz dans un récipient fermé de volume fixé. La température \\(T\\) en Kelvin et la pression \\(P\\) en kPa sont enregistrées. On trouve les résultats suivants :\n\ntemperature = [\n  406, 296, 272, 449, 483, 439, 460, 276, 321, 462, 408, 322, 285,\n  411, 491, 359, 453, 486, 413, 350, 263, 456, 390, 462, 389, 494,\n  303, 496, 336, 460\n]\n\npression = [\n  1365, 982, 898, 1486, 1596, 1481, 1506, 906, 1085, 1542, 1367,\n  1072, 955, 1379, 1633, 1186, 1499, 1606, 1378, 1156, 867, 1514,\n  1306, 1525, 1287, 1665, 1020, 1635, 1118, 1529\n]\n\n\nTracer la pression en fonction de la température. Est-ce que le graphique est linéaire ?\nConstruire la matrice \\(X = (1 | \\text{temperature})\\). La première colonne de \\(X\\) est une colonne de \\(1\\) et la deuxième colonne de \\(X\\) est le vecteur des températures.\nCalculer le vecteur \\(\\beta = (X^{\\top} X)^{-1} X^{\\top} Y\\) où \\(Y\\) est le vecteur des pressions.\nQuelle est l’interprétation physique des éléments du vecteur \\(\\beta\\) ?\nÀ quelle valeur de \\(\\beta_0\\), le premier coefficient de \\(\\beta\\), devrait-on s’attendre dans le cadre d’un gaz parfait ? Est-ce le cas ici ? Pourquoi ?\nSupposons que le volume de gaz est de \\(10\\)dm\\(^3\\). Estimer la quantité de matière \\(n\\) en mole utilisé pour avoir les données."
  }
]