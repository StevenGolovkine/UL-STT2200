[
  {
    "objectID": "informations/general.html",
    "href": "informations/general.html",
    "title": "Informations générales",
    "section": "",
    "text": "Faculté des sciences et de génie\nDépartement de mathématiques et de statistique",
    "crumbs": [
      "Informations",
      "Générales"
    ]
  },
  {
    "objectID": "informations/general.html#stt-2200-analyse-de-données",
    "href": "informations/general.html#stt-2200-analyse-de-données",
    "title": "Informations générales",
    "section": "STT-2200: Analyse de données",
    "text": "STT-2200: Analyse de données\nFormule d’enseignement: Présentiel\nTemps consacré:\n\nCours: 2h\nLaboratoire: 1h\nTravail personnel: 6h\nTotal: 9h\n\nCrédits: 3\nPréalables:\n\nACT-2000 ou STT-1000 ou STT-1300 ou STT-1900\nMAT-1200 ou ACT-2002\nSTT-1100 ou IFT-4902 ou ECN-2090 ou GLO-1901 ou IFT-1004\n\nPlage horaire:\n\n\n\nType\nJour\nHoraire\nDates\n\n\n\n\nLabo\nMardi\n\n\n\n\nClasse\nVendredi",
    "crumbs": [
      "Informations",
      "Générales"
    ]
  },
  {
    "objectID": "informations/general.html#coordonnées-et-disponibilités",
    "href": "informations/general.html#coordonnées-et-disponibilités",
    "title": "Informations générales",
    "section": "Coordonnées et disponibilités",
    "text": "Coordonnées et disponibilités\nSteven Golovkine (Enseignant)\nVCH-2200\nsteven.golovkine@mat.ulaval.ca\nDisponibilités\nJe suis disponible pour vous rencontrer sur rendez-vous par couriel ou sans rendez-vous du lundi au jeudi entre 8h et 10h.\n\nL’Université reconnaît le droit à la déconnexion des professeures et professeurs, des personnes chargées de cours et des autres membres du personnel enseignant. Cela signifie que ces personnes ne sont pas tenues de consulter les messages qui leur sont envoyés (courriel, boîte vocale, message dans un forum, etc.) pendant les soirs, fins de semaine, jours fériés et vacances. La personne qui aura envoyé un message durant ces périodes devra donc s’attendre à recevoir une réponse dans un délai raisonnable, calculé à partir de la reprise des heures normales de travail.",
    "crumbs": [
      "Informations",
      "Générales"
    ]
  },
  {
    "objectID": "informations/schedule.html",
    "href": "informations/schedule.html",
    "title": "Plan",
    "section": "",
    "text": "Le plan est donné à titre indicatif.\n\n\n\nSemaine\nDate\nDurée\nPlan\nÉvaluations",
    "crumbs": [
      "Informations",
      "Plan"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STT-2200: Analyse de données",
    "section": "",
    "text": "Bienvenue à tous au cours STT-2200 de l’automne 2025.\nVous trouverez sur ce site toute les informations nécessaire pour les cours, les différents travaux et les exams. Les contenus seront mis à jour au cours de la session avec les corrigés de certains travaux. Je vous invite à regarder les différents modules avant les cours.\nCe site a été créé avec Quarto."
  },
  {
    "objectID": "contents/supervised/02-discriminant.html",
    "href": "contents/supervised/02-discriminant.html",
    "title": "Discriminant",
    "section": "",
    "text": "La méthode a été introduite en 1936 par R. A. Fisher. Il s’intéressait à la taxonimie végétale, c’est-à-dire déterminer l’espèce de fleurs à partir de diverses mesures.\nNotation:\nSoit \\(X = (X_{i j})\\), qui est une matrice de dimension \\(n \\times p\\), où \\(n\\) est le nombre d’individus dans l’échantillon, \\(p\\) est le nombre de variables et \\(X_{i j}\\) est la valeur de la \\(j\\)e variable pour le \\(i\\)e individus.\nIdentification des groupes:\nScore de l’analyse discriminante: on a des observations dans \\(R^p\\). Pour faire de la classification à partir de \\(X_{1}, \\dots, X_p\\), on doit partionner \\(R^p\\) en \\(q\\) sous-ensembles de sorte que chacun des \\(q\\) sous-ensembles est associé à un des \\(q\\) groupes.\nOn va chercher à passer de la dimension \\(p\\) à la dimension \\(1\\) en calculant un score \\(f(x_{1}, \\dots, x_p) \\in \\mathbb{R}\\) pour chaque observation et ensuite utiliser ce score pour déterminer le groupe d’appartenance (et donc partionner \\(R\\)). Le score proposé par Fisher est une combinaison linéaire des variables, c’est-à-dire \\[f(X_{1}, \\dots, X_p) = a^{\\top} X + b = a_{1} X_{1} + \\cdots + a_p X_p + b.\\]\nOn en déduira \\(q\\) intervalles de décision \\(S_{1}, \\dots, S_q\\) associés aux groupes.\nOn voudrait choisir le vecteur \\(a\\) de sorte que les scores soient, à la fois, très différents entre les groupes et très similaires à l’intérieur d’un groupe. On s’intéresse donc à la variabilité des scores à l’intérieur des groupes et entre les groupes.\nÉtant donné \\(a \\in R^p\\), on a: \\[\\mathrm{Var}(f(X_{1}, \\dots, X_p)) = \\mathrm{Var}(a^{\\top} X) = a^{\\top} \\mathrm{Var}(X) a,\\] que nous estimons à partir des \\(n\\) observations par \\[\\widehat{\\mathrm{Var}}(f(X_{1}, \\dots, X_p)) = \\frac{1}{n} a^{\\top} S a.\\]\nLa base de l’analyse discriminante repose sur le fait que \\[S = W + B,\\] où \\(W\\) est la matrice de variance intragroupe et \\(B\\) est la matrice de variance intergroupe.\nOn peut prouver ce résultat en considérant la définition des matrices \\(S, W\\) et \\(B\\). La moyenne de la variable \\(j\\) pour tous les individus de l’échantillon est \\[\\overline{X}_j = \\frac{1}{n}\\sum_{i = 1}^{n} X_{i j}.\\]\nLa moyenne de la variable \\(j\\) pour les individus du groupe \\(k\\) est \\[\\overline{X}_{k j} = \\frac{1}{n_{k}} \\sum_{i \\in I_k} X_{i j}.\\]\nLa somme des carrés totale est \\[s_{j j^\\prime} \\sum_{i = 1}^{n} (X_{i j} - \\overline{X}_j)(X_{i j^\\prime} - \\overline{X}_{j^\\prime}).\\]\nOn tirerait de la matrice \\(S\\) une estimation de \\(\\mathrm{Cov}(X_j, X_{j^\\prime})\\) si toutes les observations provenaient d’un même groupe. On définit \\(s_{j j^\\prime}\\) comme étant \\[s_{j j^\\prime} = w_{j j^\\prime} + b_{j j^\\prime},\\] où \\[w_{j j^\\prime} = \\sum_{k = 1}^{q} \\sum_{i \\in I_k} (X_{i j} - \\overline{X}_{k j})(X_{i j^\\prime} - \\overline{X}_{k j^\\prime}),\\]\n\\[b_{j j^\\prime} = \\sum_{k = 1}^{q} n_k (\\overline{X}_{k j} - \\overline{X}_j)(\\overline{X}_{k j^\\prime} - \\overline{X}_{j^\\prime}).\\]\nPreuve:\nOn obtient \\[\\widehat{\\mathrm{Var}}(a^{\\top} X) = \\frac{1}{n} a^{\\top} S a = \\frac{1}{n} \\left( a^{\\top} W a + a^{\\top} B a \\right).\\]\nOn se rappelle que l’on veut choisir le vecteur \\(a\\) pour que les scores puissent facilement séparer les groupes. En d’autres mots, on veut des scores les plus similaires possible à l’intérieur d’un groupe et des scores les plus différents possible entre les groupes.\nOn propose de choisir le vecteur \\(a in \\mathbb{R}^p\\) pour maximiser \\[\\frac{a^{\\top} B a}{a^{\\top} W a} \\quad\\text{où}\\quad \\frac{a^{\\top} B a}{a^{\\top} S a}.\\] ce vecteur est unique à une constante près.\nCe problème peut être reformuler des faons suivantes:\nEn écrivant la troisième formulation \\[c^{\\top} \\left( S^{-1/2} B S^{-1/2} \\right) c \\quad\\text{s.c.} c^{\\top} c = 1,\\] on peut prendre \\(a = S^{-1/2} c\\), où \\(c\\) est un vecteur propre normé associé à \\(\\lambda_{1}\\) la première valeur propre de \\(S^{-1/2} B S^{-1/2}\\). De faon équivalente, de la deuxième formulation, on peut prendre \\(a\\), un vecteur propre normé associé à \\(\\lambda_{1}\\) la première valeur propre de \\(S^{-1} B\\). Notons que comme \\[S^{-1/2} B S^{-1/2} c = \\lambda c \\quad\\text{et}\\quad a = S^{-1/2} c,\\] alors \\[S^{-1/2} B a = \\lambda S^{1/2} a \\Rightarrow S^{-1} B a = \\lambda a.\\] Les valeurs propres de \\(S^{-1} B\\) et de \\(S^{-1/2} B S^{-1/2}\\) sont donc les mêmes.\nLa fonction discriminante de Fisher est donc \\[f(x) = a^{\\top} (x - \\overline{X}),\\] où \\(a\\) est le vecteur propre normé associé à la plus grande valeur propre de \\(S^{-1} B\\). Les scores \\(U_i = a^{\\top} (X_{i} - \\overline{X})\\) sont les scores linéaires en \\(X_i\\) qui ont le rapport (variance inter) / (variance intra) le plus élevé. On peut aussi prendre \\(U_i = a^{\\top} X_i\\), car ajouter la même constante à toutes les observations \\(i = 1, \\dots, n\\) ne change rien.\nPuisque la matrice \\(S^{-1/2} B S^{-1/2}\\) est symétrique et définie positive, ses valeurs propres sont toutes réelles et positives. De plus, on a que \\(S^{-1} B a - \\lambda_{1} a\\). Ainsi, \\[B a = \\lambda_{1} S a \\Rightarrow a^{\\top} B a = \\lambda_{1} a^{\\top} S a \\Rightarrow \\lambda_{1} = \\frac{a^{\\top} B a}{a^{\\top} S a}.\\]\nOn a donc \\(0 \\leq \\lambda_{1} \\leq 1\\). La valeur propre \\(\\lambda_{1}\\) peut donc être vue comme le pouvoir discriminant de \\(f\\):",
    "crumbs": [
      "Modules",
      "05 - Supervisée",
      "Discriminant"
    ]
  },
  {
    "objectID": "contents/supervised/02-discriminant.html#example",
    "href": "contents/supervised/02-discriminant.html#example",
    "title": "Discriminant",
    "section": "Example",
    "text": "Example",
    "crumbs": [
      "Modules",
      "05 - Supervisée",
      "Discriminant"
    ]
  },
  {
    "objectID": "contents/remainders/01-linear-algebra.html",
    "href": "contents/remainders/01-linear-algebra.html",
    "title": "Algèbre",
    "section": "",
    "text": "Cf. cours MAT-1200. Donner quelques reférences.\nNotons \\(M\\), \\(N\\) et \\(P\\) des matrices de taille \\(n \\times m\\), \\(A\\) et \\(B\\) des matrices carrées et \\(I_n\\) la matrice identités, de dimension \\(n \\times n\\), et \\(u\\) et \\(v\\) des vecteurs colonnes de taille \\(n\\).\n\n\n\n\n\n\nPropriétés de l’inverse\n\n\n\n\\[(AB)^{-1} = B^{-1} A^{-1}\\]\n\n\n\n\n\n\n\n\nPropriétés du déterminant\n\n\n\n\\[\\begin{align*}\n  \\text{det}(A^\\top) &= \\text{det}(A) \\\\\n  \\text{det}(A^{-1}) &= 1 / \\text{det}(A) \\\\\n  \\text{det}(AB) &= \\text{det}(A)\\text{det}(B)\n\\end{align*}\\]\n\n\n\n\n\n\n\n\nPropriétés de la trace\n\n\n\n\\[\\begin{align*}\n  \\text{tr}(A + B) &= \\text{tr}(A) + \\text{tr}(B) \\\\\n  \\text{tr}(MN) &= \\text{tr}(NM)\n\\end{align*}\\]\n\n\nPropriété de matrices:\n\nSoit \\(A\\) une matrice symmétrique de dimension \\(n \\times n\\). \\(A\\) est définie positive si elle est positive et inversible, c’est-à-dire si \\(u^\\top A u &gt; 0\\) pour tout \\(x \\in \\mathbb{R}^n\\) tel que \\(x \\neq 0\\).\nSoit \\(A\\) une matrice carrée à valeur dans \\(\\mathbb{R}\\). \\(A\\) est orthogonal si \\(A^\\top A = A A^\\top = I_n\\).\n\nValeurs et vecteurs propres:\n\nSoit \\(A\\) une matrice carrée de dimension \\(n \\times n\\). On dit que \\(\\lambda\\) est une valeur propre de \\(A\\) si il existe un vecteur \\(u \\neq 0 \\in \\mathbb{R}^n\\) tel que \\[Au = \\lambda u.\\] Le vecteur \\(u\\) est appelé vecteur propre correspondant à la valeur propre \\(\\lambda\\) et l’ensemble des nombres réels \\(\\lambda\\) satidfaisant l’équation est appelé spectre de la matrice \\(A\\) et noté \\(\\text{sp}(A)\\).\nSi \\(u\\) est un vecteur propre de \\(A\\) correspondant à une valeur propre \\(\\lambda\\), alors \\(cu\\), \\(c \\neq 0 \\in \\mathbb{R}\\) sera également un vecteur propre de \\(A\\) correspondant à \\(\\lambda\\).\nSi \\(A\\) est symmétrique et \\(u_{1}\\) et \\(u_{2}\\) sont des vecteurs propres correspondant à des valeurs propres différentes de \\(A\\), alors \\(u_{1}\\) et \\(u_{2}\\) sont orthogonaux, i.e. \\(u_{1}^\\top u_{2} = 0\\).\nSi \\(A\\) a comme valeurs propres (réelles, mais pas forcément distinctes) \\(\\lambda_{1}, \\dots, \\lambda_{n}\\), alors \\[\\text{A} = \\prod_{i = 1}^{n} \\lambda_i \\quad\\text{et}\\quad \\text{tr}(A) = \\sum_{i = 1}^{n} \\lambda_i.\\]\nSi \\(A\\) est symmétrique, toutes ses valeurs propres sont réelles.\nSi \\(A\\) est définie positive, alors toutes ses valeurs propres sont positives.\n\nDiagonalisation de matrices:\n\nSoit \\(A\\) une matrice carrée de dimension \\(n \\times n\\). On dit que \\(A\\) est diagonalisable s’il existe une matrice carrée \\(n \\times n\\) non-singulière \\(P\\) et une matrice \\(n \\times n\\) diagonale \\(D\\) telles que \\[P^{-1} A P = D \\leftrightarrow A = P D P^{-1}.\\]\n\nToute matrice carrée symmétrique est diagonalisable part une matrice orthogonal \\(P\\).\nThéorème de décomposition spectrale:\nSoit \\(A\\) une matrice carrée symmétrique de dimension \\(n \\times n\\) et ses \\(n\\) valeurs propres \\(\\lambda_{1}, \\dots, \\lambda_n\\). Alors il existe une matrice orthogonal \\(P\\) telle que \\[A = P \\Lambda P^\\top, \\quad\\text{où}\\quad \\Lambda = \\text{diag}(\\lambda_1, \\dots, \\lambda_n).\\]\nSi \\(A\\) admet \\(n\\) valeurs propres positives distinctes, alors on peut prendre \\(P\\) comme la matrice dont la \\(k\\)e coloone est le vacteur propre normé correspondant à la \\(k\\)e valeur propre \\(\\lambda_k\\).\nSoit deux matrices symétriques, \\(A\\) et \\(M\\), comment déterminer le vecteur \\(u\\) tel que \\(u^{\\top}Au\\) soit maximal, sachant que \\(u^{\\top}Mu = 1\\) ? Il faut prendre \\(u\\) comment le vecteur propre de \\(M^{-1}A\\) associé à \\(\\lambda\\) la valeur propre maximale de \\(M^{-1}A\\). On obtient ainsi \\[u^{\\top} A u = u^{\\top}\\lambda M u = \\lambda U^{\\top} M u = \\lambda.\\]",
    "crumbs": [
      "Modules",
      "02 - Révisions",
      "Algèbre"
    ]
  },
  {
    "objectID": "contents/remainders/01-linear-algebra.html#révisions-dalgèbre-linéaire",
    "href": "contents/remainders/01-linear-algebra.html#révisions-dalgèbre-linéaire",
    "title": "Algèbre",
    "section": "",
    "text": "Cf. cours MAT-1200. Donner quelques reférences.\nNotons \\(M\\), \\(N\\) et \\(P\\) des matrices de taille \\(n \\times m\\), \\(A\\) et \\(B\\) des matrices carrées et \\(I_n\\) la matrice identités, de dimension \\(n \\times n\\), et \\(u\\) et \\(v\\) des vecteurs colonnes de taille \\(n\\).\n\n\n\n\n\n\nPropriétés de l’inverse\n\n\n\n\\[(AB)^{-1} = B^{-1} A^{-1}\\]\n\n\n\n\n\n\n\n\nPropriétés du déterminant\n\n\n\n\\[\\begin{align*}\n  \\text{det}(A^\\top) &= \\text{det}(A) \\\\\n  \\text{det}(A^{-1}) &= 1 / \\text{det}(A) \\\\\n  \\text{det}(AB) &= \\text{det}(A)\\text{det}(B)\n\\end{align*}\\]\n\n\n\n\n\n\n\n\nPropriétés de la trace\n\n\n\n\\[\\begin{align*}\n  \\text{tr}(A + B) &= \\text{tr}(A) + \\text{tr}(B) \\\\\n  \\text{tr}(MN) &= \\text{tr}(NM)\n\\end{align*}\\]\n\n\nPropriété de matrices:\n\nSoit \\(A\\) une matrice symmétrique de dimension \\(n \\times n\\). \\(A\\) est définie positive si elle est positive et inversible, c’est-à-dire si \\(u^\\top A u &gt; 0\\) pour tout \\(x \\in \\mathbb{R}^n\\) tel que \\(x \\neq 0\\).\nSoit \\(A\\) une matrice carrée à valeur dans \\(\\mathbb{R}\\). \\(A\\) est orthogonal si \\(A^\\top A = A A^\\top = I_n\\).\n\nValeurs et vecteurs propres:\n\nSoit \\(A\\) une matrice carrée de dimension \\(n \\times n\\). On dit que \\(\\lambda\\) est une valeur propre de \\(A\\) si il existe un vecteur \\(u \\neq 0 \\in \\mathbb{R}^n\\) tel que \\[Au = \\lambda u.\\] Le vecteur \\(u\\) est appelé vecteur propre correspondant à la valeur propre \\(\\lambda\\) et l’ensemble des nombres réels \\(\\lambda\\) satidfaisant l’équation est appelé spectre de la matrice \\(A\\) et noté \\(\\text{sp}(A)\\).\nSi \\(u\\) est un vecteur propre de \\(A\\) correspondant à une valeur propre \\(\\lambda\\), alors \\(cu\\), \\(c \\neq 0 \\in \\mathbb{R}\\) sera également un vecteur propre de \\(A\\) correspondant à \\(\\lambda\\).\nSi \\(A\\) est symmétrique et \\(u_{1}\\) et \\(u_{2}\\) sont des vecteurs propres correspondant à des valeurs propres différentes de \\(A\\), alors \\(u_{1}\\) et \\(u_{2}\\) sont orthogonaux, i.e. \\(u_{1}^\\top u_{2} = 0\\).\nSi \\(A\\) a comme valeurs propres (réelles, mais pas forcément distinctes) \\(\\lambda_{1}, \\dots, \\lambda_{n}\\), alors \\[\\text{A} = \\prod_{i = 1}^{n} \\lambda_i \\quad\\text{et}\\quad \\text{tr}(A) = \\sum_{i = 1}^{n} \\lambda_i.\\]\nSi \\(A\\) est symmétrique, toutes ses valeurs propres sont réelles.\nSi \\(A\\) est définie positive, alors toutes ses valeurs propres sont positives.\n\nDiagonalisation de matrices:\n\nSoit \\(A\\) une matrice carrée de dimension \\(n \\times n\\). On dit que \\(A\\) est diagonalisable s’il existe une matrice carrée \\(n \\times n\\) non-singulière \\(P\\) et une matrice \\(n \\times n\\) diagonale \\(D\\) telles que \\[P^{-1} A P = D \\leftrightarrow A = P D P^{-1}.\\]\n\nToute matrice carrée symmétrique est diagonalisable part une matrice orthogonal \\(P\\).\nThéorème de décomposition spectrale:\nSoit \\(A\\) une matrice carrée symmétrique de dimension \\(n \\times n\\) et ses \\(n\\) valeurs propres \\(\\lambda_{1}, \\dots, \\lambda_n\\). Alors il existe une matrice orthogonal \\(P\\) telle que \\[A = P \\Lambda P^\\top, \\quad\\text{où}\\quad \\Lambda = \\text{diag}(\\lambda_1, \\dots, \\lambda_n).\\]\nSi \\(A\\) admet \\(n\\) valeurs propres positives distinctes, alors on peut prendre \\(P\\) comme la matrice dont la \\(k\\)e coloone est le vacteur propre normé correspondant à la \\(k\\)e valeur propre \\(\\lambda_k\\).\nSoit deux matrices symétriques, \\(A\\) et \\(M\\), comment déterminer le vecteur \\(u\\) tel que \\(u^{\\top}Au\\) soit maximal, sachant que \\(u^{\\top}Mu = 1\\) ? Il faut prendre \\(u\\) comment le vecteur propre de \\(M^{-1}A\\) associé à \\(\\lambda\\) la valeur propre maximale de \\(M^{-1}A\\). On obtient ainsi \\[u^{\\top} A u = u^{\\top}\\lambda M u = \\lambda U^{\\top} M u = \\lambda.\\]",
    "crumbs": [
      "Modules",
      "02 - Révisions",
      "Algèbre"
    ]
  },
  {
    "objectID": "contents/unsupervised/01-kmeans.html",
    "href": "contents/unsupervised/01-kmeans.html",
    "title": "\\(k\\)-means",
    "section": "",
    "text": "Algorithme des \\(k\\)-moyennes:\n\\[\\mu_{k} = \\frac{1}{N_k} \\sum_{i: C(i) = k} x_i, \\quad k = 1, \\dots, K,\\]\noù \\(N_{k}\\) est le nombre d’observations dans le group \\(k\\).\nExample\nInconvénients des \\(k\\)-means:\nExpliciter chaque nconvénients et de potentielles solutions.",
    "crumbs": [
      "Modules",
      "06 - Non-supervisée",
      "$k$-means"
    ]
  },
  {
    "objectID": "contents/unsupervised/01-kmeans.html#k-médoides",
    "href": "contents/unsupervised/01-kmeans.html#k-médoides",
    "title": "\\(k\\)-means",
    "section": "\\(k\\)-médoides",
    "text": "\\(k\\)-médoides\nOn n’utilise plus le centre, mais l’observation qui minimise les distances dans chaque groupe. La médiane plutôt que la moyenne.\nAvantages:\n\nPermet d’intégrer des variables ordinales\nRobuste\nPermet de bien spécifier la matrix de distance\n\nInconvénients:\n\nIl faut connaître le nombre de groupes.",
    "crumbs": [
      "Modules",
      "06 - Non-supervisée",
      "$k$-means"
    ]
  },
  {
    "objectID": "contents/02-revisions.html",
    "href": "contents/02-revisions.html",
    "title": "Révisions",
    "section": "",
    "text": "Slides: link",
    "crumbs": [
      "Modules",
      "02 - Révisions"
    ]
  },
  {
    "objectID": "contents/generalities/02-distance.html",
    "href": "contents/generalities/02-distance.html",
    "title": "Distances",
    "section": "",
    "text": "On a besoin d’une notion de distance et de similarité. On a besoin d’une définition de ce que sont des observations similaires ou des observations différentes. On veut quantifier la similarité ou la distance entre deux observations. Le choix de la métrique est une étape essentielle dans le processus d’analyse de données.",
    "crumbs": [
      "Modules",
      "03 - Généralités",
      "Distances"
    ]
  },
  {
    "objectID": "contents/generalities/02-distance.html#introduction",
    "href": "contents/generalities/02-distance.html#introduction",
    "title": "Distances",
    "section": "",
    "text": "On a besoin d’une notion de distance et de similarité. On a besoin d’une définition de ce que sont des observations similaires ou des observations différentes. On veut quantifier la similarité ou la distance entre deux observations. Le choix de la métrique est une étape essentielle dans le processus d’analyse de données.",
    "crumbs": [
      "Modules",
      "03 - Généralités",
      "Distances"
    ]
  },
  {
    "objectID": "contents/generalities/02-distance.html#notion-de-distance",
    "href": "contents/generalities/02-distance.html#notion-de-distance",
    "title": "Distances",
    "section": "Notion de distance",
    "text": "Notion de distance\n\n\n\n\n\n\nDéfinition de distance\n\n\n\nUne mesure de distance \\(d\\) doit satisfaire les propriétés suivantes pour tout \\(x, y, z \\in A\\), \\(A\\) étant un ensemble quelconque:\n\n\\(d(x, y) \\geq 0\\);\n\\(d(x, x) = 0\\);\n\\(d(x, y) = d(y, x)\\);\n\\(d(x, y) \\leq d(x, z) + d(y, z)\\).\n\n\n\nLa distance euclidienne:\nSi les observations sont constituées de \\(p\\) nombres réels de même ordre de grandeur, alors la distance euclidienne entre deux éléments de \\(\\mathbb{R}^p\\) est une mesure raisonnable.\nSoit \\(x, y \\in \\mathbb{R}^n\\), la distance euclidienne est données par : \\[d(x, y) = \\left\\| x - y \\right\\|_2 = \\left( \\sum_{i = 1}^{n} (x_i - y_i)^2 \\right)^{1/2}.\\]\nQue fait-on dans d’autres cas ? Plusieurs mesures ont été développées pour leur application particulière. Voici les plus classiques.\nUne distance est peu plus générale que la distance euclidienne est la distance \\(L_p\\). Soit \\(x, y \\in \\mathbb{R}^n\\), la distance \\(L_p\\) est donnée, pour \\(p &gt; 0\\), par : \\[d(x, y) = \\left\\| x - y \\right\\|_p = \\left( \\sum_{i = 1}^{n} |x_i - y_i|^p \\right)^{1 /p} .\\]\nLa distance euclidienne correspond à la distance \\(L_p\\) avec \\(p = 2\\). Cas particulier avec \\(p = 1\\), distance de Manhattan:\n\\[d(x, y) = \\left\\| x - y \\right\\|_1 = \\sum_{i = 1}^{n} | x_i - y_i |.\\]\n\n\n\n\n\n\nProperties\n\n\n\nLa distance \\(L_p\\) n’est pas invariante à un changement d’échelle, i.e., \\(L_p(x, y) \\neq L_p(\\lambda x, \\lambda y)\\).\n\n\nStandardisation",
    "crumbs": [
      "Modules",
      "03 - Généralités",
      "Distances"
    ]
  },
  {
    "objectID": "contents/generalities/02-distance.html#notion-de-similarité",
    "href": "contents/generalities/02-distance.html#notion-de-similarité",
    "title": "Distances",
    "section": "Notion de similarité",
    "text": "Notion de similarité\n\n\n\n\n\n\nDéfinition d’un indice de similarité\n\n\n\nUn indice de similarité \\(s\\) doit satisfaire les propriétés suivantes pour tout \\(x, y \\in A\\), \\(A\\) étant un ensemble quelconque:\n\n\\(s(x, y) \\geq 0\\);\n\\(s(x, y) = s(y, x)\\);\n\\(s(x, x) = 1 \\geq s(x, y)\\).\n\n\n\nUne distance peut se transformer en similarité en posant \\(s(x, y) = \\frac{1}{1 + d(x, y)}\\). L’inverse n’est pas vrai, dû à l’inégalité triangulaire. On peut aussi définir la dissemblance entre deux objects: \\(d^\\star(x, y) = 1 - s(x, y)\\).",
    "crumbs": [
      "Modules",
      "03 - Généralités",
      "Distances"
    ]
  },
  {
    "objectID": "contents/dimension/01-pca.html",
    "href": "contents/dimension/01-pca.html",
    "title": "ACP",
    "section": "",
    "text": "Il y a plusieurs raisons de vouloir changer le dimension des données. Il est possible que la dimension des données soit trop importante pour avoir une visualisation intéressante, que la dimension actuelle ne permette pas une bonne séparation des classes dans les données, etc.",
    "crumbs": [
      "Modules",
      "04 - Dimension",
      "ACP"
    ]
  },
  {
    "objectID": "contents/dimension/01-pca.html#changer-de-dimension---pourquoi-faire",
    "href": "contents/dimension/01-pca.html#changer-de-dimension---pourquoi-faire",
    "title": "ACP",
    "section": "",
    "text": "Il y a plusieurs raisons de vouloir changer le dimension des données. Il est possible que la dimension des données soit trop importante pour avoir une visualisation intéressante, que la dimension actuelle ne permette pas une bonne séparation des classes dans les données, etc.",
    "crumbs": [
      "Modules",
      "04 - Dimension",
      "ACP"
    ]
  },
  {
    "objectID": "contents/dimension/01-pca.html#analyse-en-composantes-principales",
    "href": "contents/dimension/01-pca.html#analyse-en-composantes-principales",
    "title": "ACP",
    "section": "Analyse en composantes principales",
    "text": "Analyse en composantes principales\nL’analyse en composantes principales (ACP) est une méthode permettant de réduire la dimension d’un jeu de données tout en conservant le plus d’information possible. Cette méthode est utilisée lorsque l’on a \\(n\\) observations de \\(p\\) variables continues avec \\(p\\) trop “grand” pour nos besoins.\nPourquoi l’ACP est-elle utilisée ?\n\nVisualisation d’un jeu de données;\nRéduction du nombre de variables de \\(p\\) à \\(p^{\\prime} \\ll p\\) pour faciliter la construction de modèle. Exemples: analyse de texte, analyse de données génétique;\nEffectuer une rotation d’axes pour simplifier la structure de corrélation;\nCompression de données.\n\nLa méthode a été introduite par H. Hotelling dans (Hotelling 1933).\nVisualiser, comprendre, modéliser, classifier des données sont toutes des tâches beacoup simples à accomplir si le nomnre de variables dans un jeu de données est faible. Si un jeu de données comprend un grand nombre de variables, une première question que l’on peut se poser est: “Est-il possible de réduire la dimension du problème sans trop perdre d’information ?”. En omettant tout simplement des variables, on risque de perdre beaucoup d’information utile. Une meilleure solution consiste à trouver des combinaisons linéaires des variables en vue de conserver le maximum d’information sur le jeu de données.\n\n\n\n\n\n\nExample\n\n\n\n\nComparer des équipes de hockey sur la base de six statistiques de fin de saison.\nComparer la criminalité entre états sur la base des taux de sept types de crimes différents.\nCompresser des images formées de \\(1084 \\times 1084\\) pixels.\nIdentifier le nombre de variantes d’un type de tumeur à partir du degré d’expression de millions de gènes.\n\n\n\n\nLes maths\nSoit un vecteur aléatoire composé de \\(p\\) variables \\(X = \\left( X_{1}, \\dots, X_p \\right)\\) ayant comme matrice de variance \\(\\Sigma\\). On aimerait définir une première composante principale, \\[Y_{1} = \\alpha_{1}^{\\top} X = \\sum_{i = 1}^{p} \\alpha_{1i}X_i,\\] de sorte que la variance de \\(Y_{1}\\) soit maximale. L’idée est simple: on désire combiner \\(p\\) variables en une seule, mais en “capturant” la plus grande partie possible de la variabilité.\nIl faut d’abord ajouter une contrainte sur \\(\\alpha_{1}\\), puisque sinon on n’aurait qu’à prendre \\(\\alpha_{1i} = \\pm \\infty\\) et on aurait \\(\\mathrm{Var}(Y_{1}) = +\\infty\\) ce qui est définitivement maximal ! On verra qu’il est pratique de contraindre \\(\\alpha_{1}\\) de sorte qu’il ait une norme égale à \\(1\\).\n\nCalcul de la première composante:\n\n\\[\\mathrm{Var}(Y_1) = \\alpha_1^\\top \\Sigma \\alpha_{1} \\]\nLe problème est donc de maximiser \\[F(\\alpha_{1} = \\alpha_{1}^\\top \\Sigma \\alpha_1, \\quad\\text{s.c.}\\quad \\alpha_1^\\top \\alpha_1 = 1.\\]\nOn peut récrire ce problème à l’aide des multiplicateurs de Lagrange, soit maximiser \\[F(\\alpha_1, \\lambda) = \\alpha_1^\\top \\Sigma \\alpha_1 - \\lambda (\\alpha_1^\\top \\alpha_1 -1 ),\\] où \\(\\lambda\\) est un multiplicateur de Lagrange.\nPour solutionner ce problème, on dérive \\(F\\) par rapport à \\(\\alpha_{1}\\) et à \\(\\lambda\\).\n\\[\\begin{cases}\n\\frac{\\partial F(\\alpha_{1}, \\lambda)}{\\partial \\alpha_{1}} = 2 \\Sigma \\alpha_{1} - 2 \\lambda \\alpha_{1} \\\\\n\\frac{\\partial F(\\alpha_{1}, \\lambda)}{\\partial \\lambda} = 1 - \\alpha_{1}^\\top \\alpha_{1}\n\\end{cases}\n.\\]\nEnsuite, on égalise à \\(0\\), ce qui donne: \\[\\begin{cases}\n\\Sigma \\alpha_{1} = \\lambda \\alpha_{1} \\\\\n\\alpha_{1}^\\top \\alpha_{1} = 1\n\\end{cases}\n.\\] La seconde équation est bien entendue notre contrainte. La première équation est celle qui nous intéresse. En utilisant cette équation et la définition des éléments propres, on déduit que\n\n\\(\\alpha_{1}\\) est un vecteur propre (normé) de \\(\\Sigma\\);\n\\(\\lambda\\) est la valeur propre correspondante.\n\nOn a donc que \\[\\mathrm{Var}(Y_{1}) = \\alpha_{1}^\\top \\Sigma \\alpha_{1} = \\lambda \\alpha_{1}^\\top \\alpha_1 = \\lambda.\\] Puisque l’on veut maximiser cette quantité, on conclut que:\n\n\\(\\lambda = \\lambda_{1}\\), la plus grande valeur propre de \\(\\Sigma\\);\n\\(\\alpha_{1}\\), le vecteur propre normé correspondant.\n\n\nCalcul de la deuxième composante:\n\nOn poursuit simultanément deux objectifs:\n\nConserver le maximum de variation présente dans \\(X\\);\nSimplifier la structure de dépendancce pour faciliter l’interprétation et assurer la stabilité numérique d’éventuelles méthodes qui utiliseront les composantes principales obtenues.\n\nÉtant donné \\(Y_{1}\\), la deuxième composante principale \\(Y_{2} = \\alpha_{2}^\\top X\\) est définie telle que\n\n\\(\\mathrm{Var}(Y_{2}) = \\alpha_{2}^\\top \\Sigma \\alpha_{2}\\) est maximale;\n\\(\\alpha_{2}^\\top \\alpha_{2} = 1\\);\n\\(\\mathrm{Cov}(Y_{1}, Y_{2}) = 0\\).\n\nOn a que \\[\\mathrm{Cov}(Y_{1}, Y_{2}) = \\mathrm{Cov}(\\alpha_{1}^\\top X, \\alpha_{2}^\\top X) = \\alpha_{1}^\\top \\Sigma \\alpha_{2} = \\alpha_{2}^\\top \\Sigma \\alpha_{1} = \\lambda_{1} \\alpha_{2}^\\top \\alpha_{1}.\\]\nOn cherche donc le vecteur \\(\\alpha_{2}\\) qui maximise: \\[F(\\alpha_{2}, \\lambda, \\kappa) = \\alpha_{2}^\\top \\Sigma \\alpha_{2} - \\lambda (\\alpha_{2}^\\top \\alpha_{2} - 1) - \\kappa (\\alpha_{2}^\\top \\alpha_{1} - 0).\\]\nDe même que pour la première composante, on dérive \\(F\\) par rapport à \\(\\alpha_{2}\\), \\(\\lambda\\) et \\(\\kappa\\).\n\\[\\begin{cases}\n\\frac{\\partial F(\\alpha_{2}, \\lambda, \\kappa)}{\\partial \\alpha_{2}} = 2 \\Sigma \\alpha_{2} - 2 \\lambda \\alpha_{2} - \\kappa \\alpha_1 \\\\\n\\frac{\\partial F(\\alpha_{2}, \\lambda, \\kappa)}{\\partial \\lambda} = 1 - \\alpha_{2}^\\top \\alpha_{2} \\\\\n\\frac{\\partial F(\\alpha_{2}, \\lambda, \\kappa)}{\\partial \\kappa} = - \\alpha_2^\\top \\alpha_1\n\\end{cases}\n\\]\nEn égalisant les équations à \\(0\\), on retrouve les deux équations des contraintes, ainsi que\n\\[2 \\Sigma \\alpha_{2} - 2 \\lambda \\alpha_{2} - \\kappa \\alpha_{1} = 0.\\]\nEn multipliant cette équation à gauche et à droite par \\(\\alpha_{1}^\\top\\), on trouve\n\\[2 \\alpha_{1}^\\top \\Sigma \\alpha_{2} - 2 \\alpha_{1}^\\top \\lambda \\alpha_{2} - \\kappa \\alpha_{1}^\\top \\alpha_{1} = 0.\\]\nOr \\(\\alpha_{1}^\\top \\Sigma = \\lambda_{1} \\alpha_{1}^\\top\\), et \\(\\lambda_{1}^\\top \\alpha_{1} = 1\\), donc \\[2 \\alpha_{1}^\\top \\lambda \\alpha_{2} - 2 \\alpha_{1}^\\top \\lambda \\alpha_{2} - \\kappa \\alpha_{1}^\\top \\alpha_{1} = 0 \\implies - \\kappa = 0.\\]\nEn substituant ce résulat, on obtient \\[\\Sigma \\alpha_{2} = \\lambda \\alpha_{2}.\\]\net donc \\(\\lambda\\) est une autre valeur propre de \\(\\Sigma\\). Puisque \\[\\mathrm{Var}(Y_{2} = \\alpha_{2}^\\top \\Sigma \\alpha_{2} = \\alpha_{2}^\\top \\lambda \\alpha_{2} = \\lambda,\\] on a que cette variance est maximale si \\(\\lambda = \\lambda_{2}\\), la deuxième plus grande valeur propre de \\(\\Sigma\\), et conséquemment \\(\\alpha_{2}\\) est le vecteur propre normé correspondant.\nOn peut généraliser ce résultat en utilisant des maximisations successives. On en conclut que \\[Y_k = \\alpha_k^\\top X,\\] où \\(\\alpha_k\\) est le vecteur propre normé associé à \\(\\lambda_k\\), la \\(k\\)e plus grande valeur propre de \\(\\Sigma\\).\n\nNotation matricielle:\n\nPour définir simultanément et de facon plus compacte les composantes principales, on pose \\[Y = AX,\\] où \\[A = \\left( \\alpha_{1}, \\dots, \\alpha_{p} \\right) = \\begin{pmatrix}\n  \\alpha_{1, 1} & \\alpha_{2, 1} & \\cdots & \\alpha_{p, 1} \\\\\n  \\alpha_{1, 1} & \\alpha_{2, 2} & \\cdots & \\alpha_{p, 1} \\\\\n  \\vdots & \\vdots & \\ddots & \\vdots \\\\\n  \\alpha_{1, p} & \\alpha_{2, p} & \\cdots & \\alpha_{p, p}\n\\end{pmatrix}.\\]\n\n\n\n\n\n\nPropriétés de \\(A\\)\n\n\n\n\nLes colonnes de la matrice \\(A\\) sont les vecteurs propres de \\(\\Sigma\\);\n\\(A^{\\top} A = A A^{\\top} = I_p\\);\n\\(A^{\\top} = A^{-1}\\);\n\\(\\Sigma A = A \\Lambda\\), où \\(\\Lambda = \\text{diag}(\\lambda_{1}, \\dots, \\lambda_p)\\);\n\\(\\mathrm{Var}(Y) = A^{\\top} \\Sigma A = \\Lambda \\implies \\mathrm{Cov}(Y_i, Y_j) = 0\\) si \\(i \\neq j\\) et \\(\\mathrm{Var}(Y_i) = \\lambda_i \\geq \\mathrm{Var}(Y_j) = \\lambda_j\\) si et seulement si \\(i \\leq j\\).\n\n\n\nUne mesure globale de la variation presente dans les données est donnée par la trace de la matrice \\(\\Sigma\\): \\[\\text{tr}(\\Sigma) = \\text{tr}(\\Lambda) = \\sum_{i = 1}^{p} \\lambda_i.\\]\nLa proportion de variation expliquée par la composante principale \\(Y_i\\) est \\[\\frac{\\lambda_i}{\\lambda_{1} + \\cdots + \\lambda_p}.\\]\nSimilairement, les \\(m\\) première composantes expliquent \\[\\frac{\\sum_{i = 1}^{m} \\lambda_i}{\\sum_{i = 1}^{p} \\lambda_i} \\times 100\\%.\\] de la variabilité dans les variables.",
    "crumbs": [
      "Modules",
      "04 - Dimension",
      "ACP"
    ]
  },
  {
    "objectID": "contents/dimension/03-kernel.html",
    "href": "contents/dimension/03-kernel.html",
    "title": "Noyau",
    "section": "",
    "text": "Dans certains cas, réduire la dimension des données n’est pas la chose à faire. Dans certains cas, il est même préférable d’augmenter la dimension des données ! Pourquoi ?\nRegardons l’exemple suivant: ajouter exemple en 2d, type cercle.\nDans ce cas, on s’intéresse à la méthode des noyaux (kernel trick).",
    "crumbs": [
      "Modules",
      "04 - Dimension",
      "Noyau"
    ]
  },
  {
    "objectID": "contents/dimension/03-kernel.html#la-méthode-du-noyau",
    "href": "contents/dimension/03-kernel.html#la-méthode-du-noyau",
    "title": "Noyau",
    "section": "",
    "text": "Dans certains cas, réduire la dimension des données n’est pas la chose à faire. Dans certains cas, il est même préférable d’augmenter la dimension des données ! Pourquoi ?\nRegardons l’exemple suivant: ajouter exemple en 2d, type cercle.\nDans ce cas, on s’intéresse à la méthode des noyaux (kernel trick).",
    "crumbs": [
      "Modules",
      "04 - Dimension",
      "Noyau"
    ]
  },
  {
    "objectID": "slides/01-introduction-slides.html#test",
    "href": "slides/01-introduction-slides.html#test",
    "title": "Introduction",
    "section": "Test",
    "text": "Test\nCoucou"
  },
  {
    "objectID": "contents/04-dimension.html",
    "href": "contents/04-dimension.html",
    "title": "Dimension",
    "section": "",
    "text": "Slides: link",
    "crumbs": [
      "Modules",
      "04 - Dimension"
    ]
  },
  {
    "objectID": "contents/dimension/02-mca.html",
    "href": "contents/dimension/02-mca.html",
    "title": "ACM",
    "section": "",
    "text": "L’analyse en composantes principales concerne des données continues. Que faire lorsque les variables sont rapportées dans des tableaux de contingences où alors comme réponses à des questionnaires à choix multiples ? Que faire lorsque les variables sont catégorielles ? Il est possible de faire la même chose que pour l’ACP pour des variables catégorielles, cette méthode s’appelle l’analyse des correspondances (AC). Cette méthode permet de représenter graphiquement des tableaux de fréquence. Pour des tableaux de fréquences croisant deux variables, on parlera d’analyse des correspondances bianires (ACB) ou alors d’analyse factorielle des correspondances (AFC). Pour des tableaux de fréquences croisant troix variables ou plus, on parlera d’analyse des correspondances multiples (ACM).\n\n\n\n\n\n\nExamples\n\n\n\nLa boussole électorale de Radio-Canada. Étude sur les clients d’une compagnie de télécom. Relation entre la couleur des yeux et des cheveux.",
    "crumbs": [
      "Modules",
      "04 - Dimension",
      "ACM"
    ]
  },
  {
    "objectID": "contents/dimension/02-mca.html#introduction",
    "href": "contents/dimension/02-mca.html#introduction",
    "title": "ACM",
    "section": "",
    "text": "L’analyse en composantes principales concerne des données continues. Que faire lorsque les variables sont rapportées dans des tableaux de contingences où alors comme réponses à des questionnaires à choix multiples ? Que faire lorsque les variables sont catégorielles ? Il est possible de faire la même chose que pour l’ACP pour des variables catégorielles, cette méthode s’appelle l’analyse des correspondances (AC). Cette méthode permet de représenter graphiquement des tableaux de fréquence. Pour des tableaux de fréquences croisant deux variables, on parlera d’analyse des correspondances bianires (ACB) ou alors d’analyse factorielle des correspondances (AFC). Pour des tableaux de fréquences croisant troix variables ou plus, on parlera d’analyse des correspondances multiples (ACM).\n\n\n\n\n\n\nExamples\n\n\n\nLa boussole électorale de Radio-Canada. Étude sur les clients d’une compagnie de télécom. Relation entre la couleur des yeux et des cheveux.",
    "crumbs": [
      "Modules",
      "04 - Dimension",
      "ACM"
    ]
  },
  {
    "objectID": "contents/dimension/02-mca.html#notation",
    "href": "contents/dimension/02-mca.html#notation",
    "title": "ACM",
    "section": "Notation",
    "text": "Notation\nNotation: Soit \\(K = (k_{ij})\\), où \\(k_{ij}\\) est le nombre d’individus appartenant à la classe \\(i \\in \\{ 1, \\dots, n \\}\\) et à la catégorie \\(j \\in \\{ 1, \\dots, p \\}\\).\nTableau des fréquences relatives: Comme les fréquences sont proportionnelles à la taille d’échantillon \\(n\\), le tableau des fréquences relatives contient plus d’information : \\(F = (f_{ij})\\), dans lequel \\[f_{ij} = \\frac{k_{ij}}{k_{\\bullet\\bullet}} = \\frac{k_{ij}}{\\sum_{l = 1}^{n} \\sum_{m = 1}^{p} k_{lm}}.\\]\nLes marges du tableau correspondent à la somme des colonnes pour chaque ligne et de la somme des lignes pour chaque colonne: \\[\\begin{align}\nf_{i \\bullet} &= \\sum_{j = 1}^{p} f_{ij} = \\frac{k_{i \\bullet}}{k_{\\bullet\\bullet}}, \\quad 1 \\leq i \\leq n; \\\\\nf_{\\bullet j} &= \\sum_{i = 1}^{n} f_{ij} = \\frac{k_{\\bullet j}}{k_{\\bullet\\bullet}}, \\quad 1 \\leq j \\leq p.\n\\end{align}\\]\nLes fréquences relatives estiment des probabilités. Dans le cas d’un tableau de fréquences croisant deux variables, sous l’hypothèse d’indépendance, les fréquences relatives devraient être telles qu’on ne s’éloigne pas trop de la relation \\[f_{ij} = f_{i \\bullet} f_{\\bullet j}, \\quad i \\in \\{ 1, \\dots, n \\}, j \\in \\{ 1, \\dots, p \\}.\\]\nStatistique du \\(\\chi^{2}\\): on peut tester si les différences entre \\(f_{ij}\\) et \\(f_{i \\bullet} f_{\\bullet j}\\) sont significatives à l’aide du test du \\(\\chi^2\\): \\[T = \\sum_{i = 1}^{n} \\sum_{j = 1}^{p} \\frac{\\left( k_{ij} - \\mathbb{E}(k_{ij}) \\right)^2}{\\mathbb{E}(k_{ij})} = \\sum_{i = 1}^{n} \\sum_{j = 1}^{p} \\frac{\\left( k_{ij} - \\frac{k_{i \\bullet}k_{\\bullet j}}{k_{\\bullet\\bullet}} \\right)^2}{\\left( \\frac{k_{i \\bullet} k_{\\bullet j}}{k_{\\bullet\\bullet}} \\right)}.\\]\nSi les variables sont indépendantes, la statistique \\(T\\) dit être proche de \\(0\\).\nOn peut aussi s’intéresser à la contribution de chaque cellule à la valeur globale de la statistique \\(T\\). Cela permet d’obtenir le profil de chaque ligne \\(i\\) et de chaque colonne \\(j\\). \\[\\begin{align}\nL_i &= \\left( \\frac{k_{i 1}}{k_{i \\bullet}}, \\dots, \\frac{k_{i p}}{k_{i \\bullet}} \\right) &= \\left( \\frac{f_{i 1}}{f_{i \\bullet}}, \\dots, \\frac{f_{i p}}{f_{i \\bullet}} \\right) \\\\\nC_j &= \\left( \\frac{k_{1 j}}{k_{\\bullet j}}, \\dots, \\frac{k_{n j}}{k_{\\bullet j}} \\right) &= \\left( \\frac{f_{1 j}}{f_{\\bullet j}}, \\dots, \\frac{f_{n j}}{f_{\\bullet j}} \\right)\n.\n\\end{align}\\]\nLe profil-ligne moyen est donné par \\[\\left( \\sum_{i = 1}^{n} f_{i \\bullet} \\frac{f_{i 1}}{f_{i \\bullet}}, \\dots, \\sum_{i = 1}^{n} f_{i \\bullet} \\frac{f_{i p}}{f_{i \\bullet}} \\right) = \\left( f_{\\bullet 1}, \\dots, f_{\\bullet p} \\right).\\] Le profil-colonne moyen est donné par \\[\\left( \\sum_{j = 1}^{p} f_{{\\bullet j}} \\frac{f_{1 j}}{f_{\\bullet j}}, \\dots, \\sum_{j = 1}^{p} f_{\\bullet j} \\frac{f_{n j}}{f_{\\bullet j}} \\right) = \\left( f_{{1 \\bullet}}, \\dots, f_{n \\bullet} \\right).\\]\nEn cas d’indépendance, on a, pour tout \\(i \\in \\{ 1, \\dots, n \\}\\) et \\(j \\in \\{ 1, \\dots, p \\}\\), \\[\\left( \\frac{f_{i 1}}{f_{i \\bullet}}, \\dots, \\frac{f_{i p}}{f_{i \\bullet}} \\right) = \\left( f_{\\bullet 1}, \\dots, f_{\\bullet p} \\right) \\quad\\text{et}\\quad\n\\left( \\frac{f_{1 j}}{f_{\\bullet j}}, \\dots, \\frac{f_{n j}}{f_{\\bullet j}} \\right) = \\left( f_{{1 \\bullet}}, \\dots, f_{n \\bullet} \\right).\\]\nLa dépendance entre les variables est fonction de la ressemblance entre les profils des lignes et les profils des colonnes. On peut mesurer et visualiser cette de diverses manières.\nOn peut mesurer la distance entre deux profils-lignes avec la distance du \\(\\chi^2\\), qui tient compte de l’importance de chaque colonne: \\[d^2(i, i^\\prime) = \\sum_{j = 1}^{p} \\frac{1}{f_{\\bullet j}} \\left( \\frac{f_{ij}}{f_{i \\bullet}} - \\frac{f_{i^\\prime j}}{f_{i^\\prime j}} \\right)^2.\\]\nOn peut écrire cela sous forme matricielle. Notons \\(D_n = \\text{diag}(f_{i \\bullet}\\) et \\(D_p = \\text{diag}(f_{\\bullet j}\\). On a que la matrice \\(D_n^{-1}F\\) a pour lignes les profils-lignes et la matrice \\(D_p^{-1}F^{\\top}\\) a pour lignes les profils-colonnes.\nEn utilisant ces matrices, la distance du \\(\\chi^2\\) au carré est de la forme \\(x^{\\top}D_p^{-1}x\\) pour un point-ligne \\(x \\in \\mathbb{R}^p\\) et de la forme \\(x^{\\top}D_n^{-1}x\\) pour un point-colonne \\(x \\in \\mathbb{R}^n\\).",
    "crumbs": [
      "Modules",
      "04 - Dimension",
      "ACM"
    ]
  },
  {
    "objectID": "contents/dimension/02-mca.html#analyse-factorielle-des-correspondances",
    "href": "contents/dimension/02-mca.html#analyse-factorielle-des-correspondances",
    "title": "ACM",
    "section": "Analyse factorielle des correspondances",
    "text": "Analyse factorielle des correspondances\nL’analyse factorielle des correspondances est une approche graphique permettant de représenter simultanément les profils-lignes appartenant à \\(\\mathbb{R}^p\\) et les profils-colonnes appartenant à \\(\\mathbb{R}^n\\) d’un tableau de fréquences. On cherche donc un espace en deux dimensions où projeter les profils lignes et colonnes de sorte que les points dans cet espace soient le plus près possible des points originaux au sens de la distance du \\(\\chi^2\\). L’analyse des correspondances est très similaire à une double ACP.\nAnalyse directe (sur les lignes): les lignes de la matrice \\(D_n^{-1}F \\in \\mathbb{R}^p\\). On cherche à les représenter dans cet espace muni de la distance \\(x^{\\top} D_p^{-1} x\\).\nAnalyse duale (sur les colonnes): les colonnes de la matrice \\(D_p^{-1}F^{\\top} \\in R^n\\). On cherche à les représenter dans cet espace muni de la distance \\(x^{\\top} D_n^{-1} x\\).\nPremier axe factoriel de l’analyse directe: on cherche le vecteur \\(u \\in \\mathbb{R}^p\\) tel que \\[\\left( u^{\\top} D_p^{-1} F^{\\top} D_n^{-1} \\right) D_n \\left( D_n^{-1} F D_p^{-1} u \\right).\\] soit maximal, sachant que \\(u^{\\top} D_p^{-1} u = 1\\). La solution est donnée par le vecteur propre principal de \\[D_p \\left( D_p^{-1} F^{\\top} D_n^{-1} F D_p^{-1} \\right) = F^{\\top} D_n^{-1} F D_p^{-1} \\equiv S.\\]\nLes formules des coordonnées dans le système d’axes sont plutôt complexes et pas très parlantes… mais à remarquer:\n\n\\(S\\) et \\(T\\) ont les mêmes \\(p\\) premières valeurs propres.\nEn centrant les profils lignes et colonnes, on peut illustrer le résultat des deux graphiques sur les mêmes axes.",
    "crumbs": [
      "Modules",
      "04 - Dimension",
      "ACM"
    ]
  },
  {
    "objectID": "contents/dimension/02-mca.html#centre-de-gravité-et-inertie",
    "href": "contents/dimension/02-mca.html#centre-de-gravité-et-inertie",
    "title": "ACM",
    "section": "Centre de gravité et inertie",
    "text": "Centre de gravité et inertie\nLes logiciels produisent généralement un graphique centré en \\((0, 0)\\). Il s’agit d’une analyse relative aux centres de gravité des lignes et des colonnes. Cette pratique est à la fois commune et commode. En fait, la masse de la \\(i\\)e ligne est \\(f_{i \\bullet}\\), soit la proportion des observations qui tombent sur cette ligne. De faon similaire, la masse de la \\(j\\)e colonne est \\(f_{\\bullet j}\\). Le centre de gravité des lignes est la moyenne des profils-lignes, mais pondérée par la masse de chaque ligne, et similairement pour les profils-colonnes.\nLe centre de gravité des lignes est défini par \\(G_L = \\left( g_{1}, \\dots, g_{p} \\right)^top\\), où \\[g_j = \\sum_{i = 1}^{n} f_{i \\bullet} \\frac{f_{i j}}{f_{i \\bullet}} = \\sum_{i = 1}^{n} f_{i j} = f_{\\bullet j}, \\quad 1 \\leq j \\leq p.\\]\nDe même, le centre de gravité des colonnes est défini par \\[G_C = \\left( f_{1 \\bullet}, \\dots, f_{n \\bullet} \\right)^top.\\]\nCentrage des lignes: \\[\\frac{f_{i j}}{f_{i \\bullet}} - g_{j} = \\frac{f_{i j}}{f_{i \\bullet}} - f_{\\bullet j} = \\frac{f_{i j} - f_{i \\bullet} f_{\\bullet j}}{f_{i \\bullet}}.\\] de sorte que, pour tout \\(i \\in \\{ 1, \\dots, n \\}\\), \\[\\sum_{j = 1}^{p} \\frac{f_{i j} - f_{i \\bullet}f_{\\bullet j}}{f_{i \\bullet}} = 0.\\]\nL’analyse ne se fait plus sur \\[S = F^{\\top} D_n^{-1} F D_p^{-1},\\] mais plutôt sur \\(S^\\star = (s_{j j^\\prime}^\\star),\\) où \\[s_{j j^\\prime}^\\star = \\sum_{i = 1}^{n} \\frac{\\left( f_{i j} - f_{i \\bullet} f_{\\bullet j} \\right) \\left( f_{i j^\\prime} - f_{i \\bullet} f_{\\bullet j^\\prime} \\right)}{f_{i \\bullet} f_{\\bullet j^\\prime}}.\\]\nPar définition, \\[\\text{tr}(S^\\star) = \\sum_{j = 1}^{p} \\sum_{i = 1}^{n} \\frac{\\left( f_{i j} - f_{i \\bullet}f_{\\bullet j} \\right)^2}{f_{i \\bullet} f_{\\bullet j}}.\\]\nOn retrouve l’expression de la statistique du \\(\\chi^2\\) servant à tester l’indépendance entre deux variables.\nOn peut prouver que \\(s_{j j^\\prime}^\\star = s_{j j^\\prime} - f_{\\bullet j},\\) où \\[s_{j j^\\prime} = \\sum_{i = 1}^{n} \\frac{f_{i j}f_{i j^\\prime}}{f_{i \\bullet} f_{\\bullet j^\\prime}}.\\] Ceci entraîne que ces deux matrices ont les mêmes \\(p\\) premiers vecteurs propres normalisés.\nCoordonnées des points-lignes. La projection du \\(i\\)e point-ligne sur l’axe \\(j\\) est donnée par \\[\\left( D_n^{-1} F \\phi_j \\right)_i = \\frac{1}{f_{i \\bullet}} \\sum_{j^\\prime = 1}^{p} f_{i j^\\prime} \\phi_{j j^\\prime} = \\sqrt{\\lambda_j} \\Psi_{j i} \\equiv \\widehat{\\Psi}_{j i}.\\] De plus, \\[\\sum_{i = 1}^{n} f_{i \\bullet} \\widehat{\\Psi}^2_{j i} = \\sum_{i = 1}^{n} f_{i \\bullet} \\left( \\sqrt{\\lambda_j} \\Psi_{j i} \\right)^2 = \\lambda_j.\\]\nCoordonnées des point-colonne. La projection du \\(k\\)e point-ligne sur l’axe \\(j\\) est donnée par \\[\\left( D_p^{-1} F^{\\top} \\psi_j \\right)_k = \\frac{1}{f_{\\bullet k}} \\sum_{i = 1}^{n} f_{i k} \\psi_{j i} = \\sqrt{\\lambda_j} \\Phi_{j k} \\equiv \\widehat{\\Phi}_{j k}.\\] De plus, \\[\\sum_{k = 1}^{p} f_{\\bullet k} \\widehat{\\Phi}^2_{j k} = \\sum_{k = 1}^{p} f_{\\bullet k} \\left( \\sqrt{\\lambda_j} \\Phi_{j k} \\right)^2 = \\lambda_j.\\]\nL’inertie absolue du \\(i\\)e point-ligne sur l’axe \\(j\\) est \\(f_{i \\bullet}\\widehat{\\Psi}_{j i}^2\\). L’inertie relative du \\(i\\)e point-ligne sur l’axe \\(j\\) est \\[\\frac{f_{i \\bullet} \\widehat{\\Psi}_{j i}^2}{\\lambda_j}.\\]\nL’inertie absolue du \\(k\\)e point-colonne sur l’axe \\(j\\) est \\(f_{\\bullet k}\\widehat{\\Phi}_{j k}^2\\). L’inertie relative du \\(k\\)e point-colonne sur l’axe \\(j\\) est \\[\\frac{f_{\\bullet k} \\widehat{\\Phi}_{j k}^2}{\\lambda_j}.\\]\nL’inertie totale est \\(I = T / k_{\\bullet\\bullet}\\).\nla qualité de la représentation du \\(k\\)e point-colonne dans l’axe \\(j\\) est donnée par \\[\\frac{d_j^2(k, G_C)}{d^2(k, G_c)} = \\cos^2(\\theta_{k j}),\\] où \\(\\theta_{k j}\\) est l’angle entre le point \\(k\\) et sa projection sur l’axe \\(j\\).\nInterprétation:\n\nPlus les \\(\\cos^2(\\theta_{k j})\\) sont élevés, mieux les points sont représentés sur l’axe \\(j\\).\nCeci ne signifie pas pour autant que les points sont près du centre du graphique.\nLes points éloignés du centre de gravité se distinguent du centre de gravité.\nUne interprétation semblable existe pour les points-lignes.",
    "crumbs": [
      "Modules",
      "04 - Dimension",
      "ACM"
    ]
  },
  {
    "objectID": "contents/dimension/02-mca.html#example",
    "href": "contents/dimension/02-mca.html#example",
    "title": "ACM",
    "section": "Example",
    "text": "Example",
    "crumbs": [
      "Modules",
      "04 - Dimension",
      "ACM"
    ]
  },
  {
    "objectID": "contents/dimension/02-mca.html#analyse-des-correspondances-multiples",
    "href": "contents/dimension/02-mca.html#analyse-des-correspondances-multiples",
    "title": "ACM",
    "section": "Analyse des correspondances multiples",
    "text": "Analyse des correspondances multiples\nL’analyse des correspondances multiples est une généralisation de l’analyse des correspondances binaires. Elle permet la representation graphique de tableaux de fréquences contnant plus de deux variables. Un exemple classique d’un tableau de fréquences avec plus de deux variables est le tableau présentant les réponses d’individus à un questionnaire contenant \\(Q\\) questions à choix multiples.\nTrès utile pour visualiser les résultats d’une étude par questionnaire.\nL’ACM peut aussi être vue comme une version de l’ACP quand les variables sont catégorielles:\n\nl’analyse duale permet de voir les individus ayant des profils de réponses similaires\non peut obtenir des scores continus pour les individus qui capturent une grande partie de l’information\ndonc aussi utile pour scorer les résultats d’une étude par questionnaire dans un but éventuel de partitionnement, par exemple\n\nEn général, pour un questionnaire contenant \\(Q\\) questions, on a un tableau de la forme suivante: \\[Z = \\left[ Z_1 \\mid \\cdots \\mid Z_{Q} \\right].\\]\nNotation:\n\n\\(Q\\): nombre de questions\n\\(n\\): nombre d’individus répondant au questionnaire\n\\(p_q\\): nombre de modalités (choix de réponses) de la question \\(q\\).\n\\(p = p_{1} + p_{Q}\\)\n\nPotentiel problème: plus le nombre de questions est grand, plus il y aura de cellules vides. C’est aussi le cas si le nombre de réponses aux questions est important. La proportion de cellules non vides est \\[\\frac{nQ}{np} = \\frac{Q}{p}.\\].\nSi toutes les questions ont le même nombre de choix de réponses, alors \\(p_{1} = \\dots = p_{Q} = \\frac{p}{Q}\\), de sorte que \\[\\frac{Q}{p} = \\frac{1}{p_{1}} \\longrightarrow 0, \\quad\\text{quand}~ p_{1} \\rightarrow \\infty.\\]\nLe tableau résumé est un tableau de taille \\(n \\times Q\\). Il contient le numéro de la modalité associée à la réponse de chaque individu pour chacune des questions.\nLa tableau de Burt est une autre faon de présenter un tableau de fréquences contenant plus de deux variables. Étant donné un tableau logique \\(Z = \\left[ Z_{1} \\mid \\cdots \\mid Z_{Q} \\right]\\), le tableau de Burt associé est la matrice carrée \\(p \\times p\\) définie comme étant \\[ B = \\begin{pmatrix}\n  Z_1^\\top Z_1 & \\cdots & Z_1^\\top Z_Q \\\\\n  \\vdots & \\ddots & \\vdots \\\\\n  Z_Q^\\top Z_1 & \\cdots & Z_Q^\\top Z_Q\n\\end{pmatrix}.\\]\n\n\n\n\n\n\nPropriétés de \\(Z_q^\\top Z_q\\)\n\n\n\n\n\\(Z_{q}^\\top Z_q\\) est une matrice diagonale \\(p_q \\times p_q\\) présentant les réponses à la \\(q\\)e question.\nL’élément \\((j, j)\\) de la matrice \\(Z_q^\\top Z_q\\) est égal au nombre d’individus \\(d_{jj}\\) qui appartiennent à la \\(j\\)e catégorie de la \\(q\\)e question.\n\\(Z_{q}^\\top Z_{r}\\) est le tableau de fréquences présentant les répones au x \\(q\\)e et \\(r\\)e questions.\nL’élément \\((j, k)\\) de la matrice \\(Z_q^\\top Z_r\\) est égal au nombre d’individus \\(d_{jk}\\) qui appartiennent à la \\(j\\)e catégorie de la \\(q\\)e question et à la \\(k\\)e catégorie de la \\(r\\)e question.\n\n\n\nD’un point de vue mathématique, l’ACM est une AFC effectuée sur la matrice logique \\(Z\\) ou sur le tableau de Burt \\(B\\). On peut démontrer que l’on obtient les mêmes facteurs, et ce, peu importe la matrice utilisé pour l’analyse.\n\n\n\n\n\n\nNote\n\n\n\nOn peut créé un graphique comme l’AFC. Cependant, en ACM, la distance entre les points de même couleur et la géométrie globale du graphique ne peuvent pas s’interpréter comme en AFC. En fait, on s’intéresse aux points qui sont dans une même direction par rapport à l’origine.",
    "crumbs": [
      "Modules",
      "04 - Dimension",
      "ACM"
    ]
  },
  {
    "objectID": "contents/dimension/02-mca.html#example-1",
    "href": "contents/dimension/02-mca.html#example-1",
    "title": "ACM",
    "section": "Example",
    "text": "Example",
    "crumbs": [
      "Modules",
      "04 - Dimension",
      "ACM"
    ]
  },
  {
    "objectID": "contents/dimension/04-others.html",
    "href": "contents/dimension/04-others.html",
    "title": "Divers",
    "section": "",
    "text": "L’idée est de présenter rapidement d’autres méthodes de changement de dimension.",
    "crumbs": [
      "Modules",
      "04 - Dimension",
      "Divers"
    ]
  },
  {
    "objectID": "contents/dimension/04-others.html#t-sne",
    "href": "contents/dimension/04-others.html#t-sne",
    "title": "Divers",
    "section": "\\(t\\)-SNE",
    "text": "\\(t\\)-SNE",
    "crumbs": [
      "Modules",
      "04 - Dimension",
      "Divers"
    ]
  },
  {
    "objectID": "contents/dimension/04-others.html#umap",
    "href": "contents/dimension/04-others.html#umap",
    "title": "Divers",
    "section": "UMAP",
    "text": "UMAP",
    "crumbs": [
      "Modules",
      "04 - Dimension",
      "Divers"
    ]
  },
  {
    "objectID": "contents/generalities/01-spaces.html",
    "href": "contents/generalities/01-spaces.html",
    "title": "Espace",
    "section": "",
    "text": "Variable numérique: Variable dont la valeur numérique mesure quelque chose de quantifiable et dont la différence entre les valeurs reflète la différence entre les objets. Exemples: revenu en dollars, masse, âge, …\nVariable ordinale: Variable qui ne donne pas une quantification précise d’un phénomène, mais dont les modalités peuvent être naturellement ordonnées. Examples: revenu faible, moyen ou élevé, ou niveau d’accord entre “tout-à-fait en désaccord”, “en désaccord”, “pas d’avis”, “d’accord”, “tout-à-fait d’accord”.\nNominales symétriques: Variable qualitative dont toutes les modalités sont aussi informatives l’une que l’autre. Examples: Sexe, sections d’un cours.\nNominales asymétriques: Variable qualitative dont les modalités ne contiennent pas toutes le même niveau d’information, habituellement lorsque l’une des modalités est très fréquente, un peu la modalité par defaut, mais que les autres ne le sont pas. Exemples: variable indiquant si un individu est daltonien ou non (deux individus daltoniens ont quelque chose en commun, mais deux individus non-daltoniens n’ont pas forcément quelque chose en commun), variable indiquant si une transaction est frauduleuse.\nIl y a plein d’autre type de variables: fonctions, textes, images, graphs, …",
    "crumbs": [
      "Modules",
      "03 - Généralités",
      "Espace"
    ]
  },
  {
    "objectID": "contents/generalities/01-spaces.html#types-de-variables",
    "href": "contents/generalities/01-spaces.html#types-de-variables",
    "title": "Espace",
    "section": "",
    "text": "Variable numérique: Variable dont la valeur numérique mesure quelque chose de quantifiable et dont la différence entre les valeurs reflète la différence entre les objets. Exemples: revenu en dollars, masse, âge, …\nVariable ordinale: Variable qui ne donne pas une quantification précise d’un phénomène, mais dont les modalités peuvent être naturellement ordonnées. Examples: revenu faible, moyen ou élevé, ou niveau d’accord entre “tout-à-fait en désaccord”, “en désaccord”, “pas d’avis”, “d’accord”, “tout-à-fait d’accord”.\nNominales symétriques: Variable qualitative dont toutes les modalités sont aussi informatives l’une que l’autre. Examples: Sexe, sections d’un cours.\nNominales asymétriques: Variable qualitative dont les modalités ne contiennent pas toutes le même niveau d’information, habituellement lorsque l’une des modalités est très fréquente, un peu la modalité par defaut, mais que les autres ne le sont pas. Exemples: variable indiquant si un individu est daltonien ou non (deux individus daltoniens ont quelque chose en commun, mais deux individus non-daltoniens n’ont pas forcément quelque chose en commun), variable indiquant si une transaction est frauduleuse.\nIl y a plein d’autre type de variables: fonctions, textes, images, graphs, …",
    "crumbs": [
      "Modules",
      "03 - Généralités",
      "Espace"
    ]
  },
  {
    "objectID": "contents/generalities/01-spaces.html#espaces-associés",
    "href": "contents/generalities/01-spaces.html#espaces-associés",
    "title": "Espace",
    "section": "Espaces associés",
    "text": "Espaces associés",
    "crumbs": [
      "Modules",
      "03 - Généralités",
      "Espace"
    ]
  },
  {
    "objectID": "contents/generalities/03-bias-variance.html",
    "href": "contents/generalities/03-bias-variance.html",
    "title": "Biais/Variance",
    "section": "",
    "text": "On veut prédire \\(Y\\) à partir de \\(X\\). Pour cela, on cherche à estimer \\(f(X)\\) la fonction qui relie \\(X\\) à \\(Y\\) par \\[Y = f(X) + \\epsilon\\]\nOn peut mesurer deux fonctions qui vont nous aider:\n\nFonction de perte (\\(L\\)): c’est la mesure de l’écart par rapport à ce qu’on souhaite mesurer, par example: \\[L(Y, f(X)) = (Y - f(X))^2.\\]\nFonction de risque : c’est la quantité que l’on cherche à minimiser. Il s’agit de l’espérance de la fonction de perte.\n\nComment trouver \\(f\\):\nObjectif: Trouver une fonction \\(\\widehat{f}\\) qui minimise le risque.\nComment: Supposer une certaine forme pour \\(f(X)\\) et minimiser la fonction de perte de faon analytique ou numérique.\n\nParamétrique: On donne une forme explication à \\(f(X)\\) qui dépend de paramètres. On cherche une méthode d’estimation des paramètres.\nNon-paramétrique: aucune forme forme particulière de \\(f\\), on estime une courbe ou fonction.\n\n\n\n\n\n\n\nExample: la régression linéaire simple\n\n\n\nOn suppose que \\(f(X) = \\beta_{0} + \\beta_{1}X\\), on cherche donc les valeurs \\(\\widehat{\\beta}_0\\) et \\(\\widehat{\\beta}_1\\) qui minimisent : \\[\\mathbb{E}(Y - (\\beta_{0} + \\beta_{1}X))^2.\\]\n\n\nQue faire dans le cas discret ?\nOn cherche à prédire \\(G\\) (un groupe ou facteur) à partir de \\(X\\). Supposons que nous avons \\(\\widehat{G}\\) qui prédise le classe des observations sachant \\(X\\), alors on peut définir la fonction de perte (0-1) comme le nombre d’erreur que l’on a effectué: \\[L(G, \\widehat{G}) = \\mathbb{1}_{G \\neq \\widehat{G}}.\\]\nAvec cette fonction de perte 0-1, \\(\\widehat{G}(x)\\) est la classe \\(g\\) qui maximise \\(\\mathbb{P}(g | X = x)\\).",
    "crumbs": [
      "Modules",
      "03 - Généralités",
      "Biais/Variance"
    ]
  },
  {
    "objectID": "contents/06-nonsupervisee.html",
    "href": "contents/06-nonsupervisee.html",
    "title": "Non-supervisée",
    "section": "",
    "text": "On veut partitionner \\(n\\) observations en \\(K\\) groupes avec comme objectifs:\n\nque les observations dans une même classe soient le plus similaire possible;\nque les observations dans des classes différentes soient les moins similaires possibles.\n\nOn veut donc définir une fonction, que l’on appelle classifieur, qui prend un numéro d’observation \\(i\\) en entrée et qui donne son numéro de groupe en sortie qui va remplir ces deux objectifs.\n\\[\\begin{align}\nC: \\{ 1, \\dots, n \\} &\\rightarrow \\{ 1, \\dots, K\\} \\\\\ni &\\mapsto C(i)\n\\end{align}\\]\nLa fonction objectif est \\[W(C) = \\sum_{k = 1}^{K} \\sum_{i: C(i) = k} \\sum_{j: C(j) = k} d(x_i, x_j),\\]\nou \\(d(x_i, x_j)\\)\nProblème de grande taille!\nComme on ne peut pas explorer l’espace de toutes les possibilités, nous utiliserons des algorithmes gloutons (greedy algorithm), c’est-à-dire qu’ils vont nous donner une règle \\(C\\) qui minimise \\(W(C)\\) sur un espace restreint et qui ne garantissent pas nous ayons trouvé un minimum global.\nDans notre contexte:\n\n\\(p\\) variables sont numériques/ordinales (et habituellement standardisées).\nLa valeur de \\(K\\) est fixé.",
    "crumbs": [
      "Modules",
      "06 - Non-supervisée"
    ]
  },
  {
    "objectID": "contents/01-introduction.html",
    "href": "contents/01-introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "Slides: link",
    "crumbs": [
      "Modules",
      "01 - Introduction"
    ]
  },
  {
    "objectID": "contents/01-introduction.html#quest-ce-que-lanalyse-de-données",
    "href": "contents/01-introduction.html#quest-ce-que-lanalyse-de-données",
    "title": "Introduction",
    "section": "Qu’est-ce que l’analyse de données ?",
    "text": "Qu’est-ce que l’analyse de données ?\nL’analyse de données est un ensemble de méthodes permettant de retirer de l’information d’un jeu de données. On parle aussi d’apprentissage statistique (statistical learning). L’idée est d’utiliser des modèles statistiques pour comprendre comment les données sont structurées et comment elles intéragissent l’une avec l’autre.\n\n\n\n\n\n\nExample\n\n\n\nImaginons que vous êtes le gérant d’un magasin. Vous aimeriez savoir quand ouvrir votre magasin pour maximiser votre chiffre d’affaires. Pour cela, vous pouvez comptez le nombre de clients par heure pendant une période de temps, disons un mois, et ensuite s’interesser à comment ses données sont structurées par rapport aux horaires d’ouverture.",
    "crumbs": [
      "Modules",
      "01 - Introduction"
    ]
  },
  {
    "objectID": "contents/01-introduction.html#objectifs",
    "href": "contents/01-introduction.html#objectifs",
    "title": "Introduction",
    "section": "Objectifs",
    "text": "Objectifs\nCe cours n’a pas vocation à être exhaustif, dans le sens de présenter toutes les méthodes possibles. Ce cours n’a pas non plus vocation à être à l’état de l’art, dans le sens où on ne s’intéressera pas aux derniers développements en apprentissage machine. Ce cours n’est pas non plus un cours de programmation.",
    "crumbs": [
      "Modules",
      "01 - Introduction"
    ]
  },
  {
    "objectID": "contents/01-introduction.html#pratique-de-programmation",
    "href": "contents/01-introduction.html#pratique-de-programmation",
    "title": "Introduction",
    "section": "Pratique de programmation",
    "text": "Pratique de programmation\n\nConseils généraux\nSpécifique à R\nSpécifique à Python",
    "crumbs": [
      "Modules",
      "01 - Introduction"
    ]
  },
  {
    "objectID": "contents/unsupervised/02-hierarchy.html",
    "href": "contents/unsupervised/02-hierarchy.html",
    "title": "Hierarchique",
    "section": "",
    "text": "Certains problèmes de l’algorithme \\(k\\)-means peuvent être résolu avec des algorithmes de classification hiérarchique. Par exemple, lorque l’on a à dispostion qu’une matrice de similiarité/distance entre les observations.\nLa classification hiérarchique permet d’obtenir des partitions toutes imbriquées les unes dans les autres. Il existe deux types d’algortihmes pour effectuer de la classification hiérarchique:\nDans les deux cas, on obtient \\(n\\) partitions hiérarchiques constituées de \\(1\\) à \\(n\\) groupes.",
    "crumbs": [
      "Modules",
      "06 - Non-supervisée",
      "Hierarchique"
    ]
  },
  {
    "objectID": "contents/unsupervised/02-hierarchy.html#algorithme-descendant",
    "href": "contents/unsupervised/02-hierarchy.html#algorithme-descendant",
    "title": "Hierarchique",
    "section": "Algorithme descendant",
    "text": "Algorithme descendant\nUn algorithme descendant fonctionne ainsi:\n\nAu départ, toutes les observations sont dans un seul et même groupe de \\(n\\) observations;\nÀ chaque étape, on divise le groupe le moins homogéne en deux groupes.\nÀ la fin, après \\(n\\) étapes, chaque observation à son propre groupe, c’est-à-dire qu’on obtient \\(n\\) groupes contenant une seule observation.\n\nL’exécution de cet algorithme ne donne pas une seule parition, mais \\(n\\) partitions, que l’on peut résumer à l’aide d’un graphique en forme d’arbre appelé dendogramme. Certains critères peuvent aider à choisir l’une parmi les \\(n\\) partitions proposées par l’algorithme. Ils demandent beaucoup de temps de calcul.",
    "crumbs": [
      "Modules",
      "06 - Non-supervisée",
      "Hierarchique"
    ]
  },
  {
    "objectID": "contents/unsupervised/02-hierarchy.html#algorithme-ascendant",
    "href": "contents/unsupervised/02-hierarchy.html#algorithme-ascendant",
    "title": "Hierarchique",
    "section": "Algorithme ascendant",
    "text": "Algorithme ascendant\nUn algorithme ascendant fonctionne ainsi:\n\nAu départ, chaque observation est son propre groupe, c’est-à-dire uq’on démarre avec \\(n\\) groupes content chacun une seule observation.\nÀ chaque étape, on fusionne les deux groupes les plus similaires.\nÀ la fin, après \\(n\\) étapes, on obtient un seul groupe contenant toutes les \\(n\\) observations.\n\nDifférences entre les algorithmes:\n\ncaractère ascendant ou descendant;\nleur faon de mesurer les distances / similiarité entre deux observations;\nleur faon de mesurer les distances / similiarité entre deux groupes.",
    "crumbs": [
      "Modules",
      "06 - Non-supervisée",
      "Hierarchique"
    ]
  },
  {
    "objectID": "contents/unsupervised/02-hierarchy.html#distance-entre-groupes",
    "href": "contents/unsupervised/02-hierarchy.html#distance-entre-groupes",
    "title": "Hierarchique",
    "section": "Distance entre groupes",
    "text": "Distance entre groupes\nPour mettre en oeuvre les algorithmes précédent, on doit définir la distance entre deux groupes d’observations \\(A\\) et \\(B\\), \\(d(A, B)\\).\nExample: On sait comment mesurer la distance entre les trois paires possibles de \\(\\{ 1 \\}\\), \\(\\{  2 \\}\\) et \\(\\{ 3 \\}\\), mais comment mesurer la distance entre \\(\\{ 1, 2 \\}\\) et \\(\\{ 3 \\}\\) ?\nIl existe plusieurs faons de calculer une telle distance entre deux groupes.\n\nMéthode du plus proche voisin (single linkage)\n\n\\[d(A, B) = \\min \\{ d_{i j}: i \\in A, j \\in B \\}.\\]\n\\[s(A, B) = \\max \\{ s_{i j}: i \\in A, j \\in B \\}.\\]\nLa distance/similiarité entre deux groupes d’observations est tout simplement la distance/similiarité entre les points de chaque groupe qui sont les plus rapprochés/similaires\nAvantages: * Donne de bons résultats lorsque les variables sont de types différents * Possède d’excellentes propriétés théoriques * Permet de créer des groupes dont la forme est très irrégulière * Est robuste aux données aberrantes\nDésavantages: * Tend à former un grand groupe avec plusieurs petits groupes satellites * Perd de l’efficacité si les vrais groupes sont de forme réegulière * Possède des propriétés théoriques ne semblant pas se réaliser en pratique dans certaines études\n\nMéthode du voisin le plus distant (complete linkage)\n\n\\[d(A, B) = \\max \\{ d_{i j}: i \\in A, j \\in B \\}.\\]\n\\[s(A, B) = \\min \\{ s_{i j}: i \\in A, j \\in B \\}.\\]\nLa distance/similiarité entre deux groupes d’observations est tout simplement la distance/similiarité entre les points de chaque groupe qui sont les plus éloignés/dissimilaires.\nAvantages: * Donne de bons résultats lorsque les variables sont de types différents * Tend à former des groupes de taille égale\nDésavantages: * Est extrêmement sensible aux données aberrantes * Tend à former des groupes de taille égale * Est très peu utilisée en pratique\n\nMéthode de la moyenne (average linkage)\n\n\\[d(A, B) = \\frac{1}{n_{A}n_{B}} \\sum_{i \\in A} \\sum_{j \\in B} d(x_{i}, x_{j}).\\] où \\(n_{A}\\) est le nombre d’observations dans le groupe \\(A\\) et \\(n_{B}\\) est le nonmbre d’observations dans le group \\(B\\).\nOn doit calculer les \\(n_{A} \\times n_{B}\\) distances/similiarités possibles entre les points des deux groupes, ensuite on prend la moyenne de ces distances/similiarités comme étant celle qui sépare les groupes.\nAvantages: * Tend à former des groupes de faible variance\nDésavantages: * Tend à former des groupes de même variance\n\nMéthode du centroïde (centroid method)\n\n\\[d(A, B) = d(\\overline{x}_{A}, \\overline{x}_{B}).\\] où \\[\\overline{x}_{A} = \\frac{1}{n_{A}} \\sum_{i \\in A} x_i, \\quad\\text{et}\\quad \\overline{x}_B = \\frac{1}{n_{B}} \\sum_{j \\in B} x_j\\]\nLa moyenne \\(\\overline{x}_{AB}\\) du nouveau groupe résultant de la fusion des groupes \\(A\\) et \\(B\\) se calcule comme suit: \\[\\overline{x}_{AB} = \\frac{n_{A} \\overline{x}_{A} + n_{B} \\overline{x}_{B}}{n_{A} + n_{B}}.\\]\nAvantages: * Est très robuste aux données aberrantes\nDésavantages: * Est peu efficace en l’absence de données aberrantes\n\nMéthode de la médiane (median method)\n\nÀ une étape donnée, nous avons toujours à notre disposition la distance entre les groupes déjà formés. On fusionne les deux groupes les moins distants/les plus similaires, disons \\(A\\) et \\(B\\) pour obtenir un groupe \\(AB\\). On met à jour la matrice des distances: la distance entre le nouveau groupe \\(AB\\) et tout autre groupe \\(C\\) est donnée par \\[d(AB, C) = \\frac{d(A, C) + d(B, C)}{2} - \\frac{d(A, B)}{4}.\\]\nAvantages: * Est encore plus robuste en présence de données aberrantes\nDésavantages: * Est très peu efficace en l’absence de données aberrante\n\nMéthode de Ward (Ward’s method)\n\nVariante de la méthode du centroïde pour tenir compte de la taille des groupes. Elle a été conue pour être optimale si les \\(n\\) vecteurs \\(x_{1}, \\dots, x_{n}\\) suivent des lois normales multivariées de \\(K\\) moyennes différentes mais toutes de même matrice de variance-covariance.\nBasée sur les sommes des carrées \\[SC_A = \\sum_{i \\in A} (x_i - \\overline{x}_A)^\\top (x_i - \\overline{x}_A).\\]\n\\[SC_B = \\sum_{j \\in B} (x_j - \\overline{x}_B)^\\top (x_j - \\overline{x}_B).\\]\n\\[SC_AB = \\sum_{k \\in A \\cup B} (x_k - \\overline{x}_{AB})^\\top (x_k - \\overline{x}_{AB}).\\]\noù \\(\\overline{x}_A\\), \\(\\overline{x}_B\\) et \\(\\overline{x}_{AB}\\) sont calculées comme dans la méthode du centroïde. On regroupe les classes \\(A\\) et \\(B\\) pour lesquelles \\[I_{AB} = SC_{AB} - SC_A - SC_B = \\frac{d^2(\\overline{x}_A, \\overline{x}_B)}{\\frac{1}{n_{A} + \\frac{1}{n_{B}}}}\\] est minimale.\nAvantages: * Est optimale si les observations sont approximativement distribuées selon une loi normale multidimensionnelle de même matrice de variances-covariances\nDésavantages: * Est sensible aux données aberrantes * Tend à former des groupes de petite taille * Tend à former des groupes de même taille\n\nMéthode flexible\n\nLes auteurs de cette méthode ont remarqué que pour plusieurs méthodes connues, on a les relations suivantes: \\[d(C, AB) = \\alpha_A d(C, A) + \\beta d(C, B) + \\beta d(A, B) + \\gamma \\left| d(C, A) - d(C, B) \\right|.\\]\n\n\n\n\n\n\n\n\n\n\nMéthode\n\\(\\alpha_A\\)\n\\(\\alpha_B\\)\n\\(\\beta\\)\n\\(\\gamma\\)\n\n\n\n\nPlus proche\n\\(1/2\\)\n\\(1/2\\)\n\\(0\\)\n\\(-1/2\\)\n\n\nPlus distant\n\\(1/2\\)\n\\(1/2\\)\n\\(0\\)\n\\(1/2\\)\n\n\nMédiane\n\\(1/2\\)\n\\(1/2\\)\n\\(-1/4\\)\n\\(0\\)\n\n\nMoyenne\n\\(\\frac{n_A}{n_A+n_B}\\)\n\\(\\frac{n_B}{n_A+n_B}\\)\n\\(0\\)\n\\(0\\)\n\n\nCentroïde\n\\(\\frac{n_A}{n_A+n_B}\\)\n\\(\\frac{n_B}{n_A+n_B}\\)\n\\(-\\frac{n_An_B}{n_A + n_B}\\)\n\\(0\\)\n\n\nWard\n\\(\\frac{n_A+n_C}{n_A+n_B+n_C}\\)\n\\(\\frac{n_B+n_C}{n_A+n_B+n_C}\\)\n\\(-\\frac{n_C}{n_A+n_B+n_C}\\)\n\\(0\\)\n\n\n\nAvec la méthode flexible, on impose arbitrairement les contraintes suivantes: \\[\\alpha_A + \\alpha_B + \\beta = 1, \\quad \\alpha_A = \\alpha_B, \\quad \\gamma = 0.\\]\nAinsi, \\[\\alpha_A = \\alpha_B = \\frac{1 - \\beta}{2}.\\] Et il ne reste qu’à choisir \\(\\beta\\). Les autheurs suggèrent de poser \\(\\beta = -0.25\\) sauf quand on souponne la présence de données aberrantes où l’on suggère \\(\\beta = -0.5\\).",
    "crumbs": [
      "Modules",
      "06 - Non-supervisée",
      "Hierarchique"
    ]
  },
  {
    "objectID": "contents/unsupervised/02-hierarchy.html#pratique",
    "href": "contents/unsupervised/02-hierarchy.html#pratique",
    "title": "Hierarchique",
    "section": "Pratique",
    "text": "Pratique\nL’exécution d’un algorithme nous donne une séquence de \\(n\\) partitions ayant de \\(n\\) à \\(1\\) groupes.\nQuelle partition de cette séquence devrions-nous choisir?\nL’une des \\(n\\) partitions est-elle particulièrement interprétable? L’une des \\(n\\) partitions a-t-elle a un sens pratique? Visions-nous séparer la population en un nombre \\(K\\) de groupes?\nS’il n’y a pas de réponse claire à ces questions, des critères peuvent nous guider …\nIl y a plusieurs indications pour nous aider dans le choix du nombre de classe (surtout si les variables sont continues). La librairie NbClust en contient une trentaine: https://www.rdocumentation.org/packages/NbClust/versions/3.0.1/topics/NbClust\n\nLes indicateurs basées sur l’inertie\n\n\\[I_{tot} = I_{intra-groupe}+ I_{inter-groupe}\\] Ces indicateurs sont plus pertinents avec des variables continues. Prendre garde au poids des variables et à la standardisation.\n\nPseudo- \\(R^2\\)\n\n\\[Pseudo-R^2 =\\frac{I_{inter-groupe}}{I_{tot}}\\]\n\nStatistique de Caliliski-Harabasz (CH):\n\n\\[CH = \\frac{I_{inter-groupe}/(k-1)}{I_{intra-groupe}/(n-k)}\\]\n\nIndice de Dunn: On maxime l’indice suivant:\n\n\\[D= \\frac{\\text{Distance minimale entre 2 groupes}}{\\text{Distance maximale dans un groupe}}\\]\nL’indice de Dunn cherche donc à créer des groupes denses et bien séparés.\n\nIndice de silhouette\n\nLa silhouette de l’observation \\(i\\) mesure la confiance dans le choix du groupe pour l’observation \\(i\\): \\[S(i) = \\frac{b_i-a_i}{max(b_i,a_i)}\\] où \\(a_i\\) est la distance moyenne entre l’observation \\(i\\) et les autres observations de son groupe et \\(b_i\\) est la distance moyenne entre l’observation \\(i\\) et les observations du groupe le plus proche de \\(i\\). On souhaite maximiser la silhouette moyenne des observations.\n\nCritère de classification cubique (CCC)\n\nOn fait un graphique avec le CCC en ordonnée et le nombre de groupes en abscisse. Pour le nombre de groupes, on ne considère que les partitions de \\(K=1\\) à \\(K=n/10\\). Si \\(CCC&gt;2\\), on est en présence d’une classification de bonne qualité. Si \\(0&lt;CCC&lt;2\\), on est en présence d’une classification de qualité moyenne. Si \\(CCC&lt;0\\), on est en présence d’une classification de mauvaise qualité. Pour choisir le nombre de classes à retenir, on peut considérer les nombres de classes associés aux fortes hausses du critère CCC entre deux nombres de classes subséquents. On considère les pics, atteignant des valeurs du critère supérieures à 2 ou à 3 comme étant de fortes hausses de ce critère.\nIl ne faut pas utiliser le critère CCC avec la méthode du plus proche voisin, ou lorsque l’on suspecte que les groupes sont de forme très allongée ou irrégulière. Le critères ne fonctionne pas bien quand le nombre d’observations dans certains groupes est inférieur à 10.\n\nStatistique pseudo-\\(F\\)\n\nStatistique presque distribuée selon une loi \\(F\\) lorsque la loi des données pas trop loin de la normale multivariée avec variances égales dans toutes les classes. Même si on est loin de la normalité, en pratique cette statistique peut quand-même être informative. On cherche des nombres de classes pour lesquels la statistique du pseudo-\\(F\\) se démarque par une grande valeur. Sur un graphique de la statistique du pseudo-\\(F\\) en fonction du nombre de classes, ceci se traduit par la recherche de pics. Il ne faut pas utiliser la statistique du pseudo-\\(F\\) avec la méthode du plus proche voisin.\n\nStatistique du pseudo-\\(t^2\\)\n\nStatistique presque distribuée selon une loi \\(t\\) lorsque loi des données pas trop loin de la normale multivariée avec des variances égales dans toutes les classes. En pratique, on regarde le graphique de la statistique du pseudo-\\(t^2\\) en fonction du nombre de classes de droite à gauche, on essaie de trouver des valeurs de la statistique qui sont beaucoup plus élevées que la valeur précédente. Supposons que la forte hausse se produit entre \\(k\\) et \\(k-1\\) classes. On choisit \\(k\\) classes dans le partitionnement de nos observations. Il ne faut pas utiliser la statistique du pseudo-\\(t^2\\) avec la méthode du plus proche voisin.",
    "crumbs": [
      "Modules",
      "06 - Non-supervisée",
      "Hierarchique"
    ]
  },
  {
    "objectID": "contents/remainders/03-stat.html",
    "href": "contents/remainders/03-stat.html",
    "title": "Analyse de données",
    "section": "",
    "text": "Un projet d’analyse de données se découpe en cinq étapes:\n\nDéfinition des objectifs\nDonnées\nÉlaboration et validation des modèles\nMise en oeuvre\nSuivi de la performance et amélioration\n\n\n\n\n\n\n\nRemarque\n\n\n\nDans ce cours, on s’intéressera principalement à l’élaboration et à la validation de modèles.\n\n\nTODO: Planification d’un projet\n\n\nEst-ce que l’on veut: visualiser les données ? explorer et émettre des hypothèses? tester? regrouper? comprendre? prédire?\nComment fait-on en pratique ? On pose des questions ! Tout d’abord, il faut clarifier les termes. Qui va utiliser le modèle et comment ? Quelle est la population cible ?\n\n\n\n\n\n\nExamples\n\n\n\n\nLa Banque National du Canada veut lancer un nouveau produit d’épargne et souhaite mieux connaître ses clients pour prédire s’ils veulent l’acheter.\nL’équipe de hockey des Canadiens de Montréal souhaite mieux connaître ses adversaires pour développer des nouvelles tactiques de jeu.\nPharmascience souhaite savoir si son nouveau médicament est efficace.\n\n\n\n\n\n\n\nInventaire et qualité\nConstitution de la base de données\nExploration et traitement préliminaire\n\nQu’est-ce que l’on veut dire par qualité des données ?\n\nEst-ce que les données sont représentatives de la population cible ?\nEst-ce que les données permetternt de tirer des conclusions de causalité ?\nEst-ce que les données sont fiables ?\n\nSource de données:\nQuelques liens pour récupérer des données.\nNettoyage de données: cf R (importation, nettoyage, tidyverse, types de variables, retirer les doublons, uniformiser les modalités, vérifier le format des valeurs spéciales, pivot, opérateur pipe, jointure).\nExploration des données: modalités rares, modalités trop nombreuses, asymétrie, débalancement des classes, valeurs extrêmes ou aberrantes, variables fortement corrélées, valeurs manquantes.\nStatistiques descriptives",
    "crumbs": [
      "Modules",
      "02 - Révisions",
      "Analyse de données"
    ]
  },
  {
    "objectID": "contents/remainders/03-stat.html#project-danalyse-données",
    "href": "contents/remainders/03-stat.html#project-danalyse-données",
    "title": "Analyse de données",
    "section": "",
    "text": "Un projet d’analyse de données se découpe en cinq étapes:\n\nDéfinition des objectifs\nDonnées\nÉlaboration et validation des modèles\nMise en oeuvre\nSuivi de la performance et amélioration\n\n\n\n\n\n\n\nRemarque\n\n\n\nDans ce cours, on s’intéressera principalement à l’élaboration et à la validation de modèles.\n\n\nTODO: Planification d’un projet\n\n\nEst-ce que l’on veut: visualiser les données ? explorer et émettre des hypothèses? tester? regrouper? comprendre? prédire?\nComment fait-on en pratique ? On pose des questions ! Tout d’abord, il faut clarifier les termes. Qui va utiliser le modèle et comment ? Quelle est la population cible ?\n\n\n\n\n\n\nExamples\n\n\n\n\nLa Banque National du Canada veut lancer un nouveau produit d’épargne et souhaite mieux connaître ses clients pour prédire s’ils veulent l’acheter.\nL’équipe de hockey des Canadiens de Montréal souhaite mieux connaître ses adversaires pour développer des nouvelles tactiques de jeu.\nPharmascience souhaite savoir si son nouveau médicament est efficace.\n\n\n\n\n\n\n\nInventaire et qualité\nConstitution de la base de données\nExploration et traitement préliminaire\n\nQu’est-ce que l’on veut dire par qualité des données ?\n\nEst-ce que les données sont représentatives de la population cible ?\nEst-ce que les données permetternt de tirer des conclusions de causalité ?\nEst-ce que les données sont fiables ?\n\nSource de données:\nQuelques liens pour récupérer des données.\nNettoyage de données: cf R (importation, nettoyage, tidyverse, types de variables, retirer les doublons, uniformiser les modalités, vérifier le format des valeurs spéciales, pivot, opérateur pipe, jointure).\nExploration des données: modalités rares, modalités trop nombreuses, asymétrie, débalancement des classes, valeurs extrêmes ou aberrantes, variables fortement corrélées, valeurs manquantes.\nStatistiques descriptives",
    "crumbs": [
      "Modules",
      "02 - Révisions",
      "Analyse de données"
    ]
  },
  {
    "objectID": "contents/remainders/02-probabilities.html",
    "href": "contents/remainders/02-probabilities.html",
    "title": "Probabilités et Statistiques",
    "section": "",
    "text": "Vecteurs aléatoires:\nSoit \\(X = (X_{1}, \\dots, X_{p})^\\top\\), un vecteur aléatoire de taille \\(p\\).\nEspérance: \\[\\mathbb{E}(X) = \\begin{pmatrix}\n  \\mathbb{E}(X_{1}) \\\\\n  \\vdots \\\\\n  \\mathbb{E}(X_{p})\n\\end{pmatrix} \\coloneqq \\begin{pmatrix}\n  \\mu_{1} \\\\\n  \\vdots \\\\\n  \\mu_{p}\n\\end{pmatrix} \\coloneqq \\mu.\\]\nMatrice de variance/covariance: \\[\\mathrm{Var}(X) = \\begin{pmatrix}\n  \\mathrm{Var}(X_{1}) & \\cdots & \\mathrm{Cov}(X_{1}, X_{p}) \\\\\n  \\vdots & \\ddots & \\vdots \\\\\n  \\mathrm{Cov}(X_{1}, X_{p}) & \\cdots & \\mathrm{Var}(X_p)  \n  \\end{pmatrix} \\coloneqq \\begin{pmatrix}\n  \\sigma_{1}^2 & \\cdots & \\sigma_{1, p} \\\\\n  \\vdots & \\ddots & \\vdots \\\\\n  \\sigma_{1, p} & \\cdots & \\sigma_p   \n\\end{pmatrix} \\coloneqq \\Sigma.\\]\nMatrice de corrélation: \\[\\mathrm{Cor}(X) = \\begin{pmatrix}\n  1 & \\cdots & \\mathrm{Corr}(X_{1}, X_{p})  \\\\\n  \\vdots & \\ddots & \\vdots \\\\\n  \\mathrm{Corr}(X_{1}, X_{p}) & \\cdots & 1\n  \\end{pmatrix} \\coloneqq \\begin{pmatrix}\n  1 & \\cdots & \\rho_{1, p} \\\\\n  \\vdots & \\ddots & \\vdots \\\\\n  \\rho_{1, p} & \\cdots & 1\n\\end{pmatrix} \\coloneqq R.\\]\nPropriété des moments:\n\n\n\n\n\n\nProperties\n\n\n\nSoit \\(X\\) un vecteur aléatoire de moyenne \\(\\mathbb{E}(X) = \\mu\\) et de variance \\(\\mathrm{Var}(X) = \\Sigma\\), et soit \\(M\\) une matrice de constantes et \\(v\\) un vecteur de constantes.\n\n\\(\\Sigma\\) est définie non-négative et symétrique.\n\\(\\Sigma = \\mathbb{E}\\left[ (X - \\mu)(X - \\mu)^\\top \\right] = \\mathbb{E}(X X^\\top) - \\mu \\mu^\\top\\).\n\\(\\mathbb{E}(MX + v) = \\mathbb{E}(X) + v\\).\n\\(\\mathrm{Var}(MX + v) = \\mathrm{Var}(MX) = M \\Sigma M^\\top\\).\n\\(\\Sigma = \\Delta R \\Delta \\iff R = \\Delta^{-1} \\Sigma \\Delta^{-1}\\).\n\n\n\nAjouter qqc sur l’indépendance de variables aléatoires!\nLoi normale multivariée: On dit qu’un vecteur aléatoire \\(X\\) de dimension \\(p\\) suit une loi normale multidimensionnelle de moyenne \\(\\mu\\) et de variance \\(\\Sigma \\sim \\mathcal{N}_{p}(\\mu, \\sigma^2)\\), si sa densité est données par \\[f_X(x) = \\frac{1}{(2 \\pi)^{p /2}} \\cdot \\frac{1}{(\\text{det} \\Sigma)^{1/2}} \\cdot \\exp\\left\\{ -\\frac{1}{2}\\left( x - \\mu \\right)^\\top \\Sigma^{-1} \\left( x - \\mu \\right) \\right\\}, \\quad x \\in \\mathbb{R}^p.\\]\nEstimation avec un échantillon: En practique, nous ne connaissons pas les valeurs de \\(\\mu\\) et de \\(\\Sigma\\) et nous voulons les estimer à partir d’un échantillon. Soit \\(X_{1}, \\dots, X_{n}\\), \\(n\\) réalisations indépendantes d’un vecteur aléatoire \\(X\\) de moyenne \\(\\mu\\) et de variance \\(\\Sigma\\). On estime \\(\\mu\\) et \\(\\Sigma\\) par :\n\\[\\widehat{\\mu} = \\overline{X} \\coloneqq \\frac{1}{n} \\sum_{i = 1}^{n} X_i\\]\n\\[\\widehat{\\Sigma} = S^2 \\coloneqq \\frac{1}{n - 1}\\sum_{i = 1}^{n} (X_i - \\overline{X})(X_i - \\overline{X})^\\top.\\]\nNotons \\(D = \\{\\text{diag}(S^2)\\}^{1/2}\\), la matrice des écarts-types calculés sur l’échantillon. On peut calculer la matrice des corrélations sur l’échantillon par : \\[\\widehat{R} = D^{-1} S^2 D^{-1} \\iff S^2 = D \\widehat{R} D.\\]",
    "crumbs": [
      "Modules",
      "02 - Révisions",
      "Probabilités et Statistiques"
    ]
  },
  {
    "objectID": "contents/remainders/02-probabilities.html#révisions-de-probabilité",
    "href": "contents/remainders/02-probabilities.html#révisions-de-probabilité",
    "title": "Probabilités et Statistiques",
    "section": "",
    "text": "Vecteurs aléatoires:\nSoit \\(X = (X_{1}, \\dots, X_{p})^\\top\\), un vecteur aléatoire de taille \\(p\\).\nEspérance: \\[\\mathbb{E}(X) = \\begin{pmatrix}\n  \\mathbb{E}(X_{1}) \\\\\n  \\vdots \\\\\n  \\mathbb{E}(X_{p})\n\\end{pmatrix} \\coloneqq \\begin{pmatrix}\n  \\mu_{1} \\\\\n  \\vdots \\\\\n  \\mu_{p}\n\\end{pmatrix} \\coloneqq \\mu.\\]\nMatrice de variance/covariance: \\[\\mathrm{Var}(X) = \\begin{pmatrix}\n  \\mathrm{Var}(X_{1}) & \\cdots & \\mathrm{Cov}(X_{1}, X_{p}) \\\\\n  \\vdots & \\ddots & \\vdots \\\\\n  \\mathrm{Cov}(X_{1}, X_{p}) & \\cdots & \\mathrm{Var}(X_p)  \n  \\end{pmatrix} \\coloneqq \\begin{pmatrix}\n  \\sigma_{1}^2 & \\cdots & \\sigma_{1, p} \\\\\n  \\vdots & \\ddots & \\vdots \\\\\n  \\sigma_{1, p} & \\cdots & \\sigma_p   \n\\end{pmatrix} \\coloneqq \\Sigma.\\]\nMatrice de corrélation: \\[\\mathrm{Cor}(X) = \\begin{pmatrix}\n  1 & \\cdots & \\mathrm{Corr}(X_{1}, X_{p})  \\\\\n  \\vdots & \\ddots & \\vdots \\\\\n  \\mathrm{Corr}(X_{1}, X_{p}) & \\cdots & 1\n  \\end{pmatrix} \\coloneqq \\begin{pmatrix}\n  1 & \\cdots & \\rho_{1, p} \\\\\n  \\vdots & \\ddots & \\vdots \\\\\n  \\rho_{1, p} & \\cdots & 1\n\\end{pmatrix} \\coloneqq R.\\]\nPropriété des moments:\n\n\n\n\n\n\nProperties\n\n\n\nSoit \\(X\\) un vecteur aléatoire de moyenne \\(\\mathbb{E}(X) = \\mu\\) et de variance \\(\\mathrm{Var}(X) = \\Sigma\\), et soit \\(M\\) une matrice de constantes et \\(v\\) un vecteur de constantes.\n\n\\(\\Sigma\\) est définie non-négative et symétrique.\n\\(\\Sigma = \\mathbb{E}\\left[ (X - \\mu)(X - \\mu)^\\top \\right] = \\mathbb{E}(X X^\\top) - \\mu \\mu^\\top\\).\n\\(\\mathbb{E}(MX + v) = \\mathbb{E}(X) + v\\).\n\\(\\mathrm{Var}(MX + v) = \\mathrm{Var}(MX) = M \\Sigma M^\\top\\).\n\\(\\Sigma = \\Delta R \\Delta \\iff R = \\Delta^{-1} \\Sigma \\Delta^{-1}\\).\n\n\n\nAjouter qqc sur l’indépendance de variables aléatoires!\nLoi normale multivariée: On dit qu’un vecteur aléatoire \\(X\\) de dimension \\(p\\) suit une loi normale multidimensionnelle de moyenne \\(\\mu\\) et de variance \\(\\Sigma \\sim \\mathcal{N}_{p}(\\mu, \\sigma^2)\\), si sa densité est données par \\[f_X(x) = \\frac{1}{(2 \\pi)^{p /2}} \\cdot \\frac{1}{(\\text{det} \\Sigma)^{1/2}} \\cdot \\exp\\left\\{ -\\frac{1}{2}\\left( x - \\mu \\right)^\\top \\Sigma^{-1} \\left( x - \\mu \\right) \\right\\}, \\quad x \\in \\mathbb{R}^p.\\]\nEstimation avec un échantillon: En practique, nous ne connaissons pas les valeurs de \\(\\mu\\) et de \\(\\Sigma\\) et nous voulons les estimer à partir d’un échantillon. Soit \\(X_{1}, \\dots, X_{n}\\), \\(n\\) réalisations indépendantes d’un vecteur aléatoire \\(X\\) de moyenne \\(\\mu\\) et de variance \\(\\Sigma\\). On estime \\(\\mu\\) et \\(\\Sigma\\) par :\n\\[\\widehat{\\mu} = \\overline{X} \\coloneqq \\frac{1}{n} \\sum_{i = 1}^{n} X_i\\]\n\\[\\widehat{\\Sigma} = S^2 \\coloneqq \\frac{1}{n - 1}\\sum_{i = 1}^{n} (X_i - \\overline{X})(X_i - \\overline{X})^\\top.\\]\nNotons \\(D = \\{\\text{diag}(S^2)\\}^{1/2}\\), la matrice des écarts-types calculés sur l’échantillon. On peut calculer la matrice des corrélations sur l’échantillon par : \\[\\widehat{R} = D^{-1} S^2 D^{-1} \\iff S^2 = D \\widehat{R} D.\\]",
    "crumbs": [
      "Modules",
      "02 - Révisions",
      "Probabilités et Statistiques"
    ]
  },
  {
    "objectID": "contents/05-supervisee.html",
    "href": "contents/05-supervisee.html",
    "title": "Supervisée",
    "section": "",
    "text": "On considère une population comportant \\(q\\) groupes. On observe \\(p\\) variables \\(X_{1}, \\dots, X_{p}\\) pour chaque individu/objet de la population. On cherche à obtenir un modèle/algortihme pour classer de nouveaux individus/objets dans les bons groupes, c’est-à-dire de prédire \\(Y\\) à partir de \\(X_{1}, \\dots, X_{p}\\).\n\n\n\n\n\n\nExample\n\n\n\n\nRevenue Québce désire identifier les déclarations fiscales méritant d’être examinées de faon plus approfondies (détection de fraude).\nReconnaissance automatique des chiffres et des lettres des codes postaux écrits à la main.\nIdentification de nouveaux clients potentiels.\nFiltrage de courriels indésirables.\nReconnaissance d’images.\n\n\n\nApproche générale\n\nSélectionner un certain nombre d’individus dont on connaît le groupe d’appartenance.\nMesurer \\(p\\) caractéristiques \\(X_{1}, \\dots, X_{p}\\) sur ces individus.\nDiviser ce jeu de données en deux:\n\n\nUn jeu de données pour la modélisation (entrainement, “train”)\nUn jeu de données pour la vérification (valisation, “test”)\n\n\nDévelopper un modèle/algorithme pour classer le mieux possible les individus du jeu de données d’entrainement.\nÉvaluer notre modèle/algorithme sur le jeu de données de valisation.\n(Répéter étapes 3-4-5 avec d’autres modèles/algorithmes et choisir le meilleur).\n\nQuelques méthodes:\n\nAnalyse discriminante\nArbre de classification\nRégression\nClassificateur naïf de Bayes\nMéthode des \\(k\\) plus proches voisins\nSupport vector machine\nRéseaux de neurones.",
    "crumbs": [
      "Modules",
      "05 - Supervisée"
    ]
  },
  {
    "objectID": "contents/03-generalities.html",
    "href": "contents/03-generalities.html",
    "title": "Généralités",
    "section": "",
    "text": "Slides: link",
    "crumbs": [
      "Modules",
      "03 - Généralités"
    ]
  },
  {
    "objectID": "informations/description.html",
    "href": "informations/description.html",
    "title": "Description du cours",
    "section": "",
    "text": "Dans ce cours, nous visons à introduire des méthodes qui permettront aux étudiantes et étudiants d’étudier un jeu de données de “haute dimension” (“haute” dans le sens ici où l’on ne peut faire un simple graphique de l’ensemble des observations de toutes les variables) sans avoir recours à un modèle probabiliste. Les techniques qu’on y enseigne servent à réduire la dimension des données, identifier certains liens entre les variables, visualiser les données ou à diviser le jeu de données en groupes/classes.\nSans négliger la théorie, l’accent sera mis sur l’aspect pratique de l’analyse des données et l’utilisation du langage R.",
    "crumbs": [
      "Informations",
      "Description"
    ]
  },
  {
    "objectID": "informations/description.html#objectifs",
    "href": "informations/description.html#objectifs",
    "title": "Description du cours",
    "section": "",
    "text": "Dans ce cours, nous visons à introduire des méthodes qui permettront aux étudiantes et étudiants d’étudier un jeu de données de “haute dimension” (“haute” dans le sens ici où l’on ne peut faire un simple graphique de l’ensemble des observations de toutes les variables) sans avoir recours à un modèle probabiliste. Les techniques qu’on y enseigne servent à réduire la dimension des données, identifier certains liens entre les variables, visualiser les données ou à diviser le jeu de données en groupes/classes.\nSans négliger la théorie, l’accent sera mis sur l’aspect pratique de l’analyse des données et l’utilisation du langage R.",
    "crumbs": [
      "Informations",
      "Description"
    ]
  },
  {
    "objectID": "informations/description.html#place-du-cours-dans-le-programme",
    "href": "informations/description.html#place-du-cours-dans-le-programme",
    "title": "Description du cours",
    "section": "Place du cours dans le programme",
    "text": "Place du cours dans le programme\nCe cours généralement suivi par des étudiants du baccalauréat en statistique lors de leur deuxième année de formation. Il est aussi un cours à option en actuariat, en mathématique ainsi que dans certains programmes de génie et d’administration.\nTous les étudiants doivent s’assurer d’avoir suivi au minimum un cours d’algèbre (ex. MAT1200) et un cours de statistique de base (ex. STT1000), car la majorité des méthodes d’analyse de données s’appuient sur ces notions.\nLes étudiants devraient aussi avoir une certaine familiarité avec le langage R ou avec la programmation.",
    "crumbs": [
      "Informations",
      "Description"
    ]
  },
  {
    "objectID": "informations/description.html#objectifs-spécifiques",
    "href": "informations/description.html#objectifs-spécifiques",
    "title": "Description du cours",
    "section": "Objectifs spécifiques",
    "text": "Objectifs spécifiques\nÀ la fin du cours, l’étudiant ou l’étudiante devra être capable :\n\nde comprendre et décrire succinctement les fondements théoriques des méthodes d’analyse de données étudiées;\nd’identifier correctement les situations où l’emploi de ces méthodes est indiqué;\nd’utiliser efficacement un logiciel tel SAS ou R pour mettre en oeuvre ces méthodes;\nd’analyser et d’interpréter judicieusement les résultats découlant de l’analyse;\nde formuler par écrit les conclusions de l’analyse, dans le respect des limites de la méthodologie.",
    "crumbs": [
      "Informations",
      "Description"
    ]
  },
  {
    "objectID": "informations/description.html#équipement-informatique",
    "href": "informations/description.html#équipement-informatique",
    "title": "Description du cours",
    "section": "Équipement informatique",
    "text": "Équipement informatique\nVous pourriez avoir besoin d’un ordinateur, de haut-parleurs ou d’un casque d’écoute, d’un microphone, d’une webcam et d’une connexion Internet avec fil à large bande ou sans fil. Pour vérifier les paramètres de configuration minimaux selon le système d’exploitation, nous vous invitons à visiter cette page.\nDe plus, ce cours peut nécessiter des besoins logiciels particuliers qui seront alors décrits dans d’autres sections du plan de cours, le cas échéant.",
    "crumbs": [
      "Informations",
      "Description"
    ]
  },
  {
    "objectID": "informations/description.html#fonctionnement",
    "href": "informations/description.html#fonctionnement",
    "title": "Description du cours",
    "section": "Fonctionnement",
    "text": "Fonctionnement\nLes cours et examen auront lieu en présentiel les mardi et vendredi matin, mais une portion du matériel sera disponible en ligne. De façon générale, la séance du vendredi sera une séance magistrale et la séance du mardi sera dédiée à des exercices pratiques réalisés de façon autonome par les étudiants avec le soutien de l’enseignant.",
    "crumbs": [
      "Informations",
      "Description"
    ]
  },
  {
    "objectID": "informations/description.html#approches-pédagogiques",
    "href": "informations/description.html#approches-pédagogiques",
    "title": "Description du cours",
    "section": "Approches pédagogiques",
    "text": "Approches pédagogiques\nL’approche pédagogique privilégiée est l’exposé interactif (vendredi) en alternance avec des périodes de laboratoires (mardi). L’approche pédagogique est très axée sur l’apprentissage actif et exige un engagement soutenu des étudiants tout au long de la session.",
    "crumbs": [
      "Informations",
      "Description"
    ]
  },
  {
    "objectID": "informations/evaluations.html",
    "href": "informations/evaluations.html",
    "title": "Évaluations",
    "section": "",
    "text": "Titre\nDate\nMode de travail\nPondération\n\n\n\n\nExamen 1\n\nIndividuel\n25%\n\n\nExamen 2\n\nIndividuel\n50%\n\n\nProjet\n\nEn équipe\n25%\n\n\n\n\nIdentification. Lors d’un examen, une carte d’identité avec photo admissible doit être déposée sur le coin de votre table. Les cartes admissibles sont la carte de l’Université Laval en plastique, un permis de conduire canadien, une carte d’assurance-maladie avec photo émise par une province canadienne ou un passeport canadien ou étranger.",
    "crumbs": [
      "Informations",
      "Évaluations"
    ]
  },
  {
    "objectID": "informations/evaluations.html#modalités-dévaluations",
    "href": "informations/evaluations.html#modalités-dévaluations",
    "title": "Évaluations",
    "section": "",
    "text": "Titre\nDate\nMode de travail\nPondération\n\n\n\n\nExamen 1\n\nIndividuel\n25%\n\n\nExamen 2\n\nIndividuel\n50%\n\n\nProjet\n\nEn équipe\n25%\n\n\n\n\nIdentification. Lors d’un examen, une carte d’identité avec photo admissible doit être déposée sur le coin de votre table. Les cartes admissibles sont la carte de l’Université Laval en plastique, un permis de conduire canadien, une carte d’assurance-maladie avec photo émise par une province canadienne ou un passeport canadien ou étranger.",
    "crumbs": [
      "Informations",
      "Évaluations"
    ]
  },
  {
    "objectID": "informations/evaluations.html#informations-détaillées-sur-les-évaluations",
    "href": "informations/evaluations.html#informations-détaillées-sur-les-évaluations",
    "title": "Évaluations",
    "section": "Informations détaillées sur les évaluations",
    "text": "Informations détaillées sur les évaluations\n\nExamen 1\nDate et lieu:\nMode de travail: Individuel\nPondération: 25%\nRemise de l’évaluation:\nDirectives de l’évaluation: Tout le contenu du cours est susceptible d’être a l’examen.\nMatériel autorisé: pas de restriction.\nInformation supplementaire: lien\n\n\nExamen 2\nDate et lieu:\nMode de travail: Individuel\nPondération: 50%\nRemise de l’évaluation:\nDirectives de l’évaluation: Tout le contenu du cours est susceptible d’être à l’examen.\nMatériel autorisé: pas de restriction.\n\n\nProjet\nDate et lieu:\nMode de travail: En équipe\nPondération: 25%\nRemise de l’évaluation:\nDirectives de l’évaluation:\nMatériel autorisé: pas de restriction.",
    "crumbs": [
      "Informations",
      "Évaluations"
    ]
  },
  {
    "objectID": "informations/evaluations.html#échelle-des-cotes",
    "href": "informations/evaluations.html#échelle-des-cotes",
    "title": "Évaluations",
    "section": "Échelle des cotes",
    "text": "Échelle des cotes\n\n\n\nCote\n% minimum\n% maximum\n\n\n\n\nA+\n92\n100\n\n\nA\n88\n91.99\n\n\nA-\n84\n87.99\n\n\nB+\n80\n83.99\n\n\nB\n75\n79.99\n\n\nB-\n70\n74.99\n\n\nC+\n65\n69.99\n\n\nC\n60\n64.99\n\n\nC-\n55\n59.99\n\n\nD+\n52\n54.99\n\n\nD\n50\n51.99\n\n\nE\n0\n49.99",
    "crumbs": [
      "Informations",
      "Évaluations"
    ]
  },
  {
    "objectID": "informations/evaluations.html#détails-sur-les-modalités-dévaluation",
    "href": "informations/evaluations.html#détails-sur-les-modalités-dévaluation",
    "title": "Évaluations",
    "section": "Détails sur les modalités d’évaluation",
    "text": "Détails sur les modalités d’évaluation\nConformément à la politique du Département de mathématiques et de statistique en matière d’amélioration et de consolidation de la connaissance du français, la qualité de l’écrit sera sanctionnée dans tous les travaux et examens. Un maximum de 10% des points pourra être enlevé pour la qualité de la langue et de la rédaction.\nAucun retard n’est accepté pour la remise des travaux.\nToute reprise d’évaluation accordée par la politique de reprise d’évaluation du Département aura lieu lors des dates de reprises officielles de la faculté. Pour toute demande de révision de note, vous devez suivre la procédure du chapitre 4 du Réglement des études. Vous trouverez un formulaire à remplir à l’adresse suivante: lien.",
    "crumbs": [
      "Informations",
      "Évaluations"
    ]
  },
  {
    "objectID": "informations/evaluations.html#absence-à-une-activité-obligatoire",
    "href": "informations/evaluations.html#absence-à-une-activité-obligatoire",
    "title": "Évaluations",
    "section": "Absence à une activité obligatoire",
    "text": "Absence à une activité obligatoire\nCes modalités s’appliquent en vertu des articles 4.41 et 4.42 du Réglement des études de l’Université Laval:\n\n4.41 Tout défaut de se soumettre à une activité d’ évaluation entraine la note zéro pour cette activité d’ évaluation, à moins que l’ étudiante ou l’ étudiant ne démontre que cette omission est attribuable à des motifs sérieux.\n4.42 La reprise d’ une évaluation est possible pour des motifs sérieux. Elle se fait selon les modalités prévues par l’ unité responsable de l’ activité de formation.\n\nLa reprise d’ une évaluation peut donc execptionnellement être autorisée pour des motifs jugés sérieux, dans la mesure où la procédure décrite ci-dessous est respectée.\nMotifs d’absence jugés sérieux\nLes motifs suivants sont jugés sérieux et donc acceptables pour demander une reprise d’évaluation:\n\nMaladie ou accident empêchant de se déplacer;\nHospitalisation;\nMaladie grave ou décès d’un proche;\nParticipation à une activité sportive de haut niveau;\nConvocation en cour de justice.\n\nProcédure à suivre\nDès que possible et au plus tard cinq (5) jours ouvrables après la date de l’évaluation (ou dans certains cas, avant la date de l’évaluation, dès que le motif sera connu), l’étudiante ou l’étudiant qui veut faire une demande de reprise d’évaluation doit remplir et soumettre le formulaire électronique “Demande de reprise d’une évaluation” en prenant soin d’y joindre les pièces justificatives requises.\nPour avoir plus de détail sur les procédures à suivre et les motifs sérieux pouvant donner droit à une reprise d’évaluation, consulter le document “Modalités et procédure de reprise d’une évaluation sommative à la Faculté des sciences et de génie” disponible sur le site web de la FSG.\nDans certains cas, la ou le responsable du cours pourrait adopter une procédure simplifiée de gestion des demandes de reprises d’évaluation, tout en respectant les critères décrits dans cette politique. Dans ces cas, des explications particulières seront données à cet effet dans le plan de cours et présentées lors de la première séance.",
    "crumbs": [
      "Informations",
      "Évaluations"
    ]
  },
  {
    "objectID": "informations/evaluations.html#politique-sur-lutilisation-dappareils-électroniques",
    "href": "informations/evaluations.html#politique-sur-lutilisation-dappareils-électroniques",
    "title": "Évaluations",
    "section": "Politique sur l’utilisation d’appareils électroniques",
    "text": "Politique sur l’utilisation d’appareils électroniques\nLa politique sur l’ utilisation d’appareils électroniques de la Faculté des Sciences et de Génie peut être consultée à l’ adresse: lien.",
    "crumbs": [
      "Informations",
      "Évaluations"
    ]
  },
  {
    "objectID": "informations/evaluations.html#politique-sur-le-plagiat-et-la-fraude-académique",
    "href": "informations/evaluations.html#politique-sur-le-plagiat-et-la-fraude-académique",
    "title": "Évaluations",
    "section": "Politique sur le plagiat et la fraude académique",
    "text": "Politique sur le plagiat et la fraude académique\nRègles disciplinaires\nTout étudiant qui commet une infraction au Règlement disciplinaire à l’intention des étudiants de l’Université Laval dans le cadre du présent cours, notamment en matière de plagiat, est passible des sanctions qui sont prévues dans ce règlement. Il est très important pour tout étudiant de prendre connaissance des articles 23 à 46 du Règlement disciplinaire. Celui-ci peut être consulté à l’adresse suivante: lien.\nPlagiat\nTout étudiant est tenu de respecter les règles relatives au plagiat. Constitue notamment du plagiat le fait de:\n\ncopier textuellement un ou plusieurs passages provenant d’un ouvrage sous format papier ou électronique sans mettre ces passages entre guillemets et sans en mentionner la source;\nrésumer l’idée originale d’un auteur en l’exprimant dans ses propres mots (paraphraser) sans en mentionner la source;\ntraduire partiellement ou totalement un texte sans en mentionner la provenance;\nremettre un travail copié d’un autre étudiant (avec ou sans l’accord de cet autre étudiant);\nremettre un travail téléchargé d’un site d’achat ou d’échange de travaux scolaires.\n\nL’Université Laval étant abonnée à un service de détection de plagiat, il est possible que l’enseignant soumette vos travaux pour analyse.",
    "crumbs": [
      "Informations",
      "Évaluations"
    ]
  },
  {
    "objectID": "informations/evaluations.html#étudiants-ayant-une-situation-de-handicap-liée-à-une-limitation-fonctionnelle",
    "href": "informations/evaluations.html#étudiants-ayant-une-situation-de-handicap-liée-à-une-limitation-fonctionnelle",
    "title": "Évaluations",
    "section": "Étudiants ayant une situation de handicap liée à une limitation fonctionnelle",
    "text": "Étudiants ayant une situation de handicap liée à une limitation fonctionnelle\nAfin de bénéficier de mesures d’accommodement pour les cours ou les examens, un rendez-vous avec une conseillère ou un conseiller du Centre d’aide aux étudiants travaillant en Accueil et soutien aux étudiants en situation de handicap (ACSESH) est nécessaire. Pour ce faire, les étudiants présentant une situation de handicap liée à une limitation fonctionnelle permanente doivent visiter le site “Accommodement” et prendre un rendez-vous, le plus tôt possible.\nAu cours de la semaine qui suit l’autorisation des mesures, leur activation doit être effectuée dans “Accommodement” pour assurer leur mise en place.\nLes étudiants ayant déjà obtenu des mesures d’accommodements scolaires doivent procéder à l’activation de leurs mesures pour les cours et/ou les examens dans “Accommodement” afin que celles-ci puissent être mises en place. Il est à noter que l’activation doit s’effectuer au cours de deux premières semaines de cours.\nLes étudiants concernés recevront par la suite des facultés et départements responsables de leurs cours les informations détaillées sur les modalités permettant d’appliquer les mesures d’accommodement identifiées.",
    "crumbs": [
      "Informations",
      "Évaluations"
    ]
  },
  {
    "objectID": "informations/mcq.html",
    "href": "informations/mcq.html",
    "title": "QCM",
    "section": "",
    "text": "Le premier examen est un questionaire à choix multiples avec quantification de l’incertitude.\nL’ utilisation de degrés de certitude permet à l’étudiant d’évaluer le niveau de connaissance de la réponse donnée. La barème des notes, basé sur la théorie des décision (Leclercq et al. 1993), peut sembler bizarre mais il a été fait de tel manière que:\n\ndire la vérité soit la stratégie qui rapporte le plus de points;\nceux qui s’auto-évaluent bien gagnent plus de points que si l’on appliquait un barème correctif tenant compte des probabilités d’avoir la réponse correcte aléatoirement.\n\n\n\n\nSi vous condidérez que votre réponse a une probabilité d’être correcte comprise entre…\n\n\nChoisissez le degré de certitude…\n\n\nVous obtiendrez les points suivants en case de réponse…\n\n\n\n\n\n\n\n\ncorrecte\n\n\nincorrecte\n\n\n\n\n0% et 25%\n\n\n0\n\n\n+13\n\n\n+4\n\n\n\n\n25% et 50%\n\n\n1\n\n\n+16\n\n\n+3\n\n\n\n\n50% et 70%\n\n\n2\n\n\n+17\n\n\n+2\n\n\n\n\n70% et 85%\n\n\n3\n\n\n+18\n\n\n0\n\n\n\n\n85% et 95%\n\n\n4\n\n\n+19\n\n\n-6\n\n\n\n\n95% et 100%\n\n\n5\n\n\n+20\n\n\n-10\n\n\n\nGénéralement, les étudiants s’auto-estiment avec réalisme, et sont avantagés par les degrés de certitude. Leur score obtenu est meilleur que s’il avait été calculé uniquement sur la base du nombre de réponses justes.",
    "crumbs": [
      "Informations",
      "Évaluations",
      "QCM"
    ]
  },
  {
    "objectID": "informations/mcq.html#information-sur-le-premier-examen",
    "href": "informations/mcq.html#information-sur-le-premier-examen",
    "title": "QCM",
    "section": "",
    "text": "Le premier examen est un questionaire à choix multiples avec quantification de l’incertitude.\nL’ utilisation de degrés de certitude permet à l’étudiant d’évaluer le niveau de connaissance de la réponse donnée. La barème des notes, basé sur la théorie des décision (Leclercq et al. 1993), peut sembler bizarre mais il a été fait de tel manière que:\n\ndire la vérité soit la stratégie qui rapporte le plus de points;\nceux qui s’auto-évaluent bien gagnent plus de points que si l’on appliquait un barème correctif tenant compte des probabilités d’avoir la réponse correcte aléatoirement.\n\n\n\n\nSi vous condidérez que votre réponse a une probabilité d’être correcte comprise entre…\n\n\nChoisissez le degré de certitude…\n\n\nVous obtiendrez les points suivants en case de réponse…\n\n\n\n\n\n\n\n\ncorrecte\n\n\nincorrecte\n\n\n\n\n0% et 25%\n\n\n0\n\n\n+13\n\n\n+4\n\n\n\n\n25% et 50%\n\n\n1\n\n\n+16\n\n\n+3\n\n\n\n\n50% et 70%\n\n\n2\n\n\n+17\n\n\n+2\n\n\n\n\n70% et 85%\n\n\n3\n\n\n+18\n\n\n0\n\n\n\n\n85% et 95%\n\n\n4\n\n\n+19\n\n\n-6\n\n\n\n\n95% et 100%\n\n\n5\n\n\n+20\n\n\n-10\n\n\n\nGénéralement, les étudiants s’auto-estiment avec réalisme, et sont avantagés par les degrés de certitude. Leur score obtenu est meilleur que s’il avait été calculé uniquement sur la base du nombre de réponses justes.",
    "crumbs": [
      "Informations",
      "Évaluations",
      "QCM"
    ]
  }
]