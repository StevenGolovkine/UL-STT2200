[
  {
    "objectID": "contents/03-generalities.html",
    "href": "contents/03-generalities.html",
    "title": "Generalities",
    "section": "",
    "text": "Slides:\n\nData analysis project\nSpaces\nDistances - annotated\nBias / Variance - annotated\nModel evaluation - annoté\n\nTD: link\n\nSolution Exercise 1\nSolution Exercises 2 à 4\n\nTP: link",
    "crumbs": [
      "Modules",
      "03 - Généralités"
    ]
  },
  {
    "objectID": "contents/03-generalities.html#outline",
    "href": "contents/03-generalities.html#outline",
    "title": "Generalities",
    "section": "Outline",
    "text": "Outline\n\nData analysis project\nSpaces\nDistances\nBias / Variance\nModel evaluation\n\n\n\n\nCorrelation (xkcd:552).",
    "crumbs": [
      "Modules",
      "03 - Généralités"
    ]
  },
  {
    "objectID": "contents/dimension/03-mca.html",
    "href": "contents/dimension/03-mca.html",
    "title": "Multiple correspondence analysis",
    "section": "",
    "text": "Multiple correspondence analysis (MCA) can be presented as an extension of PCA. It allows for the graphical representation of frequency tables containing more than two variables. A classic example of a frequency table with more than two qualitative variables is a table presenting individuals’ responses to a questionnaire containing \\(Q\\) multiple-choice questions. MCA is therefore very useful for visualizing the results of a questionnaire-based study.\nMCA can also be seen as a version of PCA when the variables are mixed, i.e., comprising both quantitative and qualitative variables. The joint processing of these two types of data is based on their prior transformation called complete disjunctive coding.",
    "crumbs": [
      "Modules",
      "04 - Dimension",
      "Multiple correspondence analysis"
    ]
  },
  {
    "objectID": "contents/dimension/03-mca.html#notation",
    "href": "contents/dimension/03-mca.html#notation",
    "title": "Multiple correspondence analysis",
    "section": "Notation",
    "text": "Notation\nLet \\(n\\) be the number of individuals (or observations) and \\(Q\\) the number of variables (or questions in the case of a questionnaire). Each variable has \\(J_q\\) modalities and the total number of modalities is equal to \\(J\\).\n\n\n\n\n\n\nWarningDefinition\n\n\n\nThe binary table, i.e., containing only \\(0\\)s and \\(1\\)s, with \\(n\\) rows and \\(J\\) columns is called a complete disjointive coding table. We denote it by \\(Z\\).\n\n\nThus, a variable is not treated as such but through its modalities. It is divided into modalities, and each individual is then coded \\(1\\) for the modality they possess and \\(0\\) for the others (i.e., those they do not possess, as the modalities are exclusive). This coding is immediate for qualitative variables. However, for a qualitative variable, we first divide the variable into classes. Thus, each individual belongs to only one class. This process of transforming information is called complete disjunctive coding. It is indeed coding, because the initial information is transformed; disjunctive, because each individual has at most one modality; and complete, because each individual has at least one modality.\n\n\n\n\n\n\nNoteExample\n\n\n\nLet’s take, for example, a set of products with different types (Hoodie, Joggers, and Sneakers) and different prices. We have \\(7\\) observations (\\(7\\) products). The variable Type is a qualitative variable and the variable Price is a quantitative variable.\n\nProduct dataset.\n\n\nProduct\nType\nPrice ($)\n\n\n\n\nNike Tech Fleece\nHoodie\n256.72\n\n\nPuma Joggers\nJoggers\n221.26\n\n\nOff-White Hoodie\nHoodie\n198.45\n\n\nSupreme Hoodie\nHoodie\n235.50\n\n\nJordan 1 High\nSneakers\n298.22\n\n\nNike Dunk Low\nSneakers\n273.00\n\n\nNike Tech Fleece\nHoodie\n162.38\n\n\n\nTo encode the information in a complete disjunctive coding table, we define three price classes (price below $200, price between $200 and $250, and price above $250). This allows us to encode the variable Price using the above classes. The complete disjunctive table is given in the following table.\nProduct | Hoodie | Joggers | Sneakers | $&lt;$200 | between $200 and $250 | $&gt;$250 |\n|——–:|:—– -:|:——-:|:——–:|:——:|:-:|:-:| |Nike Tech Fleece |1 | 0 | 0 | 0 | 0 | 1 | |Puma Joggers |0 | 1 | 0 | 0 | 1 | 1 | |Off-White Hoodie |1 | 0 | 0 | 1 | 0 | 0 | |Supreme Hoodie |1 | 0 | 0 | 0 | 1 | 0 | |Jordan 1 High |0 | 0 | 1 | 0 | 0 | 1 | |Nike Dunk Low |0 | 0 | 1 | 0 | 0 | 1 | |Nike Tech Fleece |1 | 0 | 0 | 1 | 0 | 0 |\n: Product dataset in complete disjunctive coding.\n\n\n\n\n\n\n\n\nTipNote\n\n\n\nWhen transforming quantitative variables into a complete disjunctive coding table, information is lost. This is because qualitative variables must be divided into classes, and class membership is less informative than a specific variable value. In the previous example, information about price is lost.\n\n\n\n\n\n\n\n\nImportantProperties\n\n\n\n\nThe sum of the elements in the same row is constant and equals \\(Q\\).\nThe sum of all the elements in the table equals \\(nQ\\).\nThe sum of the elements in the same column equals the count \\(n_j\\) with modality \\(j\\) of variable \\(q\\).\n\n\n\n\n\n\n\n\n\nNoteProof\n\n\n\n\n\nSince \\(Z\\) is a complete disjunctive array, we have \\[\\sum_{j = 1}^{J_q} z_{ij} = 1.\\] Therefore, we find that \\[z_{i \\bullet} = \\sum_{j = 1}^{J} z_{ij} = \\sum_{q = 1}^{Q} \\sum_{j = 1}^{J_q} z_{ij} = Q, \\]\n\\[z_{\\bullet j} = \\sum_{i = 1}^{n} z_{ij} = n_j,\\]\n\\[z_{\\bullet \\bullet} = \\sum_{i = 1}^{n} \\sum_{j = 1}^{J} z_{ij} = nQ.\\]",
    "crumbs": [
      "Modules",
      "04 - Dimension",
      "Multiple correspondence analysis"
    ]
  },
  {
    "objectID": "contents/dimension/03-mca.html#burts-table",
    "href": "contents/dimension/03-mca.html#burts-table",
    "title": "Multiple correspondence analysis",
    "section": "Burt’s Table",
    "text": "Burt’s Table\n\n\n\n\n\n\nWarningDefinition\n\n\n\nThe Burt’s table, denoted \\(B\\), is the product of the transpose of \\(Z\\) by \\(Z\\): \\[B = Z^{\\top} Z.\\]\n\n\n\n\n\n\n\n\nImportantProperties of \\(B\\)\n\n\n\n\nBurt’s table is square and its size is equal to the total number of modalities \\(J\\) possessed by the \\(Q\\) variables.\nThe diagonal blocks of \\(B\\) are themselves diagonal matrices. They are given by \\(B_{qq} = Z^{\\top}_q Z_q\\) and their diagonal elements correspond to the frequency of each modality for variable \\(q\\).\nThe non-diagonal blocks of \\(B\\) are given by \\(B_{qq^\\prime} = Z^{\\top}_q Z_{q^\\prime}, q \\neq q^\\prime\\). They correspond to the contingency tables crossing the variables \\(q\\) and \\(q^\\prime\\).\nBurt’s table is symmetric because \\(B_{q^\\prime q} = Z^{\\top}_{q^\\prime} Z_{q}\\) is the transpose of \\(B_{q q^\\prime} = Z^{\\top}_q Z_{q^\\prime}\\).\n\n\n\nFrom a mathematical point of view, ACM is an AFC performed on the logical matrix \\(Z\\) or on Burt’s table \\(B\\). It can be shown that the same factors are obtained, regardless of the matrix used for the analysis.",
    "crumbs": [
      "Modules",
      "04 - Dimension",
      "Multiple correspondence analysis"
    ]
  },
  {
    "objectID": "contents/dimension/03-mca.html#eigenelements-of-the-table-z",
    "href": "contents/dimension/03-mca.html#eigenelements-of-the-table-z",
    "title": "Multiple correspondence analysis",
    "section": "Eigenelements of the table \\(Z\\)",
    "text": "Eigenelements of the table \\(Z\\)\nThe eigenelements of the table \\(Z\\) can be calculated using the same method as for PCA. By analogy with PCA, we therefore seek the eigenvectors of the matrix \\[S = \\frac{1}{Q} Z^{\\top} Z D_J^{-1},\\] where \\(D_J\\) is the diagonal matrix of term \\(n_j, j = 1, \\dots, J\\). We can calculate the coordinates of the line profiles on the factorial axes in the same way: \\[\\Phi_k = n Z D_J^{-1} u_k,\\] where \\(u_k\\) is the \\(k\\)th eigenvector associated with the eigenvalue \\(\\lambda_k\\) of the matrix \\(S\\).\nWe can also look at the dual analysis of the table \\(Z\\). Again, by analogy with PCA, we look for the eigenvectors of the matrix \\[T = \\frac{1}{Q} Z D_J^{-1} Z^{\\top}.\\] Similarly, we can calculate the coordinates of the column profiles on the factorial axes: \\[\\Psi_k = n D_J^{-1} Z^{\\top} v_k.\\]",
    "crumbs": [
      "Modules",
      "04 - Dimension",
      "Multiple correspondence analysis"
    ]
  },
  {
    "objectID": "contents/dimension/03-mca.html#eigenelements-of-burts-table-b",
    "href": "contents/dimension/03-mca.html#eigenelements-of-burts-table-b",
    "title": "Multiple correspondence analysis",
    "section": "Eigenelements of Burt’s table \\(B\\)",
    "text": "Eigenelements of Burt’s table \\(B\\)\nSince Burt’s table is symmetric, the direct analysis and the dual analysis coincide. We can also analyze it in analogy with PCA. The sum of the elements of the same row (or the same column) of \\(B\\) is \\(Q n_j\\) and the sum of the elements of \\(B\\) is \\(n Q^2\\). We are looking for the eigenvectors of the matrix \\[S^\\prime = \\frac{1}{Q^2} B^{\\top} D_J^{-1} B D_J^{-1}.\\]\nWe then notice that this matrix \\(S^\\prime\\) has the same eigenvectors as the matrix \\(S\\). Indeed, \\[S^\\prime = \\frac{1}{Q^2} B^{\\top} D_J^{-1} B D_J^{-1} = \\frac{1}{Q^2} Z^{\\top} Z D_J^{-1} Z^{\\top} Z D_J^{-1}.\\] And let \\(u\\) and \\(\\lambda\\) satisfy \\(Z^{\\top} Z D_J^{-1} = \\lambda u\\), then \\[ Z^{\\top} Z D_J^{-1} Z^{\\top} Z D_J^{-1} u = Z^{\\top} Z D_J^{-1} \\lambda u = \\lambda^2 u.\\]\nFinally, the analysis of \\(Z\\) or \\(B\\) provides the same eigenvectors, and for all \\(k = 1, \\dots, Q\\), the \\(k\\)th eigenvalue of \\(B\\) is the square of the \\(k\\)th eigenvalue of \\(Z\\).",
    "crumbs": [
      "Modules",
      "04 - Dimension",
      "Multiple correspondence analysis"
    ]
  },
  {
    "objectID": "contents/dimension/03-mca.html#variable-encoding",
    "href": "contents/dimension/03-mca.html#variable-encoding",
    "title": "Multiple correspondence analysis",
    "section": "Variable encoding",
    "text": "Variable encoding\nVariable encoding, and in particular the choice of class boundaries, is essential in ACM. For continuous variables, the boundaries should be relevant to the problem being studied. For example, we would not define a class \\(&gt; 1000\\) in the previous example. To obtain relevant bounds, we can look at the distributions of the variables, e.g., with a histogram. In some specific cases, it is possible to divide the variable into equal-sized categories. However, this approach can lead to irrelevant categories.\nIn the case of qualitative variables, the choice of classes does not arise; it is given by the variable. However, “natural” modalities can lead to (very) unbalanced frequencies. In this case, we generally need to proceed with groupings. Here again, a good knowledge of the field being studied is necessary. In any case, it is preferable to group modalities rather than randomly distribute modalities with low frequencies among the other modalities (which is sometimes proposed in software).",
    "crumbs": [
      "Modules",
      "04 - Dimension",
      "Multiple correspondence analysis"
    ]
  },
  {
    "objectID": "contents/dimension/01-pca.html",
    "href": "contents/dimension/01-pca.html",
    "title": "Principal components analysis",
    "section": "",
    "text": "Working with a large number of variables can pose several practical and theoretical problems:\n\nComplicated visualization: it is impossible to visually represent data beyond 3 dimensions.\nDifficult class separation: in classification problems, the separation between groups may be hidden in a combination of variables rather than in the variables taken individually.\nHigh computational cost: complex models can become difficult to adjust when the number of variables is large.\nStrong correlations: redundant variables make models unstable or difficult to interpret.\n\nThe natural question to ask is therefore: can we reduce the dimension of the dataset without losing too much information?\nReducing the size does not simply mean removing variables. Doing so could result in the loss of information that may be useful to the model. A better approach is to construct new variables, obtained as linear combinations of the initial variables, which summarize the essential information in the dataset. One possible method for doing this is Principal Component Analysis (PCA).",
    "crumbs": [
      "Modules",
      "04 - Dimension",
      "Principal components analysis"
    ]
  },
  {
    "objectID": "contents/dimension/01-pca.html#why-change-dimensions",
    "href": "contents/dimension/01-pca.html#why-change-dimensions",
    "title": "Principal components analysis",
    "section": "",
    "text": "Working with a large number of variables can pose several practical and theoretical problems:\n\nComplicated visualization: it is impossible to visually represent data beyond 3 dimensions.\nDifficult class separation: in classification problems, the separation between groups may be hidden in a combination of variables rather than in the variables taken individually.\nHigh computational cost: complex models can become difficult to adjust when the number of variables is large.\nStrong correlations: redundant variables make models unstable or difficult to interpret.\n\nThe natural question to ask is therefore: can we reduce the dimension of the dataset without losing too much information?\nReducing the size does not simply mean removing variables. Doing so could result in the loss of information that may be useful to the model. A better approach is to construct new variables, obtained as linear combinations of the initial variables, which summarize the essential information in the dataset. One possible method for doing this is Principal Component Analysis (PCA).",
    "crumbs": [
      "Modules",
      "04 - Dimension",
      "Principal components analysis"
    ]
  },
  {
    "objectID": "contents/dimension/01-pca.html#principal-component-analysis",
    "href": "contents/dimension/01-pca.html#principal-component-analysis",
    "title": "Principal components analysis",
    "section": "Principal Component Analysis",
    "text": "Principal Component Analysis\nPCA is an unsupervised method (without variables to explain) that reduces the dimension of a dataset while retaining as much information as possible. This method is used when there are \\(n\\) observations of \\(p\\) continuous numerical variables with \\(p\\) too “large” to allow for effective modeling or visualization. The method was introduced by Hotelling in Analysis of Complex Statistical Data in 1933.\n\n\n\n\n\n\nTipCommon applications\n\n\n\n\nVisualization of a multidimensional dataset.\nReduction of the number of variables from \\(p\\) to \\(k \\ll p\\) to facilitate model construction.\nCompressing images or signals.\nExploring biological, textual, or environmental data.\n\n\n\n\n\n\n\n\n\nNoteExamples\n\n\n\n\nCompare hockey teams based on six end-of-season statistics.\nSummarize crime rates among Canadian provinces based on rates for seven different types of crimes.\nCompress images consisting of \\(1084 \\times 1084\\) pixels into a few variables.\nIdentify the number of variants of a tumor type based on the expression levels of millions of genes.\n\n\n\n\nMathematical formulation\nLet \\(X = \\left( X_{1}, \\dots, X_p \\right)^{\\top}\\) be a random vector composed of \\(p\\) variables, centered and having variance-covariance matrix \\(\\Sigma\\). Let \\(\\alpha_{1} = \\left( \\alpha_{11}, \\dots, \\alpha_{1p} \\right)^{\\top}\\) be a vector of coefficients. We are looking for a linear combination\n\\[Y_{1} = \\alpha_{1}^{\\top} X = \\sum_{k = 1}^{p} \\alpha_{1k}X_k,\\]\nsuch that the variance of \\(Y_{1}\\) is maximized. The idea is simple: we want to combine \\(p\\) variables into a single one, but while “capturing” as much of the variability as possible.\nFirst, we must add a constraint on \\(\\alpha_{1}\\), since otherwise we would only have to take \\(\\alpha_{1k} = \\pm \\infty\\) and we would have \\(\\mathrm{Var}(Y_{1}) = +\\infty\\), which is definitely maximal! We therefore constrain \\(\\alpha_{1}\\) so that it has a norm equal to \\(1\\).\nThis amounts to calculating: \\[\\max_{\\alpha_1^\\top \\alpha_1 = 1} \\mathrm{Var}(Y_1) = \\max_{\\alpha_1^\\top \\alpha_1 = 1} \\alpha_1^\\top \\Sigma \\alpha_{1}.\\]\nThis problem is solved using Lagrange multipliers. It leads to the equation \\[\\Sigma \\alpha_1 = \\lambda_{1} \\alpha_{1},\\] where \\(\\lambda_{1}\\) is the largest eigenvalue of \\(\\Sigma\\) and \\(\\alpha_{1}\\) is the associated eigenvector.\nThis defines the first principal component. The following components are constructed by imposing orthogonality conditions (linear independence) with the previous ones, which amounts to finding the following eigenvectors: \\[\\Sigma \\alpha_k = \\lambda_k \\alpha_k, \\quad \\text{with}\\quad \\lambda_{1} \\geq \\lambda_2 \\geq \\dots \\lambda_p.\\] The principal components are therefore given by \\[Y_k = \\alpha_k^\\top X, \\quad\\text{with } \\alpha_k \\text{ being the eigenvector associated with } \\lambda_k.\\]\n\n\n\n\n\n\nTipNote\n\n\n\nIf \\(\\lambda_{1} &gt; \\dots &gt; \\lambda_{p}\\), then the principal components are unique, up to sign.\n\n\n\n\n\n\n\n\nNoteProof\n\n\n\n\n\nWe seek to calculate \\[\\max_{\\alpha_1^\\top \\alpha_1 = 1} \\mathrm{Var}(Y_1) = \\max_{\\alpha_1^\\top \\alpha_1 = 1} \\alpha_1^\\top \\Sigma \\alpha_{1}.\\]\nThe problem is therefore to maximize \\[F(\\alpha_{1}) = \\alpha_{1}^\\top \\Sigma \\alpha_1, \\quad\\text{s.c.}\\quad \\alpha_1^\\top \\alpha_1 = 1.\\]\nThis problem can be rewritten using Lagrange multipliers, i.e., maximize \\[F(\\alpha_1, \\lambda) = \\alpha_1^\\top \\Sigma \\alpha_1 - \\lambda (\\alpha_1^\\top \\alpha_1 -1 ),\\] where \\(\\lambda\\) is a Lagrange multiplier.\nTo solve this problem, we differentiate \\(F\\) with respect to \\(\\alpha_{1}\\) and \\(\\lambda\\).\n\\[\\begin{cases}\n\\frac{\\partial F(\\alpha_{1}, \\lambda)} {\\partial \\alpha_{1}} = 2 \\Sigma \\alpha_{1} - 2 \\lambda \\alpha_{1},\\\\\n\\frac{\\partial F(\\alpha_{1}, \\lambda)} {\\partial \\lambda} = 1 - \\alpha_{1}^\\top \\alpha_{1}.\n\\end{cases}\n\\]\nThen, we set equal to \\(0\\), which gives: \\[\\begin{cases}\n\\Sigma \\alpha_{1} = \\lambda \\alpha_{1} \\\\\n\\alpha_{1}^\\top \\alpha_{1} = 1\n\\end{cases}\n.\\] The second equation is, of course, our constraint. The first equation is the one we are interested in. Using this equation and the definition of eigenelements, we deduce that\n\n\\(\\alpha_{1}\\) is an eigenvector (normed) of \\(\\Sigma\\);\n\\(\\lambda\\) is the corresponding eigenvalue.\n\nWe therefore have that \\[\\mathrm{Var}(Y_{1}) = \\alpha_{1}^\\top \\Sigma \\alpha_{1} = \\lambda \\alpha_{1}^\\top \\alpha_1 = \\lambda.\\] Since we want to maximize this quantity, we conclude that:\n\n\\(\\lambda = \\lambda_{1}\\), the largest eigenvalue of \\(\\Sigma\\);\n\\(\\alpha_{1}\\), the corresponding normalized eigenvector.\n\nWe then continue with the calculation of the second component. Here, we pursue two objectives simultaneously:\n\nPreserve the maximum variation present in \\(X\\);\nSimplify the dependency structure to facilitate interpretation and ensure the numerical stability of any methods that will use the principal components obtained.\n\nGiven \\(Y_{1}\\), the second principal component \\(Y_{2} = \\alpha_{2}^\\top X\\) is defined such that\n\n\\(\\mathrm{Var}(Y_{2}) = \\alpha_{2}^\\top \\Sigma \\alpha_{2}\\) is maximized;\n\\(\\alpha_{2}^\\top \\alpha_{2} = 1\\);\n\\(\\mathrm{Cov}(Y_{1}, Y_{2}) = 0\\).\n\nHowever, we have that \\[\\mathrm{Cov}(Y_{1}, Y_{2}) = \\mathrm{Cov}(\\alpha_{1}^\\top X, \\alpha_{2}^\\top X) = \\alpha_{1}^\\top \\Sigma \\alpha_{2} = \\alpha_{2}^\\top \\Sigma \\alpha_{1} = \\lambda_{1} \\alpha_{2}^\\top \\alpha_{1}.\\]\nWe are therefore looking for the vector \\(\\alpha_{2}\\) that maximizes: \\[F(\\alpha_{2}, \\lambda, \\kappa) = \\alpha_{2}^\\top \\Sigma \\alpha_{2} - \\lambda (\\alpha_{2}^\\top \\alpha_{2} - 1) - \\kappa (\\alpha_{2}^\\top \\alpha_{1} - 0).\\]\nAs with the first component, we differentiate \\(F\\) with respect to \\(\\alpha_{2}\\), \\(\\lambda\\), and \\(\\kappa\\).\n\\[\\begin{cases}\n\\frac{\\partial F(\\alpha_{2}, \\lambda, \\kappa)}{\\partial \\alpha_{2}} = 2 \\Sigma \\alpha_{2} - 2 \\lambda \\alpha_{2} - \\kappa \\alpha_1 \\\\\n\\frac{\\partial F(\\alpha_{2}, \\lambda, \\kappa)} {\\partial \\lambda} = 1 - \\alpha_{2}^\\top \\alpha_{2} \\\\\n\\frac{\\partial F(\\alpha_{2}, \\lambda, \\kappa)} {\\partial \\kappa} = - \\alpha_2^\\top \\alpha_1\n\\end{cases}\n\\]\nBy setting the equations equal to \\(0\\), we find the two constraint equations, as well as\n\\[2 \\Sigma \\alpha_{2} - 2 \\lambda \\alpha_{2} - \\kappa \\alpha_{1} = 0.\\]\nMultiplying this equation on the left and right by \\(\\alpha_{1}^\\top\\), we find\n\\[2 \\alpha_{1}^\\top \\Sigma \\alpha_{2} - 2 \\alpha_{1}^\\top \\lambda \\alpha_{2} - \\kappa \\alpha_{1}^\\top \\alpha_{1} = 0.\\]\nNow, \\(\\alpha_{1}^\\top \\Sigma = \\lambda_{1} \\alpha_{1}^\\top\\), and \\(\\alpha_{1}^\\top \\alpha_{1} = 1\\), so \\[2 \\alpha_{1}^\\top \\lambda \\alpha_{2} - 2 \\alpha_{1}^\\top \\lambda \\alpha_{2} - \\kappa \\alpha_{1}^\\top \\alpha_{1} = 0 \\implies - \\kappa = 0.\\]\nSubstituting this result, we obtain \\[\\Sigma \\alpha_{2} = \\lambda \\alpha_{2}.\\]\nand therefore \\(\\lambda\\) is another eigenvalue of \\(\\Sigma\\). Since \\[\\mathrm{Var}(Y_{2}) = \\alpha_{2}^\\top \\Sigma \\alpha_{2} = \\alpha_{2}^\\top \\lambda \\alpha_{2} = \\lambda,\\] we have that this variance is maximal if \\(\\lambda = \\lambda_{2}\\), the second largest eigenvalue of \\(\\Sigma\\), and consequently \\(\\alpha_{2}\\) is the corresponding normalized eigenvector.\nWe can generalize this result using successive maximizations. We conclude that \\[Y_k = \\alpha_k^\\top X,\\] where \\(\\alpha_k\\) is the normalized eigenvector associated with \\(\\lambda_k\\), the \\(k\\)th largest eigenvalue of \\(\\Sigma\\).\n\n\n\nIt is possible to have a more compact representation of PCA using matrices. Let \\(A = \\left( \\alpha_{1}, \\dots, \\alpha_p \\right) \\in \\mathbb{R}^{p \\times p}\\) be the matrix whose columns are the eigenvectors. We have \\(Y = AX\\) and the covariance of the principal components is written as \\[\\mathrm{Var}(Y) = A^\\top \\Sigma A.\\]\n\n\n\n\n\n\nImportantProperties of \\(A\\)\n\n\n\n\n\\(A^{\\top} A = A A^{\\top} = I_p\\);\n\\(A^{\\top} = A^{-1}\\);\n\\(\\Sigma A = A \\Lambda\\), where \\(\\Lambda = \\text{diag}(\\lambda_{1}, \\dots, \\lambda_p)\\);\n\\(\\mathrm{Var}(Y) = A^{\\top} \\Sigma A = \\Lambda \\implies \\mathrm{Cov}(Y_k, Y_l) = 0\\) if \\(k \\neq l\\) and \\(\\mathrm{Var}(Y_k) = \\lambda_k \\geq \\mathrm{Var}(Y_l) = \\lambda_l\\) if and only if \\(k \\geq l\\).\n\n\n\n\n\n\n\n\n\nNoteProofs\n\n\n\n\n\n\nBy construction, the columns of \\(A\\) are pairwise orthogonal and have norm \\(1\\) (see constraints on eigenvectors). The matrix \\(A\\) is therefore an orthogonal matrix. And so \\(A^{\\top} A = A A^{\\top} = I_p\\).\nSimilarly, since \\(A\\) is orthogonal, we have \\(A^{\\top} = A^{-1}\\).\nThe result is immediate by multiplying the matrices.\nWe have \\(\\mathrm{Var}(Y) = A^{\\top} \\Sigma A = A^{\\top} A \\Lambda = \\Lambda\\). Since \\(\\Lambda\\) is a diagonal matrix, \\(\\mathrm{Cov}(Y_k, Y_l) = 0\\) if \\(k \\neq l\\) (because it is not on the diagonal) and since \\(\\lambda_{1} \\geq \\dots \\geq \\lambda_p\\), we have \\(\\mathrm{Var}(Y_1) \\geq \\dots \\geq \\mathrm{Var}(Y_p)\\).\n\n\n\n\nAn overall measure of the variation present in the data is given by the trace of the matrix \\(\\Sigma\\): \\[\\text{tr}(\\Sigma) = \\text{tr}(\\Lambda) = \\sum_{i = 1}^{p} \\lambda_i = \\sum_{k = 1}^{p} \\mathrm{Var}(Y_k).\\]\nThe proportion of variation explained by the principal component \\(Y_k\\) is therefore given by the ratio between the eigenvalue \\(k\\) and the sum of the eigenvalues: \\[\\frac{\\lambda_k}{\\lambda_{1} + \\cdots + \\lambda_p} = \\frac{\\mathrm{Var}(Y_k)} {\\text{tr}(\\Sigma)}.\\]\nSimilarly, the first \\(m\\) components explain \\[100\\% \\times \\frac{\\sum_{k = 1}^{m} \\lambda_k}{\\sum_{k = 1}^{p} \\lambda_k} = 100\\% \\times \\frac{\\sum_{k = 1}^{m} \\mathrm{Var}(Y_k)} {\\sum_{k = 1}^{p} \\mathrm{Var}(Y_k)}\\] of the variability in the variables.",
    "crumbs": [
      "Modules",
      "04 - Dimension",
      "Principal components analysis"
    ]
  },
  {
    "objectID": "contents/dimension/01-pca.html#practice-of-pca",
    "href": "contents/dimension/01-pca.html#practice-of-pca",
    "title": "Principal components analysis",
    "section": "Practice of PCA",
    "text": "Practice of PCA\n\nEstimation of the variance-covariance matrix\nIn practice, the variance-covariance matrix \\(\\Sigma\\) is unknown. To perform PCA, it is necessary to estimate \\(\\Sigma\\) from a random sample \\(X_{1}, \\dots, X_n\\) of independent realizations of \\(X\\). An (unbiased) estimator of \\(\\Sigma\\) is given by \\[\\widehat{\\Sigma} = \\frac{1}{n - 1} \\sum_{i = 1}^{n} \\left( X_i - \\overline{X} \\right)\\left( X_i - \\widehat{X} \\right)^{\\top},\\] where \\(\\overline{X}\\) is the empirical mean of the sample.\nThe matrix \\(\\widehat{\\Sigma}\\) thus obtained is symmetric with real coefficients and therefore diagonalizable. It admits a spectral decomposition of the form \\[\\widehat{\\Sigma} = \\widehat{A} \\widehat{\\Lambda} \\widehat{A}^{\\top},\\] where \\(\\widehat{A}\\) is an orthogonal matrix whose columns are the estimators of the eigenvectors of \\(\\Sigma\\) and \\(\\widehat{\\Lambda}\\) is a diagonal matrix containing the estimators of the eigenvalues of \\(\\Sigma\\), assumed to be ordered in descending order.\nThe principal components are obtained by projecting the observations \\(X_i\\) onto the basis of eigenvectors: \\[Y_i = \\widehat{A}^{\\top} X_i.\\]\n\n\nSome remarks\n\n\n\n\n\n\nTipSensitivity to the scale of \\(X_1, \\dots, X_p\\)\n\n\n\nSince we are looking for a linear combination of \\(X_{1}, \\dots, X_p\\) that maximizes the variance, a variable \\(X_k\\) with a large variance will have a disproportionate weight in the principal components, which can distort the interpretation. One example is distance measurement. Expressing distance in meters rather than kilometers would multiply the variance of this variable by \\(1\\) million \\((10^3)^2\\). This variable would therefore have a major weight in all components.\nThus, if the variables are expressed in different units or have very different orders of magnitude, it is recommended to standardize the variables before performing PCA. This is equivalent to performing PCA on the correlation matrix.\n\n\n\n\n\n\n\n\nTipFirst step in predictive analysis\n\n\n\nSometimes PCA is performed because we want to predict the value of variable \\(Z\\) from the values of variables \\(X_{1}, \\dots, X_p\\), but \\(p\\) is simply too large. In this case, PCA is applied to obtain the first \\(k \\ll p\\) principal components \\(Y_1, \\dots, Y_k\\) and these are used to predict \\(Z\\).\nSince the principal components retain most of the information contained in the original variables, this is generally a reasonable approach.\n\n\n\n\n\n\n\n\nTipAxis rotation and representation quality\n\n\n\nSince the matrix \\(A\\) is orthogonal, the matrix product \\(Y = A^{\\top}X\\) represents a rotation of the variable space. The new axes correspond to the successive orthogonal directions of maximum variation, assuming that \\(\\lambda_{1} &gt; \\dots &gt; \\lambda_p\\). Thus, \\(Y_i = A^{\\top} X_i\\) gives the coordinates of the observation \\(X_i\\) in the new axis system. We call \\(Y_{ik}\\) the score of the observation \\(X_i\\) on the principal axis \\(k\\) and calculate it as \\[Y_{ik} = \\alpha_k^\\top X_i = \\sum_{l = 1}^{p} \\alpha_{kl}X_{il}.\\]\nThe quality of the representation of observation \\(i\\) on axis \\(k\\) is given by \\[Q_{ik} = \\frac{Y_{ik}^2}{d^2(0, Y_i)} = \\frac{Y_{ik}^2}{\\sqrt{Y_{i1}^2 + \\dots + Y_{i p}^2}}.\\]\n\n\n\n\nChoosing the number of components\nA key issue in PCA is choosing how many principal components to retain. Retaining too many does not reduce the dimension, and retaining too few can result in the loss of relevant information. Here are the main rules of thumb used:\n\n80% rule: Retain the minimum number of components necessary to explain at least 80% of the total variance. This threshold is arbitrary, but it often provides a good intuition.\nKaiser’s rule: If PCA is performed using the correlation matrix, then the average eigenvalue is 1. It is recommended to keep only those components with an eigenvalue greater than the average eigenvalue, i.e., \\(1\\).\nJoliffe’s rule: A stricter variant of Kaiser’s rule, which suggests keeping components with an eigenvalue greater than \\(0.7\\) for PCA performed using the correlation matrix.\nCattell’s rule (or elbow rule): Plot the eigenvalues \\(\\lambda_k\\) as a function of their rank \\(k\\) and look for a breakpoint in the decline. Beyond this point, additional components explain little additional variance.\n\nThese rules are decision-making tools, but the choice of the number of components also depends on the context, the objectives of the analysis, and ease of interpretation.",
    "crumbs": [
      "Modules",
      "04 - Dimension",
      "Principal components analysis"
    ]
  },
  {
    "objectID": "contents/generalities/02-spaces.html",
    "href": "contents/generalities/02-spaces.html",
    "title": "Spaces",
    "section": "",
    "text": "Before modeling or analyzing data, it is essential to fully understand the nature of the variables being manipulated. The type of variable determines:\nIn this section, we present the most common types of variables, as well as the associated spaces.",
    "crumbs": [
      "Modules",
      "03 - Généralités",
      "Spaces"
    ]
  },
  {
    "objectID": "contents/generalities/02-spaces.html#statistical-unit",
    "href": "contents/generalities/02-spaces.html#statistical-unit",
    "title": "Spaces",
    "section": "Statistical unit",
    "text": "Statistical unit\nA statistical unit is the basic element on which an observation is made. Morally, it is the “carrier” of the information that is used to determine the level of aggregation of the analysis. The statistical unit is a choice made by the modeler.\n\n\n\n\n\n\nNoteExamples\n\n\n\n\nIn the case of an income survey, the individual can be chosen as the unit.\nIn the case of a study of high school classes, the class can be chosen as the unit.\nIn the case of a medical imaging database, the image can be chosen as the unit.\n\n\n\n\n\n\n\n\n\nCautionSometimes, the same database can be analyzed at several levels.\n\n\n\nAn image is made up of pixels, each of which can be described by numerical variables (e.g., RGB values, opacity, etc.). You can choose to analyze each pixel, and therefore take the pixel as the unit, or analyze each image as a whole, and therefore take the image as the unit.",
    "crumbs": [
      "Modules",
      "03 - Généralités",
      "Spaces"
    ]
  },
  {
    "objectID": "contents/generalities/02-spaces.html#types-of-variables",
    "href": "contents/generalities/02-spaces.html#types-of-variables",
    "title": "Spaces",
    "section": "Types of variables",
    "text": "Types of variables\nThere are generally four types of variables, which are identified at the level of the smallest statistical unit in the dataset.\n\n\n\n\n\n\nWarningNumerical (or quantitative) variable\n\n\n\nA numerical (or quantitative) variable is a variable whose values are numbers representing a measurable quantity.\nExamples: income in dollars, weight, age, etc.\n\n\n\n\n\n\n\n\nWarningOrdinal variable\n\n\n\nAn ordinal variable is a qualitative (or categorical) variable whose categories can be ordered naturally, without the difference between the categories being quantifiable.\nExamples: income level (low, medium, or high), satisfaction level (“strongly disagree,” “disagree,” “no opinion,” “agree,” “strongly agree”), etc.\n\n\n\n\n\n\n\n\nWarningSymmetric nominal variable\n\n\n\nA symmetric nominal variable is a qualitative (or categorical) variable in which all categories are equally informative.\nExamples: nationality, field of study, etc.\n\n\n\n\n\n\n\n\nWarningAsymmetric nominal variable\n\n\n\nAn asymmetric nominal variable is a qualitative (or categorical) variable in which one of the categories has a special status, often being more frequent or considered the “default” value. Thus, having two observations with the “default” value of this asymmetric nominal variable does not tell us much about them, whereas we can glean much more information from two observations that do not have the “default” value.\nExamples: presence or absence of a symptom, fraudulent transaction or not, etc.\n\n\nAlthough these types of variables are the most common, there are many other types of variables. For example, we may be interested in comparing curves, texts, images, networks, etc. In these situations, the choice of representation depends on the level at which we wish to place ourselves, and therefore on the statistical unit.",
    "crumbs": [
      "Modules",
      "03 - Généralités",
      "Spaces"
    ]
  },
  {
    "objectID": "contents/generalities/02-spaces.html#associated-spaces",
    "href": "contents/generalities/02-spaces.html#associated-spaces",
    "title": "Spaces",
    "section": "Associated spaces",
    "text": "Associated spaces\nOnce our data has been collected, the first step in statistical analysis is to choose a mathematical space in which to work. This space, sometimes called the observation space and denoted by \\(\\mathcal{X}\\), depends on the type of data observed. It constitutes the formal framework in which our variables take their values, and it guides the methodological choices that follow.\n\n\n\n\n\n\nNoteCase of a numerical variable\n\n\n\nWhen observing a numerical variable (e.g., the temperature of a country), the natural space in which to work is the set of real numbers, \\(\\mathcal{X} = \\mathbb{R}\\). In some cases, this space can be restricted to a specific interval. For example, if we are interested in a person’s height, we can take \\(\\mathcal{X} = [0, +\\infty)\\) because the variable in question cannot be negative.\n\n\n\n\n\n\n\n\nNoteCase of a nominal (or qualitative, or categorical) variable\n\n\n\nFor a nominal variable, the space is a finite set of modalities, the set of modalities taken by the variable. For example, if we study the results of a dice roll, the variable can take values from \\(1\\) to \\(6\\), and the associated space will therefore be \\(\\mathcal{X} = \\{ 1, 2, 3, 4, 5, 6 \\}\\).\n\n\nWhen the data is more complex, more suitable spaces must be chosen. For curve or signal analysis, we can work in a function space. For example, we can consider the space of continuous functions on a closed interval \\([a, b]\\), denoted \\(\\mathcal{X} = \\mathcal{C}([a, b])\\). For text analysis (viewed as a sequence of characters), the workspace can be an alphabet. For example, we can consider \\(\\mathcal{X} = \\{ \\text{A}, \\text{B}, \\dots, \\text{Z} \\}\\).\nOften, several variables are observed at the same time, e.g., the height, weight, and gender of an individual. In this case, the observation space will be the Cartesian product (also called the product set) of the spaces associated with each variable: \\[\\mathcal{X} = \\mathcal{X}_1 \\times \\mathcal{X}_2 \\times \\dots \\mathcal{X}_p,\\] where \\(p\\) is the number of variables. In the case where we observe \\(p\\) numerical variables, we will simply write \\(\\mathcal{X} = \\mathbb{R}^p\\).",
    "crumbs": [
      "Modules",
      "03 - Généralités",
      "Spaces"
    ]
  },
  {
    "objectID": "contents/generalities/01-stat.html",
    "href": "contents/generalities/01-stat.html",
    "title": "Data analysis project",
    "section": "",
    "text": "Here we present the different stages of a data analysis project.",
    "crumbs": [
      "Modules",
      "03 - Généralités",
      "Data analysis project"
    ]
  },
  {
    "objectID": "contents/generalities/01-stat.html#data-analysis-project",
    "href": "contents/generalities/01-stat.html#data-analysis-project",
    "title": "Data analysis project",
    "section": "Data analysis project",
    "text": "Data analysis project\nA data analysis project can be divided into five main stages:\n\nDefining objectives\nData\nDeveloping and validating models\nImplementation\nPerformance monitoring and improvement\n\nWhen planning a project, it is important to bear in mind that each stage has a different level of importance, but also that each stage takes a different amount of time to complete. Pyle (1999) provides an estimate of the time required for each stage, as well as their importance in the success of the project (given as a percentage of the total, see Table 1).\n\n\n\nTable 1: Breakdown of a data analysis project.\n\n\n\n\n\nStage\nTime\nImportance\n\n\n\n\nUnderstanding the problem\n\\(10\\%\\)\n\\(15\\%\\)\n\n\nExploring the solution\n\\(9\\%\\)\n\\(14\\%\\)\n\n\nImplementing the solution\n\\(1\\%\\)\n\\(51\\%\\)\n\n\nPrepare the data\n\\(60\\%\\)\n\\(15\\%\\)\n\n\nAnalyze the data\n\\(15\\%\\)\n\\(3\\%\\)\n\n\nModel the data\n\\(5\\%\\)\n\\(2\\%\\)\n\n\n\n\n\n\nTwo important facts stand out: just because a step is very important does not mean it will take a long time. Implementing the solution is very important (otherwise there will be no result), but it will not usually take very long to do (possibly just a few lines of code). Conversely, data preparation is a stage of medium importance (although this is debatable), but it takes up most of the project time. For example, it is necessary to manage missing data, outliers, possible accents for French data, etc.",
    "crumbs": [
      "Modules",
      "03 - Généralités",
      "Data analysis project"
    ]
  },
  {
    "objectID": "contents/generalities/01-stat.html#setting-objectives",
    "href": "contents/generalities/01-stat.html#setting-objectives",
    "title": "Data analysis project",
    "section": "Setting objectives",
    "text": "Setting objectives\nDo we want to: visualize data? explore and make assumptions? test? group? understand? predict?\nHow do we do this in practice? We ask questions! First, we need to clarify the terms. Who will use the model and how? What is the target population?\nWhy is it important to have clear objectives when undertaking a data analysis project? It helps guide data collection and formatting. It helps define an appropriate model (e.g., classification vs. prediction). It allows you to analyze the results in light of the objective and thus allow others to judge their relevance. It is important to define the objectives before looking at the data so as not to be biased by it.\n\n\n\n\n\n\nNoteExample\n\n\n\nThe National Bank of Canada would like to launch a new savings product and is giving you access to its customer database.\nBad objective: Analyze the customer database.\nBetter objective: Can you predict which customers will buy the new savings product?\n\n\n\n\n\n\n\n\nNoteExample\n\n\n\nThe Montreal Canadiens hockey team wants to learn more about its opponents in order to develop new game tactics.\nBad objective: Analyze the opponents’ data.\nBetter objective: Can you characterize the opponents’ playing style in order to identify weaknesses?\n\n\n\n\n\n\n\n\nNoteExample\n\n\n\nPharmascience wants to know if its new drug is effective.\nBad objective: Analyze the drug data.\nBetter objective: Can you determine a testing protocol (statistics) to determine if the drug is effective?",
    "crumbs": [
      "Modules",
      "03 - Généralités",
      "Data analysis project"
    ]
  },
  {
    "objectID": "contents/generalities/01-stat.html#data",
    "href": "contents/generalities/01-stat.html#data",
    "title": "Data analysis project",
    "section": "Data",
    "text": "Data\nData is at the heart of the matter. To be useful, data must be available and of good quality. Once the objectives have been defined, preliminary processing and basic exploration of the data is carried out, followed by more developed models.\n\nWhere can data be found?\nThe simple answer is: on the Internet! Here is a non-exhaustive list of websites that collect data sets:\n\nGoogle datasets;\nKaggle;\nUC Irvine Machine Learning Repository;\nTime Series Machine Learning website;\nPhysionet Database.\n\nYou can also check out the official websites of data sources that can be found for most countries around the world:\n\nCanada: StatCan;\nFrance: data.gouv.fr;\nUSA: data.gov;\nEngland: data.gouv.uk;\netc.\n\nFor data on more specific topics, you can look at the different branches of government. For example, the Canadian Centre for Mapping and Earth Observation provides geospatial data for Canada (here).\nWhen working for a company, you generally have access to internal data sources, e.g. databases on production, customers and employees, lists of transactions and potential customers, information on website visits.\n\n\nQuality\nThere is a popular saying in computer science that also applies to data analysis: “Garbage in, garbage out.” Basically, this means that no matter how sophisticated the model is, if the input data is poor quality, biased, incomplete, etc., then the output results will be poor quality. This ensures a certain level of credibility, reproducibility, and usability for our conclusions.\nBut what do we mean by data quality? To ensure data quality, we can ask ourselves the following questions:\n\nIs the data representative of the target population?\nIs the data correct and relevant?\nIs there any missing or redundant data?\n\n\n\nBuilding the database\nOnce our data has been retrieved, we need to load it into memory so that we can then perform analyses. In Python, the pandas library can read most of the file formats we will be dealing with. In R, different libraries need to be used to load different types of data (see Table 2).\n\n\n\nTable 2: Different libraries for different file formats.\n\n\n\n\n\nFormat\nExtension\nLibrary\n\n\n\n\nText\n.txt; .csv\nreadr\n\n\nExcel\n.xlsx\nreadxl\n\n\nSAS\n.sas7bdat\nhaven\n\n\nSPSS\n.sav; .zsav\nhaven\n\n\nJSON\n.json\njsonlite\n\n\n\n\n\n\nOver the past decade, the concept of “tidy data” has emerged (see Wickham (2014)). Each “tidy” dataset follows three principles:\n\nEach variable is a column in the table.\nEach observation is a row in the table.\nEach cell in the table corresponds to a single measurement.\n\nThis allows for a unified approach to data analysis. In general, we will always try to put our dataset in “tidy” format. The tidyr package in R and the pandas library in Python allow you to format data in “tidy” format.\n\n\nExploration and preliminary processing\nOnce the data has been loaded and formatted in “tidy” format, we perform an initial exploration of the data before moving on to the model development stage itself. Although this stage is very important, it is not the focus of this course. Here are a few tips for this initial exploration:\n\nData cleaning: remove duplicates, standardize modalities, check the format of special values, etc.\nData exploration: rare modalities, too many modalities, asymmetry, class imbalance, extreme or outlier values, highly correlated variables, missing values.",
    "crumbs": [
      "Modules",
      "03 - Généralités",
      "Data analysis project"
    ]
  },
  {
    "objectID": "contents/generalities/01-stat.html#model-development-and-validation",
    "href": "contents/generalities/01-stat.html#model-development-and-validation",
    "title": "Data analysis project",
    "section": "Model development and validation",
    "text": "Model development and validation\nThis course focuses on model development and validation. For now, we can identify four main components for this:\n\na (mathematical) space in which to work;\na distance for comparing observations;\na model (or algorithm);\na function for measuring the quality of the model.\n\nWe will look at each of these components in detail in the following sections.",
    "crumbs": [
      "Modules",
      "03 - Généralités",
      "Data analysis project"
    ]
  },
  {
    "objectID": "contents/generalities/01-stat.html#implementation",
    "href": "contents/generalities/01-stat.html#implementation",
    "title": "Data analysis project",
    "section": "Implementation",
    "text": "Implementation\nOnce the model has been chosen and validated, we may want to put it into production. Putting a model into production means making it available to as many people as possible. Generally, this involves automating data collection and cleaning, then “feeding” the created model and producing analysis reports of the results. This part is called data engineering. A data engineer is therefore responsible for setting up the data processing pipeline, from data collection to model output.",
    "crumbs": [
      "Modules",
      "03 - Généralités",
      "Data analysis project"
    ]
  },
  {
    "objectID": "contents/generalities/01-stat.html#performance-monitoring-and-improvement",
    "href": "contents/generalities/01-stat.html#performance-monitoring-and-improvement",
    "title": "Data analysis project",
    "section": "Performance monitoring and improvement",
    "text": "Performance monitoring and improvement\nFinally, once the model is in production, its performance must be monitored. When new data arrives, the model that was considered may no longer be very suitable (e.g., the assumptions made are no longer correct). To prevent this from happening, the model is monitored by regularly checking its performance. You may also want to improve your model, for example because you have more accurate assumptions or better quality data.",
    "crumbs": [
      "Modules",
      "03 - Généralités",
      "Data analysis project"
    ]
  },
  {
    "objectID": "contents/02-revisions.html",
    "href": "contents/02-revisions.html",
    "title": "Remainders",
    "section": "",
    "text": "Slides:\n\nAlgèbre linéaire - annotated\nProbabilités et statistiques - annotated\nAlgorithmique / Programmation\n\nTD: link\n\nSolution Exercise 1\nSolution Exercise 2\nSolution Exercise 3\n\nTP: link",
    "crumbs": [
      "Modules",
      "02 - Révisions"
    ]
  },
  {
    "objectID": "contents/02-revisions.html#outline",
    "href": "contents/02-revisions.html#outline",
    "title": "Remainders",
    "section": "Outline",
    "text": "Outline\n\nLinear algebra\nProbabilities and statistics\nAlgorithmic / Programming\n\n\n\n\nPrediction (xkcd:2370).",
    "crumbs": [
      "Modules",
      "02 - Révisions"
    ]
  },
  {
    "objectID": "contents/01-introduction.html",
    "href": "contents/01-introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "Data analysis is a set of methods for extracting information from a data set. It’s also known as statistical learning. The idea is to use statistical models to understand how data is structured and how it interacts with each other.\n\n\n\n\n\n\nNoteExample\n\n\n\nLet’s imagine you work for the United Nations (UN). Your mission is to analyze life expectancy around the world. To do this, you’ll have a measure of life expectancy in each UN member country, of course, but also GDP per capita, health expenditure, fertility rate, urbanization rate, education level of the country, and so on. The aim of data analysis is to find links between these different variables and the variable of interest, life expectancy, to visualize these data, and eventually to predict life expectancy from the other variables.",
    "crumbs": [
      "Modules",
      "01 - Introduction"
    ]
  },
  {
    "objectID": "contents/01-introduction.html#what-is-data-analysis",
    "href": "contents/01-introduction.html#what-is-data-analysis",
    "title": "Introduction",
    "section": "",
    "text": "Data analysis is a set of methods for extracting information from a data set. It’s also known as statistical learning. The idea is to use statistical models to understand how data is structured and how it interacts with each other.\n\n\n\n\n\n\nNoteExample\n\n\n\nLet’s imagine you work for the United Nations (UN). Your mission is to analyze life expectancy around the world. To do this, you’ll have a measure of life expectancy in each UN member country, of course, but also GDP per capita, health expenditure, fertility rate, urbanization rate, education level of the country, and so on. The aim of data analysis is to find links between these different variables and the variable of interest, life expectancy, to visualize these data, and eventually to predict life expectancy from the other variables.",
    "crumbs": [
      "Modules",
      "01 - Introduction"
    ]
  },
  {
    "objectID": "contents/01-introduction.html#course-objectives",
    "href": "contents/01-introduction.html#course-objectives",
    "title": "Introduction",
    "section": "Course objectives",
    "text": "Course objectives\nIn this course, we aim to introduce methods that allow us to study a “high-dimensional” dataset (in the sense that we can’t simply graph all the variables for each observation) without having to resort to a probabilistic model. The various techniques we’ll be looking at can be used to:\n\nvisualize data\nreduce data size;\nidentify certain relationships between variables;\ndivide the dataset into groups/classes.\n\nThis course is not intended to be exhaustive, in the sense of presenting all possible methods. Nor is it intended to be state-of-the-art, in the sense that it will not cover the latest developments in machine learning. Nor is it a programming course.\nTo finish this introduction, here is a quote from Statistical Rethinking by Richard McElreath (McElreath 2020) particularly accurate in this course.\n\nStatistics courses […] tend to resemble horoscopes. There are two senses to this resemblance. First, in order to remain plausibly correct, they must remain tremendously vague. This is because the targets of the advice, for both horoscopes and statistical advices, are diverse. But only the most general advice applies to all cases. A horoscope uses only the basic facts of birth to forecast life events, and a […] statistical guide uses only the basic facts of measurement and design to dictate a model. It is easy to do better, once more detail is available. In the case of statistical analysis, it is typically only the scientist who can provide that detail, not the statistician. Second, there are strong incentives for both astrologers and statisticians to exaggerate the power and importance of their advice. No one likes an astrologer who forecasts doom, and few want a statistician who admits the answers as desired are not in the data as collected. Scientists desire results, and they will buy and attend to statisticians and statistical procedures that promise them. What we end up with is too often horoscopic: vague and optimistic, but still claiming critical importance.\n\n\n\n\nMachine learning (xkcd:1838).",
    "crumbs": [
      "Modules",
      "01 - Introduction"
    ]
  },
  {
    "objectID": "contents/misc/good-practices-python.html",
    "href": "contents/misc/good-practices-python.html",
    "title": "Good practices in Python",
    "section": "",
    "text": "This page is inspired by this document.\nLike R, Python is “just” a programming language, so it is possible to write Python code in a text editor and run it via a terminal. Although this is feasible, it is easier to use a code editor. While it is technically possible to use RStudio as an editor for Python, I would recommend using a specialized editor such as PyCharm. The notebook format is also very popular in Python. In this case, I would recommend the marimo library, which allows you to convert your .py files into notebooks.\nIn addition, it is best to make sure you are working with the latest stable version of Python that is compatible with the packages you are using (e.g., numpy, matplotlib, sklearn). Very often, there is a delay before the latest versions of packages are compatible with the latest version of Python. It is therefore important to share the version of the packages used with the analysis.\nRegarding documentation, the standard in Python is to follow the recommendations of PEP 257. The Sphinx library allows you to generate documentation from docstrings.\nA uniform syntax makes code much easier to read and understand. A style guide sets out standards for uniform syntax. The PEP 8 style guide is recommended in Python.\nLine breaks and indentation\nIndentation is part of Python, i.e. if...else and for blocks are defined using indentation. You can use either tabs or spaces. The choice is yours, but it is important to be consistent in your code.\nNaming convention\nThe use of single-character variable names should be avoided. Otherwise, variables, functions, methods, packages, and modules are named with lowercase letters, numbers, and the underscore _. Classes and exceptions should be named with uppercase letters to separate words (UneClasse). Constants are in uppercase.\nCode organization\nAs with R, when the code starts to get long, it is advantageous to split it into several files. For example, you can have one file per part of the analysis (one for data cleaning, one for analysis, one for visualization, etc.). You can also create several subfolders.\nSome things to do\n\nDo not make comparisons to True, False, or None.\n\n\nif attr:\n    print(\"True!\")\n\nif not attr:\n    print(\"False!\")\n\nif attr is None:\n    print(\"None!\")\n\n\nUse list comprehension when possible.\n\n\na = [3, 4, 5]\nb = [i for i in a if i &gt; 4]\n\n\nLoad a file with with. This ensures that the connection to the file is closed once it has been read.\n\n\nwith open(\"file.txt\") as f:\n    read(f)\n\n\nUse a maximum of 80 characters per line.\nUse parentheses to wrap long strings."
  },
  {
    "objectID": "contents/misc/good-practices-R.html",
    "href": "contents/misc/good-practices-R.html",
    "title": "Good practices in R",
    "section": "",
    "text": "This page is based on a document created by Aurélien Nicosia (ULaval) in 2023 called “Bonnes pratiques de programmation en R.” It has been updated.\nSince R is “just” a programming language, it is technically possible to use a simple text editor to write R code and a terminal to run it. However, it is much more convenient—especially for beginners—to use a code editor such as RStudio to run your code. This allows you to run your code interactively, view your working environment, test your code, and more.\nIn addition, it is best to make sure you are working with the latest version of R and the packages you need. This reduces the risk of encountering bugs and allows you to take advantage of the latest features. One point to note, however, is that I would advise against changing the version of R or the packages once you have started working on something. This change could introduce bugs in the middle of the analysis (so always report the versions of the packages used for reproducibility). You can get a summary of the working environment with the systemInfo() and packageVersion(pkg) functions.\nRegarding documentation, the roxygen package allows you to convert comments in your functions and packages into clean documentation.\nA uniform syntax makes code much easier to read and understand. A style guide sets standards for uniform syntax. The tidyverse style guide (https://style.tidyverse.org) is recommended in R. Before presenting some style conventions, note that you can quickly change the formatting of a piece of R code in RStudio in the “Code -&gt; Reformat Selection” menu.\nLine breaks and indentation\nA simple way to make your code more readable is to insert appropriate line breaks and indentations. For example, suppose we have the following character string:\n\ntext &lt;- \"Ceci est un example\"\n\nWe want to correct two errors in this sentence: the word “example” written in English rather than French and the missing period at the end of the sentence. This can be done with the following instruction:\n\npaste0(gsub(pattern = \"example\", replacement = \"example\", x = text), \".\")\n\nThis instruction contains one function call nested within another. It is much easier to read as follows:\n\npaste0(\n  gsub(\n    pattern = \"example\",\n    replacement = \"exemple\",\n    x = text),\n  \".\")\n\nAssignment operator\nIn R, &lt;- is used to assign a value to a variable, and = is used to pass values to arguments in a function call.\nNaming conventions\nThe tydiverse style guide recommends using lowercase letters, numbers, and the underscore _ to name variables and functions. Underscores are used to separate words in a name. Although other conventions may be found, these should be avoided. In any case, it is important to choose a convention and stick to it. In addition, it is best to avoid accents in variable names.\nCode organization\nWhen the code starts to get long, it becomes advantageous to separate it into several files. For example, you can have one file per part of the analysis (one for data cleaning, one for analysis, one for visualization, etc.). In addition, a data analysis does not usually consist solely of R code, e.g., C++ code files, data files, configuration files, etc. It is therefore recommended to create subfolders grouping files of the same type. RStudio projects are ideal for gathering all project-related files in one place. In addition, they make it easier to work on several projects simultaneously by managing the transition from one working directory to another.\nA few things to do\n\nWrite your code in a script and save it frequently. This prevents you from losing track of important instructions because they were written directly in the console.\nIt is best to start every R work session with an empty work environment. To do this, disable automatic session image restoration in the settings. This allows you to be aware of the presence of different objects in the working environment.\nDo not use the load function when the working environment is not empty. This prevents you from modifying an object in the working environment by overwriting it.\nDo not use the attach function. This prevents you from modifying the file search path.\nSave the graphics options and settings before modifying them.\nDo not use T and F instead of TRUE and FALSE."
  },
  {
    "objectID": "contents/remainders/01-linear-algebra.html",
    "href": "contents/remainders/01-linear-algebra.html",
    "title": "Linear Algebra",
    "section": "",
    "text": "In this section, we present some linear algebra results that are useful in the context of this course. For more information, you can refer to the MAT-1200 course, Deisenroth, Faisal, and Ong (2020) (in English), and Grifone (2024) (in French).",
    "crumbs": [
      "Modules",
      "02 - Révisions",
      "Linear Algebra"
    ]
  },
  {
    "objectID": "contents/remainders/01-linear-algebra.html#some-matrix-properties",
    "href": "contents/remainders/01-linear-algebra.html#some-matrix-properties",
    "title": "Linear Algebra",
    "section": "Some matrix properties",
    "text": "Some matrix properties\nLet \\(M_{n, m}(\\mathbb{R})\\) be the set of matrices with \\(n\\) rows and \\(m\\) columns whose entries belong to \\(\\mathbb{R}\\). Let \\(M_{n}(\\mathbb{R})\\) be the set of square matrices of size \\(n\\), i.e., with \\(n\\) rows and \\(n\\) columns whose entries belong to \\(\\mathbb{R}\\). Let \\(M\\), \\(N\\), and \\(P\\) be matrices in \\(M_{n, m}(\\mathbb{R})\\). Let \\(A\\) and \\(B\\) be matrices in \\(M_{n}(\\mathbb{R})\\). Let \\(I_n\\) be the identity matrix of size \\(n\\), i.e., containing \\(1\\)s on the diagonal and \\(0\\)s on the elements outside the diagonal. Let \\(u\\) and \\(v\\) in \\(\\mathbb{R}^n\\), i.e., column vectors of size \\(n\\).\n\n\n\n\n\n\nImportantProperties of the inverse of matrices\n\n\n\nSuppose that the matrices \\(A\\) and \\(B\\) are invertible. Then the matrix product \\(AB\\) is invertible and is given by:\n\\[(AB)^{-1} = B^{-1} A^{-1}.\\]\n\n\n\n\n\n\n\n\nNoteProof\n\n\n\n\n\nLet \\(C = AB\\) and \\(D = B^{-1} A^{-1}\\). Then\n\\[\\begin{align*}\n  CD &= A B B^{-1} A^{-1} \\\\\n     &= A A^{-1} \\\\\n     &= I_n\n\\end{align*}\\]\nSimilarly, we find that \\(DC = I_n\\). Thus, \\(AB\\) is invertible and its inverse is given by \\(B^{-1} A^{-1}\\).\n\n\n\n\n\n\n\n\n\nImportantProperties of the determinant of matrices\n\n\n\nConsidering the matrices defined at the beginning of the section, we have:\n\n\\(\\text{det}(A^\\top) = \\text{det}(A)\\),\n\\(\\text{det}(AB) = \\text{det}(A)\\text{det}(B)\\),\n\\(\\text{det}(A^{-1}) = 1 / \\text{det}(A)\\).\n\n\n\n\n\n\n\n\n\nNoteProof\n\n\n\n\n\nThe proofs of properties \\(1\\) and \\(2\\) are technical and are omitted, but can be found, for example, here. As for the third property, by definition, we have \\(A A^{-1} = I_n\\). The determinant of \\(I_n\\) is equal to \\(1\\) (product of the elements on the diagonal). Therefore, \\(\\text{det}(A A^{-1}) = 1\\). However, according to the second property, \\(\\text{det}(A A^{-1}) = \\text{det}(A)\\text{det}(A^{-1})\\). We therefore have \\(\\text{det}(A^{-1}) = 1 / \\text{det}(A)\\).\n\n\n\n\n\n\n\n\n\nImportantProperties of the trace of matrices\n\n\n\nConsidering the matrices defined at the beginning of the section, we have:\n\n\\(\\text{tr}(A) = \\text{tr}(A^{\\top})\\),\n\\(\\text{tr}(A + B) = \\text{tr}(A) + \\text{tr}(B)\\),\n\\(\\text{tr}(MN^{\\top}) = \\text{tr}(N^{\\top}M)\\).\n\n\n\n\n\n\n\n\n\nNoteProof\n\n\n\n\n\nFor a square matrix \\(A\\), let \\(a_{ij}\\) be the element of matrix \\(A\\) in row \\(i\\) and column \\(j\\). The trace of \\(A\\) is given by the sum of the diagonal elements, i.e. \\(\\text{tr}(A) = \\sum_{i = 1}^{n} a_{ii}\\).\n\nSince transposition does not change the diagonal elements, the result is straightforward.\nLet \\(C = A + B\\). Since \\(A\\) and \\(B\\) are square matrices, \\(C\\) is a square matrix. We have \\(c_{ij} = a_{ij} + b_{ij}\\) for all \\(i, j = 1, \\dots, n\\). Therefore \\[\\text{tr}(A + B) = \\text{tr}(C) = \\sum_{i = 1}^{n} c_{ii} = \\sum_{i = 1}^{n} a_{ii} + b_{ii} = \\sum_{i = 1}^{n} a_{ii} + \\sum_{i = 1}^{n} b_{ii} = \\text{tr}(A) + \\text{tr}(B).\\]\nThe matrices \\(M N^{\\top}\\) and \\(N^{\\top} M\\) are square, with dimensions \\(n \\times n\\) and \\(m \\times m\\) respectively, so we can calculate their traces. Let \\(C = M N^{\\top}\\) and \\(D = N^{\\top} M\\). \\[\\text{tr}(M N^{\\top}) = \\text{tr}(C) = \\sum_{i = 1}^{n} c_{ii} = \\sum_{i = 1}^{n} \\sum_{j = 1}^{m} m_{ij} n_{ji} = \\sum_{j = 1}^{m} \\sum_{i = 1}^{n} n_{ji} m_{ij} = \\sum_{j = 1}^{m} d_{jj} = \\text{tr}(D)  = \\text{tr}(N^{\\top} M).\\]\n\n\n\n\n\n\n\n\n\n\nWarningDefinition\n\n\n\n\nLet \\(A\\) be a symmetric matrix in \\(M_n(\\mathbb{R})\\). \\(A\\) is positive definite if \\(u^\\top A u &gt; 0\\) for all \\(u \\in \\mathbb{R}^n\\) such that \\(u \\neq 0\\).\nLet \\(A \\in M_n(\\mathbb{R})\\). \\(A\\) is orthogonal if \\(A^\\top A = A A^\\top = I_n\\).",
    "crumbs": [
      "Modules",
      "02 - Révisions",
      "Linear Algebra"
    ]
  },
  {
    "objectID": "contents/remainders/01-linear-algebra.html#eigenvalues-and-eigenvectors",
    "href": "contents/remainders/01-linear-algebra.html#eigenvalues-and-eigenvectors",
    "title": "Linear Algebra",
    "section": "Eigenvalues and eigenvectors",
    "text": "Eigenvalues and eigenvectors\n\n\n\n\n\n\nWarningDefinition\n\n\n\nLet \\(A \\in M_n(\\mathbb{R})\\). We say that \\(\\lambda \\in \\mathbb{R}\\) is an eigenvalue of \\(A\\) if there exists a nonzero vector \\(u \\in \\mathbb{R}^n\\) such that \\[Au = \\lambda u. \\tag{1}\\] The vector \\(u\\) is called an eigenvector of \\(A\\) corresponding to the eigenvalue \\(\\lambda\\).\nThe set of real numbers \\(\\lambda\\) satisfying Equation 1 is called the spectrum of the matrix \\(A\\) and is denoted by \\(\\text{sp}(A)\\).\n\n\n\n\n\n\n\n\nImportantProperty of eigenvectors\n\n\n\n\nIf \\(u\\) is an eigenvector of \\(A\\) corresponding to an eigenvalue \\(\\lambda\\), then the vector \\(cu\\), \\(c \\in \\mathbb{R}^\\star\\) is also an eigenvector of \\(A\\) corresponding to \\(\\lambda\\).\nIf \\(A\\) is symmetric and \\(u_{1}\\) and \\(u_{2}\\) are eigenvectors corresponding to different eigenvalues of \\(A\\), then \\(u_{1}\\) and \\(u_{2}\\) are orthogonal, i.e. \\(u_{1}^\\top u_{2} = 0\\).\n\n\n\n\n\n\n\n\n\nNoteProof\n\n\n\n\n\n\nLet \\(c \\in \\mathbb{R}^\\star\\) and \\(u\\) be an eigenvector of \\(A\\) associated with the eigenvalue \\(\\lambda\\). We have: \\[A(cu) = cAu = c \\lambda u = \\lambda (cu).\\] Therefore, the vector \\(cu\\) is also an eigenvector of \\(A\\) associated with the eigenvalue \\(\\lambda\\).\nLet \\(\\lambda_{1}\\) and \\(\\lambda_{2}\\) be the eigenvalues associated with \\(u_{1}\\) and \\(u_{2}\\), such that \\(\\lambda_{1} \\neq \\lambda_{2}\\). We have \\(A u_{1} = \\lambda_{1} u_{1}\\) and \\(A u_{2} = \\lambda_{2} u_{2}\\). Then \\[\\lambda_{1} u_{1}^{\\top} u_{2} = u_{1}^\\top A u_{2} = \\lambda_{2} u_{1}^\\top u_{2}.\\] This implies that \\((\\lambda_{1} - \\lambda_{2})u_{1}^\\top u_{2} = 0\\). However, \\(\\lambda_{1} \\neq \\lambda_{2}\\). Therefore, necessarily, \\(u_{1}^\\top u_{2} = 0\\).\n\n\n\n\nThis second property will be useful when we look at dimension reduction and, in particular, principal component analysis.\n\n\n\n\n\n\nImportantCharacterization of matrices with their eigenvalues\n\n\n\n\nIf \\(A\\) is symmetric, then all its eigenvalues are real.\nIf \\(A\\) is positive definite, then all its eigenvalues are strictly positive.\n\n\n\n\n\n\n\n\n\nNoteProof\n\n\n\n\n\n\nConsider the more general case where \\(A\\) is a Hermitian matrix. The matrix \\(A\\) is equal to the transpose of its conjugate, denoted \\(A^*\\). Let \\(\\lambda\\) be an eigenvalue associated with an eigenvector \\(u\\), which may be complex. We have: \\[\\begin{align}\n\\overline{u}^{\\top} A u &= \\overline{u}^\\top \\lambda u = \\lambda \\overline{u}^{\\top} u, \\\\\n\\overline{u}^\\top A u &= \\overline{u}^\\top A^* u = \\overline{Au}^\\top u = \\overline{\\lambda} \\overline{u}^\\top u.\n\\end{align}\\] This implies that \\((\\lambda - \\overline{\\lambda}) \\overline{u}^{\\top} u = 0\\). Since \\(u \\neq 0\\), we have \\(\\lambda = \\overline{\\lambda}\\). Therefore, \\(\\lambda \\in \\mathbb{R}\\).\nConsider \\(u\\), an eigenvector of \\(A\\) associated with the eigenvalue \\(\\lambda\\). We have that \\(u^{\\top} A u = \\lambda u^{\\top} u\\). However, since \\(u \\neq 0\\), \\(u^{\\top}u \\neq 0\\). Therefore, \\[\\lambda = \\frac{u^{\\top} A u}{u^{\\top} u}.\\] Since \\(A\\) is positive definite, \\(u^{\\top} A u &gt; 0\\) for all nonzero vectors \\(u\\). We can therefore deduce that \\(\\lambda &gt; 0\\).",
    "crumbs": [
      "Modules",
      "02 - Révisions",
      "Linear Algebra"
    ]
  },
  {
    "objectID": "contents/remainders/01-linear-algebra.html#diagonalization-of-matrices",
    "href": "contents/remainders/01-linear-algebra.html#diagonalization-of-matrices",
    "title": "Linear Algebra",
    "section": "Diagonalization of matrices",
    "text": "Diagonalization of matrices\n\n\n\n\n\n\nWarningDefinition\n\n\n\nLet \\(A \\in M_n(\\mathbb{R})\\). We say that \\(A\\) is diagonalizable if there exists a non-singular matrix \\(P \\in M_n(\\mathbb{R})\\) and a diagonal matrix \\(D \\in M_n(\\mathbb{R})\\) such that \\[P^{-1} A P = D \\Longleftrightarrow A = P D P^{-1}.\\]\n\n\n\n\n\n\n\n\nCautionSpectral decomposition theorem\n\n\n\nLet \\(A\\) be a symmetric matrix in \\(M_n(\\mathbb{R})\\) and \\(\\lambda_{1}, \\dots, \\lambda_n\\) its \\(n\\) eigenvalues. Then there exists an orthogonal matrix \\(P\\) in \\(M_n(\\mathbb{R})\\) such that \\[A = P \\Lambda P^\\top, \\quad\\text{where}\\quad \\Lambda = \\text{diag}(\\lambda_1, \\dots, \\lambda_n).\\]\n\n\nIf \\(A\\) has \\(n\\) distinct positive eigenvalues, then we can take \\(P\\) to be the matrix whose \\(k\\)th column is the normalized eigenvector corresponding to the \\(k\\)th eigenvalue \\(\\lambda_k\\) of \\(A\\).\nGiven two symmetric matrices, \\(A\\) and \\(B\\), how can we determine the vector \\(u\\) such that \\(u^{\\top} A u\\) is maximal, knowing that \\(u^{\\top} B u = 1\\)? We simply take \\(u\\) as the eigenvector of \\(B^{-1}A\\) associated with \\(\\lambda\\), the maximal eigenvalue of \\(B^{-1}A\\). We thus obtain \\[u^{\\top} A u = u^{\\top}\\lambda M u = \\lambda U^{\\top} M u = \\lambda.\\]\n\n\n\n\n\n\nImportantCharacterization of the determinant and trace of matrices with their eigenvalues\n\n\n\nIf \\(A\\) has eigenvalues (real, but not necessarily distinct) \\(\\lambda_{1}, \\dots, \\lambda_{n}\\), then\n\n\\(\\text{det}(A) = \\prod_{i = 1}^{n} \\lambda_i\\)\n\\(\\text{tr}(A) = \\sum_{i = 1}^{n} \\lambda_i.\\)\n\n\n\n\n\n\n\n\n\nNoteProof\n\n\n\n\n\nUsing the spectral decomposition theorem, there exists an invertible matrix \\(P\\) such that \\(A = P \\Lambda P^{-1}\\), where \\(\\Lambda\\) is a diagonal matrix containing the eigenvalues. We therefore have, for the determinant,\n\\[\\text{det}(A) = \\text{det}(P \\Lambda P^{-1}) = \\text{det}(P)\\text{det}(\\Lambda)\\text{det}(P^{-1}) = \\text{det}(P)\\text{det}(\\Lambda)\\text{det}(P)^{-1} = \\text{det}(\\lambda) = \\prod_{i = 1}^{n} \\lambda_i, \\]\nand, for the trace,\n\\[\\text{tr}(A) = \\text{tr}(P \\Lambda P^{-1}) = \\text{tr}(P^{-1} P \\Lambda) = \\text{tr}(\\Lambda) = \\sum_{i = 1}^{n} \\lambda_i.\\]",
    "crumbs": [
      "Modules",
      "02 - Révisions",
      "Linear Algebra"
    ]
  },
  {
    "objectID": "contents/remainders/02-probabilities.html",
    "href": "contents/remainders/02-probabilities.html",
    "title": "Probabilities and Statistics",
    "section": "",
    "text": "In this section, we present some results in probability and statistics within the framework of this course. For more information, you can refer to the STT-1000 course, Wasserman (2010) (in English) and Delmas (2013) (in French).",
    "crumbs": [
      "Modules",
      "02 - Révisions",
      "Probabilities and Statistics"
    ]
  },
  {
    "objectID": "contents/remainders/02-probabilities.html#modeling-randomness",
    "href": "contents/remainders/02-probabilities.html#modeling-randomness",
    "title": "Probabilities and Statistics",
    "section": "Modeling randomness",
    "text": "Modeling randomness\nMany real-world phenomena are unpredictable, and their outcomes generally contain a certain amount of variability. This variability is taken into account using a measure of uncertainty called probability measure.\n\n\n\n\n\n\nWarningDefinition\n\n\n\nThe sample space \\(S\\) is the set of all possible outcomes of a phenomenon. An event is a subset of the sample space \\(S\\).\n\n\n\n\n\n\n\n\nNoteExamples\n\n\n\n\nIf the experiment consists of tossing a coin, \\(S = \\{0, 1\\}\\). The result of this experiment cannot be known in advance. For example, \\(E = \\{1\\}\\) is an event of \\(S\\).\nIf we are interested in the lifespan of a phone, \\(S = \\mathbb{R}_{+}\\). We can also choose \\(S = [0, M]\\), because this lifespan is probably not infinite! The event \\(E = [10, \\infty)\\) represents the event “the lifespan of more than 10 time units.”\nFor the number of days without snow in Quebec City in a year, we can choose \\(S = \\mathbb{N}\\). The event \\(E = (0, 5]\\) represents the event “fewer than 5 days without snow in Quebec City in a year.”\n\n\n\n\n\n\n\n\n\nWarningDefinition\n\n\n\nA probability measure \\(\\mathbb{P}\\) on \\(S\\) is an application (function) defined on the sample space and satisfying the following properties:\n\nFor each event \\(E\\), \\(\\mathbb{P}(E) \\in [0, 1]\\).\n\\(\\mathbb{P}(S) = 1\\).\nLet \\(E_{1}, E_{2}, \\dots\\) be a sequence of mutually exclusive events (finite or infinite), i.e. \\(\\forall i \\neq j, E_{i} \\cap E_{j} = \\varnothing\\). We have \\[\\mathbb{P}(\\bigcup_{n = 1}^{\\infty} E_n) = \\sum_{n = 1}^{\\infty} \\mathbb{P}(E_n).\\]\n\nWe call \\(\\mathbb{P}(E)\\) the probability of event \\(E\\).\n\n\nThe definition of probability measures can be subjective and linked to the statistician’s experience. Let’s take example 3 on the number of days without snow in Quebec City during the year. Someone who has just arrived in Canada may want to give the same probability to each day, while a Quebecer will have more information and will be able to vary the probabilities based on this knowledge.\n\n\n\n\n\n\nWarningDefinition\n\n\n\nTwo events \\(E\\) and \\(F\\) are said to be independent if \\(\\mathbb{P}(E \\cap F) = \\mathbb{P}(E) \\times \\mathbb{P}(F)\\).\n\n\n\n\n\n\n\n\nWarningDefinition\n\n\n\nLet \\(E\\) and \\(F\\) be two events. The conditional probability that \\(E\\) occurs given that \\(F\\) has occurred is defined by: \\[\\mathbb{P}(E \\mid F) = \\frac{\\mathbb{P}(E \\cap F)} {\\mathbb{P}(F)}.\\]\n\n\nIntuitively, two events are independent if knowledge of one provides no information about the occurrence of the other. We also have \\(\\mathbb{P}(E \\mid F) = \\mathbb{P}(E)\\).",
    "crumbs": [
      "Modules",
      "02 - Révisions",
      "Probabilities and Statistics"
    ]
  },
  {
    "objectID": "contents/remainders/02-probabilities.html#random-variables",
    "href": "contents/remainders/02-probabilities.html#random-variables",
    "title": "Probabilities and Statistics",
    "section": "Random variables",
    "text": "Random variables\nIn probability theory, it is customary to express the outcome of experiments as the value of a function called a random variable. This characterization is always possible.\n\n\n\n\n\n\nWarningDefinition\n\n\n\nLet \\(X\\) be a random variable. The distribution of this random variable is defined by the application \\(A \\mapsto \\mathbb{P}(X \\in A)\\).\n\n\n\n\n\n\n\n\nWarningDefinition\n\n\n\nLet \\(X\\) be a random variable. This random variable is discrete if it takes, at most, a countable number of values. In this case, the distribution of \\(X\\) is given by the probabilities \\(\\mathbb{P}(X = x)\\) for all outcomes \\(x\\).\n\n\n\n\n\n\n\n\nWarningDefinition\n\n\n\nLet \\(X\\) be a random variable. This random variable is continuous if the probabilities \\(\\mathbb{P}(X \\in A)\\) are given by integrals of the form \\(\\int_{A} f(x) dx\\) where \\(f: \\mathbb{R}^d \\to \\mathbb{R}_+\\) is an integrable function such that \\(\\int_{\\mathbb{R}^d} f(x) dx = 1\\). Note that, for a fixed outcome \\(x\\), \\(\\mathbb{P}(X = x) = 0\\).\n\n\n\n\n\n\n\n\nWarningDefinition\n\n\n\nLet \\(X\\) be a random variable. The mathematical expectation \\(\\mathbb{E}(X)\\) of \\(X\\) is the average value of the outcome of \\(X\\) with respect to its probability distribution. The expectation is usually denoted by \\(\\mu\\).\n\n\nLet \\(F\\) be a countable set. A discrete random variable \\(X\\) has an expectation \\(\\mathbb{E}(X) = \\sum_{x \\in F} x \\mathbb{P}(X = x)\\). Let \\(X\\) be a continuous random variable with density \\(f\\). Its expectation is given by \\(\\mathbb{E}(X) = \\int_{\\mathbb{R}^d} x f(x) dx\\).\n\n\n\n\n\n\nCautionTransfer theorem\n\n\n\nLet \\(X\\) be a random variable. Let \\(g: \\mathbb{R}^d \\mapsto \\mathbb{R}\\) be a function such that \\(\\mathbb{E}\\left[ g(X) \\right]\\) exists. We have:\n\nIf \\(X\\) is a discrete random variable, \\(\\mathbb{E}\\left[ g(X) \\right] = \\sum_{x \\in F} g(x) \\mathbb{P}(X = x)\\);\nIf \\(X\\) is a continuous random variable with density \\(f\\), \\(\\mathbb{E}\\left[ g(X) \\right] = \\int_{\\mathbb{R}^d} g(x)f(x) dx\\).\n\n\n\n\n\n\n\n\n\nImportantProperties: Linearity of expectation\n\n\n\nLet \\(X\\) and \\(Y\\) be two random variables whose expectations exist, and let \\(\\lambda \\in R\\). We have:\n\n\\(\\mathbb{E}(X + Y) = \\mathbb{E}(X) + \\mathbb{E}(Y)\\);\n\\(\\mathbb{E}(\\lambda X) = \\lambda \\mathbb{E}(X)\\).\n\n\n\n\n\n\n\n\n\nNoteProof\n\n\n\n\n\nThe proof is derived from the transfer theorem and the linearity of addition and integration.\n\n\n\n\n\n\n\n\n\nWarningDefinition\n\n\n\nLet \\(X\\) be a random variable such that the expectation of its square exists. The variance of \\(X\\) is defined by \\[\\mathrm{Var}(X) = \\mathbb{E}\\left[ \\left( X - \\mathbb{E}(X) \\right)^2 \\right] = \\mathbb{E}\\left[ X^2 \\right] - \\mathbb{E}\\left[ X \\right]^2.\\]\n\n\nThe variance measures the dispersion of a random variable around its mean. We can also look at the standard deviation, defined as the square root of the variance: \\(\\sigma(X) = \\sqrt{\\mathrm{Var}(X)}\\).\n\n\n\n\n\n\nWarningDefinition\n\n\n\nLet \\(X\\) and \\(Y\\) be two random variables and \\(A\\) and \\(B\\) be two events. If the events \\(\\left\\{ X \\in A \\right\\}\\) and \\(\\left\\{ Y \\in B \\right\\}\\) are independent, then we say that the random variables \\(X\\) and \\(Y\\) are independent.\n\n\nFrom this definition, we can deduce that:\n\nfor functions \\(f\\) and \\(g\\), the random variables \\(f(X)\\) and \\(g(Y)\\) are independent;\nif the random variables \\(X\\) and \\(Y\\) are real-valued and their expectation exists, then the expectation of the product \\(XY\\) exists and \\(\\mathbb{E}(XY) = \\mathbb{E}(X) \\times \\mathbb{E}(Y)\\).\n\n\n\n\n\n\n\nWarningDefinition\n\n\n\nLet \\(X\\) be a random variable. The distribution function \\(F: \\mathbb{R} \\mapsto [0, 1]\\) of \\(X\\) is defined by \\[F(t) = \\mathbb{P}(X \\leq t), \\quad t \\in \\mathbb{R}.\\]",
    "crumbs": [
      "Modules",
      "02 - Révisions",
      "Probabilities and Statistics"
    ]
  },
  {
    "objectID": "contents/remainders/02-probabilities.html#random-vectors",
    "href": "contents/remainders/02-probabilities.html#random-vectors",
    "title": "Probabilities and Statistics",
    "section": "Random vectors",
    "text": "Random vectors\nSuppose that \\(X = (X_{1}, X_{2})\\) is a random variable of dimension \\(2\\) with density \\(f_{X}\\). Random variables of dimension greater than \\(1\\) are generally referred to as random vectors. The densities of \\(X_{1}\\) and \\(X_{2}\\) are called marginal densities. When \\(X_{1}\\) and \\(X_{2}\\) are independent, we have: \\[f_X(x, y) = f_{X_{1}}(x) \\cdot f_{X_{2}}(y), \\quad (x, y) \\in \\mathbb{R}^2.\\]\n\n\n\n\n\n\nNoteExample of the multivariate normal distribution\n\n\n\nA random vector \\(X\\) of dimension \\(p\\) is said to follow a multivariate normal distribution with mean \\(\\mu\\) and variance \\(\\Sigma\\) if its density is given by \\[f_X(x) = \\frac{1}{(2 \\pi)^{p /2}} \\cdot \\frac{1}{(\\text{det} \\Sigma)^{1/2}} \\cdot \\exp\\left\\{ -\\frac{1}{2}\\left( x - \\mu \\right)^\\top \\Sigma^{-1} \\left( x - \\mu \\right) \\right\\}, \\quad x \\in \\mathbb{R}^p.\\]\nWe denote \\(X \\sim \\mathcal{N}_{p}(\\mu, \\Sigma)\\).\n\n\nIn statistics, an important quantity to measure is the linear dependence between \\(X_{1}\\) and \\(X_{2}\\). For this, we can use the covariance or the correlation.\n\n\n\n\n\n\nWarningDefinition\n\n\n\nLet \\(X = (X_{1}, X_{2})\\) be a random vector such that the expectation of the square of \\(X_{1}\\) and \\(X_{2}\\) exists. The covariance between \\(X_{1}\\) and \\(X_{2}\\) is given by \\[\\mathrm{Cov}(X_{1}, X_{2}) = \\mathbb{E}\\left[ (X_{1} - \\mathbb{E}(X_{1})) (X_{2} - \\mathbb{E}(X_{2}))\\right].\\]\nThe correlation between \\(X_{1}\\) and \\(X_{2}\\) is a version of the covariance normalized by the standard deviation of the random variables. It is given by \\[\\mathrm{Corr}(X_{1}, X_{2}) = \\frac{\\mathrm{Cov}(X_{1}, X_{2})}{\\sigma(X_{1}) \\sigma(X_{2})}.\\]\n\n\nThe sign of the covariance and correlation can be interpreted. If they are strictly positive, \\(X_{1}\\) and \\(X_{2}\\) tend to move in the same direction. If \\(X_{1}\\) increases, then \\(X_{2}\\) also increases, and vice versa. If they are strictly negative, \\(X_{1}\\) and \\(X_{2}\\) tend to move in opposite directions. If \\(X_{1}\\) increases, then \\(X_{2}\\) decreases, and vice versa. If the covariance is equal to \\(0\\), there are no rules and \\(X_{1}\\) and \\(X_{2}\\) are said to be orthogonal.\n\n\n\n\n\n\nImportantProperties\n\n\n\nLet \\(X = (X_{1}, X_{2})\\) be a random vector. We have\n\n\\(\\mathrm{Cov}(X_{1}, X_{2}) = \\mathbb{E}(X_{1}X_{2}) - \\mathbb{E}(X_{1})\\mathbb{E}(X_{2})\\);\n\\(\\mathrm{Cov}(X_{1}, X_{2}) = \\mathrm{Cov}(X_{2}, X_{1})\\);\n\\(\\mathrm{Cov}(X_{1} + \\lambda Y_{1}, X_{2}) = \\mathrm{Cov}(X_{1}, X_{2}) + \\lambda \\mathrm{Cov}(Y_{1}, X_{2})\\).\n\n\n\n\n\n\n\n\n\nNoteProof\n\n\n\n\nWe find the result by expanding the product in the definition of the covariance.\nUsing point 1. and the commutativity of multiplication.\nUsing point 1. and the linearity of expectation.",
    "crumbs": [
      "Modules",
      "02 - Révisions",
      "Probabilities and Statistics"
    ]
  },
  {
    "objectID": "contents/remainders/02-probabilities.html#estimation",
    "href": "contents/remainders/02-probabilities.html#estimation",
    "title": "Probabilities and Statistics",
    "section": "Estimation",
    "text": "Estimation\nIn practice, we do not have perfect knowledge of our random vectors, but only some of their realizations (called samples). Let \\(x_{1}, \\dots, x_{n}\\) be \\(n\\) independent realizations of a random vector \\(X\\) with mean \\(\\mu\\) and variance \\(\\Sigma\\).\nThe estimator of the mean \\(\\mu\\) is given by \\[\\widehat{\\mu} = \\overline{X} \\coloneqq \\frac{1}{n} \\sum_{i = 1}^{n} x_i.\\]\nThe estimator of the variance \\(\\Sigma\\) is given by \\[\\widehat{\\Sigma} \\coloneqq \\frac{1}{n - 1}\\sum_{i = 1}^{n} (x_i - \\widehat{\\mu})(x_i - \\widehat{\\mu})^\\top.\\]\nWhy do we divide this sum by \\(n -1\\) and not by \\(n\\) to estimate the variance? If we divide by \\(n\\), \\(\\widehat{\\Sigma}\\) is a biased estimator of the variance. Indeed, we must take into account that we are using a biased estimator of the mean in the variance estimator and therefore correct for this estimate.\nLet \\(D = \\{\\text{diag}(\\widehat{\\Sigma})\\}^{1/2}\\) be the matrix of standard deviations calculated on the sample. We can estimate the correlation matrix on the sample by \\[\\widehat{R} = D^{-1} \\widehat{\\Sigma} D^{-1}.\\]",
    "crumbs": [
      "Modules",
      "02 - Révisions",
      "Probabilities and Statistics"
    ]
  },
  {
    "objectID": "informations/materials.html",
    "href": "informations/materials.html",
    "title": "Materials",
    "section": "",
    "text": "This page brings together various useful resources for the course.\nThe Elements of Statistical Learning: Data Mining, Inference and Prediction, 2nd Edition\nAuthors: Trevor Hastie, Robert Tibshirani and Jerome Friedman\nPublisher: Springer (New York, 2009)\nISBN: 0-387-84857-0\nLink: authors’ website\nAn Introduction to Statistical Learning: with application in R, 2nd Edition\nAuthors: Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani\nPublisher: Springer (New York, 2021)\nISBN: 1-0716-1417-4\nLink: authors’ website\nAn Introduction to Statistical Learning: with application in Python\nAuthors: Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani and Jonathan Taylor\nPublisher: Springer (Switzerland, 2023)\nISBN: 3-031-38746-3\nLink: authors’ website",
    "crumbs": [
      "Informations",
      "Matériel"
    ]
  },
  {
    "objectID": "informations/description.html",
    "href": "informations/description.html",
    "title": "Description",
    "section": "",
    "text": "In this course, we aim to introduce methods that will enable students to study a “high-dimensional” dataset (here, “high-dimensional” is taken in the sense that we can’t simply graph all the variables for all the observations) without resorting to a probabilistic model. The techniques we teach are used to reduce the dimension of the data, identify certain correlation between variables, visualize the data or divide the dataset into groups/classes.\nWithout neglecting the theory, the emphasis will be on the practical aspect of data analysis and the use of a programming language, whether R, Python or other.",
    "crumbs": [
      "Informations",
      "Description"
    ]
  },
  {
    "objectID": "informations/description.html#objectives",
    "href": "informations/description.html#objectives",
    "title": "Description",
    "section": "",
    "text": "In this course, we aim to introduce methods that will enable students to study a “high-dimensional” dataset (here, “high-dimensional” is taken in the sense that we can’t simply graph all the variables for all the observations) without resorting to a probabilistic model. The techniques we teach are used to reduce the dimension of the data, identify certain correlation between variables, visualize the data or divide the dataset into groups/classes.\nWithout neglecting the theory, the emphasis will be on the practical aspect of data analysis and the use of a programming language, whether R, Python or other.",
    "crumbs": [
      "Informations",
      "Description"
    ]
  },
  {
    "objectID": "informations/description.html#place-of-the-course-in-the-program",
    "href": "informations/description.html#place-of-the-course-in-the-program",
    "title": "Description",
    "section": "Place of the course in the program",
    "text": "Place of the course in the program\nThis course is generally taken by students in their second year of a bachelor’s degree in statistics. It is also an elective in actuarial science, mathematics and some engineering and business programs.\nAll students should ensure that they have taken at least one linear algebra course (e.g. MAT-1200) and one basic statistics course (e.g. STT-1000), as most data analysis methods are based on these concepts.\nStudents should also have some familiarity with basic algorithmic concepts, as well as with (at least) one programming language.",
    "crumbs": [
      "Informations",
      "Description"
    ]
  },
  {
    "objectID": "informations/description.html#specific-objectives",
    "href": "informations/description.html#specific-objectives",
    "title": "Description",
    "section": "Specific objectives",
    "text": "Specific objectives\nBy the end of the course, students should be able to:\n\nunderstand and describe the theoretical foundations of the data analysis methods studied;\ncorrectly identify situations where the use of these methods is indicated;\nuse a programming language effectively to implement these methods;\nanalyze and interpret the results of analysis;\nformulate the conclusions of the analysis in writing, within the limits of the methodology.",
    "crumbs": [
      "Informations",
      "Description"
    ]
  },
  {
    "objectID": "informations/description.html#computer-equipment",
    "href": "informations/description.html#computer-equipment",
    "title": "Description",
    "section": "Computer equipment",
    "text": "Computer equipment\nYou may need a computer, speakers or headphones, a microphone, a webcam and a wired broadband or wireless Internet connection. To check the minimum configuration parameters for your operating system, please visit this page.\nIn addition, this course may require specific software requirements, which will be described in other sections of the course outline as appropriate.",
    "crumbs": [
      "Informations",
      "Description"
    ]
  },
  {
    "objectID": "informations/description.html#how-it-works",
    "href": "informations/description.html#how-it-works",
    "title": "Description",
    "section": "How it works",
    "text": "How it works\nClasses and exams will take place face-to-face on Tuesday and Friday mornings, but materials will be available online. In general, the Friday session will be a lecture session, and the Tuesday session will be dedicated to practical exercises carried out independently by students with the support of the teacher.",
    "crumbs": [
      "Informations",
      "Description"
    ]
  },
  {
    "objectID": "informations/description.html#pedagogical-approaches",
    "href": "informations/description.html#pedagogical-approaches",
    "title": "Description",
    "section": "Pedagogical approaches",
    "text": "Pedagogical approaches\nThe preferred teaching approach is interactive lectures (Fridays) alternating with laboratory periods (Tuesdays). The pedagogical approach is very much geared to active learning and requires sustained commitment from students throughout the session.",
    "crumbs": [
      "Informations",
      "Description"
    ]
  },
  {
    "objectID": "informations/general.html",
    "href": "informations/general.html",
    "title": "General informations",
    "section": "",
    "text": "Faculté des sciences et de génie\nDépartement de mathématiques et de statistique",
    "crumbs": [
      "Informations",
      "Générales"
    ]
  },
  {
    "objectID": "informations/general.html#stt-2200-data-analysis",
    "href": "informations/general.html#stt-2200-data-analysis",
    "title": "General informations",
    "section": "STT-2200: Data Analysis",
    "text": "STT-2200: Data Analysis\nTeaching format: Face-to-face\nTime spent:\n\nLecture: 2h\nLaboratory: 1h\nPersonal work: 6h\nTotal: 9h\n\nCredits: 3\nPrerequisites:\n\nACT-2000 or STT-1000 or STT-1300 or STT-1900\nMAT-1200 or ACT-2002\nSTT-1100 or IFT-4902 or ECN-2090 or GLO-1901 or IFT-1004\n\nTime range:\n\n\n\nType\nDay\nTime\nDates\n\n\n\n\nLab\nTuesday\n10h30-12h20\nPLT-3928\n\n\nClass\nFriday\n08h30-10h20\nVCH-3820",
    "crumbs": [
      "Informations",
      "Générales"
    ]
  },
  {
    "objectID": "informations/general.html#contact-details-and-availability",
    "href": "informations/general.html#contact-details-and-availability",
    "title": "General informations",
    "section": "Contact details and availability",
    "text": "Contact details and availability\nTeacher: Steven Golovkine\nLocal: VCH-2209\nEmail: steven.golovkine@mat.ulaval.ca\nAvailability: I am available to meet with you by appointment by email or without appointment from Monday to Thursday between 8am and 10am.\n\nThe University recognizes the right to disconnect of professors, lecturers and other teaching staff. This means that they are not required to check messages sent to them (email, voicemail, forum messages, etc.) during evenings, weekends and vacations. The person who has sent a message during these periods should therefore expect to receive a reply within a reasonable time, calculated from the resumption of normal working hours.",
    "crumbs": [
      "Informations",
      "Générales"
    ]
  },
  {
    "objectID": "informations/evaluations.html",
    "href": "informations/evaluations.html",
    "title": "Grading",
    "section": "",
    "text": "Here are the examinations for this module.\n\n\n\nTitle\nDate\nMode of work\nWeighting\n\n\n\n\nExam 1\n24 oct. 2025\nIndividual\n25%\n\n\nExam 2\n12 déc. 2025\nIndividual\n50%\n\n\nProject\n02 déc. 2025\nTeam\n25%\n\n\n\n\nIdentification. During an exam, an eligible photo ID card must be placed on the corner of your table. Eligible cards are a plastic Université Laval card, a Canadian driver’s license, a photo health insurance card issued by a Canadian province, or a Canadian or foreign passport.",
    "crumbs": [
      "Informations",
      "Évaluations"
    ]
  },
  {
    "objectID": "informations/evaluations.html#assessment-procedures",
    "href": "informations/evaluations.html#assessment-procedures",
    "title": "Grading",
    "section": "",
    "text": "Here are the examinations for this module.\n\n\n\nTitle\nDate\nMode of work\nWeighting\n\n\n\n\nExam 1\n24 oct. 2025\nIndividual\n25%\n\n\nExam 2\n12 déc. 2025\nIndividual\n50%\n\n\nProject\n02 déc. 2025\nTeam\n25%\n\n\n\n\nIdentification. During an exam, an eligible photo ID card must be placed on the corner of your table. Eligible cards are a plastic Université Laval card, a Canadian driver’s license, a photo health insurance card issued by a Canadian province, or a Canadian or foreign passport.",
    "crumbs": [
      "Informations",
      "Évaluations"
    ]
  },
  {
    "objectID": "informations/evaluations.html#detailed-assessment-information",
    "href": "informations/evaluations.html#detailed-assessment-information",
    "title": "Grading",
    "section": "Detailed assessment information",
    "text": "Detailed assessment information\nExam 1\nDate and location: 24 october 2025, 08h30 - 09h30\nMode of work: Individual\nWeighting: 30%\nSubmission of assessment: Handoff to the teacher\nAssessment guidelines: All course content (slides, classroom presentations, readings, etc.) may be included in the exam.\nMaterials allowed: Any materials not connected to the internet are permitted. Calculators are permitted. The use of AI tools is NOT permitted for this exam.\nAdditional information: link.\nExam 2\nDate and place: 12 december 2025, 08h30 - 10h20\nMode of work: Individual\nWeighting: 50%\nSubmission of assessment: Handoff to the teacher\nAssessment guidelines: All course content (slides, classroom presentations, readings, etc.) may be included in the exam.\nMaterials allowed: Any materials not connected to the internet are permitted. Calculators are permitted. The use of AI tools is NOT permitted for this exam.\nProject\nDate and place: due 02 december 2025 - 17h00\nMode of work: Team work\nWeighting: 20%\nSubmission of assessment: Via monPortail\nAssessment guidelines: Any delay in submitting the project will result in a score of 0 for this part. There will be a poster presentation on December 9 between 10:30 a.m. and 12:30 p.m. The exact details of the presentations are still to be determined. If the posters cannot be displayed in the VCH pavilion, the presentations will take place in classroom PLT-3928. The data analysis project consists of creating a poster presenting a data analysis. The choice of subject and issue is left to your discretion. It is recommended that you use the methods presented in class, but this is not mandatory. In any case, expect to be asked questions about the methods you present. I am, of course, available to answer any questions you may have (where to find data, how to make a poster, etc.). Please submit a .zip file containing your project. This file must contain your poster in .pdf format, your code, and your data. If your data file is too large, please send me an email. I will take care of printing the posters.\nMaterials allowed: The use of AI tools is permitted for this exam. However, their use must be properly documented.",
    "crumbs": [
      "Informations",
      "Évaluations"
    ]
  },
  {
    "objectID": "informations/evaluations.html#rating-scale",
    "href": "informations/evaluations.html#rating-scale",
    "title": "Grading",
    "section": "Rating scale",
    "text": "Rating scale\n\n\n\nRating\n% minimum\n% maximum\n\n\n\n\nA+\n92\n100\n\n\nA\n88\n91.99\n\n\nA-\n84\n87.99\n\n\nB+\n80\n83.99\n\n\nB\n75\n79.99\n\n\nB-\n70\n74.99\n\n\nC+\n65\n69.99\n\n\nC\n60\n64.99\n\n\nC-\n55\n59.99\n\n\nD+\n52\n54.99\n\n\nD\n50\n51.99\n\n\nE\n0\n49.99",
    "crumbs": [
      "Informations",
      "Évaluations"
    ]
  },
  {
    "objectID": "informations/evaluations.html#assessment-details",
    "href": "informations/evaluations.html#assessment-details",
    "title": "Grading",
    "section": "Assessment details",
    "text": "Assessment details\nIn accordance with the Department of Mathematics and Statistics’ policy of improving and consolidating the knowledge of French, the quality of writing will be assessed in all assignments and examinations. A maximum of 10% of points may be deducted for the quality of language and writing.\nNo late submissions will be accepted. A delay will automatically result in a score of zero.\nAny resits granted under the Department’s resit policy will take place on the faculty’s official resit dates. To request a grade revision, you must follow the procedure set out in Chapter 4 of the Academic Regulations. You will find a form to complete at the following address: link.\nThe FSG retake exam sessions for the fall 2025 session will take place:\n\nThursday, October 30, 2025, location to be determined, from 9 a.m. to 12 p.m. and/or from 1:30 p.m. to 4:30 p.m.\nThursday, December 18, 2025, location to be determined, from 9 a.m. to 12 p.m. and/or from 1:30 p.m. to 4:30 p.m.",
    "crumbs": [
      "Informations",
      "Évaluations"
    ]
  },
  {
    "objectID": "informations/evaluations.html#absence-from-a-mandatory-activity",
    "href": "informations/evaluations.html#absence-from-a-mandatory-activity",
    "title": "Grading",
    "section": "Absence from a mandatory activity",
    "text": "Absence from a mandatory activity\nThese conditions apply in accordance with articles 4.41 and 4.42 of the Réglement des études de l’Université Laval:\n\nArticle 4.41: Any failure to submit to an evaluation activity will result in a grade of zero for that evaluation activity, unless the student demonstrates that the omission is attributable to serious reasons.\nArticle 4.42: An evaluation may be repeated for serious reasons. This is done in accordance with the procedures laid down by the unit responsible for the training activity.\n\nThe retaking of an assessment may therefore exceptionally be authorized for reasons deemed serious, as long as the procedure described below is followed.\nReasons for absence deemed to be serious\nThe following reasons are considered serious and therefore acceptable for requesting a retake of an assessment:\n\nillness or accident preventing travel;\nhospitalization;\nserious illness or death of a close relative;\nparticipation in a high-level sporting activity;\ncourt summons.\n\nProcedure to follow\nAs soon as possible and no later than five (5) working days after the evaluation date (or in certain cases, before the evaluation date, as soon as the reason is known), the student who wishes to apply for an evaluation re-sit must complete and submit the electronic form “Application to re-sit an evaluation”, taking care to attach the required supporting documents.\nFor more details on the procedures to be followed and the serious grounds for retaking an evaluation, please consult the document “Modalités et procédure de reprise d’une évaluation sommative à la Faculté des sciences et de génie” available on the FSG website.\nIn certain cases, the course leader may adopt a simplified procedure for managing requests to repeat an evaluation, while respecting the criteria described in this policy. In such cases, specific explanations will be provided in the course syllabus and presented at the first session.",
    "crumbs": [
      "Informations",
      "Évaluations"
    ]
  },
  {
    "objectID": "informations/evaluations.html#computer-lab-security-policy",
    "href": "informations/evaluations.html#computer-lab-security-policy",
    "title": "Grading",
    "section": "Computer Lab Security Policy",
    "text": "Computer Lab Security Policy\nTo use the computer labs, you must have read the Security Policy and agree to comply with it: link.\nFor technical assistance, please email [aide@fsg.ulaval.ca]. In case of emergency, call 418-656-5555, ext. 555, or use a red telephone located nearby.",
    "crumbs": [
      "Informations",
      "Évaluations"
    ]
  },
  {
    "objectID": "informations/evaluations.html#policy-on-the-use-of-electronic-devices",
    "href": "informations/evaluations.html#policy-on-the-use-of-electronic-devices",
    "title": "Grading",
    "section": "Policy on the use of electronic devices",
    "text": "Policy on the use of electronic devices\nThe Faculty of Science and Engineering’s policy on the use of electronic devices can be consulted at: link.",
    "crumbs": [
      "Informations",
      "Évaluations"
    ]
  },
  {
    "objectID": "informations/evaluations.html#plagiarism-and-academic-fraud-policy",
    "href": "informations/evaluations.html#plagiarism-and-academic-fraud-policy",
    "title": "Grading",
    "section": "Plagiarism and academic fraud policy",
    "text": "Plagiarism and academic fraud policy\nDisciplinary rules\nAny student who commits an infraction of the Disciplinary Regulations for Laval University students in the context of this course, particularly in the area of plagiarism, is subject to the sanctions provided for in these regulations. It is very important for all students to read articles 23 to 46 of the Disciplinary Regulations. These can be consulted at the following address: link.\nPlagiarism\nAll students are required to comply with the rules governing plagiarism. In particular, plagiarism is defined as:\n\ncopying verbatim one or more passages from a work in paper or electronic format without placing these passages in quotation marks and without mentioning the source;\nsummarizing an author’s original idea by expressing it in one’s own words (paraphrasing) without mentioning the source;\ntranslate a text in whole or in part without mentioning its source;\nsubmit work copied from another student (with or without that other student’s consent);\nsubmit work downloaded from a site for the purchase or exchange of school work.\n\nAs Université Laval subscribes to a plagiarism detection service, your teacher may submit your work for analysis.",
    "crumbs": [
      "Informations",
      "Évaluations"
    ]
  },
  {
    "objectID": "informations/evaluations.html#students-with-disabilities-related-to-functional-limitations",
    "href": "informations/evaluations.html#students-with-disabilities-related-to-functional-limitations",
    "title": "Grading",
    "section": "Students with disabilities related to functional limitations",
    "text": "Students with disabilities related to functional limitations\nIn order to benefit from accommodation measures for courses or exams, an appointment with a counsellor from the Centre d’aide aux étudiants travaillant en Accueil et soutien aux étudiants en situation de handicap (ACSESH) is necessary. Students with a disability related to a permanent functional limitation should visit “Accommodement” and book an appointment as soon as possible.\nDuring the week following authorization of the measures, they must be activated in “Accommodement” to ensure that they are in place.\nStudents who have already obtained academic accommodation measures must activate their measures for courses and/or exams in “Accommodement” so that they can be implemented. Please note that activation must take place within the first two weeks of classes.\nStudents concerned will then receive detailed information from the faculties and departments responsible for their courses on how to apply the accommodation measures identified.",
    "crumbs": [
      "Informations",
      "Évaluations"
    ]
  },
  {
    "objectID": "informations/mcq.html",
    "href": "informations/mcq.html",
    "title": "MCQ",
    "section": "",
    "text": "The first exam is a multiple-choice question with quantification of uncertainty.\nThe use of degrees of certainty enables the student to assess the level of knowledge of the answer given. The scoring system, based on decision theory (Leclercq et al. 1993), may seem odd but it has been designed in such a way that:\n\ntelling the truth is the strategy that earns the most points;\nthose who self-assess well earn more points than if we applied a corrective scale taking into account the probabilities of having the correct answer randomly.\n\n\n\n\nIf you find that your answer has a probability of being correct between…\n\n\nChoose the degree of certainty…\n\n\nYou’ll get the following points for … answer.\n\n\n\n\n\n\n\n\ncorrect\n\n\nincorrect\n\n\n\n\n0% and 25%\n\n\n0\n\n\n+13\n\n\n+4\n\n\n\n\n25% and 50%\n\n\n1\n\n\n+16\n\n\n+3\n\n\n\n\n50% and 70%\n\n\n2\n\n\n+17\n\n\n+2\n\n\n\n\n70% and 85%\n\n\n3\n\n\n+18\n\n\n0\n\n\n\n\n85% and 95%\n\n\n4\n\n\n+19\n\n\n-6\n\n\n\n\n95% and 100%\n\n\n5\n\n\n+20\n\n\n-10\n\n\n\nIn general, students self-assess realistically, and benefit from degrees of certainty. Their score is better than if it had been calculated solely on the basis of the number of correct answers.",
    "crumbs": [
      "Informations",
      "Évaluations",
      "MCQ"
    ]
  },
  {
    "objectID": "informations/mcq.html#information-on-the-first-exam",
    "href": "informations/mcq.html#information-on-the-first-exam",
    "title": "MCQ",
    "section": "",
    "text": "The first exam is a multiple-choice question with quantification of uncertainty.\nThe use of degrees of certainty enables the student to assess the level of knowledge of the answer given. The scoring system, based on decision theory (Leclercq et al. 1993), may seem odd but it has been designed in such a way that:\n\ntelling the truth is the strategy that earns the most points;\nthose who self-assess well earn more points than if we applied a corrective scale taking into account the probabilities of having the correct answer randomly.\n\n\n\n\nIf you find that your answer has a probability of being correct between…\n\n\nChoose the degree of certainty…\n\n\nYou’ll get the following points for … answer.\n\n\n\n\n\n\n\n\ncorrect\n\n\nincorrect\n\n\n\n\n0% and 25%\n\n\n0\n\n\n+13\n\n\n+4\n\n\n\n\n25% and 50%\n\n\n1\n\n\n+16\n\n\n+3\n\n\n\n\n50% and 70%\n\n\n2\n\n\n+17\n\n\n+2\n\n\n\n\n70% and 85%\n\n\n3\n\n\n+18\n\n\n0\n\n\n\n\n85% and 95%\n\n\n4\n\n\n+19\n\n\n-6\n\n\n\n\n95% and 100%\n\n\n5\n\n\n+20\n\n\n-10\n\n\n\nIn general, students self-assess realistically, and benefit from degrees of certainty. Their score is better than if it had been calculated solely on the basis of the number of correct answers.",
    "crumbs": [
      "Informations",
      "Évaluations",
      "MCQ"
    ]
  },
  {
    "objectID": "informations/cours.html",
    "href": "informations/cours.html",
    "title": "Classes",
    "section": "",
    "text": "Here are the recording of the classes for the semester Autumn 2025:\n\nClass of 02 september 2025: link.",
    "crumbs": [
      "Informations",
      "Cours"
    ]
  },
  {
    "objectID": "informations/schedule.html",
    "href": "informations/schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "The plan is given for information only.\n\n\n\nWeek\nPlan\nEvaluations\n\n\n\n\n1er september\nCourse presentation and remainders\n\n\n\n8 september\nRemainders\n\n\n\n15 september\nGeneralities\n\n\n\n22 september\nGeneralities\n\n\n\n29 september\nPCA\n\n\n\n6 october\nCA\n\n\n\n13 october\nMCA\n\n\n\n20 october\nDiscriminant analysis\nIntra exam\n\n\n27 october\nLecture week\nRetake exam\n\n\n3 november\nTrees\n\n\n\n10 november\nEnsemble models\n\n\n\n17 november\n\\(k\\)-means\n\n\n\n24 november\nHierarchical models + Mixture models\n\n\n\n1er décember\nRevision + University holidays\nPoster handoff\n\n\n8 décember\n\nPoster presentation + final exam\n\n\n15 décember\n\nRetake exam",
    "crumbs": [
      "Informations",
      "Plan"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STT-2200: Data Analysis",
    "section": "",
    "text": "Welcome everyone to the class STT-2200 in Autumn 2025.\nYou will find on this website every necessary information for the modules, the practicals and the exams. The contents will be updated during the session with the answers to some work. I invite you to skim through the differents modules before classes.\nThis website has been created with the help of Aurélien Nicosia and Thierry Duchesne notes.\nThis website a been built using Quarto.\nMost of the contents has been translated using DeepL. Large languages models (LLM), ChatGPT et Claude, have been used to improve notes reading, check the typos and improve the formatting of the figures. All remaining errors are strictly and directly attributable to me.\n\n\n\nData trap (xkcd:2582)."
  },
  {
    "objectID": "contents/remainders/03-programming.html",
    "href": "contents/remainders/03-programming.html",
    "title": "Programming",
    "section": "",
    "text": "In this section, we present some references on the basics of programming in different languages. This course does not specify which language to use, so you are free to choose the one that suits you best for the various exercises and your project. In any case, there are a set of best practices that will help you write code that is readable, understandable, and reusable. You can find a (quick) guide to best practices in programming at this link.",
    "crumbs": [
      "Modules",
      "02 - Révisions",
      "Programming"
    ]
  },
  {
    "objectID": "contents/remainders/03-programming.html#r",
    "href": "contents/remainders/03-programming.html#r",
    "title": "Programming",
    "section": "R",
    "text": "R\nR is a programming language specialized in statistical analysis and data visualization. It is free, open-source, and available on Windows, macOS, and Linux. Regularly updated, new packages are available every day (see CRAN).\nHere is a guide that introduces the basics of R: Learn R in Y Minutes and a guide to good practices in R. Finally, an important feature of the R language is the pipe operator. This makes the code clearer. You can find an explanation of this operator here.",
    "crumbs": [
      "Modules",
      "02 - Révisions",
      "Programming"
    ]
  },
  {
    "objectID": "contents/remainders/03-programming.html#python",
    "href": "contents/remainders/03-programming.html#python",
    "title": "Programming",
    "section": "Python",
    "text": "Python\nPython is a general-purpose programming language. It is free, open-source, and available on Windows, macOS, and Linux. Although it is a general-purpose language, there is a large community around data analysis and machine learning in Python. Thus, the usual methods have already been implemented, e.g. in the sklearn package, but if necessary, it is possible to view the code and modify it.\nHere is a guide that introduces the basics of Python: Learn Python in Y Minutes and a guide to good practices in Python.",
    "crumbs": [
      "Modules",
      "02 - Révisions",
      "Programming"
    ]
  },
  {
    "objectID": "contents/remainders/03-programming.html#julia",
    "href": "contents/remainders/03-programming.html#julia",
    "title": "Programming",
    "section": "Julia",
    "text": "Julia\nJulia is also a general-purpose programming language. It is a much newer language than its two counterparts, R and Python (Julia was created in the early 2010s, while R and Python were created in the early 1990s). It may therefore seem less mature in some respects, but most traditional data analysis methods have been implemented. Like R and Python, this language is free, open-source, and available on all major operating systems. Julia has the advantage of being faster than R and Python. It also includes native support for calling libraries in C or Fortran, and non-native support for calling libraries in R and Python.\nHere is a guide that introduces the basics of Julia: Learn Julia in Y minutes and a guide to best practices in Julia.",
    "crumbs": [
      "Modules",
      "02 - Révisions",
      "Programming"
    ]
  },
  {
    "objectID": "contents/remainders/03-programming.html#sas",
    "href": "contents/remainders/03-programming.html#sas",
    "title": "Programming",
    "section": "SAS",
    "text": "SAS\nSAS is a proprietary programming language specialized in data analysis. Although used in certain industries, e.g. pharmaceuticals, its use is declining. Furthermore, the license must be renewed annually, there is no online community (apart from the official documentation, there is not much help available), and it is impossible to view the code for the various procedures. For these reasons, I do not recommend using SAS for this course. However, if you wish, the university can provide licenses for a fee.",
    "crumbs": [
      "Modules",
      "02 - Révisions",
      "Programming"
    ]
  },
  {
    "objectID": "contents/misc/pipe.html",
    "href": "contents/misc/pipe.html",
    "title": "Pipe operator in R",
    "section": "",
    "text": "This page is based on a document created by Aurélien Nicosia (ULaval) in 2023 called “Pipe Operator.” It has been updated.\nSince version 4.1.0 of R, released in May 2021, the language has introduced the pipe operator |&gt;, inspired by what the magrittr package was doing. Starting with R 4.3.0, the style guide for the tidyverse recommends using the basic pipe operator, not the one from magrittr.\n\n\n\n\n\n\nNoteKeyboard shortcuts\n\n\n\nIn RStudio, the keyboard shortcut for inserting the pipe operator is:\n\nOn Windows: Ctrl + Shift + M\nOn macOS: ⌘ + ⇧ + M\n\n\n\nThis operator introduces a way to chain instructions and pass arguments to functions in a more readable way than the traditional method.\nTo summarize how this operator works, here is how it transforms a few function calls:\n\nf(x) becomes x |&gt; f();\nf(x, y) becomes x |&gt; f(y);\nh(g(f(x))) becomes x |&gt; f() |&gt; g() |&gt; h().\n\nThis operator allows you to highlight the sequence of actions rather than the object on which the sequence of actions is performed. This makes the code more readable (and having readable code is a good practice). Reading the instruction h(g(f(x))) from left to right, we first see the call to the function h, then the call to the function g, and finally the call to the function f. However, this instruction is evaluated in reverse order. In fact, R will first:\n\nevaluate f(x);\nthen pass the result to the function g and return the result;\nwhich will be passed to the function h and the final result will be returned.\n\nIf we want to write code that correctly reflects the order of evaluation, we could write:\n\nres1 &lt;- f(x)\nres2 &lt;- g(res1)\nh(res2)\n\nHowever, this code has the disadvantage of creating objects that we do not necessarily want to keep. The |&gt; operator does not have this disadvantage! In fact, an instruction written using the |&gt; operator allows us to follow the order of evaluations without creating unnecessary objects in memory.\nFor even greater clarity, it is possible to extend an instruction containing several |&gt; operators over several lines so that there is one function per line:\n\nx |&gt; \n  f() |&gt; \n  g() |&gt; \n  h()\n\nIf the argument we want to pass with the |&gt; operator is not the first one, we must use _ as follows with a named argument: f(y, z = x) becomes x |&gt; f(y, z = _).\nLet’s take an example to illustrate the use of the |&gt; operator. Suppose we have the following character string:\n\ntext &lt;- \"This is an example\"\n\nand we want to correct it—replace “example” with “example” and add a period at the end—with the following instruction:\n\npaste0(gsub(pattern = \"example\", replacement = \"example\", x = text), \".\")\n\n[1] \"This is an example.\"\n\n\nThis instruction is a little difficult to read because of the call to the gsub function nested within a call to the paste0 function. We could rewrite it as follows using the |&gt; operator:\n\ntext |&gt; \n  gsub(pattern = \"example\", replacement = \"example\", x = _) |&gt; \n  paste0(\".\")\n\n[1] \"This is an example.\"\n\n\nLet’s take another numerical example. We want to perform the following calculation: \\[\\frac{(2 + 4) \\times 8}{2}.\\]\nTo do this, we need a few mathematical functions.\n\nadd &lt;- function(x, y) {\n  x + y\n}\n\nmul &lt;- function(x, y) {\n  x * y\n}\n\ndiv &lt;- function(x, y) {\n  x / y\n}\n\nWe can perform the calculation in three different ways:\n\n# By creating different objects \nres1 &lt;- add(2, 4)\nres2 &lt;- mul(res1, 8)\nres3 &lt;- div(res2, 2)\nprint(res3)\n\n[1] 24\n\n# By nesting functions\nres &lt;- div(mul(add(2, 4), 8), 2)\nprint(res)\n\n[1] 24\n\n# With the pipe operator \nres &lt;- 2 |&gt;\n  add(4) |&gt; \n  mul(8) |&gt; \n  div(2)\nprint(res)\n\n[1] 24"
  },
  {
    "objectID": "contents/misc/good-practices.html",
    "href": "contents/misc/good-practices.html",
    "title": "Programming good practices",
    "section": "",
    "text": "This page is based on a document created by Aurélien Nicosia (ULaval) in 2023 called “Bonnes pratiques de programmation en R.” It has been updated and written in a more general context.\nRegardless of the programming language, employing good programming practices means adhering to certain standards in order to create “good” code. This raises the question of what constitutes “good” code. For me, “good” code is code that fulfills three objectives:\nWhy is it desirable to adopt good practices? It allows the code to be understood and used by anyone (and in particular, by yourself in the future). In the long term, good practices increase our productivity by avoiding unnecessary repetition."
  },
  {
    "objectID": "contents/misc/good-practices.html#objective-1-produce-the-expected-results",
    "href": "contents/misc/good-practices.html#objective-1-produce-the-expected-results",
    "title": "Programming good practices",
    "section": "Objective 1: Produce the expected results",
    "text": "Objective 1: Produce the expected results\nThe priority when developing any computer code is undoubtedly to write code that does what it is supposed to do. Therefore, “good” code must produce the right results. To achieve this, the code must first be functional, i.e., it must not contain any bugs. To ensure that the code works correctly, it simply needs to be tested. It is better to test frequently, with each small addition, rather than producing a lot of code before testing it. This way, there is much less debugging to do. One school of thought in computer science even advocates writing tests before writing code (test-driven development). However, a better practice is to formalize the tests so that they can be easily rerun when future changes are made to the code."
  },
  {
    "objectID": "contents/misc/good-practices.html#objective-2-easy-to-maintain-code",
    "href": "contents/misc/good-practices.html#objective-2-easy-to-maintain-code",
    "title": "Programming good practices",
    "section": "Objective 2: Easy-to-maintain code",
    "text": "Objective 2: Easy-to-maintain code\nMaintaining computer code means ensuring that it continues to function properly in the future, despite any changes made to it. Code that is used frequently is code that will need to be updated, either to add features or to fix bugs that were not detected by testing but discovered by users. Taking over code written by someone else, or written by ourselves a few months ago, is not always an easy task. However, if the code is written correctly, it should not be too difficult to understand and modify.\nCode maintenance is based on three principles: versioning, comprehensibility, and reusability.\nThe principle of versioning is to use software that records the various changes made to the code. The best known is Git. It allows you to navigate between different versions of your code, create multiple versions (called branches), and collaborate with others on the same code. It is truly an essential tool to have in your arsenal. Versioning can then be recorded on Github, which manages the underlying machinery. For example, this site uses Git and Github for versioning, and you can see the different versions here.\nComprehensible code is clear and easy to read (almost like text). It often includes instructions that are self-explanatory. These instructions are typically succinct, as overly long instructions often perform several tasks that are difficult to discern. If reading an instruction does not allow a programmer familiar with the computer language used to understand what it does, it is recommended to insert a comment in the code to explain what the instruction is for. In addition to comments explaining certain instructions, all functions should be documented. The documentation for a function should contain: an explanatory text describing what the function does, a description of the arguments accepted as input, a description of the results produced, and an example of use. When programming, it is also good practice to follow a style guide. A style guide is a set of rules that developers have agreed upon to ensure consistent syntax across different projects. In R, you can use the tidyverse style guide. In Python, you can use the PEP8 style guide written by the creator of Python. And in Julia, you can use the style guide provided with the language manual. You can use a linter, a static code analysis tool, to help you comply with these style guides. Note that these style guides are recommendations and there is no obligation to follow them. Some rules contradict each other, so I would even recommend not following some of them.\nThe most common way to make code easy to reuse is to turn it into functions that can then be shared through a package."
  },
  {
    "objectID": "contents/misc/good-practices.html#objective-3-sufficiently-fast-code",
    "href": "contents/misc/good-practices.html#objective-3-sufficiently-fast-code",
    "title": "Programming good practices",
    "section": "Objective 3: Sufficiently fast code",
    "text": "Objective 3: Sufficiently fast code\nOnce we have ensured that our code works correctly and is easy to maintain, we can focus on its execution time. Although this is not the most important criterion for defining “good” code, it is still a criterion that should not be overlooked, as code that is too slow may not be used. To produce computationally efficient code, you need to:\n\nput a few simple tricks into practice, i.e., use the optimized syntaxes of the different languages;\ncompare the execution time of different ways of programming a task;\nsometimes perform parallel calculations;\nsometimes program pieces of code in another, lower-level language."
  },
  {
    "objectID": "contents/misc/good-practices.html#in-summary",
    "href": "contents/misc/good-practices.html#in-summary",
    "title": "Programming good practices",
    "section": "In summary",
    "text": "In summary\nIn summary, to adopt good programming practices, you need to:\n\nTest your code frequently.\nUse version control software.\nDocument your code.\nFollow a style guide.\nFactorize your code by creating functions and packages.\nOptimize execution time."
  },
  {
    "objectID": "contents/04-dimension.html",
    "href": "contents/04-dimension.html",
    "title": "Dimension",
    "section": "",
    "text": "Slides:\n\nPrincipal componens analysis - annotated\nCorrespondance analysis - annotated\nMultiple correspondance analysis - annotated\n\nTD: link\nTP: link",
    "crumbs": [
      "Modules",
      "04 - Dimension"
    ]
  },
  {
    "objectID": "contents/04-dimension.html#outline",
    "href": "contents/04-dimension.html#outline",
    "title": "Dimension",
    "section": "Outline",
    "text": "Outline\n\nPrincipal componens analysis\nCorrespondance analysis\nMultiple correspondance analysis\n\n\n\n\nFlatland (xkcd:721).",
    "crumbs": [
      "Modules",
      "04 - Dimension"
    ]
  },
  {
    "objectID": "contents/generalities/04-bias-variance.html",
    "href": "contents/generalities/04-bias-variance.html",
    "title": "Bias/Variance",
    "section": "",
    "text": "This section is based on James et al. (2021), Chapter 2.",
    "crumbs": [
      "Modules",
      "03 - Généralités",
      "Bias/Variance"
    ]
  },
  {
    "objectID": "contents/generalities/04-bias-variance.html#what-is-our-objective",
    "href": "contents/generalities/04-bias-variance.html#what-is-our-objective",
    "title": "Bias/Variance",
    "section": "What is our objective?",
    "text": "What is our objective?\nWe want to model the relationship between a response variable \\(Y\\), which may be quantitative, qualitative, or of a different nature, and a set of \\(p\\) explanatory variables \\(X = (X_{1}, \\dots, X_{p})\\), also of (potentially) different types. The central idea is that there is a relationship between \\(Y\\) and the explanatory variables \\(X\\). Generally speaking, we model this relationship using the following model:\n\\[Y = f(X) + \\varepsilon. \\tag{1}\\]\nHere, \\(f\\) is a deterministic (non-random) function representing the systematic information that the explanatory variables \\(X_{1}, \\dots, X_{p}\\) provide about \\(Y\\), and \\(\\varepsilon\\) is a random error term, modeling the variations in \\(Y\\) not explained by \\(X\\). For the purposes of this course, we will make the following assumptions: the random variable \\(\\varepsilon\\) is independent of the explanatory variables \\(X\\), \\(\\mathbb{E}[\\varepsilon] = 0\\), and \\(\\mathrm{Var}(\\varepsilon) = \\sigma^2\\). The Equation 1 is general. It serves as a framework for all the methods we will study, even when the explicit form of \\(f\\) is not known.\nThe Figure 1 illustrates the different elements of the model: the observed data \\((X_i, Y_i)\\), the function \\(f\\) (in blue), and the random deviations \\(\\varepsilon_i\\) represented by dotted lines.\n\n\nCode\nlibrary(tibble)\nlibrary(dplyr)\n\ngenerate_noisy_data &lt;- function(n = 100, noise_levels = c(0, 0.1, 0.3, 0.5)) { \n# x values ​​(avoid 0 for log) \nx_vals &lt;- seq(0.01, 0.99, length.out = n) \n\n# True function \nf &lt;- function(x) 4 * x * (1 - x) * log(x) + 2 \n\n# Generate data for each noise level \ndata &lt;- lapply(noise_levels, function(sigma) { \ny_true &lt;- f(x_vals) \ny_noisy &lt;- y_true + rnorm(n, mean = 0, sd = sqrt(sigma)) \n\ntibble( \nx = x_vals, \ny = y_noisy, \nnoise = sigma \n) \n}) %&gt;% bind_rows() \n\nreturn(data)\n}\n\n# Example usage\nset.seed(123)\n\nnoise_levels &lt;- seq(0, 0.5, by=0.01)\ndf &lt;- generate_noisy_data(noise_levels = noise_levels)\nwrite.csv(df, './data.csv')\n\n\n\n\nCode\ndata = FileAttachment(\"../../include/data/bias-variance/data.csv\").csv({ typed: true })\n\nviewof noise = Inputs.range( \n[0.01, 0.5], \n{value: 0.25, step: 0.01, label: tex`\\sigma^2`}\n)\n\nfiltered = data.filter(function(df) { \nreturn df.noise == noise;\n})\n\ntrue_curve = data.filter(function(df) { \nreturn df.noise == 0;\n})\n\nerrors = filtered.map(f =&gt; { \nconst location = true_curve.find(loc =&gt; loc.x === f.x); \nreturn { ...f, y_end: location?.y };\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nPlot.plot({ \ngrid:true, \nx:{ \ndomain: [0, 1], \nlabel: \"X\", \n}, \ny: { \ndomain: [0, 3], \nlabel: \"Y\", \n}, \nmarks: [ \nPlot.dot(filtered, {x: \"x\", y: \"y\", fill: \"#444444\", r: 3}), \nPlot.line(true_curve, {x: \"x\", y: \"y\", stroke: \"#B0E1FA\", strokeWidth: 5}), \nPlot.link(errors, { \nx1: \"x\", \nx2: \"x\", \ny1: \"y\", \ny2: \"y_end\", \nstroke: \"#AAAAAA\", \nstrokeDasharray: \"5,5\", \n}) \n]\n})\n\n\n\n\n\n\n\n\n\nFigure 1: The different elements of the model. The points represent the observed data \\((X_i, Y_i)\\). The blue curve represents the function \\(f\\) and the dotted lines represent the error associated with each observation.\n\n\n\n\nIn the rest of the course, we will examine different methods for estimating the function f from data. However, before examining how to construct a \\(\\widehat{f}\\) estimator of \\(f\\), we will examine the quality of such an estimator: what does it mean to “properly estimate” f? And how can we evaluate the quality of the estimate?\n\n\n\n\n\n\nNoteExample: Simple Linear Regression\n\n\n\nIn this very simple framework, we assume that the function f is of the form: f(x) = a x + b. In this case, estimating the function f is reduced to estimating the coefficients a and b.\n\n\n\n\n\n\n\n\nTipNote: Tradeoff between accuracy and interpretability\n\n\n\nDepending on the objective of the study, we generally have to make a choice between the accuracy of our predictions and the interpretability of our model. A simple model, such as linear regression, will be easy to interpret but will poorly capture complex relationships. Conversely, a more flexible model, such as a random forest, will have better predictions but will be more difficult to interpret. The choice therefore depends on the objective of the analysis: understanding or predictive performance?\n\n\n\n\n\n\n\n\nTipNote: No free lunch in statistics\n\n\n\nWhy not simply use the “ultimate” model, the one that would always be optimal regardless of the dataset? Because such a model doesn’t exist! There is no universally best method for all datasets and all objectives. A method that performs well in a given context may fail elsewhere. It is therefore always necessary to adapt the approach to the problem (explanation, prediction, classification, etc.).",
    "crumbs": [
      "Modules",
      "03 - Généralités",
      "Bias/Variance"
    ]
  },
  {
    "objectID": "contents/generalities/04-bias-variance.html#how-to-measure-the-quality-of-an-estimator",
    "href": "contents/generalities/04-bias-variance.html#how-to-measure-the-quality-of-an-estimator",
    "title": "Bias/Variance",
    "section": "How to measure the quality of an estimator?",
    "text": "How to measure the quality of an estimator?\nOnce we have an estimator \\(\\widehat{f}\\) of the function \\(f\\), obtained from \\(n\\) observations \\((y_1, x_1), \\dots, (y_n, x_n)\\), we seek to evaluate the accuracy of the predictions \\(\\widehat{Y} = \\widehat{f}(X)\\). The idea is to verify how close \\(\\widehat{Y}\\) is to the true value of \\(Y\\).\n\n\n\n\n\n\nWarningDefinition: Mean Square Error\n\n\n\nWhen \\(Y\\) is a quantitative variable, a classic measure of the quality of \\(\\widehat{f}\\) is the mean square error (MSE): \\[MSE(Y, \\widehat{Y}) = \\frac{1}{n} \\sum_{i = 1}^{n} \\left( y_i - \\widehat{y}_i\\right)^2 = \\frac{1}{n} \\sum_{i = 1}^{n} \\left( y_i - \\widehat{f}(x_i) \\right)^2,\\] where \\(\\widehat{y}_i = \\widehat{f}(x_i)\\) is the prediction that \\(\\widehat{f}\\) gives for observation \\(x_i\\).\n\n\nA low MSE indicates that the predictions are close to the observations. We can also interpret it as the average distance between the observed values ​​and the predicted values. We therefore aim for a low average distance.\nWhen \\(Y\\) is a qualitative variable, e.g., a class or a label, we use another measure: the error rate.\n\n\n\n\n\n\nWarningDefinition: Error Rate\n\n\n\nWhen \\(Y\\) is a qualitative variable, a classic measure of the quality of \\(\\widehat{f}\\) is the error rate (*ER): \\[ER(Y, \\widehat{Y}) = \\frac{1}{n} \\sum_{i = 1}^{n} \\mathbb{1}(y_i \\neq \\widehat{y}_i) = \\frac{1}{n} \\sum_{i = 1}^{n} \\mathbb{1}(y_i \\neq \\widehat{f}(x_i)).\\] where \\(\\widehat{y}_i = \\widehat{f}(x_i)\\) is the prediction that \\(\\widehat{f}\\) gives for observation \\(x_i\\).\n\n\nThe error rate measures the proportion of incorrect predictions. It is, again, a measure of the average distance between \\(Y\\) and \\(\\widehat{Y}\\), suitable for qualitative variables.",
    "crumbs": [
      "Modules",
      "03 - Généralités",
      "Bias/Variance"
    ]
  },
  {
    "objectID": "contents/generalities/04-bias-variance.html#the-biasvariance-trade-off",
    "href": "contents/generalities/04-bias-variance.html#the-biasvariance-trade-off",
    "title": "Bias/Variance",
    "section": "The bias/variance trade-off",
    "text": "The bias/variance trade-off\nOur goal is often to minimize the prediction error, not only on observed data, but especially on new data (recovered after estimating the model). To do this, we focus on the prediction error: \\[\\mathbb{E}\\left[ \\left( Y - \\widehat{Y} \\right)^2 \\right] = \\mathbb{E}\\left[ \\left( Y - \\widehat{f}(X) \\right)^2 \\right].\\]\nThis error can be broken down into three components:\n\nBias: the error due to a systematic approximation, e.g., if we impose a linear model when the relationship is nonlinear.\nVariance: the sensitivity of the estimator to fluctuations in the training sample.\nIrreducible error: the intrinsic variance of the noise \\(\\varepsilon\\), denoted \\(\\sigma^2\\).\n\n\n\n\n\n\n\nImportantDécomposition bias/variance\n\n\n\nOn to: \\[\\mathbb{E}\\left[ (Y - \\widehat{Y})^2 \\right] = \\mathbb{E}\\left[ (Y - \\widehat{f}(X))^2 \\right] = \\mathrm{Biais}(\\widehat{f}(X))^2 + \\mathrm{Var}(\\widehat{f}(X)) + \\sigma^2.\\]\n\n\n\n\n\n\n\n\nNotePreuve\n\n\n\n\n\nOn the other hand, we point out that the hope of the estimator’s error is broken down into a reductible part and an irreductible part.\n\\[\\begin{align*}\n\\mathbb{E}\\left[ \\left( Y - \\widehat{Y} \\right)^2 \\right]\n&= \\mathbb{E}\\left[ \\left( Y - \\widehat{f}(X) \\right)^2 \\right] \\\\\n&= \\mathbb{E}\\left[ \\left( f(X) + \\varepsilon - \\widehat{f}(X) \\right)^2 \\right] \\\\\n&= \\mathbb{E}\\left[ \\left( f(X) - \\widehat{f}(X) \\right)^2 \\right] + 2\\mathbb{E}\\left[ \\left( f(X) - \\widehat{f}(X) \\right)\\varepsilon \\right] + \\mathbb{E}[\\varepsilon^2] \\\\\n&= \\mathbb{E}\\left[ \\left( f(X) - \\widehat{f}(X) \\right)^2 \\right] + 2\\mathbb{E}\\left[ \\left( f(X) - \\widehat{f}(X) \\right) \\right] \\underbrace{\\mathbb{E}\\left[ \\varepsilon \\right]}_{= 0} + \\sigma^2 \\\\\n&= \\underbrace{\\mathbb{E}\\left[ \\left( f(X) - \\widehat{f}(X) \\right)^2 \\right]}_{\\text{réductible}} + \\underbrace{\\sigma^2}_{\\text{irréductible}}.\n\\end{align*}\\]\nUse the linearity of hope and make sure that \\(X\\) and \\(\\varepsilon\\) are independent. He is interested in maintaining the “reductible” party. The trick is to make apparatus \\(\\mathbb{E}\\left[ \\widehat{f}(X) \\right]\\).\n\\[\\begin{align*}\n\\mathbb{E}\\left[ \\left( f(X) - \\widehat{f}(X) \\right)^2 \\right]\n&= \\mathbb{E}\\left[ \\left( f(X) - \\mathbb{E}\\left[ \\widehat{f}(X) \\right] + \\mathbb{E}\\left[ \\widehat{f}(X) \\right] - \\widehat{f}(X) \\right)^2 \\right] \\\\\n&= \\underbrace{\\mathbb{E}\\left[ \\left( f(X) - \\mathbb{E}\\left[ \\widehat{f}(X) \\right] \\right)^2 \\right]}_{\\text{A}} \\\\\n&\\quad - 2 \\underbrace{\\mathbb{E}\\left[ \\left( f(X) - \\mathbb{E}\\left[ \\widehat{f}(X) \\right] \\right) \\left( \\widehat{f}(X) - \\mathbb{E}\\left[ \\widehat{f}(X) \\right] \\right) \\right]}_{\\text{B}} \\\\\n&\\quad + \\underbrace{\\mathbb{E}\\left[ \\left( \\widehat{f}(X) - \\mathbb{E}\\left[ \\widehat{f}(X) \\right] \\right)^2 \\right]}_{\\text{C}}.\n\\end{align*}\\]\nA. The \\(f(X)\\) function is not aléatoire, on a \\(\\mathbb{E}\\left[ f(X) \\right] = f(X)\\) et donc\n\\[\\begin{align*}\n\\mathbb{E}\\left[ \\left( f(X) - \\mathbb{E}\\left[ \\widehat{f}(X) \\right] \\right)^2 \\right]\n&= \\mathbb{E}\\left[ \\left( \\mathbb{E}\\left[ f(X) - \\widehat{f}(X) \\right] \\right)^2 \\right] \\\\\n&= \\mathbb{E}\\left[ f(X) - \\widehat{f}(X) \\right]^2 \\\\\n&= \\text{Biais}(\\widehat{f}(X))^2.\n\\end{align*}\\]\nB. To develop the expression and use the independence of the variables, on finding that \\(B = 0\\).\nC. Using the definition of variance,\n\\[\\mathbb{E}\\left[ \\left( \\widehat{f}(X) - \\mathbb{E}\\left[ \\widehat{f}(X) \\right] \\right)^2 \\right] = \\mathrm{Var}(\\widehat{f}).\\]\nFinally, hon a\n\\[\\mathbb{E}\\left[ \\left( f(X) - \\widehat{f}(X) \\right)^2 \\right] = \\text{Biais}(\\widehat{f}(X))^2 + \\mathrm{Var}(\\widehat{f}(X)).\\]\nHence the results.\n\n\n\nThis breakdown before a fundamental compromise in analyzing women:\n\nIf you choose a more flexible model, the bias will be higher, but the variance will be easier.\nIf you choose a flexible model, the bias will be easy, but the variance will be too high.\n\nNotre objectif est donc de trouver un juste équilibre entre biais et variance, i.e. un modèle qui prédit correctement, tout en étant généralisable à de nouvelles données. La Figure 2 (a) présente un jeu de données et différents estimateurs \\(\\widehat{f}\\). En faisant varier le paramètre \\(\\lambda\\), on obtient des modèles plus ou moins flexible (lorsque \\(\\lambda = 0.15\\), le modèle est flexible et lorsque \\(\\lambda = 1\\), le modèle est rigide). La Figure 2 (b) montre la valeur du biais, de la variance et de la MSE pour les modèles estimés pour la Figure 2 (a). On remarque que plus \\(\\lambda\\) est petit, plus la variance est grande, mais le biais est petit (le modèle est flexible). Inversement, plus \\(\\lambda\\) est grand, plus le biais est grand et la variance petite (le modèle est rigide). La courbe de MSE en fonction du paramètre est une courbe en U. Comme on cherche à minimiser la MSE, i.e. à faire un compromis entre le biais et la variance, on peut prendre \\(\\lambda = 0.5\\).\n\n\nCode\n# Load packages\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\n\nset.seed(42)\n\n# 1. Simulate a single dataset\nn &lt;- 100\nsigma2 &lt;- 0.1\nx &lt;- sort(runif(n, 0.05, 1))\ny &lt;- 4 * x * (1 - x) * log(x) + 2 + rnorm(n, 0, sqrt(sigma2))\ndf &lt;- data.frame(x = x, y = y, span = 0)\n\n# 2. Define grid and spans to compare\nx_grid &lt;- seq(0.05, 1, length.out = 300)\nspans_to_plot &lt;- seq(0.1, 1, by = 0.1)\n\n# 3. Compute loess fits for each span\nfits &lt;- lapply(spans_to_plot, function(s) {\n  loess_model &lt;- loess(y ~ x, data = df, span = s)\n  y_hat &lt;- predict(loess_model, newdata = data.frame(x = x_grid))\n  data.frame(x = x_grid, y = y_hat, span = s)\n})\n\nfit_df &lt;- bind_rows(fits)\nfit_df &lt;- fit_df |&gt; add_row(df)  # Add data points\n \nwrite.csv(fit_df, './data_fit.csv')\n\n\n# Parameters\nn_sim &lt;- 100          # number of simulated datasets\nspans &lt;- seq(0.1, 1, by = 0.05)  # LOESS smoothing parameters\nx_grid &lt;- seq(0.1, 1, length.out = 200)\nf_true &lt;- 4 * x_grid * (1 - x_grid) * log(x_grid) + 2\n\n# Storage for predictions\nresults &lt;- list()\n\nfor (s in spans) {\n  pred_matrix &lt;- matrix(NA, nrow = length(x_grid), ncol = n_sim)\n  \n  for (sim in 1:n_sim) {\n    x &lt;- sort(runif(n, 0.01, 1.1))\n    y &lt;- 4 * x * (1 - x) * log(x) + 2 + rnorm(n, 0, sqrt(sigma2))\n    df &lt;- data.frame(x = x, y = y)\n    \n    # Fit loess model with span = s\n    model &lt;- loess(y ~ x, data = df, span = s, degree = 2)\n    pred &lt;- predict(model, newdata = data.frame(x = x_grid), )\n    \n    pred_matrix[, sim] &lt;- pred\n  }\n  \n  # For each point in x_grid, compute bias², variance, MSE\n  mean_pred &lt;- rowMeans(pred_matrix, na.rm = TRUE)\n  bias2 &lt;- (mean_pred - f_true)^2\n  var_pred &lt;- apply(pred_matrix, 1, var, na.rm = TRUE)\n  mse &lt;- bias2 + var_pred\n  \n  results[[as.character(s)]] &lt;- data.frame(\n    span = s,\n    Biais2 = mean(bias2),\n    Variance = mean(var_pred),\n    MSE = mean(mse)\n  )\n}\n\n# Combine and reshape results\nresults_df &lt;- bind_rows(results)\nresults_long &lt;- pivot_longer(\n  results_df,\n  cols = c(\"Biais2\", \"Variance\", \"MSE\"),\n  names_to = \"component\", values_to = \"value\"\n)\n\nwrite.csv(results_long, './data_mse.csv')\n\n\n\n\nCode\ndata_fit = FileAttachment(\"../../include/data/bias-variance/data_fit.csv\").csv({ typed: true })\ndata_mse = FileAttachment(\"../../include/data/bias-variance/data_mse.csv\").csv({ typed: true })\n\nviewof span = Inputs.range( \n[0.1, 1], \n{value: 0.5, step: 0.1, label: tex`\\lambda`}\n)\n\nfiltered_fit = data_fit.filter(function(df) { \nreturn df.span == span;\n})\n\npoints = data_fit.filter(function(df) { \nreturn df.span == 0;\n})\n\ncurrent_mse = data_mse.filter(function(df) {\nreturn df.span == span;\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nPlot.plot({ \ngrid:true, \nx:{ \ndomain: [0, 1], \nlabel: \"X\", \n}, \ny: { \ndomain: [0, 3], \nlabel: \"Y\", \n}, \nmarks: [ \nPlot.dot( \npoints, \n{x: \"x\", y: \"y\", fill: \"#444444\", r: 3} \n), \nPlot.line( \nfiltered_fit, \n{x: \"x\", y: \"y\", stroke: \"#B0E1FA\", strokeWidth: 5} \n), \n\n]\n})\n\n\nCode\nPlot.plot({ \ngrid:true, \ncolor: { \nlegend: true, \n}, \nx:{ \ndomain: [0, 1], \nlabel: \"λ\", \n}, \ny: { \ndomain: [0, 0.04], \nlabel: \"Y\",\n},\nmarks: [\nPlot.line(\ndata_mse,\n{x: \"span\", y: \"value\", stroke: \"component\", strokeWidth: 5}\n),\nPlot.dot(\ncurrent_mse,\n{x: \"span\", y: \"value\", fill: \"component\", r: 10}\n),\n]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Different \\(\\widehat{f}\\) estimators.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Different parts of the error.\n\n\n\n\n\n\nFigure 2: Illustration of the bias/variance tradeoff. The \\(\\lambda\\) parameter controls the flexibility of the model; the smaller \\(\\lambda\\), the more flexible the model.\n\n\n\n\nGenerally speaking, as flexibility increases, the decrease in bias is greater than the increase in variance, resulting in a decrease in prediction error. However, beyond a certain level of flexibility, the bias becomes negligible, and any further decrease is offset by the rapid increase in variance. Prediction error therefore begins to increase. This results in a U-shaped curve of prediction error as a function of model flexibility: a model that is too rigid generates a high bias, while a model that is too flexible leads to too much variance.\n\n\n\n\n\n\nTipNote: Why compromise?\n\n\n\nIt is always possible to build a very flexible model with zero bias, e.g., a model that passes through all observation points, but which will have enormous variance. Conversely, a model that is too rigid, e.g., a constant, will have a very large bias but almost zero variance. The bias/variance compromise consists of choosing a model that controls both of these quantities.",
    "crumbs": [
      "Modules",
      "03 - Généralités",
      "Bias/Variance"
    ]
  },
  {
    "objectID": "contents/generalities/03-distance.html",
    "href": "contents/generalities/03-distance.html",
    "title": "Distances",
    "section": "",
    "text": "In any data analysis project, it is necessary to be able to quantify the similarity (or dissimilarity) between two observations. To do this, we use the concept of distance (or similarity) between observations. The choice of this distance directly influences the results of learning, clustering, and visualization algorithms.",
    "crumbs": [
      "Modules",
      "03 - Généralités",
      "Distances"
    ]
  },
  {
    "objectID": "contents/generalities/03-distance.html#concept-of-distance",
    "href": "contents/generalities/03-distance.html#concept-of-distance",
    "title": "Distances",
    "section": "Concept of distance",
    "text": "Concept of distance\nA distance is a mathematical function that measures how far two objects are from each other in a given space. The greater the distance, the further apart the observations are.\n\n\n\n\n\n\nWarningDefinition of distance measurement\n\n\n\nA function \\(d: \\mathcal{X} \\times \\mathcal{X} \\to \\mathbb{R}\\) is a distance on a set \\(\\mathcal{X}\\) if, for all \\(x, y, z \\in \\mathcal{X}\\), the following conditions are satisfied:\n\nnon-negativity: \\(d(x, y) \\geq 0\\);\nseparation: \\(d(x, y) = 0 \\Leftrightarrow x = y\\);\nsymmetry: \\(d(x, y) = d(y, x)\\);\ntriangle inequality: \\(d(x, y) \\leq d(x, z) + d(y, z)\\).\n\n\n\n\n\n\n\n\n\nNoteEuclidean distance\n\n\n\nWhen observations are represented by numerical vectors in \\(\\mathbb{R}^p\\) of the same order of magnitude, Euclidean distance is often a good choice.\nLet \\(x, y \\in \\mathbb{R}^p\\), the Euclidean distance is given by: \\[d(x, y) = \\left\\| x - y \\right\\|_2 = \\left( \\sum_{i = 1}^{p} (x_i - y_i)^2 \\right)^{1/2}.\\]\n\n\n\n\n\n\n\n\nNoteThe \\(L_q\\) (or Minkowski) distance\n\n\n\nLet \\(x, y \\in \\mathbb{R}^p\\), the distance \\(L_q\\) is given, for \\(q &gt; 0\\), by: \\[d(x, y) = \\left\\| x - y \\right\\|_q = \\left( \\sum_{i = 1}^{p} |x_i - y_i|^q \\right)^{1 / q}.\\]\nSpecial cases:\n\nFor \\(q = 1\\), we obtain the Manhattan distance: \\[d(x, y) = \\left\\| x - y \\right\\|_1 = \\sum_{i = 1}^{p} |x_i - y_i|.\\]\nFor \\(q = 2\\), we obtain the Euclidean distance.\n\n\n\n\n\n\n\n\n\nNoteExample\n\n\n\nConsider the following dataset:\n\nAverage height and weight in Canada (Source: Statistics Canada, Canadian Community Health Survey (2008)).\n\n\nName\nHeight\nWeight\n\n\n\n\nAlice\n162.1\n66.8\n\n\nBob\n175.8\n81.6\n\n\n\nThe Euclidean distance between Alice and Bob is \\[d(\\text{Alice}, \\text{Bob}) = \\sqrt{(162.1 - 175.8)^2 + (66.8 - 81.6)^2} = 20.16.\\]\nThe Manhattan distance between Alice and Bob is \\[d(\\text{Alice}, \\text{Bob}) = |162.1 - 175.8| + |66.8 - 81.6| = 28.5.\\]\n\n\nThe distance L_q is not invariant to changes in scale. For example, if we multiply all the components of a vector by a factor of , the distance between two vectors changes by a factor of .\nIn practice, we prefer to work with standardized variables. Thus, denoting \\(\\mu_i\\) as the mean, and \\(\\sigma_i\\) as the standard deviation of variable \\(i\\), the Euclidean distance with standardized variables is given by: \\[d(x, y) = \\sum_{i = 1}^{p} \\left\\{ \\frac{x_i - \\mu_i}{\\sigma_i} - \\frac{y_i - \\mu_i}{\\sigma_i} \\right\\}^2 = \\sum_{i = 1}^{p} \\left( \\frac{x_i - y_i}{\\sigma_i} \\right)^2.\\]\n\n\n\n\n\n\nImportantProperty\n\n\n\nThe Euclidean distance with standardized variables is invariant under changes in scale.\n\n\n\n\n\n\n\n\nNoteProof\n\n\n\n\n\nLet \\(\\lambda \\neq 0\\) and let \\(X\\) be a random variable. We have \\(\\mathbb{E}(\\lambda X) = \\lambda \\mathbb{E}(X)\\) and \\(\\mathrm{Var}(\\lambda X) = \\lambda^2 \\mathrm{Var}(X)\\). So\n\\[d(\\lambda x, \\lambda y) = \\sum_{i = 1}^{p} \\left\\{ \\frac{\\lambda x_i - \\lambda \\mu_i}{\\lambda \\sigma_i} - \\frac{\\lambda y_i - \\lambda \\mu_i}{\\lambda \\sigma_i} \\right\\}^2 = \\sum_{i = 1}^{p} \\left( \\frac{x_i - y_i}{\\sigma_i} \\right)^2 = d(x, y).\\]",
    "crumbs": [
      "Modules",
      "03 - Généralités",
      "Distances"
    ]
  },
  {
    "objectID": "contents/generalities/03-distance.html#notion-of-similarity",
    "href": "contents/generalities/03-distance.html#notion-of-similarity",
    "title": "Distances",
    "section": "Notion of Similarity",
    "text": "Notion of Similarity\nIn contrast to the notion of distance, a similarity measure quantifies how close two observations are in a given space. Thus, the greater the similarity, the closer the observations are.\n\n\n\n\n\n\nWarningSimilarity Measure Definition\n\n\n\nA function \\(s: \\mathcal{X} \\times \\mathcal{X} \\to \\mathbb{R}\\) is a similarity measure on a set \\(\\mathcal{X}\\) if, for all \\(x, y \\in \\mathcal{X}\\), the following conditions hold:\n\n\\(s(x, y) \\geq 0\\);\n\\(s(x, y) = s(y, x)\\);\n\\(s(x, x) = 1 \\geq s(x, y)\\).\n\n\n\nA distance can be transformed into a similarity by setting \\[s(x, y) = \\frac{1}{1 + d(x, y)}.\\]\nThis transformation ensures that the greater the distance, the lower the similarity. However, the reverse is not always possible because a similarity measure does not necessarily respect the triangle inequality. We can also define the dissimilarity between two objects: \\[d^\\star(x, y) = 1 - s(x, y).\\]",
    "crumbs": [
      "Modules",
      "03 - Généralités",
      "Distances"
    ]
  },
  {
    "objectID": "contents/generalities/03-distance.html#case-of-qualitative-variables",
    "href": "contents/generalities/03-distance.html#case-of-qualitative-variables",
    "title": "Distances",
    "section": "Case of qualitative variables",
    "text": "Case of qualitative variables\nWhen working with qualitative variables, the usual numerical distances (such as \\(L_p\\) distances) are generally meaningless. For example, if a variable takes its values ​​from the set \\[\\mathcal{X} = \\{ \\text{Red}, \\text{Green}, \\text{Blue} \\},\\] then it makes no sense to calculate the difference between \\(\\text{Blue}\\) and \\(\\text{Red}\\), because these categories carry no intrinsic numerical structure and have no notion of order or distance.\nBad practice would be to arbitrarily assign numerical values ​​to the variables (e.g., \\(\\text{Red} = 1\\), \\(\\text{Green} = 2\\), \\(\\text{Blue} = 3\\)), which would introduce an artificial order between them. This could significantly bias the analyses.\n\nOne-of-K Encoding\nWhen we want to use a model based on a notion of distance between observations (i.e., most models), we must use a suitable encoding. One-of-K encoding (one-hot encoding) consists of encoding a categorical variable with \\(K\\) categories as a binary vector of dimension \\(K\\), in which only one entry is \\(1\\), the others are \\(0\\). Thus, for the example of \\(\\mathcal{X} = \\{ \\text{Red, Green, Blue} \\}\\), we will obtain the following encoding: “Red” gives \\((1, 0, 0)\\), “Green” gives \\((0, 1, 0)\\), and “Blue” gives \\((0, 0, 1)\\).\nThis encoding method has the advantage of not introducing an artificial order between the modalities. However, if the variable has many modalities, the representation space will be large, which can hinder the effectiveness of some analysis methods.\n\n\nDefine an appropriate distance\nOnce the modalities have been encoded, we can define a distance between two observations of qualitative variables.\n\n\n\n\n\n\nNoteDiscrete Distance (or Hamming Distance)\n\n\n\nLet \\(\\mathcal{X}\\) be a discrete set and let \\(x\\) and \\(y\\) be two observations of \\(X\\). The discrete distance is given by \\[d(x, y) = \\begin{cases}\n0, & \\text{if } x = y,\\\\\n1, & \\text{if } x \\neq y\n\\end{cases}.\\]\n\n\nFor vectors of qualitative variables (e.g., the comparison of several individuals described by several characteristics), the discrete distance is the sum of disagreements between the components: \\[d(x, y) = \\sum_{i = 1}^{p} \\mathbb{1}(x_i \\neq y_i),\\] where \\(p\\) is the number of variables.\n\n\n\n\n\n\nNoteExample\n\n\n\nConsider the characteristics of three people.\n\n\n\nTable 1: Characteristics of three people.\n\n\n\n\n\nName\nColor\nEyes\nHair\n\n\n\n\nAlice\nRed\nBlue\nBlonde\n\n\nBob\nGreen\nBlue\nRed\n\n\nChris\nRed\nGreen\nBlonde\n\n\n\n\n\n\nThe distance between two people is calculated as the number of different characteristics. Thus,\n\\[d(\\text{Alice}, \\text{Bob}) = 1 + 0 + 1 = 2,\\]\n\\[d(\\text{Alice}, \\text{Chris}) = 0 + 1 + 0 = 1,\\]\n\\[d(\\text{Bob}, \\text{Chris}) = 1 + 1 + 1 = 3.\\]\n\n\nRather than counting the differences, we can also count the agreements and normalize them:\n\\[s(x, y) = \\frac{1}{p}\\sum_{i = 1}^{p} \\mathbb{1}(x_i = y_i),\\]\nwhich gives a similarity measure between \\(0\\) (no agreement) and \\(1\\) (identical).\n\n\n\n\n\n\nNoteExample\n\n\n\nContinuing the previous example (see Table 1), we find the following similarities:\n\\[s(\\text{Alice}, \\text{Bob}) = \\frac{0 + 1 + 0}{3} = \\frac{1}{3},\\]\n\\[s(\\text{Alice}, \\text{Chris}) = \\frac{1 + 0 + 1}{3} = \\frac{2}{3},\\]\n\\[s(\\text{Bob}, \\text{Chris}) = \\frac{0 + 0 + 0}{3} = 0.\\]\n\n\n\n\nJaccard Distance\nWhen the number of binary variables is large (e.g., in the case where a 1 out of K encoding has been performed on p qualitative variables), the discrete distance is not necessarily very suitable because the number of agreements is likely to be small compared to the total number of variables ($K × p in the previous example), which will result in small distances in all cases. One solution is to focus only on attributes that equal 1, because generally, a binary variable equal to 0 does not provide any particular information (at least, less than a binary variable equal to 1). The Jaccard index (Intersection over Union, IoU) was introduced to take this into account.\n\n\n\n\n\n\nWarningDefinition: Jaccard Index\n\n\n\nConsider two observations x and y of K binary variables. All variables can take the values ​​\\(0\\) and \\(1\\).\nLet’s define the following quantities:\n\n\\(M_{11}\\), the number of \\(1\\) variables for \\(x\\) and \\(y\\);\n\\(M_{10}\\), the number of \\(1\\) variables for \\(x\\) and \\(0\\) for \\(y\\);\n\\(M_{01}\\), the number of \\(0\\) variables for \\(x\\) and \\(1\\) for \\(y\\);\n\\(M_{00}\\), the number of \\(0\\) variables for \\(x\\) and \\(y\\).\n\nSince each binary variable is necessarily counted in either \\(M_{11}\\), \\(M_{10}\\), \\(M_{01}\\), or \\(M_{00}\\), their sum is therefore equal to \\(K\\).\nThe Jaccard index is defined as \\[J(x, y) = \\frac{M_{11}}{M_{10} + M_{01} + M_{11}} = \\frac{M_{11}}{K - M_{00}}.\\]\n\n\nPaying attention to the case where the two observations consist only of \\(0\\) (we take \\(J(x, y) = 1\\) in this case), the Jaccard index is a measure of similarity.\n\n\n\n\n\n\nImportantProperty: Jaccard Distance\n\n\n\nFor two observations \\(x\\) and \\(y\\) of \\(K\\) binary variables, the Jaccard distance is given by \\[d(x, y) = 1 - J(x, y).\\]\n\n\n\n\n\n\n\n\nNoteProof\n\n\n\n\n\nTo show that the Jaccard distance is indeed a distance, we must demonstrate the four properties of distances. First, note that the Jaccard distance can be rewritten as \\[d(x, y) = \\frac{M_{10} + M_{01}}{M_{01} + M_{10} + M_{11}}.\\]\n\nAll terms in the numerator and denominator are positive, so \\(d(x, y) \\geq 0\\).\nLet us show that \\(d(x, y) = 0 \\Leftrightarrow x = y\\).\n\nSuppose that \\(d(x, y) = 0\\). Then \\(M_{01} + M_{10} = 0\\). Therefore, there are no variables that equal \\(0\\) for \\(x\\) and \\(1\\) for \\(y\\), and vice versa. Since \\(M_{01} + M_{10} + M_{11} &gt; 0\\), we have \\(x = y\\).\nNow, suppose that \\(x = y\\). Then \\(M_{01} = M_{10} = 0\\). Therefore \\(d(x, y) = 0\\).\n\nWe have that \\(d(x, y) = d(y, x)\\) because the Jaccard index is symmetric.\nThe proof of the triangle inequality will be done in an exercise (see TD)).\n\n\n\n\n\n\n\n\n\n\nNoteExample\n\n\n\nConsider a questionnaire with 5 closed questions. Suppose the answer “Yes” is coded as 1 and the answer “No” is coded as 0.\n\n\n\nTable 2: Characteristics of two people.\n\n\n\n\n\nName\nQ1\nQ2\nQ3\nQ4\nQ5\n\n\n\n\nAlice\n1\n0\n1\n0\n0\n\n\nBob\n1\n0\n0\n1\n0\n\n\n\n\n\n\nFor the distance between Alice and Bob, we have \\(M_{11} = 1\\), \\(M_{10} = 1\\), \\(M_{01} = 1\\), and \\(M_{00} = 2\\). Therefore, the Jaccard similarity is given by \\(J(\\text{Alice}, \\text{Bob}) = \\frac{1}{3}\\). Thus, the Jaccard distance is \\(d(\\text{Alice}, \\text{Bob}) = 1 - J(x, y) = \\frac{2}{3}\\).",
    "crumbs": [
      "Modules",
      "03 - Généralités",
      "Distances"
    ]
  },
  {
    "objectID": "contents/generalities/05-model-evaluation.html",
    "href": "contents/generalities/05-model-evaluation.html",
    "title": "Models evaluation",
    "section": "",
    "text": "This section is based on James et al. (2021), Chapter 5.",
    "crumbs": [
      "Modules",
      "03 - Généralités",
      "Models evaluation"
    ]
  },
  {
    "objectID": "contents/generalities/05-model-evaluation.html#evaluating-the-performance-of-a-predictive-model",
    "href": "contents/generalities/05-model-evaluation.html#evaluating-the-performance-of-a-predictive-model",
    "title": "Models evaluation",
    "section": "Evaluating the Performance of a Predictive Model",
    "text": "Evaluating the Performance of a Predictive Model\nIn the previous section, we introduced tools for measuring the quality of an estimator: the mean square error (MSE) for quantitative variables and the error rate (ER) for qualitative variables. These measures compare the predicted values ​​\\(\\widehat{Y} = \\widehat{f}(X)\\) to the observed values ​​\\(Y\\). However, if we calculate these errors only from the data used to train the model, we risk underestimating the true prediction error. Why? Because the estimator \\(\\widehat{f}\\) was adjusted to minimize the error on these same data. It therefore fits them well, and generally, too well! This can lead to the illusion that our model is performing well. Indeed, a very flexible model can have a low error on the training data simply because it captures noise rather than signal. But if the model adapts too much to the training data, it risks generalizing poorly to new data, i.e., data it has never seen before. This phenomenon is called overfitting.\n\n\n\n\n\n\nTipNote: Overfitting and Underfitting\n\n\n\nAn overly flexible model can fit the training data perfectly, including random noise. It will have a low error on this data but a high error on new observations. This is called overfitting of the model. Conversely, a model that is too rigid (e.g., a constant straight line) will not be able to capture the structure of the data, even on the training set. This is called underfitting of the model.\nThe goal is to find the right balance between flexibility and generalization capacity.\n\n\nTo objectively evaluate a model, ideally, it would be best to test it on data completely independent of the data used for training. We therefore distinguish two sets: a training set, used to adjust the model, and a test set, used to evaluate the model’s predictive performance. In practice, we generally don’t have access to a test set to perform this evaluation. In this section, we will discuss two approaches to circumvent this problem.",
    "crumbs": [
      "Modules",
      "03 - Généralités",
      "Models evaluation"
    ]
  },
  {
    "objectID": "contents/generalities/05-model-evaluation.html#validation-dataset",
    "href": "contents/generalities/05-model-evaluation.html#validation-dataset",
    "title": "Models evaluation",
    "section": "Validation Dataset",
    "text": "Validation Dataset\nWhen we only have a single dataset, a simple solution is to randomly divide it into two subsets: a training set to adjust the model, and a validation set to estimate the prediction error. This is called a validation set approach. Figure 1 presents a schematic of this approach, and ?@fig-trainval-1-pdf and ?@fig-trainval-2-pdf present an example of a training and validation set, along with the associated MSE. Figure 2 presents several combinations of training and validation datasets, along with the associated MSE.\n\n\n\n\n\n\nFigure 1: Diagram of the validation set approach.\n\n\n\n\n\nCode\n# Load packages\nlibrary(tidyverse)\nlibrary(dplyr)\n\n# Set seed for reproducibility\nset.seed(42)\n\n# Function definition\nf &lt;- function(x) {\n  4 * x * (1 - x) * log(x) + 2\n}\n\ngenerate_splits &lt;- function(data, train_prop = 0.7, K = 10) {\n  rows &lt;- list()\n  \n  for (k in 1:K) {\n    idx_train &lt;- sample(nrow(data), size = floor(train_prop * nrow(data)))\n    train &lt;- data[idx_train, ]\n    validation &lt;- data[-idx_train, ]\n    \n    train_tbl &lt;- tibble(\n      split = k,\n      set = \"train\",\n      x = train$x,\n      y = train$y\n    )\n    \n    validation_tbl &lt;- tibble(\n      split = k,\n      set = \"validation\",\n      x = validation$x,\n      y = validation$y\n    )\n    \n    rows[[k]] &lt;- bind_rows(train_tbl, validation_tbl)\n  }\n  \n  bind_rows(rows)\n}\n\n# Generate N data points in the interval (0, 1]\nN &lt;- 200\nspans &lt;- seq(0.1, 1, by = 0.05)  # LOESS smoothing parameters\nx_all &lt;- runif(N, min = 0.01, max = 1)  # avoid 0 due to log(x)\ny_all &lt;- f(x_all) + rnorm(N, mean = 0, sd = 0.1)  # add Gaussian noise\n\n# Put everything in a data.frame\ndata &lt;- data.frame(x = x_all, y = y_all)\n\n# Generate 10 training/validation splits\nsplits &lt;- generate_splits(data, train_prop = 0.7, K = 10)\nwrite.csv(splits, './splits.csv')\n\n\n# Fit a model with different smoothing parameters on each split\nsplits_number &lt;- unique(splits$split)\n\nresults &lt;- list()\nfor (n_split in splits_number) {\n  \n  df_split &lt;- splits |&gt; dplyr::filter(split == n_split)\n  train &lt;- df_split |&gt; \n    dplyr::filter(set == 'train') |&gt; \n    dplyr::select(x, y)\n  validation &lt;- df_split |&gt; \n    dplyr::filter(set == 'validation') |&gt; \n    dplyr::select(x, y)\n  \n  results_splits &lt;- list()\n  for (s in spans) {\n\n    # Fit loess model with span = s on train and predict on validation\n    model &lt;- loess(y ~ x, data = train, span = s, degree = 2)\n    pred &lt;- predict(model, newdata = validation$x)\n      \n    # For each point in x_grid, compute bias², variance, MSE\n    bias2 &lt;- mean((pred - validation$y)^2, na.rm = TRUE)\n    var_pred &lt;- var(pred, na.rm = TRUE)\n    mse &lt;- bias2 + var_pred\n    \n    results_splits[[as.character(s)]] &lt;- data.frame(\n      split = n_split,\n      span = s,\n      MSE = mse\n    )\n  }\n  results_splits_df &lt;- bind_rows(results_splits)\n  results[[as.character(n_split)]] &lt;- results_splits_df\n}\n\nresults &lt;- results |&gt; bind_rows()\nwrite.csv(results, './splits_mse.csv')\n\n\n\n\nCode\ndata = FileAttachment(\"../../include/data/model-evaluation/splits.csv\").csv({ typed: true })\ndata_mse = FileAttachment(\"../../include/data/model-evaluation/splits_mse.csv\").csv({ typed: true })\n\nviewof split = Inputs.range(\n  [1, 10], \n  {value: 1, step: 1, label: \"Autre jeu de validation\"}\n)\n\nfiltered = data.filter(function(df) {\n  return df.split == split;\n})\n\ncurrent_mse = data_mse.filter(function(df) {\n  return df.split == split;\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nPlot.plot({\n  grid: true,\n  x: {\n    domain: [0, 1],\n    label: \"X\",\n  },\n  y: {\n    domain: [0, 3],\n    label: \"Y\",\n  },\n  color: {\n    domain: [\"train\", \"validation\"],\n    range: [\"#f7a941\", \"#e30513\"],\n    legend: true\n  },\n  marks: [\n    Plot.dot(\n      filtered,\n      {x: \"x\", y: \"y\", fill: \"set\", r: 5}\n    ),\n  ]\n})\n\n\nCode\nPlot.plot({\n  grid: true,\n  color: {\n    legend: true,\n  },\n  x: {\n    domain: [0, 1],\n    label: \"λ\",\n  },\n  y: {\n    domain: [0.1, 0.2],\n    label: \"Y\",\n  },\n  marks: [\n    Plot.line(\n      data_mse, \n      {\n        x: \"span\", y: \"MSE\", z: \"split\",\n        stroke: \"#AAAAAA\", strokeWidth: 2\n      }\n    ),\n    Plot.line(\n      current_mse,\n      {x: \"span\", y: \"MSE\", stroke: \"#e30513\", strokeWidth: 5}\n    ),\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Différents jeux d’entraînement et validation.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) MSE pour différents jeux de validation.\n\n\n\n\n\n\nFigure 2: Illustration de l’approche par jeu de données de validation.\n\n\n\n\n\n\n\n\n\n\nTipHow to choose the size of the subsets?\n\n\n\nIn general:\n\nIf we have a large number of observations (say, several thousand), we can do a 50-50 split.\nIf we have fewer observations, we would prefer to keep more observations for training. We can, for example, do a 70-30 or 80-20 split.\n\nHowever, there is no universal rule. The right choice depends on the context, the complexity of the model, and the amount of data available.\n\n\nHowever, this method has two drawbacks. The first is that the error estimate is unstable. Indeed, the value of the prediction error depends on the observations in the validation set. A different validation set may give a different result. The second is that there is less data to fit the model. Since part of the data is reserved for validation, the model is trained on a smaller set, and this may therefore overestimate its true error compared to if it had been trained on the full dataset.",
    "crumbs": [
      "Modules",
      "03 - Généralités",
      "Models evaluation"
    ]
  },
  {
    "objectID": "contents/generalities/05-model-evaluation.html#cross-validation",
    "href": "contents/generalities/05-model-evaluation.html#cross-validation",
    "title": "Models evaluation",
    "section": "Cross-validation",
    "text": "Cross-validation\nTo overcome the limitations of the previous approach, cross-validation is often used. This method is more robust and stable. The principle is to repeat the validation set approach several times on different subsets of the dataset.\nThe approach consists of randomly dividing the set of observations into K subsets of equal size (called folds). The first fold is used as the validation dataset and the model is fitted on the remaining K - 1 folds. The prediction error is calculated on the first fold. This procedure is repeated K times; each time, a different fold is used as the validation dataset. At the end, we have \\(K\\) values ​​for the prediction error. Finally, we calculate the average of the \\(K\\) prediction values. Figure 3 presents a diagram of this approach.\n\n\n\n\n\n\nFigure 3: Diagram of the cross-validation approach\n\n\n\n\n\n\n\n\n\nTipHow to choose the number of subsets \\(K\\)?\n\n\n\nThe choice of the number of subsets \\(K\\) has an impact on the quality of the prediction error estimation, as well as on the computational cost of the procedure. In practice, \\(K = 5\\) or \\(K = 10\\) are often used. This choice is based on a compromise between the accuracy of the error estimation and the required computational time. Indeed, for each value of \\(K\\), the model is fitted \\(K\\) times. Therefore, the larger \\(K\\) is, the greater the computational cost.\nIn the limiting case where \\(K = n\\), i.e. \\(K\\) is equal to the number of observations in the dataset, this is called leave-one-out cross-validation (LOOCV). In this case, each observation is used as validation once, and the model is trained on the other \\(n - 1\\) observations.\nThis choice of \\(K = n\\) minimizes bias in the prediction error estimate, because at each iteration, the model is fitted to almost all the observations in the dataset. However, this comes at the cost of high variance. Indeed, since the training sets are almost identical, the prediction errors are highly correlated, making the overall error estimate unstable.\nConversely, lower values ​​of \\(K\\) introduce a slight bias in the error estimate (because the models are fitted to sets containing fewer observations), but reduce the variance of this estimate. This bias/variance tradeoff, coupled with a significant reduction in computation time, explains why \\(K = 5\\) or \\(K = 10\\) are standard choices in practice.\n\n\nFinally, cross-validation is a general method that can be applied to most models.",
    "crumbs": [
      "Modules",
      "03 - Généralités",
      "Models evaluation"
    ]
  },
  {
    "objectID": "contents/dimension/02-ca.html",
    "href": "contents/dimension/02-ca.html",
    "title": "Correspondence analysis",
    "section": "",
    "text": "Correspondence analysis (CA) is an exploratory analysis method that aims to graphically represent the relationships between the modalities of two qualitative variables. It allows the row profiles (in \\(\\mathbb{R}^p\\)) and column profiles (in \\(\\mathbb{R}^n\\)) of a contingency table to be represented simultaneously in a low-dimensional space, while preserving the \\(\\chi^2\\) distance as closely as possible. The objective of AFC is to find a two-dimensional (or three-dimensional) representation in which the geometric proximity between points best reflects the similarities between the modalities.",
    "crumbs": [
      "Modules",
      "04 - Dimension",
      "Correspondence analysis"
    ]
  },
  {
    "objectID": "contents/dimension/02-ca.html#notation",
    "href": "contents/dimension/02-ca.html#notation",
    "title": "Correspondence analysis",
    "section": "Notation",
    "text": "Notation\nConsider a contingency table \\(K = (k_{ij})\\), where \\(k_{ij}\\) is the number of individuals belonging to class \\(i \\in \\{ 1, \\dots, n \\}\\) and category \\(j \\in \\{ 1, \\dots, p \\}\\). We then work with the relative frequency table by normalizing this table. Since the frequencies are proportional to the sample size \\(n\\), the relative frequency table contains more information. Let \\(F = (f_{ij})\\), where \\[f_{ij} = \\frac{k_{ij}}{k_{\\bullet\\bullet}} = \\frac{k_{ij}}{\\sum_{l = 1}^{n} \\sum_{m = 1}^{p} k_{lm}}.\\]\nThe row (resp. column) margins of the table correspond to the sum of the columns for each row (resp. the sum of the rows for each column): \\[\\begin{align}\nf_{i \\bullet} &= \\sum_{j = 1}^{p} f_{ij} = \\frac{k_{i \\bullet}}{k_{\\bullet\\bullet}}, \\quad 1 \\leq i \\leq n; \\\\\nf_{\\bullet j} &= \\sum_{i = 1}^{n} f_{ij} = \\frac{k_{\\bullet j}}{k_{\\bullet\\bullet}}, \\quad 1 \\leq j \\leq p.\n\\end{align}\\]\nWe have \\(f_{\\bullet \\bullet} = \\sum_{i = 1}^{n} f_{i \\bullet} = \\sum_{j = 1}^{p} f_{\\bullet j} = 1\\).\n\n\n\n\n\n\nNoteExample\n\n\n\nAs an example, let’s consider the major and admission type of students enrolled in the STT-2200 course in the fall of 2025.\n\nContingency table of students enrolled in the STT-2200 course (Fall 2025) cross-referencing their major and type of admission.\n\n\n\nCollege\nLaval University\nOther university\nOutside Quebec\n\n\n\n\nActuarial Science\n2\n0\n0\n1\n\n\nStatistics\n2\n4\n1\n0\n\n\nBioinformatics\n4\n2\n0\n2\n\n\nFinance\n2\n0\n0\n0\n\n\nMath\n1\n0\n0\n0\n\n\nComputer Science\n2\n1\n0\n1\n\n\n\nHere, we find \\(k_{\\bullet \\bullet} = 25\\). This is simply the number of students enrolled in the course. We therefore find the following frequency table:\n\nFrequency table associated with the previous contingency table.\n\n\n\n\n\n\n\n\n\n\n\nCollege\nLaval University\nOther university\nOutside Quebec\n\\(f_{i \\bullet}\\)\n\n\n\n\nActuarial Science\n0.08\n0\n0\n0.04\n0.12\n\n\nStatistics\n0.08\n0.16\n0.04\n0\n0.28\n\n\nBioinformatics\n0.16\n0.08\n0\n0.08\n0.32\n\n\nFinance\n0.08\n0\n0\n0\n0.08\n\n\nMath\n0.04\n0\n0\n0\n0.04\n\n\nInfo\n0.08\n0.04\n0\n0.04\n0.16\n\n\n\\(f_{\\bullet j}\\)\n0.52\n0.28\n0.04\n0.16\n1",
    "crumbs": [
      "Modules",
      "04 - Dimension",
      "Correspondence analysis"
    ]
  },
  {
    "objectID": "contents/dimension/02-ca.html#statistical-independence",
    "href": "contents/dimension/02-ca.html#statistical-independence",
    "title": "Correspondence analysis",
    "section": "Statistical independence",
    "text": "Statistical independence\nThe relative frequency table \\(F = (f_{ij})\\) can be interpreted as an estimate of the joint probabilities of the modalities of the two qualitative variables. If the two variables are statistically independent, we expect the joint probability to approach the product of the marginal probabilities: \\[f_{ij} \\approx f_{i \\bullet} f_{\\bullet j}, \\quad i \\in \\{ 1, \\dots, n \\},~ j \\in \\{ 1, \\dots, p \\}.\\]\nTo test whether the observed differences between \\(f_{ij}\\) and \\(f_{i \\bullet} f_{\\bullet j}\\) are significant, we use the \\(\\chi^2\\) test of independence: \\[T = \\sum_{i = 1}^{n} \\sum_{j = 1}^{p} \\frac{\\left( k_ {ij} - \\mathbb{E}(k_{ij}) \\right)^2}{\\mathbb{E}(k_{ij})} = \\sum_{i = 1}^{n} \\sum_{j = 1}^{p} \\frac{\\left( k_ {ij} - \\frac{k_{i \\bullet}k_{\\bullet j}}{k_{\\bullet\\bullet}} \\right)^2}{\\left( \\frac{k_{i \\bullet} k_{\\bullet j}}{k_{\\bullet\\bullet}} \\right)}.\\] Under the assumption of independence, this statistic approximately follows a \\(\\chi^2\\) distribution. If the variables are independent, the statistic \\(T\\) should be close to \\(0\\).",
    "crumbs": [
      "Modules",
      "04 - Dimension",
      "Correspondence analysis"
    ]
  },
  {
    "objectID": "contents/dimension/02-ca.html#row-profiles-and-column-profiles",
    "href": "contents/dimension/02-ca.html#row-profiles-and-column-profiles",
    "title": "Correspondence analysis",
    "section": "Row profiles and column profiles",
    "text": "Row profiles and column profiles\nTo analyze the structures in the contingency table, we introduce the concept of a profile. Each row of the table can be viewed as a row profile \\[L_i = \\left( \\frac{k_{i 1}}{k_{i \\bullet}}, \\dots, \\frac{k_{i p}}{k_{i \\bullet}} \\right) = \\left( \\frac{f_{i 1}}{f_{i \\bullet}}, \\dots, \\frac{f_{i p}}{f_{i \\bullet}} \\right).\\] The line profile represents the distribution of the modalities \\(i\\) of the first variable among the modalities of the second.\nSimilarly, each column of the table can be viewed as a column profile \\[C_j = \\left( \\frac{k_{1 j}}{k_{\\bullet j}}, \\dots, \\frac{k_{n j}}{k_{\\bullet j}} \\right) = \\left( \\frac{f_{1 j}}{f_{\\bullet j}}, \\dots, \\frac{f_{n j}}{f_{\\bullet j}} \\right)\n.\\] The column profile represents the distribution of the modalities \\(j\\) of the second variable among the modalities of the first.\nWe can then look at the mean row profile (resp. mean column profile) obtained as the weighted average of the row profiles (resp. column profiles). In other words, they correspond to the marginal frequencies of the columns (resp. marginal frequencies of the rows). The mean row profile is given by \\[\\left( \\sum_{i = 1}^{n} f_{i \\bullet} \\frac{f_{i 1}}{f_{i \\bullet}}, \\dots, \\sum_{i = 1}^{n} f_{i \\bullet} \\frac{f_{i p}}{f_{i \\bullet}} \\right) = \\left( f_{\\bullet 1}, \\dots, f_{\\bullet p} \\right),\\] and the average column profile is given by \\[\\left( \\sum_{j = 1}^{p} f_{{\\bullet j}} \\frac{f_{1 j}}{f_{\\bullet j}}, \\dots, \\sum_{j = 1}^{p} f_{\\bullet j} \\frac{f_{n j}}{f_{\\bullet j}} \\right) = \\left( f_{{1 \\bullet}}, \\dots, f_{n \\bullet} \\right).\\]\nIf the variables are independent, all profiles are equal to their respective average profiles. In other words, for all \\(i \\in \\{ 1, \\dots, n \\}\\) and \\(j \\in \\{ 1, \\dots, p \\}\\), \\[\\left( \\frac{f_{i 1}}{f_{i ⊗}}, \\dots, \\frac{f_{i p}}{f_{i ⊗}} \\right) = \\left( f_{\\bullet 1}, \\dots, f_{\\bullet p} \\right) \\quad\\text{and}\\quad\n\\left( \\frac{f_{1 j}}{f_{\\bullet j}}, \\dots, \\frac{f_{n j}}{f_{\\bullet j}} \\right) = \\left( f_{{1 \\bullet}}, \\dots, f_{n \\bullet} \\right).\\] Thus, the further the profiles deviate from their means, the more the variables show dependence.\nTo measure the difference between two line profiles, we use the \\(\\chi^2\\) distance weighted by marginal frequencies: \\[d^2(L_i, L_{i^\\prime}) = \\sum_{j = 1}^{p} \\frac{1}{f_{\\bullet j}} \\left( \\frac{f_{ij}}{f_{i \\bullet}} - \\frac{f_{i^\\prime j}}{f_{i^\\prime \\bullet}} \\right)^2.\\] We can do the same for the difference between two column profiles: \\[d^2(C_j, C_{j^\\prime}) = \\sum_{i = 1}^{n} \\frac{1}{f_{i \\bullet}} \\left( \\frac{f_{ij}}{f_{\\bullet j}} - \\frac{f_{i j^\\prime}}{f_{\\bullet j^\\prime}} \\right)^2.\\]\nThis can be written in matrix form. Let \\(D_n = \\text{diag}(f_{i \\bullet})\\) be the diagonal matrix of row weights and \\(D_p = \\text{diag}(f_{\\bullet j})\\) be the diagonal matrix of column weights. The matrix \\(D_n^{-1}F\\) has the row profiles as its rows, and the matrix \\(D_p^{-1}F^{\\top}\\) has the column profiles as its rows. The \\(\\chi^2\\) distance between two row profiles \\(L_i\\) and \\(L_{i^\\prime}\\) is then written as \\[d^2(L_i, L_{i^\\prime}) = (L_i - L_{i^\\prime})^\\top D_p^{-1} (L_i - L_{i^\\prime}),\\] and similarly for two column profiles \\(C_j\\) and \\(C_{j^\\prime}\\) \\[d^2(C_j, C_{j^\\prime}) = (C_j - C_{j^\\prime})^\\top D_n^{-1} (C_j - C_{j^\\prime}).\\]\nThese distances form the basis of geometric representation in correspondence analysis, where we seek a projection of the profiles into a low-dimensional space that best preserves these distances.",
    "crumbs": [
      "Modules",
      "04 - Dimension",
      "Correspondence analysis"
    ]
  },
  {
    "objectID": "contents/dimension/02-ca.html#estimation-of-eigenvalues",
    "href": "contents/dimension/02-ca.html#estimation-of-eigenvalues",
    "title": "Correspondence analysis",
    "section": "Estimation of Eigenvalues",
    "text": "Estimation of Eigenvalues\nThe analysis of line profiles is called direct analysis. We consider the line profiles contained in the matrix \\(D_n^{-1}F \\in \\mathbb{R}^{n \\times p}\\). The row profiles are projected into a space equipped with the \\(\\chi^2\\) metric on the columns, defined by \\[\\left\\langle x, y \\right\\rangle = x^{\\top} D_p^{-1} y.\\]\nThe analysis of the column profiles is called dual analysis. We consider the column profiles contained in the matrix \\(D_p^{-1} F^{\\top} \\in \\mathbb{R}^{p \\times n}\\). We project the column profiles into a space equipped with the \\(\\chi^2\\) metric on the rows, defined by \\[\\left\\langle x, y \\right\\rangle = x^{\\top} D_n^{-1} y.\\]\nFor direct analysis, we seek the first factorial axis, i.e., the direction \\(u \\in R^p\\) that maximizes the projected variance of the row profiles, under the constraint that \\(u\\) is normalized. We therefore seek \\[\\max_{u} u^{\\top} D_p^{-1} F^{\\top} D_n^{-1} F D_p^{-1} u, \\quad\\text{s.c.}\\quad u^{\\top} D_p^{-1} u = 1.\\] This optimization problem is equivalent to finding the first eigenvector of the matrix \\[S = F^{\\top} D_n^{-1} F D_p^{-1}.\\] The matrix \\(S\\) plays a role analogous to the covariance matrix in PCA. The first eigenvector \\(u_1\\) therefore satisfies the relation \\[S u_{1} = F^{\\top} D_n^{-1} F D_p^{-1} u_1 = \\lambda_{1} u_1,\\] where \\(\\lambda_1\\) is the eigenvalue associated with \\(u_1\\). The eigenvectors of the matrix \\(S\\) give the factorial axes in the column space. The coordinates of the line profiles on the first factorial axis are obtained by the relation \\[\\Phi_1 = D_n^{-1} F D_p^{-1} u_1.\\] The other pairs of eigenvalues and eigenvectors, as well as the coordinates of the line profiles on the associated factorial axes, are obtained in a similar manner.\nThe dual analysis is performed in a similar way. We seek the first eigenvector of the matrix \\[T = F D_p^{-1} F^{\\top} D_n^{-1}.\\] The first eigenvector \\(v_1\\) therefore satisfies the relation \\[T v_{1} = F D_p^{-1} F^{\\top} D_n^{-1} v_1 = \\mu_1 v_1,\\] where \\(\\mu_{1}\\) is the eigenvalue associated with \\(v_{1}\\). The eigenvectors of the matrix \\(T\\) give the factorial axes in the row space. The coordinates of the column profiles on the first factorial axis are obtained by the relation \\[\\Psi_1 = D_p^{-1} F^{\\top} D_n^{-1} v_1 .\\] The other pairs of eigenvalues and eigenvectors, as well as the coordinates of the column profiles on the associated factorial axes, are obtained in a similar manner.\n\n\n\n\n\n\nImportantProperty\n\n\n\nThe matrices \\(S\\) and \\(T\\) have the same \\(r = \\min(n - 1, p - 1)\\) first positive eigenvalues. This guarantees a consistent representation of the rows and columns in the same reduced space. For \\(k = 1, \\dots, r\\), the relationships between the eigenvectors \\(u_k\\) and \\(v_k\\) are \\[u_k = \\frac{1}{\\sqrt{\\lambda_k}} F^{\\top} D_n^{-1} v_k \\quad \\text{and} \\quad v_k = \\frac{1}{\\sqrt{\\lambda_k}} F D_p^{-1} u_k.\\]\n\n\n\n\n\n\n\n\nTipProof\n\n\n\n\n\nStarting from the equation \\[T v_{1} = F D_p^{-1} F^{\\top} D_n^{-1} v_1 = \\mu_1 v_1,\\] multiplying on the left by \\(F^{\\top} D_n^{-1}\\), we obtain: \\[F^{\\top} D_n^{-1} F D_p^{-1} F^{\\top} D_n^{-1} v_1 = \\mu_1 F^{\\top} D_n^{-1} v_1.\\] Thus, the vector \\(F^{\\top} D_n^{-1} v_1\\) is an eigenvector of the matrix \\(F^{\\top} D_n^{-1} F D_p^{-1}\\) associated with the eigenvalue \\(\\mu_1\\). Since \\(\\lambda_1\\) is the largest eigenvalue of \\(F^{\\top} D_n^{-1} F D_p^{-1}\\), we can deduce that \\(\\mu_1 \\leq \\lambda_{1}\\). Proceeding in the same way, starting from \\(S u_1 = \\lambda_{1} u_{1}\\), we deduce that \\(\\lambda_{1} \\leq \\mu_{1}\\). Therefore, \\(\\lambda_{1} = \\mu_{1}\\). We can then do the same for the first \\(r\\) eigenvalues. We can also deduce the relationships between the eigenvalues.\n\n\n\n\n\n\n\n\n\nTipNote\n\n\n\nBy centering the profiles, we can project the line profiles and column profiles onto the same coordinate system, thus facilitating joint geometric interpretation.\n\n\nCenter of gravity and inertia\nIn statistical software outputs, the scatter plot generated by an AFC is generally centered at (0, 0). This convention reflects an analysis relative to the centers of gravity of the line profiles and column profiles. This centering is both practical and interpretable. It shows the distances between the modalities relative to their weighted mean, i.e., relative to the average behavior in the population.\nEach modality (row or column) is associated with a weight corresponding to its marginal frequency: the weight of the \\(i\\)th row is \\(f_{i \\bullet}\\) and the weight of the \\(j\\)th column is \\(f_{\\bullet j}\\). The center of gravity of the rows is the weighted average of the row profiles:\n\\[G_L = \\left( g_{1}, \\dots, g_{p} \\right)^{\\top}, \\quad \\text{where}\\quad g_j = \\sum_{i = 1}^{n} f_{i \\bullet} \\frac{f_{i j}}{f_{i \\bullet}} = \\sum_{i = 1}^{n} f_{i j} = f_{\\bullet j}, j \\in \\{ 1, \\dots, p \\}.\\]\nSimilarly, the center of gravity of the columns is \\[G_C = \\left( f_{1 \\bullet}, \\dots, f_{n \\bullet} \\right)^{\\top}.\\]\nTo re-center the profiles around the center of gravity, we subtract their mean value: \\[\\frac{f_{i j}}{f_{i \\bullet}} - g_{j} = \\frac{f_{i j}}{f_{i \\bullet}} - f_{\\bullet j} = \\frac{f_{i j} - f_{i \\bullet} f_{\\bullet j}}{f_{i \\bullet}}.\\] This centering ensures that each line profile \\(i \\in \\{ 1, \\dots, n \\}\\) is averaged to zero: \\[\\sum_{j = 1}^{p} \\frac{f_{i j} - f_{i \\bullet}f_{\\bullet j}}{f_{i \\bullet}} = 0.\\]\nAFC is therefore no longer performed on matrix \\(S\\) but rather on a centered matrix \\(S^\\star = (s_{j j^\\prime}^\\star),\\) where \\[s_{j j^\\prime}^\\star = \\sum_{i = 1}^{n} \\frac{\\left( f_{i j} - f_{i \\bullet} f_{\\bullet j} \\right) \\left( f_{i j^\\prime} - f_{i \\bullet} f_{\\bullet j^\\prime} \\right)}{f_{i \\bullet} f_{\\bullet j^\\prime}}.\\]\nBy definition, the trace of the matrix \\(S^\\star\\) gives the total inertia: \\[\\text{tr}(S^\\star) = \\sum_{j = 1}^{p} \\sum_{i = 1}^{n} \\frac{\\left( f_{i j} - f_{i \\bullet}f_{\\bullet j} \\right)^2}{f_{i \\bullet} f_{\\bullet j}}.\\] This corresponds to the normalized \\(\\chi^2\\) statistic used to test the independence between variables.\n\n\n\n\n\n\nImportantProperty\n\n\n\nWe have that, for all \\(j, j^\\prime \\in \\{ 1, \\dots, p \\}\\), \\(s_{j j^\\prime}^\\star = s_{j j^\\prime} - f_{\\bullet j},\\) where \\[s_{j j^\\prime} = \\sum_{i = 1}^{n} \\frac{f_{i j}f_{i j^\\prime}}{f_{i \\bullet} f_{\\bullet j^\\prime}}.\\]\n\n\n\n\n\n\n\n\nTipProof\n\n\n\n\n\n\n\n\n\nThe previous property implies that the matrices \\(S\\) and \\(S^\\star\\) have the same eigenvectors for the first \\(p\\) dimensions, which allows us to perform factorial analysis on the centered version.",
    "crumbs": [
      "Modules",
      "04 - Dimension",
      "Correspondence analysis"
    ]
  },
  {
    "objectID": "contents/dimension/02-ca.html#factor-coordinates",
    "href": "contents/dimension/02-ca.html#factor-coordinates",
    "title": "Correspondence analysis",
    "section": "Factor coordinates",
    "text": "Factor coordinates\nWe have that, for all \\(k = 1, \\dots, r\\), \\[\\Phi_k = D_n^{-1} F D_p^{-1} u_k \\quad \\text{and} \\quad \\Psi_k = D_p^{-1} F^{\\top} D_n^{-1} v_k.\\] However, we have also seen the relationships between the eigenvectors \\(u_k\\) and \\(v_k\\), \\[u_k = \\frac{1}{\\sqrt{\\lambda_k}} F^{\\top} D_n^{-1} v_k \\quad \\text{and} \\quad v_k = \\frac{1}{\\sqrt{\\lambda_k}} F D_p^{-1} u_k.\\] We can therefore deduce the relationships between the factorial coordinates of the row profiles and those of the column profiles: \\[\\Phi_k = \\frac{1}{\\sqrt{\\lambda_k}} D_n^{-1} F \\Psi_k \\quad \\text{and} \\quad \\Psi_k = \\frac{1}{\\sqrt{\\lambda_k}} D_p^{-1} F^{\\top} \\Phi_k.\\]\nWe can now examine these relationships on each of the components: \\[\\left[ \\Phi_k \\right]_i = \\frac{1}{\\sqrt{\\lambda_k}} \\sum_{j = 1}^{p} \\frac{f_{ij}}{f_{i \\bullet}} \\left[ \\Psi_k \\right]_j \\quad \\text{and} \\quad \\left[ \\Psi_k \\right]_j = \\frac{1}{\\sqrt{\\lambda_k}} \\sum_{i = 1}^{n} \\frac{f_{ij}}{f_{\\bullet j}} \\left[ \\Phi_k \\right]_i,\\] where \\(\\left[ \\Phi_k \\right]_i\\) denotes the coordinate of the line profile \\(L_i\\) on the \\(k\\)th factorial axis and \\(\\left[ \\Psi_k \\right]_j\\) denotes the coordinate of the column profile \\(C_j\\) on the same factorial axis. These relations express, to within a factor of \\(1 / \\sqrt{\\lambda_k}\\), that each line profile is at the barycenter of the projections of the column profiles assigned the weight of column \\(j\\) in line \\(i\\) and that each column profile is at the barycenter of the projections of the line profiles assigned the weight of line \\(i\\) in column \\(j\\).\n\n\n\n\n\n\nTipNote\n\n\n\nThus, in AFC, we have a double barycentric representation. On the factorial axes, each point in one cloud is at the barycenter of the points in the other cloud.",
    "crumbs": [
      "Modules",
      "04 - Dimension",
      "Correspondence analysis"
    ]
  }
]