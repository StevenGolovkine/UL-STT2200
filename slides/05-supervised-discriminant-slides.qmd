--- 
title: "Supervisée"
subtitle: "Analyse discriminante"
author: "Steven Golovkine"
lang: fr
date: today
date-format: "DD MMM YYYY"
slide-number: c/t
title-logo: ../include/logo-ul.png
format:
  revealjs:
    theme: [default, ../include/ulaval-slide.scss]
    chalkboard: true
    width: 1280
    height: 720
    template-partials:
      - title-slide.html
--- 

# Analyse discriminante

## Introduction

::: {.incremental}
- **Objectif**: Classer des individus dans différents groupes
- **Méthode supervisée**: Les groupes sont connus dans les données d'apprentissage
- **Variables**: Variables explicatives continues
- **But**: Apprendre une règle de classification optimale
:::

## Notation

::: {.fragment}
**Données**
- $X = (X_{ij}) \in \mathbb{R}^{n \times p}$ : matrice de données
- $n$ : nombre d'individus
- $p$ : nombre de variables
:::

::: {.fragment}
**Groupes**
- $K$ groupes dans la population
- $I_k$ : ensemble des individus du groupe $k$
- $n_k$ : nombre d'observations dans $I_k$
- $\sum_{k=1}^{K} n_k = n$
:::

## Objectif principal

::: {.fragment}
**Principe**
- Partitionner $\mathbb{R}^p$ en $K$ sous-ensembles
- Passer de $\mathbb{R}^p$ à $\mathbb{R}$ via un score
:::

::: {.fragment}
**Score de Fisher**
$$f(X_1, \ldots, X_p) = a^{\top} X + b = a_1 X_1 + \cdots + a_p X_p + b$$
:::

::: {.fragment}
**Simplification**
En centrant les variables: $-b = a^{\top} \overline{X}$

Il reste à déterminer le vecteur $a = (a_1, \ldots, a_p)$
:::

## Critère de Fisher

::: {.fragment}
**Idée centrale**
Optimiser le rapport entre:
- Variabilité **inter-groupes** (maximiser)
- Variabilité **intra-groupe** (minimiser)
:::

::: {.fragment}
**Matrices de variance-covariance**

- $S$ : matrice totale
- $W$ : matrice intra-groupe  
- $B$ : matrice inter-groupe

**Relation fondamentale**: $S = W + B$
:::

## Matrices de variance-covariance

**Matrice totale**
$$S = \sum_{i=1}^{n} (X_i - \overline{X})(X_i - \overline{X})^{\top}$$

**Matrice intra-groupe**
$$W = \sum_{k=1}^{K} \sum_{i \in I_k} (X_i - \overline{X}_k)(X_i - \overline{X}_k)^{\top}$$

**Matrice inter-groupe**
$$B = \sum_{k=1}^{K} n_k (\overline{X}_k - \overline{X})(\overline{X}_k - \overline{X})^{\top}$$

## Critère d'optimisation

::: {.fragment}
**Critère de Fisher**
$$J(a) = \frac{a^{\top} B a}{a^{\top} W a} = \frac{a^{\top} B a}{a^{\top} S a}$$
:::

::: {.fragment}
**Formulations équivalentes**
1. Maximiser $J(a)$ sous contrainte $a^{\top} a = 1$
2. Maximiser $a^{\top} B a$ sous contrainte $a^{\top} S a = 1$
3. Maximiser $c^{\top} S^{-1/2} B S^{-1/2} c$ sous contrainte $c^{\top} c = 1$
:::

## Solution optimale

::: {.fragment}
**Résultat**
$a$ est un vecteur propre normé associé à $\lambda_1$, la **plus grande valeur propre** de $S^{-1} B$
:::

::: {.fragment}
**Fonction discriminante**
$$f(x) = a^{\top} (x - \overline{X})$$
:::

::: {.fragment}
**Scores des observations**
$$U_i = a^{\top} (X_i - \overline{X})$$
Ces scores maximisent le rapport variance inter-groupe / variance intra-groupe
:::

## Pouvoir discriminant

::: {.fragment}
**Interprétation de $\lambda_1$**
$$\lambda_1 = \frac{a^{\top} B a}{a^{\top} S a} \quad \text{avec } 0 \leq \lambda_1 \leq 1$$
:::

::: {.incremental}
- $\lambda_1 = 1$ : **Cas idéal**
  - 100% de variabilité entre les groupes
  - Aucune variabilité intra-groupe

- $\lambda_1 = 0$ : **Analyse inutile**
  - Aucune variabilité entre les groupes
  - Toute la variabilité est intra-groupe
:::

## Règle de classification

::: {.fragment}
**Score moyen par groupe**
$$m_k = a^{\top} (\overline{X}_{k1}, \ldots, \overline{X}_{kp})^{\top}$$
:::

::: {.fragment}
**Classification d'une nouvelle observation $X_0$**
1. Calculer le score: $f(X_0) = a^{\top} X_0$
2. Assigner au groupe $k^*$ tel que:
$$k^* = \arg\min_{k} |a^{\top} X_0 - m_k|$$
:::

## Cas binaire ($K = 2$)

::: {.fragment}
**Solution explicite**
$$C = \sqrt{\frac{n_1 n_2}{n}} (\overline{X}_1 - \overline{X}_2)$$
$$B = CC^{\top}, \quad a = S^{-1}C$$
:::

::: {.fragment}
**Règle de classification**
Si $m_1 > m_2$, classer $X_0$ dans le groupe 1 si:
$$a^{\top} X_0 > \frac{m_1 + m_2}{2}$$
:::

::: {.fragment}
**Forme équivalente**
$$(\overline{X}_1 - \overline{X}_2)^{\top} S^{-1} X_0 > \frac{1}{2}(\overline{X}_1 - \overline{X}_2)^{\top} S^{-1} (\overline{X}_1 + \overline{X}_2)$$
:::

## Évaluation de la performance

::: {.fragment}
**Matrice de confusion**
- Appliquer la règle de classification à l'échantillon d'apprentissage
- Comparer les classifications prédites aux vraies classes
- Estimer les risques de mauvaise classification
:::

::: {.fragment}
**Avantages de l'analyse discriminante**
- Méthode linéaire simple et interprétable
- Solution analytique explicite
- Réduction de dimension naturelle ($\mathbb{R}^p \to \mathbb{R}$)
:::

## Merci pour votre attention

**Questions ?**

::: {.fragment}
*L'analyse discriminante de Fisher : une méthode élégante pour la classification supervisée*
:::
