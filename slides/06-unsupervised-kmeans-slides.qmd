--- 
title: "Non-supervisée"
subtitle: "$k$-moyennes"
author: "Steven Golovkine"
lang: fr
date: "28 nov. 2025"
date-format: "DD MMM YYYY"
slide-number: c/t
title-logo: ../include/logo-ul.png
format:
  revealjs:
    theme: [default, ../include/ulaval-slide.scss]
    chalkboard: true
    width: 1280
    height: 720
    template-partials:
      - title-slide.html
--- 


## Plan


<br>

1. Généralités

2. $k$-moyennes

3. Extension

4. Exemple


## Fonction de classification

<br>

$$C: \{1, \ldots, n\} \to \{1, \ldots, K\}$$
Associe à chaque observation $i$ une étiquette de groupe $C(i)$.

## Fonction de coût

<br>

$$W(C) = \sum_{k=1}^{K} \sum_{i: C(i)=k} \sum_{j: C(j)=k} d(X_i, X_j)$$
où $d(X_i, X_j)$ mesure la dissimilarité entre observations.

. . .

<br>

Objectif → Minimiser $W(C)$ (cohésion intra-groupe).


## Complexité du problème

<br>

Problème d'optimisation combinatoire :

- $K^n$ combinaisons possibles.

- Exploration exhaustive impossible même pour petites valeurs.

. . .

Solution pratique : Algorithmes gloutons 

- Exploration d'un sous-ensemble restreint.

- Amélioration itérative progressive.

- Convergence vers un minimum local.


## Hypothèses pour les $k$-moyennes

<br>

1. Variables numériques, souvent centrées-réduites.

2. Nombre de groupes $K$ fixé à l'avance.


## Algorithme des $k$-moyennes

<br>

Objectif → Regrouper $n$ observations en $K$ groupes homogènes.

. . .

Mesure de dissimilarité → généralement distance euclidienne.

. . .

Stratégie :

- Observations proches au sein d'un groupe.

- Groupes éloignés entre eux.


## Algorithme détaillé

::: {.incremental}
1. Choix de $K$ : Nombre de groupes fixé à l'avance.

2. Initialisation : Partition aléatoire ou centres initiaux aléatoires.

3. Calcul des centroïdes :
   $$\mu_k = \frac{1}{N_k} \sum_{i: C(i)=k} X_i$$

4. Réaffectation : Chaque observation au centre le plus proche.

5. Itération : Répéter 3-4 jusqu'à stabilisation.
:::

## Convergence

<br>

Garantie de convergence en nombre fini d'itérations.

. . .

Chaque étape réduit l'inertie intra-groupe (somme des distances observations / centroïdes).

. . .

⚠️ Limitation importante → Convergence vers un minimum local, pas nécessairement global.


## Limites des $k$-moyennes

::: {.incremental}
1. Sensibilité à l'initialisation

   - Résultats variables selon centres initiaux.

   - Solution : Algorithme $k$-means++.

2. Choix de $K$ à l'avance

   - Pas toujours évident.

   - Critères : Silhouette, méthode du coude, BIC/AIC.

3. Coût computationnel

   - Recalcul de toutes les distances à chaque itération.

   - Problématique pour grand $N$ ou $P$.
:::

## Limites

<br>

4. Sensibilité aux valeurs extrêmes

   - La moyenne est influencée par les observations atypiques.

   - Les centroïdes sont faussés.

   - Les regroupements sont incohérents.

. . .

Conséquence → Nécessité de prétraiter les données (détection d'outliers).


## $k$-médoïdes

<br>

Différence avec $k$-means → Utilise des observations réelles comme représentants (pas des moyennes).

. . .

Médoïde → Observation qui minimise la somme des distances aux autres observations du même groupe.

. . .

Avantage conceptuel → Représentants concrets et interprétables


## Avantages des $k$-médoïdes

<br>

::: {.incremental}
1. Robustesse aux valeurs extrêmes

   - Médoïdes moins sensibles que moyennes.

   - Meilleure résistance aux outliers.

2. Flexibilité

   - Matrice de dissimilarité personnalisées..

   - Adaptée à la nature des données (variables ordinales ou catégorielles).
:::

## Inconvénients des $k$-médoïdes

<br>

Problèmes partagés avec $k$-moyennes :

- Choix de $K$ à l'avance

- Convergence vers minimum local


. . .

Coût computationnel plus élevé :

- Calculs plus complexes pour identifier médoïdes

- Particulièrement avec distances non-euclidiennes

- Problématique pour grands jeux de données


## Comparaison des méthodes

<br>

| Aspect | $k$-moyennes | $k$-médoïdes |
|--------|--------------|--------------|
| Représentants | Centroïdes (moyennes) | Observations réelles |
| Robustesse | Sensible aux outliers | Plus robuste |
| Types de variables | Numériques | Numériques + ordinales |
| Coût computationnel | Modéré | Plus élevé |
| Interprétabilité | Abstraite | Concrète |


## Critères de choix de $K$

::: {.incremental}
1. Méthode du coude (Elbow method)

   - Graphique inertie vs $K$

   - Recherche du "coude"

2. Coefficient de silhouette

   - Mesure qualité des groupes

   - Valeurs entre -1 et 1

3. Critères d'information

   - BIC, AIC en cadre probabiliste

   - Pénalisent la complexité
:::


## Exemple {.smaller}

::: panel-tabset
### Code

```{r}
#| echo: true
#| eval: true
#| warning: false
library(tidyverse)

temp <- read.table('../include/data/unsupervised/temperature.csv', header = TRUE, sep = ";", dec = '.', row.names = 1)
df <- temp[1:23, 1:12]
city <- row.names(temp[1:23,])

model <- kmeans(df, centers = 3)
```


### Model

```{r}
#| echo: true
#| eval: true
#| warning: false
model
```

### Choix du nombre de clusters

```{r}
#| echo: true
#| eval: true
#| warning: false
n_clusters <- 10
wss <- numeric(n_clusters)
for (i in 1:n_clusters) {
  model <- kmeans(df, centers = i)
  wss[i] <- model$tot.withinss
}
plot(1:n_clusters, wss)
```


### Graphique 

```{r}
#| echo: true
#| eval: true
#| warning: false
model <- kmeans(df, centers = 3)
plot(df$January, df$September, col = model$cluster)
```

:::


## Récapitulatif

<br>

$k$-moyennes : Méthode classique, efficace mais limitations.

. . .

$k$-médoïdes : Alternative robuste mais a un coût plus élevé.

. . .

Choix du nombre de groupes : Combinaison de critères pratiques et statistiques


. . .

<br>

Prochaine étape → GMM