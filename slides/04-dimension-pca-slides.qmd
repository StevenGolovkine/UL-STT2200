--- 
title: "Dimension"
subtitle: "Analyse en composantes principales"
author: "Steven Golovkine"
lang: fr
date: today
date-format: "DD MMM YYYY"
slide-number: c/t
title-logo: ../include/logo-ul.png
format:
  revealjs:
    theme: [default, ../include/ulaval-slide.scss]
    chalkboard: true
    width: 1280
    height: 720
    template-partials:
      - title-slide.html
--- 

## Pourquoi réduire la dimension ?

Travailler avec un grand nombre de variables pose plusieurs défis :

- **Visualisation compliquée** : impossible de représenter au-delà de 3 dimensions
- **Séparation des classes difficile** : structure cachée dans des combinaisons
- **Coût computationnel élevé** : modèles complexes difficiles à ajuster
- **Corrélations fortes** : variables redondantes, modèles instables

**Question centrale** : Peut-on réduire la dimension sans perdre trop d'information ?

## L'approche PCA

**Principe** : Ne pas simplement supprimer des variables, mais construire de nouvelles variables.

**Méthode** : Créer des combinaisons linéaires des variables initiales qui résument l'information essentielle.

**Analyse en Composantes Principales (ACP)** : Méthode non-supervisée pour réduire la dimension tout en conservant le maximum d'information.

Introduite par Hotelling (1933).

## Applications courantes

1. **Visualisation** : Représenter des données multidimensionnelles

2. **Réduction dimensionnelle** : Passer de $p$ à $k \ll p$ variables

3. **Compression** : Images ou signaux

4. **Exploration** : Données biologiques, textuelles, environnementales

## Exemples concrets

- **Sport** : Comparer des équipes de hockey sur 6 statistiques
- **Criminologie** : Résumer la criminalité entre provinces sur 7 types de crimes
- **Imagerie** : Compresser des images de $1084 \times 1084$ pixels
- **Biologie** : Identifier des variants de tumeurs à partir de millions de gènes

## Formulation mathématique

Soit $X = (X_1, \ldots, X_p)^{\top}$ un vecteur aléatoire centré avec matrice de covariance $\Sigma$.

On cherche une combinaison linéaire :
$$Y_1 = \alpha_1^{\top} X = \sum_{k=1}^p \alpha_{1k} X_k$$

**Objectif** : Maximiser $\text{Var}(Y_1)$ sous contrainte $\|\alpha_1\| = 1$

## Problème d'optimisation

$$\max_{\alpha_1^{\top} \alpha_1 = 1} \text{Var}(Y_1) = \max_{\alpha_1^{\top} \alpha_1 = 1} \alpha_1^{\top} \Sigma \alpha_1$$

**Solution** (multiplicateurs de Lagrange) :
$$\Sigma \alpha_1 = \lambda_1 \alpha_1$$

où $\lambda_1$ est la plus grande valeur propre de $\Sigma$ et $\alpha_1$ le vecteur propre associé.

## Construction des composantes

Les composantes suivantes sont construites par orthogonalité :

$$\Sigma \alpha_k = \lambda_k \alpha_k, \quad \lambda_1 \geq \lambda_2 \geq \ldots \geq \lambda_p$$

**Composantes principales** :
$$Y_k = \alpha_k^{\top} X$$

où $\alpha_k$ est le vecteur propre associé à $\lambda_k$.

## Représentation matricielle

Soit $A = (\alpha_1, \ldots, \alpha_p) \in \mathbb{R}^{p \times p}$ la matrice des vecteurs propres.

$$Y = A^{\top} X$$

**Propriétés de $A$** :
- $A^{\top} A = AA^{\top} = I_p$ (orthogonale)
- $A^{\top} = A^{-1}$
- $\Sigma A = A \Lambda$ où $\Lambda = \text{diag}(\lambda_1, \ldots, \lambda_p)$

## Variance des composantes

$$\text{Var}(Y) = A^{\top} \Sigma A = \Lambda$$

**Conséquences** :
- $\text{Cov}(Y_k, Y_l) = 0$ si $k \neq l$ (orthogonalité)
- $\text{Var}(Y_k) = \lambda_k$ (variance décroissante)
- $\text{Var}(Y_1) \geq \text{Var}(Y_2) \geq \ldots \geq \text{Var}(Y_p)$

## Proportion de variance expliquée

**Variation totale** :
$$\text{tr}(\Sigma) = \sum_{i=1}^p \lambda_i = \sum_{k=1}^p \text{Var}(Y_k)$$

**Proportion expliquée par $Y_k$** :
$$\frac{\lambda_k}{\lambda_1 + \cdots + \lambda_p} = \frac{\text{Var}(Y_k)}{\text{tr}(\Sigma)}$$

**Proportion expliquée par les $m$ premières** :
$$100\% \times \frac{\sum_{k=1}^m \lambda_k}{\sum_{k=1}^p \lambda_k}$$

## Estimation pratique

En pratique, $\Sigma$ est inconnue. On estime par :

$$\widehat{\Sigma} = \frac{1}{n-1} \sum_{i=1}^n (X_i - \overline{X})(X_i - \overline{X})^{\top}$$

**Décomposition spectrale** :
$$\widehat{\Sigma} = \widehat{A} \widehat{\Lambda} \widehat{A}^{\top}$$

**Composantes estimées** :
$$Y_i = \widehat{A}^{\top} X_i$$

## Sensibilité à l'échelle

**Problème** : Variables avec grandes variances dominent l'ACP.

**Exemple** : Distance en mètres vs kilomètres multiplie la variance par $10^6$.

**Solution** : Standardiser les variables si elles ont des unités différentes.

**Équivalence** : ACP sur matrice de corrélations au lieu de covariances.

## Interprétation géométrique

**Rotation des axes** : $Y = A^{\top}X$ représente une rotation orthogonale.

**Nouveaux axes** : Directions de variation maximale successive.

**Scores** : $Y_{ik} = \alpha_k^{\top} X_i$ (coordonnées dans le nouveau système).

**Qualité de représentation** :
$$Q_{ik} = \frac{Y_{ik}^2}{\sum_{j=1}^p Y_{ij}^2}$$

## Choix du nombre de composantes

**1. Règle des 80%** : Retenir assez de composantes pour expliquer 80% de la variance.

**2. Règle de Kaiser** : Conserver les composantes avec $\lambda_k > 1$ (si matrice de corrélations).

**3. Règle de Joliffe** : Variante plus stricte, $\lambda_k > 0.7$.

**4. Règle de Cattell (coude)** : Chercher un point de rupture dans le graphique des valeurs propres.

## Règles de choix - Détails

**Règle des 80%** :
- Seuil arbitraire mais pratique
- Bon équilibre information/simplicité

**Règle de Kaiser** :
- Basée sur la moyenne des valeurs propres = 1
- Applicable uniquement avec standardisation

**Règle du coude** :
- Graphique $\lambda_k$ vs rang $k$
- Chercher changement de pente
- Plus subjective mais intuitive

## Considérations pratiques

**Usage en prédiction** : Utiliser les $k$ premières composantes comme prédicteurs.

**Avantages** :
- Réduit la dimension
- Élimine la multicolinéarité
- Conserve l'information essentielle

**Compromis** :
- Trop de composantes : pas de simplification
- Trop peu : perte d'information pertinente

## Conclusion

L'ACP est un outil puissant pour la réduction de dimension qui :

- **Préserve** l'information maximale
- **Élimine** la redondance
- **Facilite** la visualisation et la modélisation
- **Nécessite** des choix méthodologiques importants

Le succès dépend du choix approprié du nombre de composantes selon le contexte d'application.