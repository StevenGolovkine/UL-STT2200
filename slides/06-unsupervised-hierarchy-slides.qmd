--- 
title: "Non-supervisée"
subtitle: "Classification hiérarchique"
author: "Steven Golovkine"
lang: fr
date: "21 nov. 2025"
date-format: "DD MMM YYYY"
slide-number: c/t
title-logo: ../include/logo-ul.png
format:
  revealjs:
    theme: [default, ../include/ulaval-slide.scss]
    chalkboard: true
    width: 1280
    height: 720
    template-partials:
      - title-slide.html
--- 


## Plan

<br>

1. Généralités

2. Classifications ascendante et descendante

3. Méthodes d'aggrégation

4. En pratique : choix du nombre de groupes, cohérence de la partition

5. Exemple

## Motivation

<br>

- Série de partitions imbriquées

- De $n$ groupes (chaque observation isolée) à 1 groupe (toutes les observations ensemble)

- Représentation : Dendrogramme


## Types d'algorithmes


1. Algorithmes ascendants

   - Départ : $n$ groupes individuels

   - Processus : Fusions successives

. . .

2. Algorithmes descendants

   - Départ : 1 groupe avec toutes les observations

   - Processus : Divisions successives

. . .

Résultat commun : $n$ partitions hiérarchiques de 1 à $n$ groupes.


## Algorithmes descendants

<br>

1. Démarrage avec toutes les observations dans un groupe.

2. Division du groupe le moins homogène à chaque étape.

3. Maximisation de la dissimilarité entre sous-groupes.

. . .

Inconvénient majeur : Coût computationnel très élevé (évaluation de toutes les divisions possibles).


## Algorithmes ascendants

<br>

1. Démarrage avec $n$ groupes distincts.

2. Fusion des deux groupes les plus similaires à chaque étape.

3. Continuation jusqu'à un seul groupe.

. . .

Avantage : Plus efficace computationnellement que le descendant.

. . .

<br>

Enjeu clé dans les deux cas : Définir la distance entre groupes.



## Distance du plus proche voisin (*Single Linkage*)

<br>

Distance = plus petite distance entre individus des deux groupes.


$$d(A, B) = \min \{d_{ij}: i \in A, j \in B\}$$



## Distance du plus proche voisin (*Single Linkage*)

Avantages :

- Bon avec des variables de types différents.

- Groupes de formes irrégulières.

- Robuste aux données aberrantes.

- Bonnes propriétés théoriques.

. . .

Inconvénients :

- Groupes déséquilibrés.

- Moins performant pour groupes de forme régulière.


## Distance du voisin le plus distant (*Complete Linkage*)

<br>

Distance = plus grande distance entre individus des deux groupes.

$$d(A, B) = \max \{d_{ij}: i \in A, j \in B\}$$


## Distance du voisin le plus distant (*Complete Linkage*)

Avantages :

- Groupes réguliers et homogènes en taille.

- Adapté aux variables de types différents.

. . .

Inconvénients :

- Extrêmement sensible aux données aberrantes.

- Force des groupes de même taille.


## Distance de la moyenne (*Average Linkage*)

<br>

Distance = moyenne de toutes les distances inter-groupes.


$$d(A, B) = \frac{1}{n_A n_B} \sum_{i \in A} \sum_{j \in B} d(X_i, X_j)$$

## Distance de la moyenne (*Average Linkage*)


Avantages :

- Considère toutes les interactions.

- Produit des groupes homogènes (variance faible).

. . .

Inconvénients :

- Privilégie des groupes de variance similaire.

- Moins performant pour groupes de forme irrégulière.


## Méthode du centroïde

::: {.fragment}
**Définition**
$$d(A, B) = d(\overline{X}_A, \overline{X}_B)$$
Distance entre les moyennes des groupes
:::

::: {.fragment}
**Mise à jour après fusion**
$$\overline{X}_{AB} = \frac{n_A \overline{X}_A + n_B \overline{X}_B}{n_A + n_B}$$
:::

::: {.fragment}
**Propriété** : Assez robuste aux données aberrantes, mais peu performant sans elles
:::

## Distance de la médiane

::: {.fragment}
**Principe** : Mise à jour récursive des distances
:::

::: {.fragment}
**Formule de mise à jour**
$$d(AB, C) = \frac{d(A, C) + d(B, C)}{2} - \frac{d(A, B)}{4}$$
:::

::: {.fragment}
**Caractéristiques**
- Très robuste aux données aberrantes
- Peu efficace sans valeurs extrêmes
:::

## Méthode de Ward

::: {.fragment}
**Principe** : Minimisation de l'augmentation d'inertie intra-groupe
:::

::: {.fragment}
**Critère d'optimisation**
$$I_{AB} = SC_{AB} - SC_A - SC_B = \frac{d^2(\overline{X}_A, \overline{X}_B)}{\frac{1}{n_A} + \frac{1}{n_B}}$$
:::

::: {.fragment}
**Conditions optimales**
- Observations gaussiennes multivariées
- Même matrice de variance-covariance
- Moyennes différentes
:::

## Ward : Avantages et limites

::: {.fragment}
**Avantages**
- Très efficace sous hypothèses gaussiennes
- Groupes homogènes et de taille comparable
- Méthode très populaire en pratique
:::

::: {.fragment}
**Inconvénients**
- Sensible aux données aberrantes
- Tend à former des groupes de même taille
- Hypothèses gaussiennes restrictives
:::

## Distance flexible

::: {.fragment}
**Formule générale**
$$d(C, AB) = \alpha_A d(C, A) + \alpha_B d(C, B) + \beta d(A, B) + \gamma |d(C, A) - d(C, B)|$$
:::

::: {.fragment}
**Flexibilité**
Selon $(\alpha_A, \alpha_B, \beta, \gamma)$, on retrouve toutes les méthodes précédentes
:::

::: {.fragment}
**Paramétrage standard**
- $\alpha_A = \alpha_B$, $\alpha_A + \alpha_B + \beta = 1$, $\gamma = 0$
- Choix courant : $\beta = -0.25$
- Données aberrantes : $\beta = -0.5$
:::

# Choix de la partition

## Le problème du choix

::: {.fragment}
**Question centrale**
Parmi les $n$ partitions possibles, laquelle choisir ?
:::

::: {.fragment}
**Critères pratiques**
- Interprétabilité scientifique ou métier
- Pertinence opérationnelle
- Nombre de groupes $K$ déterminé à l'avance
:::

::: {.fragment}
**Critères statistiques**
Quand les considérations pratiques ne suffisent pas
:::

## Outils disponibles

::: {.fragment}
**Librairies spécialisées**
- R `NbClust` : ~30 critères
- Python `sklearn` : ~10 critères
:::

::: {.fragment}
**Adaptation**
Critères surtout adaptés aux variables continues
:::

# Critères basés sur l'inertie

## Décomposition de l'inertie

::: {.fragment}
**Inertie totale**
$$I_{\text{tot}} = \frac{1}{n}\sum_{i=1}^{n} d(X_i, G)$$
:::

::: {.fragment}
**Décomposition fondamentale**
$$I_{\text{tot}} = I_{\text{intra-groupe}} + I_{\text{inter-groupe}}$$
:::

::: {.fragment}
**Objectif** : Maximiser $I_{\text{inter-groupe}}$ (séparation des groupes)
:::

## Inertie inter-groupe et intra-groupe

::: {.fragment}
**Inertie inter-groupe** (séparation)
$$I_{\text{inter-groupe}} = \frac{1}{n}\sum_{k=1}^{K} n_k d(G_k, G)$$
:::

::: {.fragment}
**Inertie intra-groupe** (compacité)
$$I_{\text{intra-groupe}} = \frac{1}{n}\sum_{k=1}^{K} \sum_{i \in C_k} d(X_i, G_k)$$
:::

::: {.fragment}
**Principe** : Groupes compacts $\Rightarrow$ faible inertie intra-groupe
:::

## Critères dérivés

::: {.fragment}
**Pseudo-$R^2$**
$$\text{pseudo-}R^2 = \frac{I_{\text{inter-groupe}}}{I_{\text{tot}}}$$
Proportion d'inertie expliquée par la partition
:::

::: {.fragment}
**Statistique Calinski-Harabasz (CH)**
$$\text{CH} = \frac{I_{\text{inter-groupe}} / (K-1)}{I_{\text{intra-groupe}} / (n-K)}$$
Équilibre entre compacité et séparation
:::

# Critères basés sur la distance

## Indice de Dunn

::: {.fragment}
**Définition**
$$D = \frac{\text{Distance minimale entre 2 groupes}}{\text{Distance maximale dans un groupe}}$$
:::

::: {.fragment}
**Objectif** : Maximiser $D$
- Groupes denses (faible distance interne)
- Groupes bien séparés (grande distance externe)
:::

## Indice de silhouette

::: {.fragment}
**Pour une observation $X_i$**
$$S(X_i) = \frac{b_i - a_i}{\max(b_i, a_i)}$$
où :
- $a_i$ : distance moyenne aux observations du même groupe
- $b_i$ : distance moyenne au groupe le plus proche
:::

::: {.fragment}
**Interprétation**
- $S(X_i) \approx 1$ : Bien classée
- $S(X_i) \approx 0$ : À la frontière
- $S(X_i) \approx -1$ : Mal classée
:::

## Utilisation de la silhouette

::: {.fragment}
**Silhouette moyenne**
Indicateur de cohérence globale de la partition
:::

::: {.fragment}
**Avantage** : Intuitive et facilement interprétable
:::

::: {.fragment}
**Usage** : Souvent utilisée pour choisir le nombre optimal de groupes
:::

## Recommandations pratiques

::: {.incremental}
1. **Croiser les approches**
   - Connaissance métier
   - Visualisation (dendrogramme)
   - Critères statistiques multiples

2. **Aucun critère n'est parfait**
   - Complémentarité des indicateurs
   - Décision éclairée et contextualisée

3. **Validation**
   - Interprétabilité des groupes obtenus
   - Stabilité des résultats
:::

## Récapitulatif

::: {.fragment}
**Classification hiérarchique**
- Alternative robuste au $k$-means
- Exploration complète de la hiérarchie de groupes
- Choix de méthode de distance crucial
:::

::: {.fragment}
**Méthodes populaires**
- Ward (sous hypothèses gaussiennes)
- Average linkage (bon compromis)
- Complete linkage (groupes réguliers)
:::

::: {.fragment}
**Choix du nombre de groupes**
Combinaison de critères pratiques et statistiques
:::

## Merci pour votre attention

**Questions ?**

::: {.fragment}
*Classification hiérarchique : explorer la structure naturelle des données*
:::