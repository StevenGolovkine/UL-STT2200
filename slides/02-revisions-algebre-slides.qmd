--- 
title: "Révisions"
subtitle: "Algèbre linéaire"
author: "Steven Golovkine"
lang: fr
date: today
date-format: "DD MMM YYYY"
slide-number: c/t
title-logo: ../include/logo-ul.png
format:
  revealjs:
    theme: [default, ../include/ulaval-slide.scss]
    chalkboard: true
    width: 1280
    height: 720
    template-partials:
      - title-slide.html
--- 

## Plan de la présentation

- Propriétés matricielles
- Valeurs et vecteurs propres
- Diagonalisation de matrices
- Applications pratiques

# Propriétés matricielles

## Notations de base

::: {.incremental}
- $M_{n, m}(\mathbb{R})$ : matrices à $n$ lignes et $m$ colonnes
- $M_{n}(\mathbb{R})$ : matrices carrées de taille $n$  
- $I_n$ : matrice identité de taille $n$
- $u, v \in \mathbb{R}^n$ : vecteurs colonnes
:::

## Inverse de matrices {.smaller}

::: {.callout-important}
**Propriété fondamentale**

Si $A$ et $B$ sont inversibles, alors :
$(AB)^{-1} = B^{-1} A^{-1}$
:::

::: {.fragment}
**Exemple pratique :**
$A = \begin{pmatrix} 2 & 0 \\ 0 & 3 \end{pmatrix}, \quad B = \begin{pmatrix} 1 & 1 \\ 0 & 1 \end{pmatrix}$

$AB = \begin{pmatrix} 2 & 2 \\ 0 & 3 \end{pmatrix}, \quad (AB)^{-1} = \begin{pmatrix} 1/2 & -1/3 \\ 0 & 1/3 \end{pmatrix}$

Et effectivement : $B^{-1}A^{-1} = \begin{pmatrix} 1 & -1 \\ 0 & 1 \end{pmatrix} \begin{pmatrix} 1/2 & 0 \\ 0 & 1/3 \end{pmatrix} = \begin{pmatrix} 1/2 & -1/3 \\ 0 & 1/3 \end{pmatrix}$
:::

## Propriétés du déterminant

::: {.callout-important}
**Propriétés essentielles**

1. $\det(A^\top) = \det(A)$
2. $\det(AB) = \det(A)\det(B)$
3. $\det(A^{-1}) = \frac{1}{\det(A)}$
:::

::: {.fragment}
**Exemple concret :**
$A = \begin{pmatrix} 1 & 2 \\ 3 & 4 \end{pmatrix}, \quad B = \begin{pmatrix} 2 & 0 \\ 1 & 1 \end{pmatrix}$

$\det(A) = 4 - 6 = -2$, $\det(B) = 2$

$AB = \begin{pmatrix} 4 & 2 \\ 10 & 4 \end{pmatrix}$, $\det(AB) = 16 - 20 = -4$

Vérification : $\det(A) \cdot \det(B) = (-2) \times 2 = -4$ ✓
:::

## Exemple : Propriétés de la trace

::: {.fragment}
**Calcul pratique :**
$A = \begin{pmatrix} 1 & 2 \\ 3 & 4 \end{pmatrix}, \quad B = \begin{pmatrix} 5 & 0 \\ 1 & 2 \end{pmatrix}$

$\tr(A) = 1 + 4 = 5$, $\tr(B) = 5 + 2 = 7$

$A + B = \begin{pmatrix} 6 & 2 \\ 4 & 6 \end{pmatrix}$, $\tr(A + B) = 6 + 6 = 12$

Vérification : $\tr(A) + \tr(B) = 5 + 7 = 12$ ✓
:::

## Propriétés de la trace

::: {.callout-important}
**Propriétés de la trace**

1. $\tr(A) = \tr(A^{\top})$
2. $\tr(A + B) = \tr(A) + \tr(B)$
3. $\tr(MN^{\top}) = \tr(N^{\top}M)$
:::

::: {.fragment}
**Rappel :** La trace est la somme des éléments diagonaux
$\tr(A) = \sum_{i=1}^{n} a_{ii}$
:::

## Exemple : Matrices définies positives

::: {.fragment}
**Test de positivité :**
$A = \begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix}$

Pour tout $u = \begin{pmatrix} x \\ y \end{pmatrix} \neq 0$ :
$u^\top A u = \begin{pmatrix} x & y \end{pmatrix} \begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix} \begin{pmatrix} x \\ y \end{pmatrix} = 2x^2 + 2xy + 2y^2$
$= x^2 + y^2 + (x+y)^2 > 0$

Donc $A$ est définie positive !
:::

## Matrices spéciales

::: {.callout-warning}
**Définitions importantes**

**Matrice définie positive :** $A$ est définie positive si 
$$u^\top A u > 0 \text{ pour tout } u \neq 0$$

**Matrice orthogonale :** $A$ est orthogonale si 
$$A^\top A = A A^\top = I_n$$
:::

# Valeurs et vecteurs propres

## Définition fondamentale

::: {.callout-warning}
**Valeurs et vecteurs propres**

$\lambda \in \mathbb{R}$ est une **valeur propre** de $A$ s'il existe $u \neq 0$ tel que :
$Au = \lambda u$

$u$ est le **vecteur propre** correspondant à $\lambda$.

Le **spectre** $\text{sp}(A)$ est l'ensemble des valeurs propres.
:::

## Exemple simple 2×2

::: {.fragment}
$A = \begin{pmatrix} 3 & 1 \\ 0 & 2 \end{pmatrix}$

**Calcul des valeurs propres :**
$\det(A - \lambda I) = \det\begin{pmatrix} 3-\lambda & 1 \\ 0 & 2-\lambda \end{pmatrix} = (3-\lambda)(2-\lambda) = 0$

Donc $\lambda_1 = 3$ et $\lambda_2 = 2$
:::

::: {.fragment}
**Vecteurs propres :**
- Pour $\lambda_1 = 3$ : $u_1 = \begin{pmatrix} 1 \\ 0 \end{pmatrix}$
- Pour $\lambda_2 = 2$ : $u_2 = \begin{pmatrix} 1 \\ -1 \end{pmatrix}$

**Vérification :** $A u_1 = \begin{pmatrix} 3 \\ 0 \end{pmatrix} = 3 u_1$ ✓
:::

## Propriétés des vecteurs propres

::: {.callout-important}
**Propriétés essentielles**

1. Si $u$ est vecteur propre pour $\lambda$, alors $cu$ l'est aussi (pour $c \neq 0$)

2. **Pour matrices symétriques :** Si $u_1$ et $u_2$ correspondent à des valeurs propres différentes, alors :
$$u_1^\top u_2 = 0 \text{ (orthogonalité)}$$
:::

## Preuve de l'orthogonalité {.smaller}

Soient $\lambda_1 \neq \lambda_2$ et $Au_1 = \lambda_1 u_1$, $Au_2 = \lambda_2 u_2$

::: {.fragment}
$$\lambda_1 u_1^\top u_2 = u_1^\top A u_2 = \lambda_2 u_1^\top u_2$$
:::

::: {.fragment}
$$(\lambda_1 - \lambda_2)u_1^\top u_2 = 0$$
:::

::: {.fragment}
Comme $\lambda_1 \neq \lambda_2$, on a nécessairement $u_1^\top u_2 = 0$
:::

::: {.fragment}
**→ Cette propriété est cruciale pour l'analyse en composantes principales !**
:::

## Caractérisation par les valeurs propres

::: {.callout-important}
**Théorèmes importants**

1. **Matrice symétrique** → toutes les valeurs propres sont **réelles**

2. **Matrice définie positive** → toutes les valeurs propres sont **strictement positives**
:::

::: {.fragment}
**Exemple illustratif :**
$A = \begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix} \text{ (symétrique et définie positive)}$

Valeurs propres : $\lambda_1 = 3, \lambda_2 = 1$ (toutes réelles et > 0)

Vecteurs propres : $u_1 = \begin{pmatrix} 1 \\ 1 \end{pmatrix}, u_2 = \begin{pmatrix} 1 \\ -1 \end{pmatrix}$

**Vérification orthogonalité :** $u_1^\top u_2 = 1 - 1 = 0$ ✓
:::

# Diagonalisation

## Définition de la diagonalisation

::: {.callout-warning}
**Matrice diagonalisable**

$A$ est diagonalisable s'il existe $P$ inversible et $D$ diagonale telles que :
$$P^{-1} A P = D \Leftrightarrow A = P D P^{-1}$$
:::

::: {.fragment}
**Interprétation :** On peut "simplifier" $A$ en changeant de base
:::

## Théorème de décomposition spectrale

::: {.callout-caution}
**Décomposition spectrale**

Pour toute matrice **symétrique** $A$, il existe une matrice **orthogonale** $P$ telle que :
$A = P \Lambda P^\top$
où $\Lambda = \text{diag}(\lambda_1, \ldots, \lambda_n)$
:::

::: {.fragment}
**Exemple concret :**
$A = \begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix}, \quad P = \frac{1}{\sqrt{2}}\begin{pmatrix} 1 & 1 \\ 1 & -1 \end{pmatrix}, \quad \Lambda = \begin{pmatrix} 3 & 0 \\ 0 & 1 \end{pmatrix}$

**Vérification :** $P \Lambda P^\top = \frac{1}{2}\begin{pmatrix} 1 & 1 \\ 1 & -1 \end{pmatrix}\begin{pmatrix} 3 & 0 \\ 0 & 1 \end{pmatrix}\begin{pmatrix} 1 & 1 \\ 1 & -1 \end{pmatrix} = \begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix} = A$ ✓
:::

## Applications pratiques {.smaller}

**Problème d'optimisation :**
Maximiser $u^\top A u$ sous la contrainte $u^\top B u = 1$

::: {.fragment}
**Exemple numérique :**
$A = \begin{pmatrix} 5 & 2 \\ 2 & 2 \end{pmatrix}, \quad B = \begin{pmatrix} 2 & 0 \\ 0 & 1 \end{pmatrix}$

$B^{-1}A = \begin{pmatrix} 1/2 & 0 \\ 0 & 1 \end{pmatrix}\begin{pmatrix} 5 & 2 \\ 2 & 2 \end{pmatrix} = \begin{pmatrix} 5/2 & 1 \\ 2 & 2 \end{pmatrix}$

Plus grande valeur propre : $\lambda_{\max} \approx 3.56$

Solution optimale : $u^* = $ vecteur propre correspondant
:::

::: {.fragment}
**→ Principe fondamental de l'analyse en composantes principales**
:::

## Lien avec déterminant et trace

::: {.callout-important}
**Relations fondamentales**

Si $A$ a pour valeurs propres $\lambda_1, \ldots, \lambda_n$ :

1. $\det(A) = \prod_{i=1}^{n} \lambda_i$
2. $\tr(A) = \sum_{i=1}^{n} \lambda_i$
:::

::: {.fragment}
**Exemple de vérification :**
$A = \begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix}, \quad \lambda_1 = 3, \lambda_2 = 1$

$\det(A) = 4 - 1 = 3 = 3 \times 1 = \lambda_1 \lambda_2$ ✓

$\tr(A) = 2 + 2 = 4 = 3 + 1 = \lambda_1 + \lambda_2$ ✓
:::

## Exemple complet : ACP

::: {.fragment}
**Données : positions de points en 2D**
$X = \begin{pmatrix} 1 & 2 \\ 3 & 1 \\ 2 & 3 \\ 4 & 2 \end{pmatrix}$

**Matrice de covariance :**
$C = \frac{1}{n-1}X^\top X = \begin{pmatrix} 2.33 & 0.33 \\ 0.33 & 1.33 \end{pmatrix}$

**Composantes principales :** vecteurs propres de $C$
- 1ère composante : direction de variance maximale
- 2ème composante : direction orthogonale de variance restante
:::

# Conclusions

## Points clés à retenir

::: {.incremental}
- **Matrices symétriques** : valeurs propres réelles, vecteurs propres orthogonaux
- **Décomposition spectrale** : outil puissant pour l'analyse
- **Applications** : optimisation, réduction de dimension
- **Liens** : déterminant et trace via les valeurs propres
:::

## Applications en apprentissage automatique

::: {.incremental}
- **Analyse en composantes principales (ACP)**
- **Optimisation** de fonctions quadratiques
- **Méthodes de réduction de dimension**
- **Analyse de stabilité** des algorithmes
:::

## Questions ?

**Ressources recommandées :**
- Cours MAT-1200
- @deisenrothMathematicsMachineLearning2020 (anglais)
- @grifoneAlgebreLineaire2024 (français)

---

**Merci pour votre attention !**