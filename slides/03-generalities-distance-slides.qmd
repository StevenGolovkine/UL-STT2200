--- 
title: "Généralités"
subtitle: "Distances"
author: "Steven Golovkine"
lang: fr
date: "12 sept. 2025"
date-format: "DD MMM YYYY"
slide-number: c/t
title-logo: ../include/logo-ul.png
format:
  revealjs:
    theme: [default, ../include/ulaval-slide.scss]
    chalkboard: true
    width: 1280
    height: 720
    template-partials:
      - title-slide.html
--- 


## Pourquoi les distances ?

<br>

Dans tout projet d'analyse de données, il faut pouvoir quantifier la **ressemblance** ou la **dissemblance** entre observations.

. . .

Impact direct sur :

- Algorithmes d'apprentissage

- Méthodes de regroupement  

- Techniques de visualisation


## Plan

<br>

1. Notion de distance - Définition mathématique

2. Distances numériques - $L_q$, euclidienne, Manhattan

3. Notion de similarité - Concept opposé

4. Variables qualitatives - Encodages et distances adaptées

5. Distance de Jaccard - Cas des variables binaires


## Définition mathématique

::: {.callout-warning icon=false}
## Distance sur $\mathcal{X}$

Une fonction $d: \mathcal{X} \times \mathcal{X} \to \mathbb{R}$ est une distance si :

1. Non-négativité : $d(x, y) \geq 0$

2. Séparation : $d(x, y) = 0 \Leftrightarrow x = y$  

3. Symétrie : $d(x, y) = d(y, x)$

4. Inégalité triangulaire : $d(x, y) \leq d(x, z) + d(y, z)$

:::

## Distance euclidienne

::: {.callout-note icon=false}
## Définition

Pour $x, y \in \mathbb{R}^p$ :

$$d(x, y) = \left\| x - y \right\|_2 = \sqrt{\sum_{i=1}^{p} (x_i - y_i)^2}$$


Quand l'utiliser : Variables numériques de même ordre de grandeur.

:::

## Distance euclidienne (preuve)



## Distance $L_q$ (Minkowski)

::: {.callout-note icon=false}
## Formule générale :

$$d(x, y) = \left( \sum_{i=1}^{p} |x_i - y_i|^q \right)^{1/q}$$

Cas particuliers :

- $q = 1$ : Distance de Manhattan → $d(x, y) = \sum_{i=1}^{p} |x_i - y_i|$

- $q = 2$ : Distance euclidienne
:::

## Exemple concret (WOOCLAP: KFFPDJ)

| Nom | Taille (cm) | Poids (kg) |
|:---:|:----------:|:---------:|
| Alice | 165 | 70 |
| Bob | 177 | 75 |




## Problème d'échelle

::: {.callout-important icon=false}
## Attention ! 

Les distances $L_q$ ne sont **pas invariantes** aux changements d'échelle.

:::

## Problème d'échelle - solution

<br>

Standardiser les variables !

. . .

Distance euclidienne standardisée :

$$d(x, y) = \sum_{i=1}^{p} \left( \frac{x_i - y_i}{\sigma_i} \right)^2$$

où $\sigma_i$ = écart-type de la variable $i$.



## Notion de similarité

<br>

::: {.callout-warning icon=false}
## Mesure de similarité sur $\mathcal{X}$

Une fonction $s: \mathcal{X} \times \mathcal{X} \to \mathbb{R}$ telle que :

1. $s(x, y) \geq 0$

2. $s(x, y) = s(y, x)$ (symétrie)

3. $s(x, x) = 1 \geq s(x, y)$ (maximum pour soi-même)

:::


## Conversion distance ↔ similarité

<br>

Distance → Similarité :

$$s(x, y) = \frac{1}{1 + d(x, y)}$$

. . .

⚠️ Une mesure de similarité ne respecte généralement pas l'inégalité triangulaire.

. . .

Similarité → Dissemblance :

$$d^*(x, y) = 1 - s(x, y)$$



## Variables qualitatives 

<br>

Problème : pour $\mathcal{X} = \{\text{Rouge}, \text{Vert}, \text{Bleu}\}$.

. . .

<br>

❌ Mauvaise pratique : 

Rouge = 1, Vert = 2, Bleu = 3 → Introduit un ordre artificiel !

. . .

<br>

✅ Bonne pratique : Encodage adapté

## Encodage "1 parmi K" (*One-hot encoding*)

<br>

- Rouge → $(1, 0, 0)$

- Vert → $(0, 1, 0)$  

- Bleu → $(0, 0, 1)$

. . .

Avantages : Pas d'ordre artificiel

Inconvénients : Grande dimension si beaucoup de modalités


## Distance discrète

::: {.callout-note icon=false}
## Distance de Hamming


Pour des vecteurs :

$$d(x, y) = \sum_{i=1}^{p} \mathbb{1}(x_i \neq y_i)$$

:::

## Exemple : Distance discrète (WOOCLAP: KFFPDJ)

| Nom | Couleur | Yeux | Cheveux |
|:---:|:-------:|:----:|:-------:|
| Alice | Rouge | Vert | Blond |
| Bob | Vert | Bleu | Roux |
| Chris | Rouge | Vert | Blond |




## Similarité par accords

<br>

Compter les accords normalisés :

$$s(x, y) = \frac{1}{p}\sum_{i=1}^{p} \mathbb{1}(x_i = y_i)$$

. . .

Exemple précédent :

- $s(\text{Alice}, \text{Bob}) = 0$

- $s(\text{Alice}, \text{Chris}) = 1$  

- $s(\text{Bob}, \text{Chris}) = 0$


## Problème des variables binaires

<br>

Avec l'encodage "1 parmi K" :

- Beaucoup de variables binaires

- Beaucoup de 0, peu de 1

- Distance discrète peu informative

. . .

**Solution :** Se concentrer sur les attributs à 1 → Distance de Jaccard


## Indice de Jaccard : Définition

<br>

Pour deux observations de $K$ variables binaires :

- $M_{11}$ : nombre de variables à 1 pour x et y

- $M_{10}$ : nombre de variables à 1 pour x, 0 pour y  

- $M_{01}$ : nombre de variables à 0 pour x, 1 pour y

- $M_{00}$ : nombre de variables à 0 pour x et y

. . .

$$J(x, y) = \frac{M_{11}}{M_{10} + M_{01} + M_{11}} = \frac{M_{11}}{K - M_{00}}$$


## Distance de Jaccard

<br>

::: {.callout-important icon=false}
## Distance de Jaccard :

$$d(x, y) = 1 - J(x, y) = \frac{M_{10} + M_{01}}{M_{01} + M_{10} + M_{11}}$$
:::

Intuition : Proportion de désaccords parmi les variables "actives".

## Exemple : Distance de Jaccard

| Nom | Q1 | Q2 | Q3 | Q4 | Q5 |
|:---:|:--:|:--:|:--:|:--:|:--:|
| Alice | 1 | 0 | 1 | 0 | 1 |
| Bob | 1 | 0 | 0 | 1 | 0 |




## Récapitulatif des distances

<br>

| Type de données | Distance utilisable |
|:----------------|:-------------------|
| Numériques | Euclidienne (standardisée) |
| Qualitatives | Discrète (Hamming) |
| Binaires nombreuses | Jaccard |
| Mixtes | Combinaison adaptée |


## Choix pratique de la distance

<br>

Questions à se poser :

1. Quel type de variables ? (numériques/qualitatives)

2. Même échelle ou standardisation nécessaire ?

3. Beaucoup de variables binaires ?

4. Les double-absences sont-elles informatives ?

. . .

→ Le choix de distance influence directement les résultats !


## Conclusion

<br>

::: {.incremental}
1. Distance → mesure de dissemblance (4 propriétés mathématiques)

2. Similarité → mesure de ressemblance (concept opposé)  

3. Variables numériques → Distance euclidienne (standardisée)

4. Variables qualitatives → Encodage + distance adaptée

5. Variables binaires → Distance de Jaccard

:::

. . .

Prochaine étape → Compromis biais/variance
