--- 
title: "Révisions"
subtitle: "Probabilité et statistique"
author: "Steven Golovkine"
lang: fr
date: "05 sept. 2025"
date-format: "DD MMM YYYY"
slide-number: c/t
title-logo: ../include/logo-ul.png
format:
  revealjs:
    theme: [default, ../include/ulaval-slide.scss]
    chalkboard: true
    width: 1280
    height: 720
    template-partials:
      - title-slide.html
--- 


## Plan de la présentation

<br>

1. Modéliser le hasard

2. Variables aléatoires

3. Vecteurs aléatoires

4. Estimation statistique


# Modéliser le hasard

## Espace d'événements et événements

<br>

::: {.callout-important icon=false}
## Définitions

- **Espace d'événements** $S$ : ensemble de tous les résultats possibles d'une expérience.

- **Événement** : sous-ensemble de $S$.

:::

## Espace d'événements et événements : exemples

<br>

**Exemple 1 : Lancer de pièce**

- $S = \{0, 1\}$ (pile = 0, face = 1).

- Événement $E = \{1\}$ : "obtenir face".

. . .


**Exemple 2 : Durée de vie d'un téléphone**

- $S = \mathbb{R}_+ = [0, +\infty)$ (en années).

- Événement $E = [10, \infty)$ : "durée de vie > 10 ans".


## Mesure de probabilité

::: {.callout-important icon=false}
## Définition

Une mesure de probabilité $\mathbb{P}$ satisfait :

1. $\mathbb{P}(E) \in [0, 1]$ pour tout événement $E$;

2. $\mathbb{P}(S) = 1$ (certitude totale);

3. Pour des événements disjoints : $\mathbb{P}(E_1 \cup E_2 \cup \cdots) = \mathbb{P}(E_1) + \mathbb{P}(E_2) + \cdots$.

:::

## Mesure de probabilité : exemple (Wooclap: KFFPDJ)

<br>

**Exemple : Lancer d'un dé équilibré**

::: {.incremental}
- Espace d'événements :  $S = \{1, 2, 3, 4, 5, 6\}$.

- $\mathbb{P}(\{i\}) = 1/6$ pour $i = 1, \ldots, 6$.

- $\mathbb{P}(\{2, 4, 6\}) = 1/6 + 1/6 + 1/6 = 1/2$ (nombre pair).
:::


## Subjectivité de la probabilité

<br>

**Contexte :** Nombre de jours sans neige à Québec dans l'année

- Espace d'événements : $S = \{0, 1, 2, \ldots, 365\}$.

- Événement $E_1 = [0, 100]$ : "moins de 100 jours sans neige".

- Événement $E_2 = [100, 200]$ : "entre 100 et 200 jours sans neige".


**Perspective subjective :**

- **Nouvel arrivant :** probabilités uniformes sur $S$.

- **Québécois :** probabilités plus élevées pour $E_1$ que $E_2$.



## Indépendance et probabilité conditionnelle

::: {.callout-important icon=false}
## Définitions

**Indépendance :** Les événements $E$ et $F$ sont indépendants si
$$\mathbb{P}(E \cap F) = \mathbb{P}(E) \times \mathbb{P}(F).$$
**Probabilité conditionnelle :** La probabilité de $E$ sachant $F$ est
$$\mathbb{P}(E \mid F) = \frac{\mathbb{P}(E \cap F)}{\mathbb{P}(F)}.$$
:::

## Exemple : Cartes à jouer

<br>

**Contexte :** Tirage d'une carte dans un jeu standard (52 cartes)

- $E$ : "tirer un roi" → $\mathbb{P}(E) = 4/52 = 1/13$.

- $F$ : "tirer un cœur" → $\mathbb{P}(F) = 13/52 = 1/4$.


**Test d'indépendance :**

- $E \cap F$ : "tirer le roi de cœur" → $\mathbb{P}(E \cap F) = 1/52$.

- $\mathbb{P}(E) \times \mathbb{P}(F) = (1/13) \times (1/4) = 1/52$.

**Conclusion :** $E$ et $F$ sont indépendants !

# Variables aléatoires

## Variables discrètes

<br>

::: {.callout-important icon=false}
## Définition

Une variable aléatoire **discrète** prend au plus un nombre dénombrable de valeurs.

Sa distribution donnée par $\mathbb{P}(X = x)$ pour chaque valeur $x$.

:::

## Variables discrètes : exemple

<br>

**Exemple : Lancer de deux dés**

- Variable aléatoire $X$ = somme des deux dés.

- Espace d'événements : $S = \{2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12\}$.

- $\mathbb{P}(X = 7) = 6/36 = 1/6$ (combinaisons : 1+6, 2+5, 3+4, 4+3, 5+2, 6+1).

- $\mathbb{P}(X = 2) = 1/36$ (une seule combinaison : 1+1).


## Variables continues

::: {.callout-important icon=false}
## Définition

Les probabilités d'une variable aléatoire **continue** sont données par les intégrales
$$\mathbb{P}(X \in A) = \int_A f(x) dx$$
où $f(x) \geq 0$ et $\int_{-\infty}^{\infty} f(x) dx = 1$.

**Important :** $\mathbb{P}(X = x) = 0$ pour tout $x$ fixé !
:::

## Variables continues : exemple

**Variable uniforme sur $[0, 1]$ :**
$$f(x) = \begin{cases} 1 & \text{si } x \in [0, 1] \\ 0 & \text{sinon} \end{cases}.$$

**Calculs de probabilités :**

- $\mathbb{P}(X \leq 0.5) = \int_0^{0.5} 1 \, dx = 0.5$.

- $\mathbb{P}(0.2 < X < 0.7) = \int_{0.2}^{0.7} 1 \, dx = 0.5$.

- $\mathbb{P}(X = 0.3) = 0$ (point isolé).


## Espérance mathématique

::: {.callout-important icon=false}
## Définition

L'**espérance** $\mathbb{E}(X)$ est la valeur moyenne de $X$.

- **Cas discret :** $\mathbb{E}(X) = \sum_x x \mathbb{P}(X = x)$.

- **Cas continu :** $\mathbb{E}(X) = \int_{-\infty}^{\infty} x  f(x) dx$.

::: 

## Espérance mathématique : exemple (Wooclap: KFFPDJ)

**Exemple 1 : Dé équilibré**

. . .

$$\mathbb{E}(X) = 1 \cdot \frac{1}{6} + 2 \cdot \frac{1}{6} + \cdots + 6 \cdot \frac{1}{6} = \frac{21}{6} = 3.5.$$

. . .

**Exemple 2 : Loi uniforme sur $[0, 1]$**

. . .

$$\mathbb{E}(X) = \int_0^1 x \cdot 1 \, dx = \left[\frac{x^2}{2}\right]_0^1 = \frac{1}{2}.$$


## Théorème de transfert

::: {.callout-caution icon=false}
## Théorème

Pour une fonction $g$, l'espérance de $g(X)$ est :

- **Cas discret :** $\mathbb{E}[g(X)] = \sum_x g(x) \mathbb{P}(X = x)$.

- **Cas continu :** $\mathbb{E}[g(X)] = \int_{-\infty}^{\infty} g(x) f(x) dx$.

:::

## Théorème de transfert : exemple (Wooclap: KFFPDJ)

<br>

Pour $X$ suivant une loi uniforme sur $[0, 1]$ et $g(x) = x^2$ :

. . .

$$\mathbb{E}[X^2] = \int_0^1 x^2 \cdot 1 \, dx = \left[\frac{x^3}{3}\right]_0^1 = \frac{1}{3}.$$


## Variance et écart-type

<br>

::: {.callout-important icon=false}
## Définitions

**Variance :** $\text{Var}(X) = \mathbb{E}[(X - \mathbb{E}(X))^2] = \mathbb{E}[X^2] - \mathbb{E}[X]^2$

**Écart-type :** $\sigma(X) = \sqrt{\text{Var}(X)}$

:::

## Variance et écart-type : exemple

<br>

**Loi uniforme sur $[0, 1]$**

- $\mathbb{E}(X) = 1/2$

- $\mathbb{E}(X^2) = 1/3$

- $\text{Var}(X) = 1/3 - (1/2)^2 = 1/3 - 1/4 = 1/12$

- $\sigma(X) = \sqrt{1/12} \approx 0.289$


## Fonction de répartition

<br>

::: {.callout-important icon=false}
## Définition

La **fonction de répartition** est donnée par :
$$F(t) = \mathbb{P}(X \leq t).$$
:::

## Fonction de répartition : exemple

**Loi uniforme sur $[0, 1]$**

$$F(t) = \begin{cases} 
0 & \text{si } t < 0 \\
t & \text{si } 0 \leq t \leq 1 \\
1 & \text{si } t > 1
\end{cases}.$$

**Calcul :** $\mathbb{P}(0.2 < X \leq 0.7) = F(0.7) - F(0.2) = 0.7 - 0.2 = 0.5$.



# Vecteurs aléatoires

## Densités marginales et indépendance

**Vecteur aléatoire** : $X = (X_1, X_2)^\top$ avec densité conjointe $f_X(x, y)$.

**Densités marginales :**

- $f_{X_1}(x) = \int_{-\infty}^{\infty} f_X(x, y) dy$

- $f_{X_2}(y) = \int_{-\infty}^{\infty} f_X(x, y) dx$

**Indépendance :** $f_X(x, y) = f_{X_1}(x) \cdot f_{X_2}(y)$.


## Exemple : Variables indépendantes


**Contexte :** Soient $X_1$ et $X_2$ uniformes indépendantes sur $[0, 1]$.

$$f_X(x, y) = \begin{cases} 1 & \text{si } (x,y) \in [0,1]^2 \\ 0 & \text{sinon} \end{cases}$$

. . .

**Calculs de probabilités :**

- $\mathbb{P}(X_1 \leq 0.5, X_2 \leq 0.8) = \int_0^{0.5} \int_0^{0.8} 1 \, dy \, dx = 0.5 \times 0.8 = 0.4$.

- $\mathbb{P}(X_1 + X_2 \leq 1) = \int \int_{x+y \leq 1} 1 \, dy \, dx = 0.5$ (aire du triangle).


## Loi normale multivariée

::: {.callout-important icon=false}
## Loi normale $p$-dimensionnelle

Soit $X$ un vecteur aléatoire suivant une loi normale $p$-dimensionnelle. Sa densité est donnée par :
$$f_X(x) = \frac{1}{(2\pi)^{p/2}} \frac{1}{(\det \Sigma)^{1/2}} \exp\left\{-\frac{1}{2}(x-\mu)^T \Sigma^{-1} (x-\mu)\right\}.$$

**Notation :** $X \sim \mathcal{N}_p(\mu, \Sigma)$.

:::

## Exemple : Loi normale bivariée

**Cas simple :** $X \sim \mathcal{N}_2(\mu, I_2)$ avec $\mu = \begin{pmatrix} 0 \\ 0 \end{pmatrix}$

$$f_X(x, y) = \frac{1}{2\pi} \exp\left\{-\frac{x^2 + y^2}{2}\right\}$$



## Covariance et corrélation

::: {.callout-important icon=false}
## Définitions

Soit $X = (X_1, X_2)^\top$ un vecteur aléatoire.

**Covariance :**
$\text{Cov}(X_1, X_2) = \mathbb{E}[(X_1 - \mathbb{E}(X_1))(X_2 - \mathbb{E}(X_2))].$

**Corrélation :**
$\text{Corr}(X_1, X_2) = \frac{\text{Cov}(X_1, X_2)}{\sigma(X_1) \sigma(X_2)}.$
:::

## Interprétation des signes

- **Corrélation positive** ($> 0$) : $X_1 \nearrow ~\Rightarrow ~ X_2 \nearrow$.

- **Corrélation négative** ($< 0$) : $X_1 \searrow ~\Rightarrow ~ X_2 \searrow$.

- **Corrélation nulle** ($= 0$) : pas de relation linéaire (variables orthogonales)


## Interprétation des signes : exemple (Wooclap: KFFPDJ)

Soit $X = (X_1, X_2, X_3)^\top$ avec
$$\text{Var}(X) = \begin{pmatrix} 4 & 0 & 3 \\ 0 & 2 & 0 \\ 3 & 0 & 9 \end{pmatrix}.$$
Calculer $\text{Corr}(X_1, X_3)$.

. . .

$$\text{Corr}(X_1, X_3) = \frac{\text{Cov}(X_1, X_3)}{\sigma(X_1)\sigma(X_3)}
= \frac{3}{\sqrt{4} \times \sqrt{9}} = \frac{1}{2}.$$


## Propriétés de la covariance

::: {.callout-note icon=false}
## Propriétés

1. $\text{Cov}(X_1, X_2) = \mathbb{E}(X_1 X_2) - \mathbb{E}(X_1)\mathbb{E}(X_2)$.

2. $\text{Cov}(X_1, X_2) = \text{Cov}(X_2, X_1)$ (symétrie).

3. $\text{Cov}(X_1 + \lambda Y_1, X_2) = \text{Cov}(X_1, X_2) + \lambda \text{Cov}(Y_1, X_2)$ (linéarité).

:::


# Estimation statistique

## Le problème de l'estimation

<br>

- **Réalité :** On ne connaît pas exactement la distribution de $X$.

- **Données :** Échantillon $x_1, x_2, \ldots, x_n$ (observations).

- **Objectif :** Estimer les paramètres inconnus ($\mu$, $\Sigma$, etc.).


## Estimation de la moyenne

::: {.callout-note icon=false}
## Estimateur de la moyenne

$$\widehat{\mu} = \overline{X} = \frac{1}{n} \sum_{i=1}^n x_i.$$
:::

. . .

**Exemple pratique :**
Tailles en cm : 170, 175, 168, 180, 172

$$\widehat{\mu} = \frac{170 + 175 + 168 + 180 + 172}{5} = \frac{865}{5} = 173 \text{cm.}$$


## Estimation de la variance

::: {.callout-note icon=false}
## Estimateur de la variance

$$\widehat{\Sigma} = \frac{1}{n-1} \sum_{i=1}^n (x_i - \widehat{\mu})(x_i - \widehat{\mu})^T$$

**Pourquoi diviser par $n-1$ ?** Correction du biais dû à l'utilisation de $\widehat{\mu}$.

:::

## Exemple complet de calcul

**Données :** 170, 175, 168, 180, 172 (tailles en cm)

- $\widehat{\mu} = 173$

- Écarts au carré : $(170-173)^2 = 9$, $(175-173)^2 = 4$, $(168-173)^2 = 25$, $(180-173)^2 = 49$, $(172-173)^2 = 1$.

**Variance estimée :**

$$\widehat{\sigma^2} = \frac{9 + 4 + 25 + 49 + 1}{5-1} = \frac{88}{4} = 22.$$


## Estimation de la corrélation

**Matrice de corrélation estimée :** $\widehat{R} = D^{-1} \widehat{\Sigma} D^{-1}$
où $D$ est la matrice diagonale des écarts-types.

. . .

**Exemple 2D :**
Si $\widehat{\Sigma} = \begin{pmatrix} 4 & 1.5 \\ 1.5 & 9 \end{pmatrix}$. Alors,

$$D = \begin{pmatrix} 2 & 0 \\ 0 & 3 \end{pmatrix}, \quad\text{et}\quad\widehat{R} = \begin{pmatrix} 1 & 0.25 \\ 0.25 & 1 \end{pmatrix}.$$

**Interprétation :** corrélation faible positive (0.25) entre les variables.

