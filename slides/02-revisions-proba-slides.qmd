--- 
title: "Révisions"
subtitle: "Probabilité et statistique"
author: "Steven Golovkine"
lang: fr
date: today
date-format: "DD MMM YYYY"
slide-number: c/t
title-logo: ../include/logo-ul.png
format:
  revealjs:
    theme: [default, ../include/ulaval-slide.scss]
    chalkboard: true
    width: 1280
    height: 720
    template-partials:
      - title-slide.html
--- 


## Plan de la présentation

- Modéliser le hasard
- Variables aléatoires
- Vecteurs aléatoires
- Estimation statistique
- Applications pratiques

# Modéliser le hasard

## Espace d'événements et événements

::: {.callout-warning}
**Définitions de base**

- **Espace d'événements** $S$ : ensemble de tous les résultats possibles
- **Événement** : sous-ensemble de $S$
:::

::: {.fragment}
**Exemple 1 : Lancer de pièce**
- $S = \{0, 1\}$ (pile = 0, face = 1)
- Événement $E = \{1\}$ : "obtenir face"
:::

::: {.fragment}
**Exemple 2 : Durée de vie d'un téléphone**
- $S = \mathbb{R}_+ = [0, +\infty)$ (en années)
- Événement $E = [10, \infty)$ : "durée de vie > 10 ans"
:::

## Exemple détaillé : Jours sans neige

::: {.fragment}
**Contexte :** Nombre de jours sans neige à Québec dans l'année
- $S = \{0, 1, 2, \ldots, 365\}$
- Événement $E_1 = [0, 5]$ : "moins de 6 jours sans neige"
- Événement $E_2 = [100, 200]$ : "entre 100 et 200 jours sans neige"
:::

::: {.fragment}
**Perspective subjective :**
- **Nouvel arrivant :** probabilités uniformes sur $S$
- **Québécois :** probabilités plus élevées pour $E_2$ que $E_1$
:::

## Mesure de probabilité

::: {.callout-warning}
**Définition formelle**

Une mesure de probabilité $\mathbb{P}$ satisfait :

1. $\mathbb{P}(E) \in [0, 1]$ pour tout événement $E$
2. $\mathbb{P}(S) = 1$ (certitude totale)
3. Pour événements disjoints : $\mathbb{P}(E_1 \cup E_2 \cup \cdots) = \mathbb{P}(E_1) + \mathbb{P}(E_2) + \cdots$
:::

::: {.fragment}
**Exemple pratique :**
Lancer d'un dé équilibré : $S = \{1, 2, 3, 4, 5, 6\}$
- $\mathbb{P}(\{i\}) = 1/6$ pour $i = 1, \ldots, 6$
- $\mathbb{P}(\{2, 4, 6\}) = 1/6 + 1/6 + 1/6 = 1/2$ (nombre pair)
:::

## Indépendance et probabilité conditionnelle

::: {.callout-warning}
**Définitions**

**Indépendance :** $E$ et $F$ sont indépendants si
$$\mathbb{P}(E \cap F) = \mathbb{P}(E) \times \mathbb{P}(F)$$

**Probabilité conditionnelle :**
$$\mathbb{P}(E \mid F) = \frac{\mathbb{P}(E \cap F)}{\mathbb{P}(F)}$$
:::

## Exemple : Cartes à jouer

::: {.fragment}
**Contexte :** Tirage d'une carte dans un jeu standard (52 cartes)
- $E$ : "tirer un roi" → $\mathbb{P}(E) = 4/52 = 1/13$
- $F$ : "tirer un cœur" → $\mathbb{P}(F) = 13/52 = 1/4$
:::

::: {.fragment}
**Test d'indépendance :**
- $E \cap F$ : "tirer le roi de cœur" → $\mathbb{P}(E \cap F) = 1/52$
- $\mathbb{P}(E) \times \mathbb{P}(F) = (1/13) \times (1/4) = 1/52$ ✓

**Conclusion :** $E$ et $F$ sont indépendants !
:::

::: {.fragment}
**Probabilité conditionnelle :**
$\mathbb{P}(E \mid F) = \frac{1/52}{1/4} = \frac{1}{13} = \mathbb{P}(E)$ (cohérent avec l'indépendance)
:::

# Variables aléatoires

## Variables discrètes

::: {.callout-warning}
**Définition**

Variable aléatoire **discrète** : prend au plus un nombre dénombrable de valeurs

Distribution donnée par : $\mathbb{P}(X = x)$ pour chaque valeur $x$
:::

::: {.fragment}
**Exemple : Lancer de deux dés**
- $X$ = somme des deux dés
- $S = \{2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12\}$
- $\mathbb{P}(X = 7) = 6/36 = 1/6$ (combinaisons : 1+6, 2+5, 3+4, 4+3, 5+2, 6+1)
- $\mathbb{P}(X = 2) = 1/36$ (une seule combinaison : 1+1)
:::

## Variables continues

::: {.callout-warning}
**Définition**

Variable aléatoire **continue** : probabilités données par des intégrales
$$\mathbb{P}(X \in A) = \int_A f(x) dx$$
où $f(x) \geq 0$ et $\int_{-\infty}^{\infty} f(x) dx = 1$

**Important :** $\mathbb{P}(X = x) = 0$ pour tout $x$ fixé !
:::

## Exemple : Loi uniforme

::: {.fragment}
**Variable uniforme sur $[0, 1]$ :**
$$f(x) = \begin{cases} 1 & \text{si } x \in [0, 1] \\ 0 & \text{sinon} \end{cases}$$
:::

::: {.fragment}
**Calculs de probabilités :**
- $\mathbb{P}(X \leq 0.5) = \int_0^{0.5} 1 \, dx = 0.5$
- $\mathbb{P}(0.2 < X < 0.7) = \int_{0.2}^{0.7} 1 \, dx = 0.5$
- $\mathbb{P}(X = 0.3) = 0$ (point isolé)
:::

## Espérance mathématique

::: {.callout-warning}
**Définition**

L'**espérance** $\mathbb{E}(X) = \mu$ est la valeur moyenne de $X$

- **Cas discret :** $\mathbb{E}(X) = \sum_x x \cdot \mathbb{P}(X = x)$
- **Cas continu :** $\mathbb{E}(X) = \int_{-\infty}^{\infty} x \cdot f(x) dx$
:::

## Exemples de calculs d'espérance

::: {.fragment}
**Exemple 1 : Dé équilibré**
$$\mathbb{E}(X) = 1 \cdot \frac{1}{6} + 2 \cdot \frac{1}{6} + \cdots + 6 \cdot \frac{1}{6} = \frac{21}{6} = 3.5$$
:::

::: {.fragment}
**Exemple 2 : Loi uniforme sur $[0, 1]$**
$$\mathbb{E}(X) = \int_0^1 x \cdot 1 \, dx = \left[\frac{x^2}{2}\right]_0^1 = \frac{1}{2}$$
:::

::: {.fragment}
**Exemple 3 : Somme de deux dés**
Par linéarité : $\mathbb{E}(X_1 + X_2) = \mathbb{E}(X_1) + \mathbb{E}(X_2) = 3.5 + 3.5 = 7$
:::

## Théorème de transfert

::: {.callout-caution}
**Théorème de transfert**

Pour une fonction $g$, l'espérance de $g(X)$ est :

- **Cas discret :** $\mathbb{E}[g(X)] = \sum_x g(x) \mathbb{P}(X = x)$
- **Cas continu :** $\mathbb{E}[g(X)] = \int_{-\infty}^{\infty} g(x) f(x) dx$
:::

::: {.fragment}
**Exemple pratique :**
Pour $X$ uniforme sur $[0, 1]$ et $g(x) = x^2$ :
$$\mathbb{E}[X^2] = \int_0^1 x^2 \cdot 1 \, dx = \left[\frac{x^3}{3}\right]_0^1 = \frac{1}{3}$$
:::

## Variance et écart-type

::: {.callout-warning}
**Définitions**

**Variance :** $\text{Var}(X) = \mathbb{E}[(X - \mathbb{E}(X))^2] = \mathbb{E}[X^2] - \mathbb{E}[X]^2$

**Écart-type :** $\sigma(X) = \sqrt{\text{Var}(X)}$
:::

::: {.fragment}
**Exemple : Loi uniforme sur $[0, 1]$**
- $\mathbb{E}(X) = 1/2$
- $\mathbb{E}(X^2) = 1/3$
- $\text{Var}(X) = 1/3 - (1/2)^2 = 1/3 - 1/4 = 1/12$
- $\sigma(X) = \sqrt{1/12} \approx 0.289$
:::

## Fonction de répartition

::: {.callout-warning}
**Définition**

La **fonction de répartition** est :
$$F(t) = \mathbb{P}(X \leq t)$$
:::

::: {.fragment}
**Exemple : Loi uniforme sur $[0, 1]$**
$$F(t) = \begin{cases} 
0 & \text{si } t < 0 \\
t & \text{si } 0 \leq t \leq 1 \\
1 & \text{si } t > 1
\end{cases}$$

**Vérification :** $\mathbb{P}(0.2 < X \leq 0.7) = F(0.7) - F(0.2) = 0.7 - 0.2 = 0.5$ ✓
:::

# Vecteurs aléatoires

## Densités marginales et indépendance

::: {.callout-warning}
**Vecteur aléatoire**

$X = (X_1, X_2)$ avec densité conjointe $f_X(x, y)$

**Densités marginales :**
- $f_{X_1}(x) = \int_{-\infty}^{\infty} f_X(x, y) dy$
- $f_{X_2}(y) = \int_{-\infty}^{\infty} f_X(x, y) dx$

**Indépendance :** $f_X(x, y) = f_{X_1}(x) \cdot f_{X_2}(y)$
:::

## Exemple : Variables indépendantes

::: {.fragment}
**Contexte :** $X_1$ et $X_2$ uniformes indépendantes sur $[0, 1]$

$$f_X(x, y) = \begin{cases} 1 & \text{si } (x,y) \in [0,1]^2 \\ 0 & \text{sinon} \end{cases}$$
:::

::: {.fragment}
**Calculs de probabilités :**
- $\mathbb{P}(X_1 \leq 0.5, X_2 \leq 0.8) = \int_0^{0.5} \int_0^{0.8} 1 \, dy \, dx = 0.5 \times 0.8 = 0.4$
- $\mathbb{P}(X_1 + X_2 \leq 1) = \int \int_{x+y \leq 1} 1 \, dy \, dx = 0.5$ (aire du triangle)
:::

## Loi normale multivariée

::: {.callout-note}
**Loi normale $p$-dimensionnelle**

$$f_X(x) = \frac{1}{(2\pi)^{p/2}} \frac{1}{(\det \Sigma)^{1/2}} \exp\left\{-\frac{1}{2}(x-\mu)^T \Sigma^{-1} (x-\mu)\right\}$$

**Notation :** $X \sim \mathcal{N}_p(\mu, \Sigma)$
:::

## Exemple : Loi normale bivariée

::: {.fragment}
**Cas simple :** $X \sim \mathcal{N}_2(\mu, I_2)$ avec $\mu = \begin{pmatrix} 0 \\ 0 \end{pmatrix}$

$$f_X(x, y) = \frac{1}{2\pi} \exp\left\{-\frac{x^2 + y^2}{2}\right\}$$
:::

::: {.fragment}
**Propriétés :**
- Les courbes de niveau sont des cercles centrés à l'origine
- $X_1$ et $X_2$ sont indépendantes (car $\Sigma = I_2$)
- $\mathbb{E}(X_1) = \mathbb{E}(X_2) = 0$, $\text{Var}(X_1) = \text{Var}(X_2) = 1$
:::

## Covariance et corrélation

::: {.callout-warning}
**Définitions**

**Covariance :**
$$\text{Cov}(X_1, X_2) = \mathbb{E}[(X_1 - \mathbb{E}(X_1))(X_2 - \mathbb{E}(X_2))]$$

**Corrélation :**
$$\text{Corr}(X_1, X_2) = \frac{\text{Cov}(X_1, X_2)}{\sigma(X_1) \sigma(X_2)}$$
:::

## Interprétation des signes

::: {.incremental}
- **Corrélation positive** ($> 0$) : $X_1 \uparrow \Rightarrow X_2 \uparrow$
- **Corrélation négative** ($< 0$) : $X_1 \uparrow \Rightarrow X_2 \downarrow$
- **Corrélation nulle** ($= 0$) : pas de relation linéaire (variables orthogonales)
:::

::: {.fragment}
**Exemple numérique :**
Données : $(1,2)$, $(2,3)$, $(3,5)$, $(4,6)$

- $\mathbb{E}(X_1) = 2.5$, $\mathbb{E}(X_2) = 4$
- $\text{Cov}(X_1, X_2) = \mathbb{E}(X_1 X_2) - \mathbb{E}(X_1)\mathbb{E}(X_2) = 11.5 - 10 = 1.5 > 0$

**Interprétation :** relation positive entre $X_1$ et $X_2$
:::

## Propriétés de la covariance

::: {.callout-important}
**Propriétés essentielles**

1. $\text{Cov}(X_1, X_2) = \mathbb{E}(X_1 X_2) - \mathbb{E}(X_1)\mathbb{E}(X_2)$
2. $\text{Cov}(X_1, X_2) = \text{Cov}(X_2, X_1)$ (symétrie)
3. $\text{Cov}(X_1 + \lambda Y_1, X_2) = \text{Cov}(X_1, X_2) + \lambda \text{Cov}(Y_1, X_2)$ (linéarité)
:::

::: {.fragment}
**Exemple de linéarité :**
$$\text{Cov}(2X_1 + 3Y_1, X_2) = 2\text{Cov}(X_1, X_2) + 3\text{Cov}(Y_1, X_2)$$
:::

# Estimation statistique

## Le problème de l'estimation

::: {.incremental}
- **Réalité :** On ne connaît pas exactement la distribution
- **Données :** Échantillon $x_1, x_2, \ldots, x_n$ (observations)
- **Objectif :** Estimer les paramètres inconnus ($\mu$, $\Sigma$, etc.)
:::

## Estimation de la moyenne

::: {.callout-warning}
**Estimateur de la moyenne**
$$\widehat{\mu} = \overline{X} = \frac{1}{n} \sum_{i=1}^n x_i$$
:::

::: {.fragment}
**Exemple pratique :**
Tailles d'étudiants (cm) : 170, 175, 168, 180, 172

$\widehat{\mu} = \frac{170 + 175 + 168 + 180 + 172}{5} = \frac{865}{5} = 173$ cm
:::

## Estimation de la variance

::: {.callout-warning}
**Estimateur de la variance**
$$\widehat{\Sigma} = \frac{1}{n-1} \sum_{i=1}^n (x_i - \widehat{\mu})(x_i - \widehat{\mu})^T$$

**Pourquoi $n-1$ ?** Correction du biais dû à l'utilisation de $\widehat{\mu}$
:::

## Exemple complet de calcul

::: {.fragment}
**Données :** 170, 175, 168, 180, 172 (tailles en cm)
- $\widehat{\mu} = 173$
- Écarts au carré : $(170-173)^2 = 9$, $(175-173)^2 = 4$, $(168-173)^2 = 25$, $(180-173)^2 = 49$, $(172-173)^2 = 1$
:::

::: {.fragment}
**Variance estimée :**
$$\widehat{\sigma}^2 = \frac{9 + 4 + 25 + 49 + 1}{5-1} = \frac{88}{4} = 22$$

**Écart-type estimé :** $\widehat{\sigma} = \sqrt{22} \approx 4.69$ cm
:::

## Estimation de la corrélation

::: {.fragment}
**Matrice de corrélation estimée :**
$$\widehat{R} = D^{-1} \widehat{\Sigma} D^{-1}$$
où $D$ est la matrice des écarts-types
:::

::: {.fragment}
**Exemple 2D :**
Si $\widehat{\Sigma} = \begin{pmatrix} 4 & 1.5 \\ 1.5 & 9 \end{pmatrix}$

Alors $D = \begin{pmatrix} 2 & 0 \\ 0 & 3 \end{pmatrix}$ et $\widehat{R} = \begin{pmatrix} 1 & 0.25 \\ 0.25 & 1 \end{pmatrix}$

**Interprétation :** corrélation faible positive (0.25) entre les variables
:::

# Applications pratiques

## Exemple : Analyse de données météorologiques

::: {.fragment}
**Contexte :** Température et humidité quotidiennes
- Échantillon : 30 jours de mesures
- $X_1$ = température (°C), $X_2$ = humidité (%)
:::

::: {.fragment}
**Résultats estimés :**
- $\widehat{\mu} = \begin{pmatrix} 22 \\ 65 \end{pmatrix}$ (température moyenne : 22°C, humidité : 65%)
- Corrélation estimée : $\widehat{\rho} = -0.6$ (négative forte)

**Interprétation :** Quand la température augmente, l'humidité tend à diminuer
:::

## Application : Contrôle qualité

::: {.fragment}
**Contexte :** Production de pièces manufacturées
- $X$ = diamètre des pièces (mm)
- Spécification : $\mu = 50$ mm, $\sigma = 0.1$ mm
:::

::: {.fragment}
**Échantillon de contrôle :** 49.9, 50.1, 49.8, 50.2, 50.0

$\widehat{\mu} = 50.0$ mm ✓ (dans les spécifications)

$\widehat{\sigma} = 0.15$ mm ⚠️ (variabilité plus élevée que prévu)

**Action :** Ajuster le processus de production
:::

## Points clés à retenir

::: {.incremental}
- **Probabilités** : modélisation de l'incertitude
- **Variables aléatoires** : quantification des phénomènes aléatoires
- **Espérance et variance** : caractéristiques centrales des distributions
- **Covariance/corrélation** : mesure de dépendance linéaire
- **Estimation** : inférence à partir d'échantillons
:::

## Applications en apprentissage automatique

::: {.incremental}
- **Classification probabiliste** (Bayes naïf, régression logistique)
- **Réduction de dimension** (ACP via matrices de covariance)
- **Apprentissage de paramètres** (maximum de vraisemblance)
- **Incertitude en prédiction** (intervalles de confiance)
- **Analyse de données** (statistiques descriptives)
:::

## Questions ?

**Ressources recommandées :**
- Cours STT-1000
- @wassermanAllStatisticsConcise2010 (anglais)
- @delmasIntroductionAuCalcul2013 (français)

---

**Merci pour votre attention !**