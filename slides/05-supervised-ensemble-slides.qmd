--- 
title: "Supervisée"
subtitle: "Méthodes ensemblistes"
author: "Steven Golovkine"
lang: fr
date: "14 nov. 2025"
date-format: "DD MMM YYYY"
slide-number: c/t
title-logo: ../include/logo-ul.png
format:
  revealjs:
    theme: [default, ../include/ulaval-slide.scss]
    chalkboard: true
    width: 1280
    height: 720
    template-partials:
      - title-slide.html
--- 

## Plan

<br>

1. Généralités

2. Bagging 

3. Forêts aléatoires

4. Boosting


## Principe

<br>

Combiner les prédictions de plusieurs modèles simples (classificateurs faibles, *weak classifier*) plutôt qu'un unique modèle complexe :

. . .

1. Meilleure performance globale

2. Plus grande robustesse au bruit

3. Réduction du sur-ajustement

4. Stabilité des prédictions améliorée


## Stratégies de combinaison

<br>


1. Moyenne des probabilités prédites par chaque modèle

2. Vote de majorité : Classe la plus souvent prédite

3. Sélection de modèles : Analogie avec la sélection de variables

4. Méthodes automatisées : Bagging, boosting, forêts aléatoires

## Illustration 

Supposons que pour un exercice de classification binaire pour une observation, trois méthodes me donnent respectivement des probabilités $0.3$, $0.505$ et $0.515$ que cette observation soit "positive" (et donc des probabilités $0.7$, $0.495$ et $0.485$ que cette observation soit "négative"). 

. . .

1. Moyenne des probabilités : $(0.3 + 0.505 + 0.515) / 3 = 0.44 < 0.5$, donc on prédit "négatif".

2. Vote de majorité: deux méthodes prédisent "positif" et une prédit "négatif", donc on prédit "positif".

. . .

Remarque: on peut pondérer les moyennes ou les votes !

## Bagging (Bootstrap Aggregating)

<br>

Construire plusieurs versions d'un même modèle sur des échantillons différents.

. . .

Algorithme :

1. Générer $B$ échantillons *bootstrap* (tirage avec remise).

2. Obtenir $B$ arbres de classification (ou autre).

3. Prédire par vote de majorité ou moyenne des probabilités.


## Bootstrap

<br>

Le bootstrap est une technique basée sur l'échantillonage aléatoire, ce qui permet d'estimer la distribution (échantillonale) de (presque) toutes les statistiques.



## Avantages et inconvénients

<br>

Réduction de la variance des prédictions

Augmentation de la stabilité des prédictions

. . .

<br>

Perte d'interprétabilité des variables individuelles

. . .

<br>

Un *bagging* d'un bon classificateur peut le rendre meilleur, un *bagging* d'un mauvais classificateur peut le rendre moins bon.


## Évaluation de l'importance des variables

<br>

1. Sommer les réductions d'indice de Gini par variable.

2. Moyenner sur tous les arbres.

3. Exprimer en pourcentage relatif.


## Exemple


## Forêts aléatoires

<br>

Problème du bagging : Les arbres se ressemblent (dominance des classes fréquentes).

. . .

Solution : Introduction d'une source supplémentaire d'aléatoire.

. . .

À chaque division, sélectionner aléatoirement $m$ variables parmi $p$ disponibles.

. . .

Choix usuel: $m \approx \sqrt{p}$.


## Bénéfices de la randomisation

<br>

Décorrélation des arbres construits.

. . .

Diversification des modèles.

. . .

Amélioration de la performance globale.

. . .

Robustesse aux valeurs abérrantes accrue.

. . .

Meilleure généralisation que le *bagging* simple.

## Algorithme

<br>

1. Générer $B$ échantillons *bootstrap* (tirage avec remise).

2. Obtenir $B$ arbres de classification construit sur $m$ variables choisies aléatoirement parmi toutes les variables disponibles.

3. Prédire par vote de majorité ou moyenne des probabilités.


## Exemple


## Boosting

<br>

Construction séquentielle au lieu de parallèle → à l'opposé du *bagging*.

. . .

1. Construire une série de modèles simples.

2. Accorder progressivement plus d'importance aux erreurs.

3. Corriger les erreurs du modèle précédent.

4. Améliorer les prédictions.


## Condition de succès

<br>

L'idée est d'utiliser des classifieurs faibles (*weak learners*).

. . .

Laisser au processus de boosting le soin de combiner optimalement les prédictions.


## Quelques variantes

<br>

1. AdaBoost (Adaptive Boosting)

- Ajuste les poids des observations à chaque itération.

- Méthode historique et bien établie.

. . .


2. Gradient Boosting


- Procédure itérative inspirée de la descente du gradient.

- Très utilisé aujourd'hui.


## Exemple (AdaBoost)




## Paramètres par méthode

<br>

Atteindre un bon compromis entre performance et robustesse

. . .

<br>
Bagging → Nombre d'échantillons bootstrap $B$.

. . .

Forêts aléatoires → *Bagging* + Nombre de variables $m$ à considérer à chaque division

. . .

Boosting → Taille des arbres + Fraction du jeu de données échantillonné + Nombre d'itérations + Paramètres de régularisation


## Méthode d'optimisation: Validation croisée

<br>

→ Nécessaire pour chaque paramètre afin d'éviter le sur-ajustement.

. . .

1. Définir une grille de valeurs pour chaque paramètre.

2. Évaluer par validation croisée.

3. Sélectionner la combinaison optimale.

4. Tester sur un jeu de données indépendant.



## Conclusion

<br>

Ensemble > Individuel : Combinaison de modèles faibles.

Compromis : Performance vs Interprétabilité.

Optimisation des paramètres : Cruciale pour la performance finale.

Applications : Base de nombreux algorithmes modernes (XGBoost, LightGBM, etc.)

<br>

. . .

Prochaine étape → La classification non-supervisée (*clustering*)