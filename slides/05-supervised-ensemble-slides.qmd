--- 
title: "Supervisée"
subtitle: "Méthodes ensemblistes"
author: "Steven Golovkine"
lang: fr
date: "14 nov. 2025"
date-format: "DD MMM YYYY"
slide-number: c/t
title-logo: ../include/logo-ul.png
format:
  revealjs:
    theme: [default, ../include/ulaval-slide.scss]
    chalkboard: true
    width: 1280
    height: 720
    template-partials:
      - title-slide.html
--- 

## Plan

<br>

1. Généralités

2. Bagging 

3. Forêts aléatoires

4. Boosting


## Principe

<br>

Combiner les prédictions de plusieurs modèles simples (classificateurs faibles, *weak classifier*) plutôt qu'un unique modèle complexe :

. . .

1. Meilleure performance globale

2. Plus grande robustesse au bruit

3. Réduction du sur-ajustement

4. Stabilité des prédictions améliorée


## Stratégies de combinaison

<br>


1. Moyenne des probabilités prédites par chaque modèle

2. Vote de majorité : Classe la plus souvent prédite

3. Sélection de modèles : Analogie avec la sélection de variables

4. Méthodes automatisées : Bagging, boosting, forêts aléatoires

## Illustration 

Supposons que pour un exercice de classification binaire pour une observation, trois méthodes me donnent respectivement des probabilités $0.3$, $0.505$ et $0.515$ que cette observation soit "positive" (et donc des probabilités $0.7$, $0.495$ et $0.485$ que cette observation soit "négative"). 

. . .

1. Moyenne des probabilités : $(0.3 + 0.505 + 0.515) / 3 = 0.44 < 0.5$, donc on prédit "négatif".

2. Vote de majorité: deux méthodes prédisent "positif" et une prédit "négatif", donc on prédit "positif".

. . .

Remarque: on peut pondérer les moyennes ou les votes !

## Bagging (Bootstrap Aggregating)

<br>

Construire plusieurs versions d'un même modèle sur des échantillons différents.

. . .

Algorithme :

1. Générer $B$ échantillons *bootstrap* (tirage avec remise).

2. Construire un arbre pour chaque échantillon.

3. Obtenir $B$ arbres de classification (ou autre).

4. Prédire par vote de majorité ou moyenne des probabilités.

## Bootstrap

<br>

Le bootstrap est une technique basée sur l'échantillonage aléatoire, ce qui permet d'estimer la distribution (échantillonale) de (presque) toutes les statistiques.



## Avantages et inconvénients

<br>

Réduction de la variance des prédictions

Augmentation de la stabilité des prédictions

. . .

<br>

Perte d'interprétabilité des variables individuelles

. . .

<br>

Un *bagging* d'un bon classificateur peut le rendre meilleur, un *bagging* d'un mauvais classificateur peut le rendre moins bon.


## Évaluation de l'importance des variables

<br>

1. Sommer les réductions d'indice de Gini par variable.

2. Moyenner sur tous les arbres.

3. Exprimer en pourcentage relatif.


## Exemple


## Forêts aléatoires

<br>

Problème du bagging : Les arbres se ressemblent (dominance des classes fréquentes).

. . .

Solution : Introduction d'une source supplémentaire d'aléatoire.

. . .

À chaque division, sélectionner aléatoirement $m$ variables parmi $p$ disponibles.

. . .

Choix usuel: $m \approx \sqrt{p}$.


## Bénéfices de la randomisation

<br>

Décorrélation des arbres construits.

Diversification des modèles.

Amélioration de la performance globale.

Robustesse aux valeurs abérrantes accrue.

Meilleure généralisation que le *bagging* simple.

## Algorithme

## Exemple

# Boosting

## Approche séquentielle

::: {.fragment}
**Philosophie opposée au bagging**
Construction séquentielle au lieu de parallèle
:::

::: {.fragment}
**Principe**
1. **Construire** une série de modèles simples
2. **Accorder progressivement** plus d'importance aux erreurs
3. **Corriger** les erreurs du modèle précédent
4. **Atteindre** une très haute précision globale
:::

## Condition de succès

::: {.fragment}
**Exigence fondamentale**
Utiliser des **modèles faibles** (weak learners)
:::

::: {.fragment}
**Raison**
Laisser au processus de boosting le soin de combiner optimalement les prédictions
:::

## Variantes principales

::: {.fragment}
**1. AdaBoost** (Adaptive Boosting)
- Ajuste les poids des observations à chaque itération
- Méthode historique et bien établie
:::

::: {.fragment}
**2. Gradient Boosting**
- Minimisation d'une fonction de perte
- Procédure itérative inspirée de la descente du gradient
- Très utilisé aujourd'hui
:::

# Optimisation des hyper-paramètres

## Nécessité de l'optimisation

::: {.fragment}
**Observation empirique**
Plus une méthode est puissante, plus l'optimisation des hyper-paramètres est complexe et sensible
:::

::: {.fragment}
**Objectif**
Atteindre un bon compromis entre performance et robustesse
:::

## Paramètres par méthode

::: {.fragment}
**Bagging**
- Nombre d'échantillons bootstrap $B$
:::

::: {.fragment}
**Forêts aléatoires**
- Nombre d'échantillons bootstrap $B$
- Nombre de variables $m$ à considérer à chaque division
:::

::: {.fragment}
**Boosting**
- Taille des arbres
- Fraction du jeu de données échantillonné
- Nombre d'itérations
- Paramètres de régularisation
:::

## Méthode d'optimisation

::: {.fragment}
**Validation croisée**
Nécessaire pour chaque paramètre afin d'éviter le sur-ajustement
:::

::: {.fragment}
**Processus**
1. **Définir** une grille de valeurs pour chaque paramètre
2. **Évaluer** par validation croisée
3. **Sélectionner** la combinaison optimale
4. **Tester** sur un jeu de données indépendant
:::

## Récapitulatif des méthodes

::: {.fragment}
| Méthode | Approche | Avantage principal |
|---------|----------|-------------------|
| **Bagging** | Parallèle | Réduction variance |
| **Forêts aléatoires** | Parallèle + aléa | Décorrélation |
| **Boosting** | Séquentielle | Correction d'erreurs |
:::

## Points clés à retenir

::: {.incremental}
- **Ensemble > Individuel** : Combinaison de modèles faibles
- **Diversité** : Clé du succès des méthodes ensemblistes
- **Trade-off** : Performance vs Interprétabilité
- **Optimisation** : Cruciale pour la performance finale
- **Applications** : Base de nombreux algorithmes modernes (XGBoost, LightGBM, etc.)
:::

## Conclusion

<br>



. . .

Prochaine étape → La classification non-supervisée (*clustering*)