---
title: "Discriminant"
format:
  html: default
  pdf:
    header-includes:
      - \usepackage{mathtools}
---

La méthode a été introduite en 1936 par R. A. Fisher. Il s'intéressait à la taxonimie végétale, c'est-à-dire déterminer l'espèce de fleurs à partir de diverses mesures.

Notation:

Soit $X = (X_{i j})$, qui est une matrice de dimension $n \times p$, où $n$ est le nombre d'individus dans l'échantillon, $p$ est le nombre de variables et $X_{i j}$ est la valeur de la $j$e variable pour le $i$e individus.

Identification des groupes:

* $I_k = $ ensemble des individus du groupe $k$

* $n_k = |I_k| = $ cardinalité de $I_k$.

* $n_{1} + \dots + n_q = n$, où $q$ est le nombre de groupes.

Score de l'analyse discriminante: on a des observations dans $R^p$. Pour faire de la classification à partir de $X_{1}, \dots, X_p$, on doit partionner $R^p$ en $q$ sous-ensembles de sorte que chacun des $q$ sous-ensembles est associé à un des $q$ groupes.

On va chercher à passer de la dimension $p$ à la dimension $1$ en calculant un score $f(x_{1}, \dots, x_p) \in \mathbb{R}$ pour chaque observation et ensuite utiliser ce score pour déterminer le groupe d'appartenance (et donc partionner $R$). Le score proposé par Fisher est une combinaison linéaire des variables, c'est-à-dire
$$f(X_{1}, \dots, X_p) = a^{\top} X + b = a_{1} X_{1} + \cdots + a_p X_p + b.$$

On en déduira $q$ intervalles de décision $S_{1}, \dots, S_q$ associés aux groupes.

::: {.callout-tip icon=false}
## Remarque

Sans perte de généralité, on peut choisir
$$-b = a_{1} \overline{X}_1 + \dots + a_p \overline{X}_p = a^{\top} \overline{X}$$
ce qui permet de centrer les variables en enlevant le vecteur de moyenne
$$\overline{X} = \left( \overline{X}_1, \dots, \overline{X}_p \right).$$

Il ne reste plus qu'à choisir le vecteur $a = (a_{1}, \dots, a_p)$.
:::

On voudrait choisir le vecteur $a$ de sorte que les scores soient, à la fois, très différents entre les groupes et très similaires à l'intérieur d'un groupe. On s'intéresse donc à la variabilité des scores à l'intérieur des groupes et entre les groupes.

Étant donné $a \in R^p$, on a:
$$\mathrm{Var}(f(X_{1}, \dots, X_p)) = \mathrm{Var}(a^{\top} X) = a^{\top} \mathrm{Var}(X) a,$$
que nous estimons à partir des $n$ observations par 
$$\widehat{\mathrm{Var}}(f(X_{1}, \dots, X_p)) = \frac{1}{n} a^{\top} S a.$$

La base de l'analyse discriminante repose sur le fait que 
$$S = W + B,$$
où $W$ est la matrice de variance intragroupe et $B$ est la matrice de variance intergroupe.

On peut prouver ce résultat en considérant la définition des matrices $S, W$ et $B$.
La moyenne de la variable $j$ pour tous les individus de l'échantillon est
$$\overline{X}_j = \frac{1}{n}\sum_{i = 1}^{n} X_{i j}.$$

La moyenne de la variable $j$ pour les individus du groupe $k$ est 
$$\overline{X}_{k j} = \frac{1}{n_{k}} \sum_{i \in I_k} X_{i j}.$$

La somme des carrés totale est 
$$s_{j j^\prime} \sum_{i = 1}^{n} (X_{i j} - \overline{X}_j)(X_{i j^\prime} - \overline{X}_{j^\prime}).$$

On tirerait de la matrice $S$ une estimation de $\mathrm{Cov}(X_j, X_{j^\prime})$ si toutes les observations provenaient d'un même groupe.
On définit $s_{j j^\prime}$ comme étant 
$$s_{j j^\prime} = w_{j j^\prime} + b_{j j^\prime},$$
où
$$w_{j j^\prime} = \sum_{k = 1}^{q} \sum_{i \in I_k} (X_{i j} - \overline{X}_{k j})(X_{i j^\prime} - \overline{X}_{k j^\prime}),$$

$$b_{j j^\prime} = \sum_{k = 1}^{q} n_k (\overline{X}_{k j} - \overline{X}_j)(\overline{X}_{k j^\prime} - \overline{X}_{j^\prime}).$$

Preuve:

1. Poser $X_{ij} - \overline{X}_j = X_{ij} - \overline{X}_{kj} + \overline{X}_{kj} - \overline{X}_j$, dans la définition de $s_{j j^\prime}$, iden pour $X_{i j^\prime}$.

2. Développer les produits.

3. Remplacer $\sum_{i = 1}^{n}$ par $\sum_{k = 1}^{q} \sum_{i \in I_k}$.

4. Faire les simplications appropriées.

On obtient 
$$\widehat{\mathrm{Var}}(a^{\top} X) = \frac{1}{n} a^{\top} S a = \frac{1}{n} \left( a^{\top} W a + a^{\top} B a \right).$$

On se rappelle que l'on veut choisir le vecteur $a$ pour que les scores puissent facilement séparer les groupes. En d'autres mots, on veut des scores les plus similaires possible à l'intérieur d'un groupe et des scores les plus différents possible entre les groupes.

On propose de choisir le vecteur $a in \mathbb{R}^p$ pour maximiser
$$\frac{a^{\top} B a}{a^{\top} W a} \quad\text{où}\quad \frac{a^{\top} B a}{a^{\top} S a}.$$
ce vecteur est unique à une constante près.

Ce problème peut être reformuler des fa\c{c}ons suivantes:

* Maximiser $a^{\top} B a / a^{\top} S a$ sous la contrainte que $a^{\top} a = 1$.

* Maximiser $a^{\top} B a$ sous la contrainte que $a^{\top} S a = 1$.

* Maximiser $c^{\top} S^{-1/2} B S^{-1/2} c$ sous la contrainte que $c^{\top} c = 1$, où $c = S^{1/2} a$.

En écrivant la troisième formulation
$$c^{\top} \left( S^{-1/2} B S^{-1/2} \right) c \quad\text{s.c.} c^{\top} c = 1,$$
on peut prendre $a = S^{-1/2} c$, où $c$ est un vecteur propre normé associé à $\lambda_{1}$ la première valeur propre de $S^{-1/2} B S^{-1/2}$. 
De fa\c{c}on équivalente, de la deuxième formulation, on peut prendre $a$, un vecteur propre normé associé à $\lambda_{1}$ la première valeur propre de $S^{-1} B$.
Notons que comme
$$S^{-1/2} B S^{-1/2} c = \lambda c \quad\text{et}\quad a = S^{-1/2} c,$$
alors 
$$S^{-1/2} B a = \lambda S^{1/2} a \Rightarrow S^{-1} B a = \lambda a.$$
Les valeurs propres de $S^{-1} B$ et de $S^{-1/2} B S^{-1/2}$ sont donc les mêmes.

La fonction discriminante de Fisher est donc 
$$f(x) = a^{\top} (x - \overline{X}),$$
où $a$ est le vecteur propre normé associé à la plus grande valeur propre de $S^{-1} B$. 
Les scores $U_i = a^{\top} (X_{i} - \overline{X})$ sont les scores linéaires en $X_i$ qui ont le rapport (variance inter) / (variance intra) le plus élevé. On peut aussi prendre $U_i = a^{\top} X_i$, car ajouter la même constante à toutes les observations $i = 1, \dots, n$ ne change rien.

* Pouvoir discriminant

Puisque la matrice $S^{-1/2} B S^{-1/2}$ est symétrique et définie positive, ses valeurs propres sont toutes réelles et positives. De plus, on a que $S^{-1} B a - \lambda_{1} a$. Ainsi,
$$B a = \lambda_{1} S a \Rightarrow a^{\top} B a = \lambda_{1} a^{\top} S a \Rightarrow \lambda_{1} = \frac{a^{\top} B a}{a^{\top} S a}.$$

On a donc $0 \leq \lambda_{1} \leq 1$. La valeur propre $\lambda_{1}$ peut donc être vue comme le pouvoir discriminant de $f$:

* $\lambda_{1} = 1 \Rightarrow a^{\top} B a = a^{\top} S a$, donc $100\%$ de la variabilité entre les groupes et $0$ variabilité à l'intérieur des groupes. 

* $\lambda_{1} = 0 \Rightarrow a^{\top} B a = 0$, donc $0$ variabilité entre les gorupes et $100\%$ de la variabilité à l'intérieur des groupes. 

### Règle de classification

* Score moyen des groupes: Après avoir défini la fonction discriminante $f(x)$, on peut calculer le score moyen de chaque groupe défini comme étant 
$$m_k = a^{\top} \left( \overline{X}_{k 1}, \dots, \overline{X}_{k p} \right)^{\top},$$
où $\overline{X}_{k j}$ est la moyenne de la $j$e variable pour les individus appartenant au $k$e groupe. 

* Stratégie de classement des individus. Considérons une nouvelle observations $X_{0} \in \mathbb{R}^p$. Pour classer ce nouvel individu dans un groupe de la population, on calcule son score $f(X_{0}) = a^{\top} X_{0}$. Ensuite, on l'assigne au groupe $k_{0}$ qui lui ressemble le plus, c'est-à-dire le groupe tel que 
$$\left| a^{\top} X_{0} - m_{k_0} \right| = \min_{1 <= k <= q} \left| a^{\top} X_{0} - m_k \right|.$$

En applicant cette règle à l'échantillon $X_{1}, \dots, X_n$, on peut estimer les risques de mauvaise classification avec la matrice de confusion.

### Cas particulier de la classification binaire 

On peut montrer que le vectuer propre de l'analyse discriminante dans la cas où il n'y que deux populations peut être défini ainsi:
$$a = S^{-1} C = \sqrt{\frac{n_{1} n_{2}}{n}} S^{-1} (\widetilde{X}_1 - \widetilde{X}_2),$$
où
$$C = \sqrt{\frac{n_{1} n_{2}}{n}} (\widetilde{x}_1 - \widetilde{x}_2) \quad\text{et}\quad B = C C^{\top}.$$
et $\widetilde{x}_i, i = 1, 2$ sont les moyennes des caractériques $x$ dans chaque groupe.

Supposons que 
$$ m_{1} = a^{\top} \widetilde{x}_1 > a^{\top} \widetilde{x}_2 = m_{2}.$$
Alors, on classe un individu dans le premier groupe si
$$a^{\top} x > \overline{m} = \frac{m_{1} + m_{2}}{2} = a^{\top} \left( \frac{\widetilde{x}_1 + \widetilde{x}_2}{2} \right).$$

Ceci est équivalent à 
$$(\widetilde{x}_1 - \widetilde{x}_2)^\top S^{-1} x > (\widetilde{x}_1 - \widetilde{x}_2)^\top S^{-1} \left( \frac{\widetilde{x}_1 + \widetilde{x}_2}{2} \right).$$


## Example
