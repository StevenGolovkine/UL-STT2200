--- 
title: "Distances"
format: 
  html: default
  pdf:
    header-includes: 
      - \usepackage{mathtools}
--- 

Dans tout projet d'analyse de données, il est nécessaire de pouvoir quantifier la ressemblance (ou la dissemblance) entre deux observations. Pour cela, on utilise la notion de **distance** (ou **similarité**) entre les observations. Le choix de cette distance influence directement les résultats des algorithmes d'apprentissage, de regroupement et de visualisations.


## Notion de distance 

Une **distance** est une fonction mathématique mesurant à quel point deux objets sont éloigné l'un de l'autre dans un espace donnée. Plus la distance est grande, plus les observations somt éloigné.

::: {.callout-warning icon=false}
## Définition de mesure de distance

Une fonction $d: \mathcal{X} \times \mathcal{X} \to \mathbb{R}$ est une distance sur un ensemble $\mathcal{X}$ si, pour tout $x, y, z \in \mathcal{X}$, les conditions suivantes sont vérifiées : 

1. non-négativité: $d(x, y) \geq 0$;

2. séparation: $d(x, y) = 0 \Leftrightarrow x = y$;

3. symétrie: $d(x, y) = d(y, x)$;

4. inégalité triangulaire: $d(x, y) \leq d(x, z) + d(y, z)$. 

:::

::: {.callout-note icon=false}
## La distance euclidienne 

Lorsque les observations sont représentées par des vecteurs numériques dans $\mathbb{R}^p$ de même ordre de grandeur, la **distance euclidienne** est souvent un bon choix.

Soit $x, y \in \mathbb{R}^p$, la distance euclidienne est données par : 
$$d(x, y) = \left\| x - y \right\|_2 = \left( \sum_{i = 1}^{p} (x_i - y_i)^2 \right)^{1/2}.$$

:::

::: {.callout-note icon=false}
## La distance $L_q$ (ou de Minkowski) 

Soit $x, y \in \mathbb{R}^p$, la distance $L_q$ est donnée, pour $q > 0$, par : 
$$d(x, y) = \left\| x - y \right\|_q = \left( \sum_{i = 1}^{p} |x_i - y_i|^q \right)^{1 / q}.$$

Cas particuliers: 

* Pour $q = 1$, on obtient la distance de Manhattan :
$$d(x, y) = \left\| x - y \right\|_1 = \sum_{i = 1}^{p} |x_i - y_i|.$$

* Pour $q = 2$, on obtient la distance euclidienne.

:::

::: {.callout-note icon=false}
## Exemple

Considérons le jeu de données suivant :

| Nom | Taille | Poids |
| :---: | ---: | ----: |
| Alice | 162.1 | 66.8 |
| Bob | 175.8 | 81.6 |

: Taille et poids moyens au Canada (Source: Statistique Canada, Enquête sur la santé dans les collectivités canadiennes (2008)).

La distance euclidienne entre Alice et Bob est 
$$d(\text{Alice}, \text{Bob}) = \sqrt{(162.1 - 175.8)^2 + (66.8 - 81.6)^2} = 20.16.$$

La distance de Manhattan entre Alice et Bob est
$$d(\text{Alice}, \text{Bob}) = |162.1 - 175.8| + |66.8 - 81.6| = 28.5.$$

:::


La distance $L_q$ n'est pas invariante aux changements d'échelle. Par exemple, si on multiplie toutes les composantes d'un vecteur par un facteur $\lambda$, la distance entre deux vecteurs change du facteur $\lambda$.

En practique, on préfère travailler avec des variables standardisées. Ainsi, en notant, $\mu_i$, la moyenne, et $\sigma_i$, l'écart-type de la variable $i$, la distance euclidienne avec des variables standardisées est donnée par :
$$d(x, y) = \sum_{i = 1}^{p} \left\{ \frac{x_i - \mu_i}{\sigma_i} - \frac{y_i - \mu_i}{\sigma_i} \right\}^2 = \sum_{i = 1}^{p} \left( \frac{x_i - y_i}{\sigma_i} \right)^2.$$

::: {.callout-important icon=false}
## Propriété

La distance euclidienne avec des variables standardisées est invariante par changement d'échelle.

:::

::: {.callout-note icon=false collapse=true}
## Preuve

Soit $\lambda \neq 0$ et soit $X$ une variable aléatoire. On a $\mathbb{E}(\lambda X) = \lambda \mathbb{E}(X)$ et $\mathrm{Var}(\lambda X) = \lambda^2 \mathrm{Var}(X)$. Donc 

$$d(\lambda x, \lambda y) = \sum_{i = 1}^{p} \left\{ \frac{\lambda x_i - \lambda \mu_i}{\lambda \sigma_i} - \frac{\lambda y_i - \lambda \mu_i}{\lambda \sigma_i} \right\}^2 = \sum_{i = 1}^{p} \left( \frac{x_i - y_i}{\sigma_i} \right)^2 = d(x, y).$$

:::

## Notion de similarité

À l'opposé de la notion de distance, une **mesure de similarité** quantifie à quel point deux observations sont proches dans un espace donné. Ainsi, plus la similarité est grande, plus les observations sont proches.

::: {.callout-warning icon=false}
## Définition de mesure de similarité

Une fonction $s: \mathcal{X} \times \mathcal{X} \to \mathbb{R}$ est une mesure de similarité sur un ensemble $\mathcal{X}$ si, pour tout $x, y \in \mathcal{X}$, les conditions suivantes sont vérifiées : 

1. $s(x, y) \geq 0$;

2. $s(x, y) = s(y, x)$;

3. $s(x, x) = 1 \geq s(x, y)$.

:::

Une distance peut se transformer en similarité en posant
$$s(x, y) = \frac{1}{1 + d(x, y)}.$$

Cette transformation garantit que plus la distance est grande, plus la similarité est faible. Toutefois, l'inverse n'est pas toujours possible car une mesure de similarité ne respecte pas forcément l'inégalité triangulaire. On peut aussi définir la dissemblance entre deux objets:
$$d^\star(x, y) = 1 - s(x, y).$$


## Cas de variables qualitatives

Pour une variable qualitative, les distances $L_p$ n'ont pas forcément de sens. En effet, si notre variable prend ses valeurs dans l'ensemble $\{ \text{Rouge, Vert, Bleu} \}$, il n'y a pas de sens à calculer $\text{Bleu} - \text{Rouge}$. Pour résoudre ce problème, on a besoin de convertir les modalités en nombre. Cependant, leur donner des valeurs artificielles (e.g. $\text{Rouge} = 1$, $\text{Vert} = 2$, $\text{Bleu} = 3$) introduirait un ordre artificiel, ce qui n'est généralement pas souhaitable. 

### Encodage 1 parmi n

L'**encodage 1 parmi n** (*one-hot encoding*) consiste à encoder une variable qualitative à $K$ modalités en $K$ variables binaires dont une seule prend la valeur $1$. Ainsi, en reprenant l'exemple de la variable qualitative $\{ \text{Rouge, Vert, Bleu} \}$, on obtiendra trois variables binaires, et on pourra encoder la modalité "Rouge" par le vecteur $(1, 0, 0)$, la modalité "Vert" par le vecteur $(0, 1, 0)$ et la modalité "Bleu" par le vecteur $(0, 0, 1)$.

Cette méthode d'encodage a l'avantage de ne pas introduire d'ordre artificiel entre les modalités, mais peut potentiellement introduire beaucoup de nouvelles variables si la variable qualitative a beaucoup de modalités.

### La distance discrète

Maintenant que l'on a un moyen d'encoder nos variables qualitatives, on peut définir une distance entre deux observations. 

Dans ce cas, on peut simplement définir la distance à $0$ si deux observations prennent la même modalité et $1$ sinon.

::: {.callout-note icon=false}
## La distance discrète

Soit $\mathcal{X}$ un ensemble discret et $X$ une variable aéatoire discrète à valeur dans $\mathcal{X}$. Soit $x$ et $y$ deux réalisations de $X$, si $x = y$, $d(x, y) = 0$, sinon $d(x, y) = 1$.

:::

Dans le cas de vecteurs, on compte simplement le nombre de désaccord entre les deux observations.

Nombre d'accord entre $x$ et $y$ : $\sum_{i = 1}^{p} \mathbb{1}(x_i = y_i)$

::: {.callout-note icon=false}
## Exemple

Prenons en exemple les caractéristiques de trois personnes. 

| Nom | Couleur | Yeux | Cheveux |
| :---: | ---: | ----: | ----: | 
| Alice | Rouge | Bleu | Blond |
| Bob | Vert | Bleu | Roux |
| Chris | Rouge | Vert | Blond |

: Caractéristiques de trois personnes. {#tbl-carac}

Pour calculer la distance entre ces personnes, on compte simplement le nombre de désaccord entre les modalités de leurs variables. Ainsi,

$$d(\text{Alice}, \text{Bob}) = 1 + 0 + 1 = 2, \quad d(\text{Alice}, \text{Chris}) = 0 + 1 + 0 = 1, \quad d(\text{Bob}, \text{Chris}) = 1 + 1 + 1 = 3.$$

:::

Dans ce cas, il peut être plus pratique de travailler avec la similarité; au lieu de compter le nombre de désaccord, on compte le nombre d'accord que l'on normalise par le nombre de variables pour avoir une similarité égale à $1$ lorsque deux observations sont identiques. Ainsi,

$$s(x, y) = \frac{1}{p}\sum_{i = 1}^{p} \mathbb{1}(x_i = y_i).$$

::: {.callout-note icon=false}
## Exemple

En reprenant l'exemple précédent (cf. @tbl-carac), on trouve les similarités suivantes :

$$s(\text{Alice}, \text{Bob}) = \frac{0 + 1 + 0}{3} = \frac{1}{3}, \quad s(\text{Alice}, \text{Chris}) = \frac{1 + 0 + 1}{3} = \frac{2}{3}, \quad s(\text{Bob}, \text{Chris}) = \frac{0 + 0 + 0}{3} = 0.$$

:::

### Distance de Jaccard 

::: {.callout-warning icon=false}
## Définition

Considérons deux observations $X$ et $Y$ de $K$ variables binaires. Chaques variables peut prendre les valeurs $0$ et $1$. 

Définissons les quantités suivantes :

* $M_{11}$, le nombre de variables à $1$ pour $X$ et $Y$;

* $M_{10}$, le nombre de variables à $1$ pour $X$ et $0$ pour $Y$;

* $M_{01}$, le nombre de variables à $0$ pour $X$ et $1$ pour $Y$;

* $M_{00}$, le nombre de variables à $0$ pour $X$ et $Y$.

Chaque variable binaire est forcément comptée, soit dans $M_{11}$, soit dans $M_{10}$, soit dans $M_{01}$, soit dans $M_{00}$. Donc $K = M_{11} + M_{10} + M_{01} + M_{00}$.

La **distance de Jaccard** est définie comme 
$$d(x, y) = \frac{M_{11}}{M_{10} + M_{01} + M_{11}} = \frac{M_{11}}{K - M_{00}}.$$
:::

::: {.callout-note icon=false}
## Exemple

Prenons un questionnaire de $5$ questions fermées. Supposons que la réponse "Oui" est encodée par $1$ et la réponse "Non" est encodée par $0$. 

| Nom | Q1 | Q2 | Q3 | Q4 | Q5 |
| :-: | --:| --:| --:| --:| --:|
| Alice | 1 | 0 | 1 | 0 | 0 |
| Bob   | 1 | 0 | 0 | 1 | 0 |

Pour la distance entre Alice et Bob, on a $M_{11} = 1$, $M_{10} = 1$, $M_{01} = 1$ et $M_{00} = 2$. Donc $d(\text{Alice}, \text{Bob}) = \frac{1}{3}$.

:::
