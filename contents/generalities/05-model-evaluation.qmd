--- 
title: "Évaluation de modèles"
format: 
  html: default
  pdf:
    header-includes: 
      - \usepackage{mathtools}
---


Cette section est basée sur @jamesIntroductionStatisticalLearning2021, chapitre 5.

## Évaluer la performance d'un modèle prédictif

Dans la section précédente, nous avons introduit des outils pour mesurer la qualité d'un estimateur : l'erreur quadratique moyenne (MSE) pour les variables quantitatives et le taux d'erreur (ER) pour les variables qualitatives. Ces mesures comparent les valeurs prédites $\widehat{Y} = \widehat{f}(X)$ aux valeurs observées $Y$.  Cependant, si l'on calcule ces erreurs uniquement à partir des données qui ont servi à entraîner le modèle, on risque de **sous-estimer** la véritable erreur de prédiction. Pourquoi ? Parce que l'estimateur $\widehat{f}$ a été ajusté pour minimiser l'erreur sur ces mêmes données. Il s'y adapte donc bien, et généralement, trop bien ! Cela peut conduire à l'illlusion que notre modèle est performant. En effet, un modèle très flexible peut avoir une erreur faible sur les données d'entraînement simplement parce qu'il capture le bruit plutôt que le signal. Mais si le modèle s'adapte trop aux données d'entraînement, il risque de mal généraliser à de nouvelles données, i.e. des données qu'il n'a jamais vues. Ce phénomène s'appelle le **sur-ajustement** (*overfitting*).

::: {.callout-tip icon=false}
## Remarque: Sur-ajustement et sous-ajustement

Un modèle trop flexible peut s'adapter parfaitement aux données d'entraînement, y compris au bruit aléatoire. Il aura une erreur faible sur ces données mais une erreur élevée sur de nouvelles observations. On dira qu'il y a **sur-ajustement** (*overfitting*) du modèle. À l'inverse, un modèle trop rigide (par exemple, une droite constante) ne pourra pas capturer la structure des données, même sur l'ensemble d'entraînement. On dira qu'il y a **sous-ajustement** (*underfitting*) du modèle.

L'objectif est de trouver le bon compromis entre flexibilité et capacité de généralisation.

:::

Pour évaluer objectivement un modèle, l'idéal serait de le tester sur des données complètement indépendantes de celles utilisé pour l'apprentissage. On distingue donc deux ensembles : un **jeu d'entraînement**, utilisé pour ajuster le modèle et un **jeu de test**, utilisé pour évaluer la performance prédictive du modèle. En practique, nous n'avons généralement pas accès à un jeu de test pour faire cette évaluation. Dans cette section, nous allons deux approches permettant de contourner ce problème.


## Jeu de données de validation

Quand on ne dispose que d'un seul jeu de données,  une solution simple consiste à le diviser **aléatoirement** en deux sous-ensembles : un **jeu d'entraînement** pour ajuster le modèle et un **jeu de validation** pour estimer l'erreur de prédiction. On parle alors d'approche par jeu de validation. La @fig-schema-val présente un schéma de cette approche.

![Schéma de l'approche par jeu de validation.](../../include/validation-set.png){#fig-schema-val}


::: {.content-visible when-format="html"}


```{r}
#| eval: false

# Load packages
library(tidyverse)
library(dplyr)

# Set seed for reproducibility
set.seed(42)

# Function definition
f <- function(x) {
  4 * x * (1 - x) * log(x) + 2
}

generate_splits <- function(data, train_prop = 0.7, K = 10) {
  rows <- list()
  
  for (k in 1:K) {
    idx_train <- sample(nrow(data), size = floor(train_prop * nrow(data)))
    train <- data[idx_train, ]
    validation <- data[-idx_train, ]
    
    train_tbl <- tibble(
      split = k,
      set = "train",
      x = train$x,
      y = train$y
    )
    
    validation_tbl <- tibble(
      split = k,
      set = "validation",
      x = validation$x,
      y = validation$y
    )
    
    rows[[k]] <- bind_rows(train_tbl, validation_tbl)
  }
  
  bind_rows(rows)
}

# Generate N data points in the interval (0, 1]
N <- 200
spans <- seq(0.1, 1, by = 0.05)  # LOESS smoothing parameters
x_all <- runif(N, min = 0.01, max = 1)  # avoid 0 due to log(x)
y_all <- f(x_all) + rnorm(N, mean = 0, sd = 0.1)  # add Gaussian noise

# Put everything in a data.frame
data <- data.frame(x = x_all, y = y_all)

# Generate 10 training/validation splits
splits <- generate_splits(data, train_prop = 0.7, K = 10)
write.csv(splits, './splits.csv')


# Fit a model with different smoothing parameters on each split
splits_number <- unique(splits$split)

results <- list()
for (n_split in splits_number) {
  
  df_split <- splits |> dplyr::filter(split == n_split)
  train <- df_split |> 
    dplyr::filter(set == 'train') |> 
    dplyr::select(x, y)
  validation <- df_split |> 
    dplyr::filter(set == 'validation') |> 
    dplyr::select(x, y)
  
  results_splits <- list()
  for (s in spans) {

    # Fit loess model with span = s on train and predict on validation
    model <- loess(y ~ x, data = train, span = s, degree = 2)
    pred <- predict(model, newdata = validation$x)
      
    # For each point in x_grid, compute bias², variance, MSE
    bias2 <- mean((pred - validation$y)^2, na.rm = TRUE)
    var_pred <- var(pred, na.rm = TRUE)
    mse <- bias2 + var_pred
    
    results_splits[[as.character(s)]] <- data.frame(
      split = n_split,
      span = s,
      MSE = mse
    )
  }
  results_splits_df <- bind_rows(results_splits)
  results[[as.character(n_split)]] <- results_splits_df
}

results <- results |> bind_rows()
write.csv(results, './splits_mse.csv')

```


```{ojs}
//| warning: false
//| eval: true
//| echo: false

data = FileAttachment("../../include/data/model-evaluation/splits.csv").csv({ typed: true })
data_mse = FileAttachment("../../include/data/model-evaluation/splits_mse.csv").csv({ typed: true })

viewof split = Inputs.range(
  [1, 10], 
  {value: 1, step: 1, label: "Autre jeu de validation"}
)

filtered = data.filter(function(df) {
  return df.split == split;
})

current_mse = data_mse.filter(function(df) {
  return df.split == split;
})


```

```{ojs}
//| label: fig-trainval-html
//| fig-cap: Illustration de l'approche par jeu de données de validation.
//| fig-subcap:
//|   - "Différents jeux d'entraînement et validation."
//|   - "MSE pour différents jeux de validation."
//| warning: false
//| eval: true
//| echo: false

Plot.plot({
  grid: true,
  x: {
    domain: [0, 1],
    label: "X",
  },
  y: {
    domain: [0, 3],
    label: "Y",
  },
  color: {
    domain: ["train", "validation"],
    range: ["#f7a941", "#e30513"],
    legend: true
  },
  marks: [
    Plot.dot(
      filtered,
      {x: "x", y: "y", fill: "set", r: 5}
    ),
  ]
})

Plot.plot({
  grid: true,
  color: {
    legend: true,
  },
  x: {
    domain: [0, 1],
    label: "λ",
  },
  y: {
    domain: [0.1, 0.2],
    label: "Y",
  },
  marks: [
    Plot.line(
      data_mse, 
      {
        x: "span", y: "MSE", z: "split",
        stroke: "#AAAAAA", strokeWidth: 2
      }
    ),
    Plot.line(
      current_mse,
      {x: "span", y: "MSE", stroke: "#e30513", strokeWidth: 5}
    ),
  ]
})
```
:::


::: {.callout-tip icon=false}
## Comment choisir la taille des sous-ensembles ? 

En général :

* Si l'on dispose d'un grand nombre d'observations (disons plusieurs milliers), on peut faire une division $50-50$.

* Si l'on dispose de moins d'observations, on préférera garder plus d'observations pour l'entraînement. On peut, par exemple, faire une division $70-30$ ou $80-20$.

Cependant, il n'existe pas de règle universelle. Le bon choix dépend du contexte, de la complexité du modèle et de la quantité de données disponibles.

:::

La méthode a cependant deux inconvénients. Le premier est que l'estimation de l'erreur est instable. En effet, la valeur de l'erreur de prédiction dépend des observations qui sont dans le jeu de validation. Un autre jeu de validation peut donner un résultat différent. Le deuxième est qu'il y a moins de données pour ajuster le modèle. Comme une partie des données est réservée à la validation, le modèle est appris sur un ensemble plus petit, et cela peut donc surestimer son erreur réelle par rapport à s'il avait été appris sur l'ensemble de données complet.



## Validation croisée 

Pour contourner les limites de l'approche précédente, on utilise souvent la **validation croisée** (*cross-validation*). Cette méthode est plus robuste et plus stable. Le principe est de répéter l'approche par jeu de validation plusieurs sur différents sous-ensembles du jeu de données.

L'approche consiste à découper aléatoirement l'ensemble des observations en $K$ sous-ensembles de taille équivalentes (appelés *folds*). Le premier *fold* est utilisé comme jeu de données de validation et le modèle est ajusté sur les $K - 1$ *folds* restant. L'erreur de prédiction est calculé sur le premier *fold*. Cette procédure est répèté $K$ fois; à chaque fois, un différent *fold* est utilisé comme jeu de données de validation. À la fin, on a donc $K$ valeurs pour l'erreur de prédiction. On calcule enfin la moyenne des $K$ valeurs de prédiction. La @fig-schema-cv présente un schéma de cette approche.

![Schéma de l'approche par validation croisée](../../include/cross-validation.png){#fig-schema-cv}

Add exemple!

::: {.callout-tip icon=false}
## Comment choisir le nombre de sous-ensembles $K$ ? 

En pratique, on utilise souvent $K = 5$ ou $K = 10$. L'avantage de limiter $K$ est de limiter le coût computationnel car le modèle doit être ajusté $K$ fois. Dans le cas limite où $K = n$, i.e. $K$ est égal au nombre d'observations dans le jeu de données, on parle de validation croisée *leave-one-out* (LOOCV). Dans ce cas, chaque observation sert une fois de validation. 

:::
